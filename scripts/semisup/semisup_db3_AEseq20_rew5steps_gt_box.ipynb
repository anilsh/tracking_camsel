{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "#from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2 as cv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import time, math\n",
    " \n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "#use_cuda=False   #uncomment this if you dont want to use cuda variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import collections\n",
    "# import hickle as hkl\n",
    "# import ttictoc as tt\n",
    "\n",
    "sys.path.insert(0, '../data/')\n",
    "import get_pid_train_test as db\n",
    "import auxiliary as af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(0,'../py-MDNet/modules')\n",
    "# from sample_generator import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def plot_current_state(ped, c,fno):\n",
    "    # load image for current location\n",
    "    img,bb = load_image(ped,c,fno,db_no)\n",
    "\n",
    "    dpi = 80.0\n",
    "    #figsize = (img.size[0]/dpi, img.size[1]/dpi)\n",
    "    figsize = (img.shape[0]/dpi, img.shape[1]/dpi)\n",
    "    fig = plt.figure(frameon=False, figsize=figsize, dpi=dpi)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "\n",
    "    # get image and rect handle\n",
    "    imAX = ax.imshow(img, aspect='normal')\n",
    "    rect = plt.Rectangle(tuple(bb[0,:2]),bb[0,2],bb[0,3], \n",
    "        linewidth=3, edgecolor=\"#ff0000\", zorder=1, fill=False)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    plt.pause(.01)\n",
    "    plt.draw()\n",
    "    #fig.savefig(os.path.join(savefig_dir,'0000.jpg'),dpi=dpi)\n",
    "    \n",
    "    return imAX, rect\n",
    "    \n",
    "def plot_second(ped,c,curr_frame, imAX,rect):\n",
    "    img,bb =  load_image(ped,c,curr_frame,db_no)\n",
    "    #if np.array(img).shape[0] > 0:\n",
    "    if img != []:\n",
    "        imAX.set_data(img)\n",
    "    #print (bb)\n",
    "\n",
    "    #if bb.shape[0] > 0:\n",
    "    if bb != []:\n",
    "        rect.set_xy(bb[0,:2])\n",
    "        rect.set_width(bb[0,2])\n",
    "        rect.set_height(bb[0,3])\n",
    "        print ('Correct camera')\n",
    "    elif c!= num_camera-1:\n",
    "        print ('Wrong camera')\n",
    "\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    plt.pause(1)\n",
    "    plt.draw()\n",
    "    #fig.savefig(os.path.join(savefig_dir,'%04d.jpg'%(i)),dpi=dpi)\n",
    "\n",
    "def get_reward_gt(ped, curr_frame, c):\n",
    "    y = afc.find_target_camera(ped,curr_frame)\n",
    "    # get reward (give reward at end of episode)\n",
    "    if y == num_camera-1 and y == c:\n",
    "        reward = 0\n",
    "    elif y == c:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = -1\n",
    "        \n",
    "    return reward,y\n",
    "\n",
    "def get_next_step(ped,c,curr_frame, state):\n",
    "    # update current state and history\n",
    "    ispresent,this_state = get_state_vector(ped, c,curr_frame)\n",
    "    if ispresent:\n",
    "        next_state = this_state\n",
    "    else:\n",
    "        # use previous state\n",
    "        next_state = state\n",
    "    \n",
    "    # get correct label from ground truth\n",
    "    reward,y = get_reward_gt(ped, curr_frame,c)\n",
    "\n",
    "    return next_state,reward,y,ispresent\n",
    "\n",
    "def test_func(pTest, iloc='first', eloc='last', fixLoc=-1, isdebug=0, req_inc=1):\n",
    "    policy_net.eval()\n",
    "    rsT,accT = [],[]\n",
    "    Qvalues = []\n",
    "    numTrAllP = []\n",
    "    \n",
    "    for p in range(pTest.shape[0]): \n",
    "        reward_sum = 0\n",
    "        accP = []\n",
    "        inc = 1\n",
    "        aaa = 1\n",
    "        Qval_1p = []\n",
    "        numTr = 0\n",
    "        \n",
    "        # load p'th person data\n",
    "        ped = np.copy(pTest[p])\n",
    "        # camera index and frame index starts from zero\n",
    "        ped[:,0] -= 1\n",
    "        ped[:,1] -= 1\n",
    "        \n",
    "        # Initialize with current state with start frame\n",
    "        if iloc == 'first':\n",
    "            startIDX = 0\n",
    "        elif iloc == 'rand':\n",
    "            startIDX = np.random.randint( 0,ped.shape[0]-20 )\n",
    "        elif iloc == 'fix':\n",
    "            startIDX = fixLoc\n",
    "        if startIDX > ped.shape[0]:\n",
    "            continue\n",
    "        myPos = ped[startIDX,0:]\n",
    "        print ('Initial position: ',myPos)\n",
    "        \n",
    "        curr_camera = myPos[0]\n",
    "        curr_frame = myPos[1]\n",
    "        \n",
    "        # Initialize history variable (one-hot encoding)\n",
    "        ch = np.zeros((h_len,duke_cam))\n",
    "        occ_len = 0.01\n",
    "        # Make initial state\n",
    "        x_t,c_t,te_tau,r_t = make_state_vector(ped, curr_camera,curr_frame,ch,occ_len)\n",
    "        prev_rt = r_t[0:4]\n",
    "        #print (state.size())\n",
    "        num_steps = 0\n",
    "        prev_camera = curr_camera\n",
    "        count_curr_c = 0\n",
    "        \n",
    "        if render: # show current location\n",
    "            plot_current_state(ped, curr_camera,curr_frame)\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "        # select an action from the current state\n",
    "        hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "        #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "        state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "        state = torch.cat([state_xt, hidden[1,].detach()], dim=1)\n",
    "        \n",
    "        \n",
    "        while(curr_frame <= ped[-1,1]): # alltime-6):\n",
    "            \n",
    "            if use_cuda:\n",
    "                state_in = Variable(state)\n",
    "                value_c = policy_net(state_in)\n",
    "            else:\n",
    "                state_in = Variable(state)\n",
    "                value_c = policy_net(state_in)\n",
    "                \n",
    "            # Only exploitation for testing\n",
    "            camera_index = torch.argmax(value_c)\n",
    "            c = camera_index.detach().cpu().numpy()\n",
    "            \n",
    "            occ_max_val = 12000000\n",
    "            aaa += 1\n",
    "            if aaa > 1 and occ_len > occ_max_val:\n",
    "                c = c #np.array(num_camera-1)\n",
    "            if occ_len > occ_max_val and aaa%50 == 0:\n",
    "                aaa = 1\n",
    "                c = np.array(np.random.randint(num_camera))\n",
    "\n",
    "            # find target for the next frame\n",
    "            curr_frame += fpsc\n",
    "            num_steps += 1\n",
    "            \n",
    "            # get correct label from ground truth\n",
    "            reward,y = get_reward_gt(ped, curr_frame,c)\n",
    "            #if req_inc:\n",
    "            if inc==1 and y!=num_camera-1:\n",
    "                # inside a camera\n",
    "                if req_inc:\n",
    "                    accP.append((y,y))\n",
    "                    c = y\n",
    "                else:\n",
    "                    accP.append((y,c.item(0)))\n",
    "            elif inc==0 and y==c.item(0) and y!=num_camera-1:\n",
    "                # transitioning to second camera\n",
    "                accP.append((y,c.item(0)))\n",
    "                inc = 1\n",
    "                numTr += 1\n",
    "            elif inc==1 and y==num_camera-1:\n",
    "                # moving out of a camera FOV\n",
    "                inc = 0\n",
    "                accP.append((y,c.item(0)))\n",
    "            else:\n",
    "                # Making transition\n",
    "                accP.append((y,c.item(0)))\n",
    "                #print ('Another case',y,c.item(0))\n",
    "                    \n",
    "            #else:\n",
    "            #    accP.append((y,c.item(0)))\n",
    "            \n",
    "            # get the current bounding box\n",
    "            bbox = ped[ np.logical_and(ped[:,0]==c,ped[:,1]==curr_frame),2:]\n",
    "            if bbox.shape[0] > 0: # and np.random.rand < 0.95:\n",
    "                bbox = bbox[0]\n",
    "                rt = np.zeros((8))\n",
    "                rt[0] = bbox[0]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[1] = bbox[1]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[2] = bbox[2]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[3] = bbox[3]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[4] = rt[0] - prev_rt[0] if occ_len < 0.2 else 0\n",
    "                rt[5] = rt[1] - prev_rt[1] if occ_len < 0.2 else 0\n",
    "                rt[6] = rt[2] - prev_rt[2] if occ_len < 0.2 else 0\n",
    "                rt[7] = rt[3] - prev_rt[3] if occ_len < 0.2 else 0\n",
    "                curr_camera = c\n",
    "                \n",
    "                # make next_state vector\n",
    "                this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "                x_t = np.concatenate((this_cam, rt.ravel()))\n",
    "                x_t[x_t==0] = -10\n",
    "                x_t[x_t==1] = 10\n",
    "                x_t = x_t.reshape(1,-1)\n",
    "                if use_cuda:\n",
    "                    x_t = torch.from_numpy(x_t).float().cuda()\n",
    "                \n",
    "                \n",
    "                ispresent = 1\n",
    "                prev_rt = rt[0:4]\n",
    "                    \n",
    "            else:\n",
    "                ispresent = 0\n",
    "                \n",
    "            # count the time of prev_camera selection\n",
    "            if ispresent:\n",
    "                occ_len = 0.01\n",
    "            else:\n",
    "                occ_len += 1\n",
    "            #hcount = np.array(-occ_max_val + (occ_len/500)*(occ_max_val-(-occ_max_val)))\n",
    "            hcount = np.array(10*np.log(occ_len))\n",
    "            \n",
    "            # update current state and history\n",
    "            ch[1:,] = ch[0:-1,]\n",
    "            ch[0,0:num_camera] = afc.make_one_hot_camera(c)\n",
    "            ch[0,num_camera:] = 0\n",
    "            this_cam = afc.make_one_hot_camera(c)\n",
    "            c_t = this_cam.reshape(1,-1)\n",
    "            \n",
    "            if use_cuda:\n",
    "                c_t = torch.from_numpy(c_t).float().cuda()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float().cuda()\n",
    "            else:\n",
    "                c_t = torch.from_numpy(c_t).float()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float()\n",
    "                \n",
    "            if isdebug:\n",
    "                print ( np.where(rt.ravel()))\n",
    "                print ( np.where(ch))\n",
    "                print (c, curr_frame)\n",
    "                print ('isPresent', ispresent)\n",
    "                \n",
    "            # make next_state vector\n",
    "            hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "            #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "            next_state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "            next_state = torch.cat([next_state_xt, hidden[1,].detach()], dim=1)\n",
    "            \n",
    "            # store current reward\n",
    "            reward_sum += reward\n",
    "            Qval_1p.append((list(value_c.detach().cpu().numpy()[0]),hcount.ravel()[0],reward,False,y,c,state.detach().cpu().numpy()))\n",
    "                        \n",
    "            #state = next_state\n",
    "            #state_xt = next_state_xt\n",
    "            state = next_state #torch.cat([state_xt, enc_history], dim=1)\n",
    "            prev_camera = c\n",
    "            \n",
    "            if render:\n",
    "                plot_second()\n",
    "            if eloc != 'last':\n",
    "                if num_steps > eloc:\n",
    "                    break\n",
    "            \n",
    "        # stack episodic reward \n",
    "        Qvalues.append((np.stack(Qval_1p)))\n",
    "        rsT.append((reward_sum,num_steps))\n",
    "        accT.append(accP)\n",
    "        numTrAllP.append(numTr)\n",
    "        \n",
    "    return rsT, accT, Qvalues, numTrAllP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 1500\n",
    "replay_memory_size = 20000\n",
    "#epsilon = 0.1\n",
    "gamma = 0.99\n",
    "\n",
    "resume = False # resume from previous checkpoint\n",
    "render = False\n",
    "eps = np.finfo(np.float32).eps.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of person in data set:  (1, 14)\n",
      "Total number of person in data set:  (1, 14)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "db_no = 3\n",
    "[pALL,num_camera,alltime,fps] = db.get_pid(set_no=db_no, train_flag='train')\n",
    "num_camera += 1  # occlusion is also considered as a FOV\n",
    "fpsc = 2\n",
    "pALL = np.array(pALL)\n",
    "\n",
    "# load test set for current data set\n",
    "[pTest,num_camera,alltime,fps] = db.get_pid(set_no=db_no, train_flag='test')\n",
    "num_camera += 1  # occlusion is also considered as a FOV\n",
    "fpsc = 2\n",
    "pTest = np.array(pTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of full ground truth (18339, 6)\n"
     ]
    }
   ],
   "source": [
    "camGTT = pd.read_csv(os.path.expanduser('~/8tb/hpc-storage/nlpr/annotation_files/annotation/Dataset' + str(db_no) + '/fullgt.csv'), header=None).values.astype(np.int)[:,:7]\n",
    "tmp = np.copy(camGTT[:,1])\n",
    "camGTT[:,1] = camGTT[:,2]\n",
    "camGTT[:,2] = tmp\n",
    "camGT = camGTT[:,[0,2,3,4,5,6]]\n",
    "\n",
    "# In NLPR GT, frames already start with 0, only cam needs to be decremented\n",
    "camGT[:,0]-=1\n",
    "# camGT[:,1]-=1\n",
    "# camGT = matlab.double(camGT)\n",
    "print('Shape of full ground truth', camGT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpoch = 100000\n",
    "d = 10\n",
    "region_size = (d,d)\n",
    "\n",
    "h_len = 10\n",
    "\n",
    "# Load auxiliary functions using an object\n",
    "afc = af.AuxiliaryFunction(num_camera=num_camera, d=d, h_len=h_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize required parameters\n",
    "lstm_size = 256\n",
    "hidden_size1 = 4096\n",
    "hidden_size2 = 2048\n",
    "hidden_size3 = 256\n",
    "\n",
    "input_size = lstm_size + num_camera+ 4*2 +1\n",
    "\n",
    "# Required network\n",
    "class NextCamera(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NextCamera, self).__init__()\n",
    "        \n",
    "        # make decoder layers\n",
    "        self.fch1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fch2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fch3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fco = nn.Linear(hidden_size3, num_camera)\n",
    "        \n",
    "        # Activation function \n",
    "        self.tanh = nn.Tanh() #ReLU()\n",
    "        self.relu = nn.ReLU() #ReLU()\n",
    "        #self.linear = nn.Linear() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.fch1(x))\n",
    "        x = self.relu(self.fch2(x))\n",
    "        x = self.relu(self.fch3(x))\n",
    "        x = self.fco(x)\n",
    "            \n",
    "        return x # nn.functional.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "if use_cuda:\n",
    "    policy_net = NextCamera().float().cuda()\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    \n",
    "else:\n",
    "    policy_net = NextCamera().float()\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "if use_cuda:\n",
    "    target_net = NextCamera().cuda()\n",
    "    target_net.float().cuda()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# use ADAM as optimizer since we can load the whole data to train\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_network(replay_memory_pos,pos_prob, replay_memory_neg, replay_memory_cx, update_criteria):\n",
    "\n",
    "    # sample random minibatch\n",
    "    minibatch_pos = random.choices(replay_memory_pos, k=min(len(replay_memory_pos), 500), weights=pos_prob)\n",
    "    #minibatch_pos = random.sample(replay_memory_pos, min(len(replay_memory_pos), 300)) #int(batch_size/3)))\n",
    "    minibatch_posneg = minibatch_pos + random.sample(replay_memory_neg, min(len(replay_memory_neg), 500)) # int(batch_size/3)))\n",
    "    minibatch = minibatch_posneg + random.sample(replay_memory_cx, min(len(replay_memory_cx), 500)) #int(batch_size/3)))\n",
    "    \n",
    "    # unpack minibatch\n",
    "    #state_xt = tuple(d[0] for d in minibatch)\n",
    "    state = torch.cat(tuple(d[0] for d in minibatch))\n",
    "    #prev_ch = tuple(d[1] for d in minibatch)\n",
    "    action = torch.cat(tuple(d[1] for d in minibatch))\n",
    "    reward = torch.cat(tuple(d[2] for d in minibatch))\n",
    "    #next_state_xt = tuple(d[4] for d in minibatch)\n",
    "    next_state = torch.cat(tuple(d[3] for d in minibatch))\n",
    "    #ch = tuple(d[5] for d in minibatch)\n",
    "    \n",
    "    # num samples of different categories\n",
    "    numRew = torch.stack([torch.sum(reward>=0.2),torch.sum(reward==-1),torch.sum(reward==0.01)]).data.cpu().numpy()\n",
    "    \n",
    "    # get output for the next state\n",
    "    next_output = target_net(next_state)\n",
    "\n",
    "    # set y_j to r_j for terminal state, otherwise to r_j + gamma*max(Q)\n",
    "    y = torch.cat(tuple(reward[i] if minibatch[i][4] \\\n",
    "                        else reward[i] + gamma * torch.max(next_output[i]) \\\n",
    "                        for i in range(len(minibatch))))\n",
    "\n",
    "    # extract Q-value\n",
    "    q_value = torch.sum(policy_net(state) * action, dim=1)\n",
    "\n",
    "    # PyTorch accumulates gradients by default, so they need to be reset in each pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # returns a new Tensor, detached from the current graph, the result will never require gradient\n",
    "    y = y.detach()\n",
    "\n",
    "    #print (y, q_value)\n",
    "    # calculate loss\n",
    "    loss = criterion(q_value, y)\n",
    "\n",
    "    # do backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # copy weights from policy_net to target_net\n",
    "    if update_criteria == 10:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        update_criteria = 0\n",
    "    update_criteria += 1\n",
    "    \n",
    "    return loss.data,numRew,update_criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = src #self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size, dim]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size, dim]\n",
    "        \n",
    "        embedded = input #self.dropout(self.embedding(input))\n",
    "        #embedded[np.arange(embedded.size),a] = 1\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.1):\n",
    "        \n",
    "        #src = [src len, batch size, dim]\n",
    "        #trg = [trg len, batch size, dim]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output #.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "duke_cam = 9\n",
    "INPUT_DIM = duke_cam\n",
    "OUTPUT_DIM = duke_cam\n",
    "ENC_EMB_DIM = duke_cam\n",
    "DEC_EMB_DIM = duke_cam\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).float().cuda()\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).float().cuda()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Seq2Seq(enc, dec, device).float().to(device)\n",
    "criterion_ae = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(9, 9)\n",
       "    (rnn): LSTM(9, 256, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(9, 9)\n",
       "    (rnn): LSTM(9, 256, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=256, out_features=9, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load encoder model\n",
    "#enc.load_state_dict(torch.load('enc-model_manyDB_state64.pt'))\n",
    "#enc.load_state_dict(torch.load('enc-model_manyDB.pt'))\n",
    "#enc.eval()\n",
    "#dec.load_state_dict(torch.load('dec-model_manyDB_state64.pt'))\n",
    "#dec.eval()\n",
    "model.load_state_dict(torch.load('../eccv2020/tut1-model_duke_lstmSize128_manyDB_2.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "running_reward = None\n",
    "xs,rs,cprs = [],[],[]\n",
    "episode_number = 0\n",
    "episode_durations = []\n",
    "episode_reward = []\n",
    "validation_reward= []\n",
    "replay_memory_pos = []\n",
    "pos_prob = []\n",
    "replay_memory_neg = []\n",
    "replay_memory_cx = []\n",
    "M = np.zeros((num_camera,num_camera))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state_vector(ped, curr_camera,curr_frame, ch,occ_len):\n",
    "    numSamples = 30\n",
    "    overlap_thres = [0.9, 1]\n",
    "        \n",
    "    # read image\n",
    "    img,bbox,p = afc.load_image(ped,curr_camera,curr_frame,db_no)\n",
    "    imw, imh = (320,240) #img.size\n",
    "    hc = np.array(10*np.log(occ_len))\n",
    "    \n",
    "    if p:\n",
    "        rt = np.zeros((8))\n",
    "        rt[0] = bbox[0]/imw -(np.random.rand()-0.5)/100\n",
    "        rt[1] = bbox[1]/imh -(np.random.rand()-0.5)/100\n",
    "        rt[2] = bbox[2]/imw -(np.random.rand()-0.5)/100\n",
    "        rt[3] = bbox[3]/imh -(np.random.rand()-0.5)/100\n",
    "        rt[4] = 0\n",
    "        rt[5] = 0\n",
    "        rt[6] = 0\n",
    "        rt[7] = 0\n",
    "        #print (np.where(rt.ravel()))\n",
    "        \n",
    "        # make next_state vector\n",
    "        #this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "        #state = np.concatenate((this_cam, rt.ravel()))\n",
    "        #state = np.concatenate((state, hc.ravel()))\n",
    "        #state = np.concatenate((state, ch.ravel()))\n",
    "        #state = state.reshape(1,-1)\n",
    "        #state[state==0] = -10\n",
    "        #state[state==1] = 10\n",
    "        \n",
    "        # make next_state vector\n",
    "        this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "        xt = np.concatenate((this_cam, rt.ravel()))\n",
    "        xt[xt==0] = -10\n",
    "        xt[xt==1] = 10\n",
    "        xt = xt.reshape(1,-1)\n",
    "        \n",
    "        # make history vector\n",
    "        c_t = this_cam.reshape(1,-1)\n",
    "        \n",
    "        if use_cuda:\n",
    "            xt = torch.from_numpy(xt).float().cuda()\n",
    "            c_t = torch.from_numpy(c_t).float().cuda()\n",
    "            hc = torch.from_numpy(hc.reshape(1,-1)).float().cuda()\n",
    "        else:\n",
    "            xt = torch.from_numpy(xt).float()\n",
    "            c_t = torch.from_numpy(c_t).float()\n",
    "            hc = torch.from_numpy(hc.reshape(1,-1)).float()\n",
    "    else:\n",
    "        print ('Target is not present in ',c,curr_frame)\n",
    "        xt,h_t = [],[]\n",
    "    \n",
    "    return xt,c_t,hc,rt #p,state,rt\n",
    "\n",
    "def append_reward(rs,num_steps):\n",
    "    if len(rs) > 0:\n",
    "        # stack episodic reward \n",
    "        epR = np.vstack(rs)\n",
    "        rs = []\n",
    "\n",
    "        # append the episodic reward\n",
    "        #episode_number += 1\n",
    "        #episode_durations.append(num_steps)\n",
    "        reward_stat = [num_steps,np.std(epR),np.sum(epR)]\n",
    "        episode_reward.append(reward_stat)\n",
    "    \n",
    "    return rs\n",
    "\n",
    "def reinit_ae(ch):\n",
    "    # Initialize history variable (one-hot encoding)\n",
    "    if use_cuda:\n",
    "        ch = torch.from_numpy(ch).float().cuda()\n",
    "        enc_h = torch.zeros(1,lstm_size).float().cuda()\n",
    "        enc_c = torch.zeros(1,lstm_size).float().cuda()\n",
    "    else:\n",
    "        enc_h = torch.zeros(1,lstm_size).float()\n",
    "        enc_c = torch.zeros(1,lstm_size).float()\n",
    "        \n",
    "    # encode whole camera history\n",
    "    for i in range(seq_len-1,-1,-1):\n",
    "        #print (ch[i,:])\n",
    "        x = ch[i,:].view(1,-1)\n",
    "        h_lstm,enc = ae_enc((enc_h,enc_c), x)\n",
    "        (enc_h,enc_c) = h_lstm\n",
    "\n",
    "    return h_lstm,enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "occ_max_val = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpData = []\n",
    "allEpData = []\n",
    "numRew=[]\n",
    "\n",
    "numUpdateRew=[]\n",
    "update_criteria = 0\n",
    "episode_count = 0\n",
    "steps_count = 0\n",
    "initialEpsilon = 0.4\n",
    "finalEpsilon = 0.01\n",
    "epsilon = initialEpsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trExplored = {}\n",
    "for i in range(num_camera):\n",
    "    for j in range(num_camera):\n",
    "        trExplored[str(i)+'-'+str(j)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "if resume:\n",
    "    epoch = 600\n",
    "    epsilon = 0.170962\n",
    "    aaa = np.load('./EpData/.npy', allow_pickle=True)\n",
    "    episode_count = aaa[2]\n",
    "    steps_count = aaa[3]\n",
    "    episode_reward_pre = aaa[0]\n",
    "    validation_reward_pre = aaa[1]\n",
    "    policy_net.load_state_dict(torch.load('./models/')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # convert to x1,y1,x2,y2\n",
    "    boxA[2] = boxA[2] + boxA[0]\n",
    "    boxA[3] = boxA[3] + boxA[1]\n",
    "    boxB[2] = boxB[2] + boxB[0]\n",
    "    boxB[3] = boxB[3] + boxB[1]\n",
    "    \n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def find_nearest_box(c,frame_no, prev_box):\n",
    "    # find all bounding boxes\n",
    "    all_dets = np.copy(camGT[np.logical_and(camGT[:,0]==c, camGT[:,1]==frame_no), 2:])\n",
    "    \n",
    "    #print (all_dets, prev_box)\n",
    "    \n",
    "    # find the nearest box\n",
    "    maxIOU = -1\n",
    "    box = np.array([])\n",
    "    for i in range(len(all_dets)):\n",
    "        this_box = np.copy(all_dets[i])\n",
    "        iou = bb_intersection_over_union(this_box, prev_box)\n",
    "        #print (iou, this_box)\n",
    "        if iou > maxIOU and iou > 0.4:\n",
    "            maxIOU = iou\n",
    "            box = np.copy(all_dets[i])\n",
    "            \n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def discount_rewards(r):\n",
    "#     \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "#     discounted_r = np.zeros_like(r)\n",
    "#     running_add = 0\n",
    "#     for t in reversed(range(0, r.size)):\n",
    "#         if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "#         running_add = running_add * gamma + r[t]\n",
    "#         discounted_r[t] = running_add\n",
    "#     return discounted_r\n",
    "\n",
    "# def discount_rewards(r):\n",
    "#     \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "#     discounted_r = [] #np.zeros_like(r)\n",
    "#     running_add = 0\n",
    "#     for t in reversed(range(0, len(r))):\n",
    "#         if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "#         running_add = running_add * gamma + r[t]\n",
    "#         #discounted_r[t] = running_add\n",
    "#         discounted_r.append(running_add)\n",
    "#     return discounted_r\n",
    "\n",
    "def discount_rewards(r,c):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = [] #np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    pivot = c[-1]\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        if r[t] != 0: \n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "            pivot = c[t]\n",
    "        if c[t] == pivot:\n",
    "            running_add = running_add * gamma + r[t]\n",
    "            #discounted_r[t] = running_add\n",
    "            discounted_r.append(running_add)\n",
    "        else:\n",
    "            running_add = running_add * gamma + 0\n",
    "            discounted_r.append(0)\n",
    "            \n",
    "        \n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0: ep_len:665 episode reward: total was -110.930000. running mean: -110.930000\n",
      "ep 0: ep_len:510 episode reward: total was -80.450000. running mean: -110.625200\n",
      "ep 0: ep_len:550 episode reward: total was -100.930000. running mean: -110.528248\n",
      "ep 0: ep_len:580 episode reward: total was -50.480000. running mean: -109.927766\n",
      "ep 0: ep_len:2 episode reward: total was 0.000000. running mean: -108.828488\n",
      "ep 0: ep_len:233 episode reward: total was -26.880000. running mean: -108.009003\n",
      "ep 0: ep_len:1100 episode reward: total was -106.940000. running mean: -107.998313\n",
      "epsilon:0.399863 episode_count: 7. steps_count: 3640.000000\n",
      "ep 1: ep_len:610 episode reward: total was -59.020000. running mean: -107.508530\n",
      "ep 1: ep_len:830 episode reward: total was -100.440000. running mean: -107.437845\n",
      "ep 1: ep_len:1215 episode reward: total was -149.320000. running mean: -107.856666\n",
      "ep 1: ep_len:670 episode reward: total was -98.810000. running mean: -107.766199\n",
      "ep 1: ep_len:3 episode reward: total was 0.000000. running mean: -106.688537\n",
      "ep 1: ep_len:585 episode reward: total was -53.910000. running mean: -106.160752\n",
      "ep 1: ep_len:570 episode reward: total was -88.930000. running mean: -105.988445\n",
      "epsilon:0.399727 episode_count: 14. steps_count: 8123.000000\n",
      "ep 2: ep_len:875 episode reward: total was -147.420000. running mean: -106.402760\n",
      "ep 2: ep_len:605 episode reward: total was -93.370000. running mean: -106.272432\n",
      "ep 2: ep_len:665 episode reward: total was -115.470000. running mean: -106.364408\n",
      "ep 2: ep_len:560 episode reward: total was -91.970000. running mean: -106.220464\n",
      "ep 2: ep_len:3 episode reward: total was 0.000000. running mean: -105.158259\n",
      "ep 2: ep_len:510 episode reward: total was -59.370000. running mean: -104.700377\n",
      "ep 2: ep_len:1110 episode reward: total was -133.280000. running mean: -104.986173\n",
      "epsilon:0.399590 episode_count: 21. steps_count: 12451.000000\n",
      "ep 3: ep_len:120 episode reward: total was -11.470000. running mean: -104.051011\n",
      "ep 3: ep_len:570 episode reward: total was -73.420000. running mean: -103.744701\n",
      "ep 3: ep_len:550 episode reward: total was -102.970000. running mean: -103.736954\n",
      "ep 3: ep_len:565 episode reward: total was -100.460000. running mean: -103.704185\n",
      "ep 3: ep_len:102 episode reward: total was -18.990000. running mean: -102.857043\n",
      "ep 3: ep_len:505 episode reward: total was -87.960000. running mean: -102.708072\n",
      "ep 3: ep_len:580 episode reward: total was -100.940000. running mean: -102.690392\n",
      "epsilon:0.399454 episode_count: 28. steps_count: 15443.000000\n",
      "ep 4: ep_len:570 episode reward: total was -89.460000. running mean: -102.558088\n",
      "ep 4: ep_len:575 episode reward: total was -93.390000. running mean: -102.466407\n",
      "ep 4: ep_len:450 episode reward: total was -62.480000. running mean: -102.066543\n",
      "ep 4: ep_len:565 episode reward: total was -85.400000. running mean: -101.899877\n",
      "ep 4: ep_len:3 episode reward: total was 0.000000. running mean: -100.880879\n",
      "ep 4: ep_len:685 episode reward: total was -63.030000. running mean: -100.502370\n",
      "ep 4: ep_len:755 episode reward: total was -87.980000. running mean: -100.377146\n",
      "epsilon:0.399317 episode_count: 35. steps_count: 19046.000000\n",
      "ep 5: ep_len:255 episode reward: total was -23.860000. running mean: -99.611975\n",
      "ep 5: ep_len:870 episode reward: total was -93.760000. running mean: -99.553455\n",
      "ep 5: ep_len:630 episode reward: total was -101.870000. running mean: -99.576620\n",
      "ep 5: ep_len:510 episode reward: total was -83.940000. running mean: -99.420254\n",
      "ep 5: ep_len:83 episode reward: total was -16.000000. running mean: -98.586052\n",
      "ep 5: ep_len:500 episode reward: total was -62.490000. running mean: -98.225091\n",
      "ep 5: ep_len:337 episode reward: total was -44.970000. running mean: -97.692540\n",
      "epsilon:0.399181 episode_count: 42. steps_count: 22231.000000\n",
      "ep 6: ep_len:505 episode reward: total was -80.480000. running mean: -97.520415\n",
      "ep 6: ep_len:570 episode reward: total was -80.440000. running mean: -97.349611\n",
      "ep 6: ep_len:358 episode reward: total was -34.380000. running mean: -96.719915\n",
      "ep 6: ep_len:585 episode reward: total was -84.470000. running mean: -96.597415\n",
      "ep 6: ep_len:126 episode reward: total was -16.470000. running mean: -95.796141\n",
      "ep 6: ep_len:530 episode reward: total was -80.380000. running mean: -95.641980\n",
      "ep 6: ep_len:304 episode reward: total was -50.910000. running mean: -95.194660\n",
      "epsilon:0.399044 episode_count: 49. steps_count: 25209.000000\n",
      "ep 7: ep_len:655 episode reward: total was -114.420000. running mean: -95.386913\n",
      "ep 7: ep_len:510 episode reward: total was -62.910000. running mean: -95.062144\n",
      "ep 7: ep_len:575 episode reward: total was -78.790000. running mean: -94.899423\n",
      "ep 7: ep_len:695 episode reward: total was -99.830000. running mean: -94.948729\n",
      "ep 7: ep_len:3 episode reward: total was 0.000000. running mean: -93.999241\n",
      "ep 7: ep_len:735 episode reward: total was -63.820000. running mean: -93.697449\n",
      "ep 7: ep_len:310 episode reward: total was -35.270000. running mean: -93.113174\n",
      "epsilon:0.398908 episode_count: 56. steps_count: 28692.000000\n",
      "ep 8: ep_len:735 episode reward: total was -112.770000. running mean: -93.309743\n",
      "ep 8: ep_len:525 episode reward: total was -64.360000. running mean: -93.020245\n",
      "ep 8: ep_len:383 episode reward: total was -35.440000. running mean: -92.444443\n",
      "ep 8: ep_len:505 episode reward: total was -67.430000. running mean: -92.194298\n",
      "ep 8: ep_len:49 episode reward: total was -9.000000. running mean: -91.362355\n",
      "ep 8: ep_len:560 episode reward: total was -81.390000. running mean: -91.262632\n",
      "ep 8: ep_len:198 episode reward: total was -32.450000. running mean: -90.674506\n",
      "epsilon:0.398771 episode_count: 63. steps_count: 31647.000000\n",
      "ep 9: ep_len:1045 episode reward: total was -148.180000. running mean: -91.249560\n",
      "ep 9: ep_len:510 episode reward: total was -27.510000. running mean: -90.612165\n",
      "ep 9: ep_len:520 episode reward: total was -65.740000. running mean: -90.363443\n",
      "ep 9: ep_len:535 episode reward: total was -39.570000. running mean: -89.855509\n",
      "ep 9: ep_len:87 episode reward: total was -11.970000. running mean: -89.076654\n",
      "ep 9: ep_len:176 episode reward: total was -12.460000. running mean: -88.310487\n",
      "ep 9: ep_len:1045 episode reward: total was -180.430000. running mean: -89.231682\n",
      "epsilon:0.398635 episode_count: 70. steps_count: 35565.000000\n",
      "ep 10: ep_len:500 episode reward: total was -57.960000. running mean: -88.918965\n",
      "ep 10: ep_len:500 episode reward: total was -39.710000. running mean: -88.426876\n",
      "ep 10: ep_len:610 episode reward: total was -91.330000. running mean: -88.455907\n",
      "ep 10: ep_len:590 episode reward: total was -79.770000. running mean: -88.369048\n",
      "ep 10: ep_len:3 episode reward: total was 0.000000. running mean: -87.485358\n",
      "ep 10: ep_len:805 episode reward: total was -89.500000. running mean: -87.505504\n",
      "ep 10: ep_len:690 episode reward: total was -107.870000. running mean: -87.709149\n",
      "epsilon:0.398498 episode_count: 77. steps_count: 39263.000000\n",
      "ep 11: ep_len:970 episode reward: total was -96.750000. running mean: -87.799557\n",
      "ep 11: ep_len:790 episode reward: total was -64.530000. running mean: -87.566862\n",
      "ep 11: ep_len:565 episode reward: total was -65.660000. running mean: -87.347793\n",
      "ep 11: ep_len:500 episode reward: total was -39.060000. running mean: -86.864915\n",
      "ep 11: ep_len:44 episode reward: total was 4.000000. running mean: -85.956266\n",
      "ep 11: ep_len:520 episode reward: total was -65.290000. running mean: -85.749603\n",
      "ep 11: ep_len:595 episode reward: total was -84.810000. running mean: -85.740207\n",
      "epsilon:0.398362 episode_count: 84. steps_count: 43247.000000\n",
      "ep 12: ep_len:645 episode reward: total was -105.430000. running mean: -85.937105\n",
      "ep 12: ep_len:615 episode reward: total was -67.220000. running mean: -85.749934\n",
      "ep 12: ep_len:795 episode reward: total was -88.330000. running mean: -85.775735\n",
      "ep 12: ep_len:525 episode reward: total was -48.240000. running mean: -85.400378\n",
      "ep 12: ep_len:3 episode reward: total was 0.000000. running mean: -84.546374\n",
      "ep 12: ep_len:551 episode reward: total was -63.790000. running mean: -84.338810\n",
      "ep 12: ep_len:910 episode reward: total was -132.220000. running mean: -84.817622\n",
      "epsilon:0.398225 episode_count: 91. steps_count: 47291.000000\n",
      "ep 13: ep_len:197 episode reward: total was -11.920000. running mean: -84.088646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 13: ep_len:540 episode reward: total was -66.830000. running mean: -83.916059\n",
      "ep 13: ep_len:520 episode reward: total was -66.840000. running mean: -83.745299\n",
      "ep 13: ep_len:580 episode reward: total was -64.200000. running mean: -83.549846\n",
      "ep 13: ep_len:3 episode reward: total was 0.000000. running mean: -82.714347\n",
      "ep 13: ep_len:525 episode reward: total was -50.510000. running mean: -82.392304\n",
      "ep 13: ep_len:650 episode reward: total was -66.630000. running mean: -82.234681\n",
      "epsilon:0.398089 episode_count: 98. steps_count: 50306.000000\n",
      "ep 14: ep_len:595 episode reward: total was -55.820000. running mean: -81.970534\n",
      "ep 14: ep_len:690 episode reward: total was -106.320000. running mean: -82.214029\n",
      "ep 14: ep_len:430 episode reward: total was -40.870000. running mean: -81.800588\n",
      "ep 14: ep_len:530 episode reward: total was -50.860000. running mean: -81.491182\n",
      "ep 14: ep_len:3 episode reward: total was 0.000000. running mean: -80.676271\n",
      "ep 14: ep_len:169 episode reward: total was -5.470000. running mean: -79.924208\n",
      "ep 14: ep_len:520 episode reward: total was -90.400000. running mean: -80.028966\n",
      "epsilon:0.397952 episode_count: 105. steps_count: 53243.000000\n",
      "ep 15: ep_len:565 episode reward: total was -72.210000. running mean: -79.950776\n",
      "ep 15: ep_len:950 episode reward: total was -113.170000. running mean: -80.282968\n",
      "ep 15: ep_len:595 episode reward: total was -61.040000. running mean: -80.090539\n",
      "ep 15: ep_len:48 episode reward: total was -2.940000. running mean: -79.319033\n",
      "ep 15: ep_len:3 episode reward: total was 0.000000. running mean: -78.525843\n",
      "ep 15: ep_len:685 episode reward: total was -50.540000. running mean: -78.245985\n",
      "ep 15: ep_len:520 episode reward: total was -89.430000. running mean: -78.357825\n",
      "epsilon:0.397816 episode_count: 112. steps_count: 56609.000000\n",
      "ep 16: ep_len:535 episode reward: total was -67.820000. running mean: -78.252446\n",
      "ep 16: ep_len:575 episode reward: total was -44.330000. running mean: -77.913222\n",
      "ep 16: ep_len:675 episode reward: total was -71.140000. running mean: -77.845490\n",
      "ep 16: ep_len:56 episode reward: total was -2.450000. running mean: -77.091535\n",
      "ep 16: ep_len:3 episode reward: total was 0.000000. running mean: -76.320620\n",
      "ep 16: ep_len:292 episode reward: total was -18.310000. running mean: -75.740513\n",
      "ep 16: ep_len:334 episode reward: total was -32.370000. running mean: -75.306808\n",
      "epsilon:0.397679 episode_count: 119. steps_count: 59079.000000\n",
      "ep 17: ep_len:505 episode reward: total was -40.880000. running mean: -74.962540\n",
      "ep 17: ep_len:605 episode reward: total was -49.750000. running mean: -74.710415\n",
      "ep 17: ep_len:640 episode reward: total was -73.730000. running mean: -74.700611\n",
      "ep 17: ep_len:45 episode reward: total was -3.460000. running mean: -73.988204\n",
      "ep 17: ep_len:3 episode reward: total was 0.000000. running mean: -73.248322\n",
      "ep 17: ep_len:550 episode reward: total was -51.320000. running mean: -73.029039\n",
      "ep 17: ep_len:665 episode reward: total was -74.820000. running mean: -73.046949\n",
      "epsilon:0.397543 episode_count: 126. steps_count: 62092.000000\n",
      "ep 18: ep_len:895 episode reward: total was -89.360000. running mean: -73.210079\n",
      "ep 18: ep_len:660 episode reward: total was -48.680000. running mean: -72.964779\n",
      "ep 18: ep_len:925 episode reward: total was -119.500000. running mean: -73.430131\n",
      "ep 18: ep_len:680 episode reward: total was -65.530000. running mean: -73.351129\n",
      "ep 18: ep_len:3 episode reward: total was 0.000000. running mean: -72.617618\n",
      "ep 18: ep_len:500 episode reward: total was -42.890000. running mean: -72.320342\n",
      "ep 18: ep_len:675 episode reward: total was -72.650000. running mean: -72.323639\n",
      "epsilon:0.397406 episode_count: 133. steps_count: 66430.000000\n",
      "ep 19: ep_len:215 episode reward: total was -15.950000. running mean: -71.759902\n",
      "ep 19: ep_len:635 episode reward: total was -88.360000. running mean: -71.925903\n",
      "ep 19: ep_len:760 episode reward: total was -71.120000. running mean: -71.917844\n",
      "ep 19: ep_len:535 episode reward: total was -49.810000. running mean: -71.696766\n",
      "ep 19: ep_len:118 episode reward: total was -17.500000. running mean: -71.154798\n",
      "ep 19: ep_len:690 episode reward: total was -83.660000. running mean: -71.279850\n",
      "ep 19: ep_len:500 episode reward: total was -60.230000. running mean: -71.169352\n",
      "epsilon:0.397270 episode_count: 140. steps_count: 69883.000000\n",
      "ep 20: ep_len:730 episode reward: total was -55.080000. running mean: -71.008458\n",
      "ep 20: ep_len:540 episode reward: total was -65.270000. running mean: -70.951073\n",
      "ep 20: ep_len:570 episode reward: total was -54.660000. running mean: -70.788163\n",
      "ep 20: ep_len:575 episode reward: total was -70.260000. running mean: -70.782881\n",
      "ep 20: ep_len:3 episode reward: total was 0.000000. running mean: -70.075052\n",
      "ep 20: ep_len:505 episode reward: total was -78.940000. running mean: -70.163702\n",
      "ep 20: ep_len:550 episode reward: total was -85.430000. running mean: -70.316365\n",
      "epsilon:0.397133 episode_count: 147. steps_count: 73356.000000\n",
      "ep 21: ep_len:510 episode reward: total was -45.320000. running mean: -70.066401\n",
      "ep 21: ep_len:505 episode reward: total was -40.860000. running mean: -69.774337\n",
      "ep 21: ep_len:580 episode reward: total was -48.760000. running mean: -69.564194\n",
      "ep 21: ep_len:775 episode reward: total was -61.470000. running mean: -69.483252\n",
      "ep 21: ep_len:3 episode reward: total was 0.000000. running mean: -68.788419\n",
      "ep 21: ep_len:500 episode reward: total was -46.870000. running mean: -68.569235\n",
      "ep 21: ep_len:770 episode reward: total was -66.000000. running mean: -68.543543\n",
      "epsilon:0.396997 episode_count: 154. steps_count: 76999.000000\n",
      "ep 22: ep_len:510 episode reward: total was -55.200000. running mean: -68.410107\n",
      "ep 22: ep_len:745 episode reward: total was -50.220000. running mean: -68.228206\n",
      "ep 22: ep_len:575 episode reward: total was -72.860000. running mean: -68.274524\n",
      "ep 22: ep_len:795 episode reward: total was -100.220000. running mean: -68.593979\n",
      "ep 22: ep_len:89 episode reward: total was -13.970000. running mean: -68.047739\n",
      "ep 22: ep_len:515 episode reward: total was -72.790000. running mean: -68.095162\n",
      "ep 22: ep_len:1085 episode reward: total was -146.160000. running mean: -68.875810\n",
      "epsilon:0.396860 episode_count: 161. steps_count: 81313.000000\n",
      "ep 23: ep_len:530 episode reward: total was -82.840000. running mean: -69.015452\n",
      "ep 23: ep_len:655 episode reward: total was -68.130000. running mean: -69.006597\n",
      "ep 23: ep_len:73 episode reward: total was -11.000000. running mean: -68.426532\n",
      "ep 23: ep_len:525 episode reward: total was -58.290000. running mean: -68.325166\n",
      "ep 23: ep_len:91 episode reward: total was -15.490000. running mean: -67.796815\n",
      "ep 23: ep_len:635 episode reward: total was -79.710000. running mean: -67.915946\n",
      "ep 23: ep_len:600 episode reward: total was -60.250000. running mean: -67.839287\n",
      "epsilon:0.396724 episode_count: 168. steps_count: 84422.000000\n",
      "ep 24: ep_len:241 episode reward: total was -7.880000. running mean: -67.239694\n",
      "ep 24: ep_len:535 episode reward: total was -67.740000. running mean: -67.244697\n",
      "ep 24: ep_len:565 episode reward: total was -77.790000. running mean: -67.350150\n",
      "ep 24: ep_len:500 episode reward: total was -61.270000. running mean: -67.289349\n",
      "ep 24: ep_len:87 episode reward: total was -14.490000. running mean: -66.761355\n",
      "ep 24: ep_len:645 episode reward: total was -80.780000. running mean: -66.901542\n",
      "ep 24: ep_len:575 episode reward: total was -73.210000. running mean: -66.964626\n",
      "epsilon:0.396587 episode_count: 175. steps_count: 87570.000000\n",
      "ep 25: ep_len:595 episode reward: total was -54.320000. running mean: -66.838180\n",
      "ep 25: ep_len:615 episode reward: total was -54.320000. running mean: -66.712998\n",
      "ep 25: ep_len:79 episode reward: total was -8.500000. running mean: -66.130868\n",
      "ep 25: ep_len:575 episode reward: total was -61.780000. running mean: -66.087359\n",
      "ep 25: ep_len:3 episode reward: total was 0.000000. running mean: -65.426486\n",
      "ep 25: ep_len:505 episode reward: total was -62.360000. running mean: -65.395821\n",
      "ep 25: ep_len:840 episode reward: total was -127.350000. running mean: -66.015363\n",
      "epsilon:0.396451 episode_count: 182. steps_count: 90782.000000\n",
      "ep 26: ep_len:600 episode reward: total was -48.790000. running mean: -65.843109\n",
      "ep 26: ep_len:610 episode reward: total was -74.220000. running mean: -65.926878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 26: ep_len:715 episode reward: total was -62.420000. running mean: -65.891809\n",
      "ep 26: ep_len:500 episode reward: total was -49.700000. running mean: -65.729891\n",
      "ep 26: ep_len:3 episode reward: total was 0.000000. running mean: -65.072592\n",
      "ep 26: ep_len:175 episode reward: total was -3.400000. running mean: -64.455866\n",
      "ep 26: ep_len:545 episode reward: total was -35.150000. running mean: -64.162808\n",
      "epsilon:0.396314 episode_count: 189. steps_count: 93930.000000\n",
      "ep 27: ep_len:224 episode reward: total was -10.890000. running mean: -63.630080\n",
      "ep 27: ep_len:690 episode reward: total was -82.240000. running mean: -63.816179\n",
      "ep 27: ep_len:525 episode reward: total was -60.220000. running mean: -63.780217\n",
      "ep 27: ep_len:520 episode reward: total was -60.850000. running mean: -63.750915\n",
      "ep 27: ep_len:3 episode reward: total was 0.000000. running mean: -63.113406\n",
      "ep 27: ep_len:500 episode reward: total was -59.890000. running mean: -63.081172\n",
      "ep 27: ep_len:510 episode reward: total was -48.810000. running mean: -62.938460\n",
      "epsilon:0.396178 episode_count: 196. steps_count: 96902.000000\n",
      "ep 28: ep_len:655 episode reward: total was -69.220000. running mean: -63.001275\n",
      "ep 28: ep_len:625 episode reward: total was -66.110000. running mean: -63.032363\n",
      "ep 28: ep_len:550 episode reward: total was -51.350000. running mean: -62.915539\n",
      "ep 28: ep_len:505 episode reward: total was -35.840000. running mean: -62.644784\n",
      "ep 28: ep_len:3 episode reward: total was 0.000000. running mean: -62.018336\n",
      "ep 28: ep_len:620 episode reward: total was -72.260000. running mean: -62.120752\n",
      "ep 28: ep_len:600 episode reward: total was -58.600000. running mean: -62.085545\n",
      "epsilon:0.396041 episode_count: 203. steps_count: 100460.000000\n",
      "ep 29: ep_len:131 episode reward: total was -10.960000. running mean: -61.574289\n",
      "ep 29: ep_len:505 episode reward: total was -48.810000. running mean: -61.446647\n",
      "ep 29: ep_len:505 episode reward: total was -53.880000. running mean: -61.370980\n",
      "ep 29: ep_len:117 episode reward: total was -7.440000. running mean: -60.831670\n",
      "ep 29: ep_len:103 episode reward: total was -9.980000. running mean: -60.323154\n",
      "ep 29: ep_len:530 episode reward: total was -75.850000. running mean: -60.478422\n",
      "ep 29: ep_len:515 episode reward: total was -59.720000. running mean: -60.470838\n",
      "epsilon:0.395905 episode_count: 210. steps_count: 102866.000000\n",
      "ep 30: ep_len:940 episode reward: total was -113.580000. running mean: -61.001929\n",
      "ep 30: ep_len:510 episode reward: total was -28.800000. running mean: -60.679910\n",
      "ep 30: ep_len:560 episode reward: total was -60.810000. running mean: -60.681211\n",
      "ep 30: ep_len:415 episode reward: total was -43.330000. running mean: -60.507699\n",
      "ep 30: ep_len:3 episode reward: total was 0.000000. running mean: -59.902622\n",
      "ep 30: ep_len:665 episode reward: total was -71.080000. running mean: -60.014396\n",
      "ep 30: ep_len:645 episode reward: total was -78.130000. running mean: -60.195552\n",
      "epsilon:0.395768 episode_count: 217. steps_count: 106604.000000\n",
      "ep 31: ep_len:229 episode reward: total was -14.440000. running mean: -59.737996\n",
      "ep 31: ep_len:178 episode reward: total was -10.460000. running mean: -59.245216\n",
      "ep 31: ep_len:580 episode reward: total was -56.760000. running mean: -59.220364\n",
      "ep 31: ep_len:557 episode reward: total was -66.790000. running mean: -59.296060\n",
      "ep 31: ep_len:3 episode reward: total was 0.000000. running mean: -58.703100\n",
      "ep 31: ep_len:248 episode reward: total was -38.460000. running mean: -58.500669\n",
      "ep 31: ep_len:505 episode reward: total was -45.780000. running mean: -58.373462\n",
      "epsilon:0.395632 episode_count: 224. steps_count: 108904.000000\n",
      "ep 32: ep_len:237 episode reward: total was -29.450000. running mean: -58.084228\n",
      "ep 32: ep_len:500 episode reward: total was -56.780000. running mean: -58.071185\n",
      "ep 32: ep_len:75 episode reward: total was -7.000000. running mean: -57.560473\n",
      "ep 32: ep_len:500 episode reward: total was -64.330000. running mean: -57.628169\n",
      "ep 32: ep_len:3 episode reward: total was 0.000000. running mean: -57.051887\n",
      "ep 32: ep_len:710 episode reward: total was -54.540000. running mean: -57.026768\n",
      "ep 32: ep_len:610 episode reward: total was -80.790000. running mean: -57.264400\n",
      "epsilon:0.395495 episode_count: 231. steps_count: 111539.000000\n",
      "ep 33: ep_len:630 episode reward: total was -70.180000. running mean: -57.393556\n",
      "ep 33: ep_len:500 episode reward: total was -55.820000. running mean: -57.377821\n",
      "ep 33: ep_len:515 episode reward: total was -55.190000. running mean: -57.355943\n",
      "ep 33: ep_len:635 episode reward: total was -68.250000. running mean: -57.464883\n",
      "ep 33: ep_len:3 episode reward: total was 0.000000. running mean: -56.890234\n",
      "ep 33: ep_len:575 episode reward: total was -70.250000. running mean: -57.023832\n",
      "ep 33: ep_len:580 episode reward: total was -70.340000. running mean: -57.156994\n",
      "epsilon:0.395359 episode_count: 238. steps_count: 114977.000000\n",
      "ep 34: ep_len:236 episode reward: total was -13.920000. running mean: -56.724624\n",
      "ep 34: ep_len:196 episode reward: total was -14.950000. running mean: -56.306878\n",
      "ep 34: ep_len:675 episode reward: total was -77.230000. running mean: -56.516109\n",
      "ep 34: ep_len:550 episode reward: total was -49.750000. running mean: -56.448448\n",
      "ep 34: ep_len:3 episode reward: total was 0.000000. running mean: -55.883963\n",
      "ep 34: ep_len:600 episode reward: total was -47.710000. running mean: -55.802224\n",
      "ep 34: ep_len:184 episode reward: total was -20.960000. running mean: -55.453801\n",
      "epsilon:0.395222 episode_count: 245. steps_count: 117421.000000\n",
      "ep 35: ep_len:670 episode reward: total was -79.170000. running mean: -55.690963\n",
      "ep 35: ep_len:510 episode reward: total was -50.700000. running mean: -55.641054\n",
      "ep 35: ep_len:535 episode reward: total was -53.190000. running mean: -55.616543\n",
      "ep 35: ep_len:510 episode reward: total was -58.310000. running mean: -55.643478\n",
      "ep 35: ep_len:3 episode reward: total was 0.000000. running mean: -55.087043\n",
      "ep 35: ep_len:515 episode reward: total was -33.590000. running mean: -54.872073\n",
      "ep 35: ep_len:183 episode reward: total was -26.470000. running mean: -54.588052\n",
      "epsilon:0.395086 episode_count: 252. steps_count: 120347.000000\n",
      "ep 36: ep_len:535 episode reward: total was -70.760000. running mean: -54.749771\n",
      "ep 36: ep_len:510 episode reward: total was -53.290000. running mean: -54.735174\n",
      "ep 36: ep_len:63 episode reward: total was -2.500000. running mean: -54.212822\n",
      "ep 36: ep_len:505 episode reward: total was -52.240000. running mean: -54.193094\n",
      "ep 36: ep_len:3 episode reward: total was 0.000000. running mean: -53.651163\n",
      "ep 36: ep_len:605 episode reward: total was -49.770000. running mean: -53.612351\n",
      "ep 36: ep_len:575 episode reward: total was -58.750000. running mean: -53.663728\n",
      "epsilon:0.394949 episode_count: 259. steps_count: 123143.000000\n",
      "ep 37: ep_len:735 episode reward: total was -58.250000. running mean: -53.709590\n",
      "ep 37: ep_len:305 episode reward: total was -30.390000. running mean: -53.476394\n",
      "ep 37: ep_len:585 episode reward: total was -59.640000. running mean: -53.538030\n",
      "ep 37: ep_len:525 episode reward: total was -57.770000. running mean: -53.580350\n",
      "ep 37: ep_len:3 episode reward: total was 0.000000. running mean: -53.044547\n",
      "ep 37: ep_len:610 episode reward: total was -84.810000. running mean: -53.362201\n",
      "ep 37: ep_len:535 episode reward: total was -62.360000. running mean: -53.452179\n",
      "epsilon:0.394813 episode_count: 266. steps_count: 126441.000000\n",
      "ep 38: ep_len:600 episode reward: total was -57.250000. running mean: -53.490157\n",
      "ep 38: ep_len:520 episode reward: total was -48.690000. running mean: -53.442156\n",
      "ep 38: ep_len:515 episode reward: total was -66.210000. running mean: -53.569834\n",
      "ep 38: ep_len:700 episode reward: total was -45.080000. running mean: -53.484936\n",
      "ep 38: ep_len:3 episode reward: total was 0.000000. running mean: -52.950087\n",
      "ep 38: ep_len:665 episode reward: total was -67.190000. running mean: -53.092486\n",
      "ep 38: ep_len:580 episode reward: total was -70.340000. running mean: -53.264961\n",
      "epsilon:0.394676 episode_count: 273. steps_count: 130024.000000\n",
      "ep 39: ep_len:800 episode reward: total was -108.750000. running mean: -53.819811\n",
      "ep 39: ep_len:515 episode reward: total was -57.750000. running mean: -53.859113\n",
      "ep 39: ep_len:560 episode reward: total was -59.100000. running mean: -53.911522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 39: ep_len:510 episode reward: total was -48.760000. running mean: -53.860007\n",
      "ep 39: ep_len:92 episode reward: total was -13.470000. running mean: -53.456107\n",
      "ep 39: ep_len:560 episode reward: total was -61.680000. running mean: -53.538346\n",
      "ep 39: ep_len:630 episode reward: total was -50.890000. running mean: -53.511862\n",
      "epsilon:0.394540 episode_count: 280. steps_count: 133691.000000\n",
      "ep 40: ep_len:525 episode reward: total was -60.770000. running mean: -53.584444\n",
      "ep 40: ep_len:383 episode reward: total was -51.390000. running mean: -53.562499\n",
      "ep 40: ep_len:715 episode reward: total was -81.130000. running mean: -53.838174\n",
      "ep 40: ep_len:585 episode reward: total was -39.090000. running mean: -53.690692\n",
      "ep 40: ep_len:3 episode reward: total was 0.000000. running mean: -53.153785\n",
      "ep 40: ep_len:695 episode reward: total was -71.100000. running mean: -53.333248\n",
      "ep 40: ep_len:307 episode reward: total was -28.410000. running mean: -53.084015\n",
      "epsilon:0.394403 episode_count: 287. steps_count: 136904.000000\n",
      "ep 41: ep_len:780 episode reward: total was -98.250000. running mean: -53.535675\n",
      "ep 41: ep_len:342 episode reward: total was -40.800000. running mean: -53.408318\n",
      "ep 41: ep_len:635 episode reward: total was -63.190000. running mean: -53.506135\n",
      "ep 41: ep_len:610 episode reward: total was -59.210000. running mean: -53.563174\n",
      "ep 41: ep_len:3 episode reward: total was 0.000000. running mean: -53.027542\n",
      "ep 41: ep_len:740 episode reward: total was -71.480000. running mean: -53.212067\n",
      "ep 41: ep_len:520 episode reward: total was -56.360000. running mean: -53.243546\n",
      "epsilon:0.394267 episode_count: 294. steps_count: 140534.000000\n",
      "ep 42: ep_len:200 episode reward: total was -15.370000. running mean: -52.864810\n",
      "ep 42: ep_len:605 episode reward: total was -26.740000. running mean: -52.603562\n",
      "ep 42: ep_len:870 episode reward: total was -112.050000. running mean: -53.198027\n",
      "ep 42: ep_len:645 episode reward: total was -77.660000. running mean: -53.442646\n",
      "ep 42: ep_len:53 episode reward: total was -5.500000. running mean: -52.963220\n",
      "ep 42: ep_len:545 episode reward: total was -55.210000. running mean: -52.985688\n",
      "ep 42: ep_len:875 episode reward: total was -106.570000. running mean: -53.521531\n",
      "epsilon:0.394130 episode_count: 301. steps_count: 144327.000000\n",
      "ep 43: ep_len:122 episode reward: total was -1.420000. running mean: -53.000516\n",
      "ep 43: ep_len:316 episode reward: total was -37.870000. running mean: -52.849210\n",
      "ep 43: ep_len:585 episode reward: total was -74.150000. running mean: -53.062218\n",
      "ep 43: ep_len:382 episode reward: total was -27.330000. running mean: -52.804896\n",
      "ep 43: ep_len:3 episode reward: total was 0.000000. running mean: -52.276847\n",
      "ep 43: ep_len:620 episode reward: total was -66.700000. running mean: -52.421079\n",
      "ep 43: ep_len:535 episode reward: total was -56.690000. running mean: -52.463768\n",
      "epsilon:0.393994 episode_count: 308. steps_count: 146890.000000\n",
      "ep 44: ep_len:720 episode reward: total was -78.400000. running mean: -52.723130\n",
      "ep 44: ep_len:575 episode reward: total was -38.540000. running mean: -52.581299\n",
      "ep 44: ep_len:71 episode reward: total was -4.470000. running mean: -52.100186\n",
      "ep 44: ep_len:500 episode reward: total was -36.270000. running mean: -51.941884\n",
      "ep 44: ep_len:100 episode reward: total was -12.460000. running mean: -51.547065\n",
      "ep 44: ep_len:595 episode reward: total was -66.700000. running mean: -51.698595\n",
      "ep 44: ep_len:605 episode reward: total was -66.300000. running mean: -51.844609\n",
      "epsilon:0.393857 episode_count: 315. steps_count: 150056.000000\n",
      "ep 45: ep_len:510 episode reward: total was -47.920000. running mean: -51.805363\n",
      "ep 45: ep_len:590 episode reward: total was -74.350000. running mean: -52.030809\n",
      "ep 45: ep_len:625 episode reward: total was -76.700000. running mean: -52.277501\n",
      "ep 45: ep_len:550 episode reward: total was -56.180000. running mean: -52.316526\n",
      "ep 45: ep_len:54 episode reward: total was -4.000000. running mean: -51.833361\n",
      "ep 45: ep_len:645 episode reward: total was -69.070000. running mean: -52.005727\n",
      "ep 45: ep_len:705 episode reward: total was -59.900000. running mean: -52.084670\n",
      "epsilon:0.393721 episode_count: 322. steps_count: 153735.000000\n",
      "ep 46: ep_len:510 episode reward: total was -56.200000. running mean: -52.125823\n",
      "ep 46: ep_len:655 episode reward: total was -69.640000. running mean: -52.300965\n",
      "ep 46: ep_len:645 episode reward: total was -54.770000. running mean: -52.325655\n",
      "ep 46: ep_len:590 episode reward: total was -36.600000. running mean: -52.168399\n",
      "ep 46: ep_len:3 episode reward: total was 0.000000. running mean: -51.646715\n",
      "ep 46: ep_len:550 episode reward: total was -47.840000. running mean: -51.608647\n",
      "ep 46: ep_len:500 episode reward: total was -53.200000. running mean: -51.624561\n",
      "epsilon:0.393584 episode_count: 329. steps_count: 157188.000000\n",
      "ep 47: ep_len:1190 episode reward: total was -110.560000. running mean: -52.213915\n",
      "ep 47: ep_len:364 episode reward: total was -34.410000. running mean: -52.035876\n",
      "ep 47: ep_len:540 episode reward: total was -54.720000. running mean: -52.062717\n",
      "ep 47: ep_len:55 episode reward: total was -4.450000. running mean: -51.586590\n",
      "ep 47: ep_len:105 episode reward: total was -12.460000. running mean: -51.195324\n",
      "ep 47: ep_len:705 episode reward: total was -98.280000. running mean: -51.666171\n",
      "ep 47: ep_len:610 episode reward: total was -65.690000. running mean: -51.806409\n",
      "epsilon:0.393448 episode_count: 336. steps_count: 160757.000000\n",
      "ep 48: ep_len:248 episode reward: total was -7.870000. running mean: -51.367045\n",
      "ep 48: ep_len:520 episode reward: total was -59.220000. running mean: -51.445575\n",
      "ep 48: ep_len:685 episode reward: total was -61.600000. running mean: -51.547119\n",
      "ep 48: ep_len:535 episode reward: total was -59.240000. running mean: -51.624048\n",
      "ep 48: ep_len:3 episode reward: total was 0.000000. running mean: -51.107807\n",
      "ep 48: ep_len:500 episode reward: total was -48.870000. running mean: -51.085429\n",
      "ep 48: ep_len:530 episode reward: total was -79.860000. running mean: -51.373175\n",
      "epsilon:0.393311 episode_count: 343. steps_count: 163778.000000\n",
      "ep 49: ep_len:705 episode reward: total was -97.230000. running mean: -51.831743\n",
      "ep 49: ep_len:830 episode reward: total was -67.120000. running mean: -51.984626\n",
      "ep 49: ep_len:735 episode reward: total was -74.610000. running mean: -52.210880\n",
      "ep 49: ep_len:500 episode reward: total was -44.150000. running mean: -52.130271\n",
      "ep 49: ep_len:3 episode reward: total was 0.000000. running mean: -51.608968\n",
      "ep 49: ep_len:500 episode reward: total was -32.710000. running mean: -51.419978\n",
      "ep 49: ep_len:540 episode reward: total was -35.060000. running mean: -51.256379\n",
      "epsilon:0.393175 episode_count: 350. steps_count: 167591.000000\n",
      "ep 50: ep_len:620 episode reward: total was -62.350000. running mean: -51.367315\n",
      "ep 50: ep_len:500 episode reward: total was -30.910000. running mean: -51.162742\n",
      "ep 50: ep_len:605 episode reward: total was -73.680000. running mean: -51.387914\n",
      "ep 50: ep_len:590 episode reward: total was -58.310000. running mean: -51.457135\n",
      "ep 50: ep_len:3 episode reward: total was 0.000000. running mean: -50.942564\n",
      "ep 50: ep_len:655 episode reward: total was -61.830000. running mean: -51.051438\n",
      "ep 50: ep_len:500 episode reward: total was -57.200000. running mean: -51.112924\n",
      "epsilon:0.393038 episode_count: 357. steps_count: 171064.000000\n",
      "ep 51: ep_len:610 episode reward: total was -75.660000. running mean: -51.358395\n",
      "ep 51: ep_len:590 episode reward: total was -46.550000. running mean: -51.310311\n",
      "ep 51: ep_len:500 episode reward: total was -51.390000. running mean: -51.311107\n",
      "ep 51: ep_len:615 episode reward: total was -32.270000. running mean: -51.120696\n",
      "ep 51: ep_len:67 episode reward: total was -6.960000. running mean: -50.679089\n",
      "ep 51: ep_len:580 episode reward: total was -57.820000. running mean: -50.750499\n",
      "ep 51: ep_len:530 episode reward: total was -68.800000. running mean: -50.930994\n",
      "epsilon:0.392902 episode_count: 364. steps_count: 174556.000000\n",
      "ep 52: ep_len:610 episode reward: total was -79.200000. running mean: -51.213684\n",
      "ep 52: ep_len:615 episode reward: total was -50.190000. running mean: -51.203447\n",
      "ep 52: ep_len:765 episode reward: total was -82.090000. running mean: -51.512312\n",
      "ep 52: ep_len:500 episode reward: total was -31.050000. running mean: -51.307689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 52: ep_len:133 episode reward: total was -15.960000. running mean: -50.954212\n",
      "ep 52: ep_len:555 episode reward: total was -67.800000. running mean: -51.122670\n",
      "ep 52: ep_len:590 episode reward: total was -58.840000. running mean: -51.199843\n",
      "epsilon:0.392765 episode_count: 371. steps_count: 178324.000000\n",
      "ep 53: ep_len:500 episode reward: total was -37.890000. running mean: -51.066745\n",
      "ep 53: ep_len:500 episode reward: total was -55.850000. running mean: -51.114578\n",
      "ep 53: ep_len:630 episode reward: total was -63.300000. running mean: -51.236432\n",
      "ep 53: ep_len:520 episode reward: total was -43.260000. running mean: -51.156668\n",
      "ep 53: ep_len:3 episode reward: total was 0.000000. running mean: -50.645101\n",
      "ep 53: ep_len:735 episode reward: total was -77.590000. running mean: -50.914550\n",
      "ep 53: ep_len:530 episode reward: total was -51.710000. running mean: -50.922504\n",
      "epsilon:0.392629 episode_count: 378. steps_count: 181742.000000\n",
      "ep 54: ep_len:590 episode reward: total was -55.200000. running mean: -50.965279\n",
      "ep 54: ep_len:166 episode reward: total was -15.960000. running mean: -50.615226\n",
      "ep 54: ep_len:500 episode reward: total was -54.400000. running mean: -50.653074\n",
      "ep 54: ep_len:132 episode reward: total was -12.450000. running mean: -50.271043\n",
      "ep 54: ep_len:106 episode reward: total was -3.480000. running mean: -49.803133\n",
      "ep 54: ep_len:650 episode reward: total was -71.190000. running mean: -50.017002\n",
      "ep 54: ep_len:625 episode reward: total was -67.130000. running mean: -50.188132\n",
      "epsilon:0.392492 episode_count: 385. steps_count: 184511.000000\n",
      "ep 55: ep_len:610 episode reward: total was -69.790000. running mean: -50.384150\n",
      "ep 55: ep_len:500 episode reward: total was -40.910000. running mean: -50.289409\n",
      "ep 55: ep_len:500 episode reward: total was -52.720000. running mean: -50.313715\n",
      "ep 55: ep_len:575 episode reward: total was -41.800000. running mean: -50.228578\n",
      "ep 55: ep_len:3 episode reward: total was 0.000000. running mean: -49.726292\n",
      "ep 55: ep_len:505 episode reward: total was -62.390000. running mean: -49.852929\n",
      "ep 55: ep_len:510 episode reward: total was -41.030000. running mean: -49.764700\n",
      "epsilon:0.392356 episode_count: 392. steps_count: 187714.000000\n",
      "ep 56: ep_len:675 episode reward: total was -78.150000. running mean: -50.048553\n",
      "ep 56: ep_len:510 episode reward: total was -29.370000. running mean: -49.841767\n",
      "ep 56: ep_len:500 episode reward: total was -87.410000. running mean: -50.217449\n",
      "ep 56: ep_len:500 episode reward: total was -40.270000. running mean: -50.117975\n",
      "ep 56: ep_len:3 episode reward: total was 0.000000. running mean: -49.616795\n",
      "ep 56: ep_len:640 episode reward: total was -80.140000. running mean: -49.922027\n",
      "ep 56: ep_len:530 episode reward: total was -65.700000. running mean: -50.079807\n",
      "epsilon:0.392219 episode_count: 399. steps_count: 191072.000000\n",
      "ep 57: ep_len:750 episode reward: total was -85.680000. running mean: -50.435809\n",
      "ep 57: ep_len:500 episode reward: total was -14.870000. running mean: -50.080151\n",
      "ep 57: ep_len:595 episode reward: total was -41.180000. running mean: -49.991149\n",
      "ep 57: ep_len:510 episode reward: total was -37.160000. running mean: -49.862838\n",
      "ep 57: ep_len:132 episode reward: total was -15.460000. running mean: -49.518809\n",
      "ep 57: ep_len:690 episode reward: total was -73.190000. running mean: -49.755521\n",
      "ep 57: ep_len:293 episode reward: total was -27.400000. running mean: -49.531966\n",
      "epsilon:0.392083 episode_count: 406. steps_count: 194542.000000\n",
      "ep 58: ep_len:660 episode reward: total was -97.830000. running mean: -50.014946\n",
      "ep 58: ep_len:500 episode reward: total was -47.780000. running mean: -49.992597\n",
      "ep 58: ep_len:570 episode reward: total was -60.210000. running mean: -50.094771\n",
      "ep 58: ep_len:55 episode reward: total was -6.470000. running mean: -49.658523\n",
      "ep 58: ep_len:3 episode reward: total was 0.000000. running mean: -49.161938\n",
      "ep 58: ep_len:500 episode reward: total was -47.200000. running mean: -49.142319\n",
      "ep 58: ep_len:286 episode reward: total was -31.870000. running mean: -48.969596\n",
      "epsilon:0.391946 episode_count: 413. steps_count: 197116.000000\n",
      "ep 59: ep_len:570 episode reward: total was -64.860000. running mean: -49.128500\n",
      "ep 59: ep_len:575 episode reward: total was -70.840000. running mean: -49.345615\n",
      "ep 59: ep_len:500 episode reward: total was -55.410000. running mean: -49.406258\n",
      "ep 59: ep_len:585 episode reward: total was -45.340000. running mean: -49.365596\n",
      "ep 59: ep_len:111 episode reward: total was -11.980000. running mean: -48.991740\n",
      "ep 59: ep_len:500 episode reward: total was -54.810000. running mean: -49.049923\n",
      "ep 59: ep_len:715 episode reward: total was -55.360000. running mean: -49.113023\n",
      "epsilon:0.391810 episode_count: 420. steps_count: 200672.000000\n",
      "ep 60: ep_len:620 episode reward: total was -36.960000. running mean: -48.991493\n",
      "ep 60: ep_len:201 episode reward: total was -20.890000. running mean: -48.710478\n",
      "ep 60: ep_len:393 episode reward: total was -13.860000. running mean: -48.361973\n",
      "ep 60: ep_len:510 episode reward: total was -53.750000. running mean: -48.415854\n",
      "ep 60: ep_len:101 episode reward: total was -15.470000. running mean: -48.086395\n",
      "ep 60: ep_len:550 episode reward: total was -80.830000. running mean: -48.413831\n",
      "ep 60: ep_len:610 episode reward: total was -74.760000. running mean: -48.677293\n",
      "epsilon:0.391673 episode_count: 427. steps_count: 203657.000000\n",
      "ep 61: ep_len:505 episode reward: total was -42.300000. running mean: -48.613520\n",
      "ep 61: ep_len:545 episode reward: total was -51.830000. running mean: -48.645685\n",
      "ep 61: ep_len:515 episode reward: total was -58.780000. running mean: -48.747028\n",
      "ep 61: ep_len:156 episode reward: total was -9.920000. running mean: -48.358758\n",
      "ep 61: ep_len:3 episode reward: total was 0.000000. running mean: -47.875170\n",
      "ep 61: ep_len:740 episode reward: total was -99.690000. running mean: -48.393318\n",
      "ep 61: ep_len:285 episode reward: total was -32.910000. running mean: -48.238485\n",
      "epsilon:0.391537 episode_count: 434. steps_count: 206406.000000\n",
      "ep 62: ep_len:570 episode reward: total was -52.750000. running mean: -48.283600\n",
      "ep 62: ep_len:292 episode reward: total was -52.980000. running mean: -48.330564\n",
      "ep 62: ep_len:79 episode reward: total was -5.990000. running mean: -47.907159\n",
      "ep 62: ep_len:500 episode reward: total was -47.740000. running mean: -47.905487\n",
      "ep 62: ep_len:3 episode reward: total was 0.000000. running mean: -47.426432\n",
      "ep 62: ep_len:605 episode reward: total was -85.290000. running mean: -47.805068\n",
      "ep 62: ep_len:585 episode reward: total was -53.640000. running mean: -47.863417\n",
      "epsilon:0.391400 episode_count: 441. steps_count: 209040.000000\n",
      "ep 63: ep_len:570 episode reward: total was -30.250000. running mean: -47.687283\n",
      "ep 63: ep_len:550 episode reward: total was -78.760000. running mean: -47.998010\n",
      "ep 63: ep_len:565 episode reward: total was -80.270000. running mean: -48.320730\n",
      "ep 63: ep_len:515 episode reward: total was -41.740000. running mean: -48.254923\n",
      "ep 63: ep_len:104 episode reward: total was -13.480000. running mean: -47.907174\n",
      "ep 63: ep_len:186 episode reward: total was -13.940000. running mean: -47.567502\n",
      "ep 63: ep_len:610 episode reward: total was -74.740000. running mean: -47.839227\n",
      "epsilon:0.391264 episode_count: 448. steps_count: 212140.000000\n",
      "ep 64: ep_len:535 episode reward: total was -50.380000. running mean: -47.864634\n",
      "ep 64: ep_len:764 episode reward: total was -95.120000. running mean: -48.337188\n",
      "ep 64: ep_len:500 episode reward: total was -44.100000. running mean: -48.294816\n",
      "ep 64: ep_len:555 episode reward: total was -32.750000. running mean: -48.139368\n",
      "ep 64: ep_len:107 episode reward: total was -8.470000. running mean: -47.742674\n",
      "ep 64: ep_len:500 episode reward: total was -47.400000. running mean: -47.739248\n",
      "ep 64: ep_len:585 episode reward: total was -60.840000. running mean: -47.870255\n",
      "epsilon:0.391127 episode_count: 455. steps_count: 215686.000000\n",
      "ep 65: ep_len:525 episode reward: total was -67.790000. running mean: -48.069453\n",
      "ep 65: ep_len:624 episode reward: total was -67.770000. running mean: -48.266458\n",
      "ep 65: ep_len:79 episode reward: total was -7.960000. running mean: -47.863394\n",
      "ep 65: ep_len:535 episode reward: total was -42.170000. running mean: -47.806460\n",
      "ep 65: ep_len:109 episode reward: total was -11.470000. running mean: -47.443095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 65: ep_len:715 episode reward: total was -69.160000. running mean: -47.660264\n",
      "ep 65: ep_len:565 episode reward: total was -39.010000. running mean: -47.573761\n",
      "epsilon:0.390991 episode_count: 462. steps_count: 218838.000000\n",
      "ep 66: ep_len:605 episode reward: total was -45.720000. running mean: -47.555224\n",
      "ep 66: ep_len:274 episode reward: total was -20.390000. running mean: -47.283572\n",
      "ep 66: ep_len:640 episode reward: total was -56.200000. running mean: -47.372736\n",
      "ep 66: ep_len:500 episode reward: total was -40.640000. running mean: -47.305408\n",
      "ep 66: ep_len:50 episode reward: total was -5.500000. running mean: -46.887354\n",
      "ep 66: ep_len:565 episode reward: total was -65.260000. running mean: -47.071081\n",
      "ep 66: ep_len:550 episode reward: total was -59.180000. running mean: -47.192170\n",
      "epsilon:0.390854 episode_count: 469. steps_count: 222022.000000\n",
      "ep 67: ep_len:223 episode reward: total was -10.400000. running mean: -46.824248\n",
      "ep 67: ep_len:530 episode reward: total was -64.830000. running mean: -47.004306\n",
      "ep 67: ep_len:620 episode reward: total was -48.240000. running mean: -47.016663\n",
      "ep 67: ep_len:52 episode reward: total was -3.470000. running mean: -46.581196\n",
      "ep 67: ep_len:3 episode reward: total was 0.000000. running mean: -46.115384\n",
      "ep 67: ep_len:540 episode reward: total was -52.190000. running mean: -46.176130\n",
      "ep 67: ep_len:695 episode reward: total was -74.160000. running mean: -46.455969\n",
      "epsilon:0.390718 episode_count: 476. steps_count: 224685.000000\n",
      "ep 68: ep_len:580 episode reward: total was -52.230000. running mean: -46.513709\n",
      "ep 68: ep_len:595 episode reward: total was -31.150000. running mean: -46.360072\n",
      "ep 68: ep_len:640 episode reward: total was -64.240000. running mean: -46.538872\n",
      "ep 68: ep_len:403 episode reward: total was -34.320000. running mean: -46.416683\n",
      "ep 68: ep_len:3 episode reward: total was 0.000000. running mean: -45.952516\n",
      "ep 68: ep_len:575 episode reward: total was -74.290000. running mean: -46.235891\n",
      "ep 68: ep_len:186 episode reward: total was -16.470000. running mean: -45.938232\n",
      "epsilon:0.390581 episode_count: 483. steps_count: 227667.000000\n",
      "ep 69: ep_len:640 episode reward: total was -80.210000. running mean: -46.280950\n",
      "ep 69: ep_len:545 episode reward: total was -54.260000. running mean: -46.360740\n",
      "ep 69: ep_len:385 episode reward: total was -32.880000. running mean: -46.225933\n",
      "ep 69: ep_len:510 episode reward: total was -39.180000. running mean: -46.155473\n",
      "ep 69: ep_len:49 episode reward: total was -1.500000. running mean: -45.708919\n",
      "ep 69: ep_len:635 episode reward: total was -49.250000. running mean: -45.744329\n",
      "ep 69: ep_len:520 episode reward: total was -49.310000. running mean: -45.779986\n",
      "epsilon:0.390445 episode_count: 490. steps_count: 230951.000000\n",
      "ep 70: ep_len:227 episode reward: total was -12.430000. running mean: -45.446486\n",
      "ep 70: ep_len:615 episode reward: total was -59.710000. running mean: -45.589121\n",
      "ep 70: ep_len:570 episode reward: total was -65.750000. running mean: -45.790730\n",
      "ep 70: ep_len:500 episode reward: total was -38.730000. running mean: -45.720123\n",
      "ep 70: ep_len:3 episode reward: total was 0.000000. running mean: -45.262922\n",
      "ep 70: ep_len:565 episode reward: total was -40.140000. running mean: -45.211693\n",
      "ep 70: ep_len:530 episode reward: total was -52.700000. running mean: -45.286576\n",
      "epsilon:0.390308 episode_count: 497. steps_count: 233961.000000\n",
      "ep 71: ep_len:118 episode reward: total was -2.940000. running mean: -44.863110\n",
      "ep 71: ep_len:590 episode reward: total was -42.320000. running mean: -44.837679\n",
      "ep 71: ep_len:580 episode reward: total was -58.770000. running mean: -44.977002\n",
      "ep 71: ep_len:167 episode reward: total was -13.420000. running mean: -44.661432\n",
      "ep 71: ep_len:3 episode reward: total was 0.000000. running mean: -44.214818\n",
      "ep 71: ep_len:304 episode reward: total was -20.880000. running mean: -43.981469\n",
      "ep 71: ep_len:630 episode reward: total was -69.140000. running mean: -44.233055\n",
      "epsilon:0.390172 episode_count: 504. steps_count: 236353.000000\n",
      "ep 72: ep_len:645 episode reward: total was -57.630000. running mean: -44.367024\n",
      "ep 72: ep_len:257 episode reward: total was -27.940000. running mean: -44.202754\n",
      "ep 72: ep_len:73 episode reward: total was -7.990000. running mean: -43.840626\n",
      "ep 72: ep_len:605 episode reward: total was -62.330000. running mean: -44.025520\n",
      "ep 72: ep_len:95 episode reward: total was -12.990000. running mean: -43.715165\n",
      "ep 72: ep_len:302 episode reward: total was -27.430000. running mean: -43.552313\n",
      "ep 72: ep_len:351 episode reward: total was -27.900000. running mean: -43.395790\n",
      "epsilon:0.390035 episode_count: 511. steps_count: 238681.000000\n",
      "ep 73: ep_len:535 episode reward: total was -62.240000. running mean: -43.584232\n",
      "ep 73: ep_len:630 episode reward: total was -58.130000. running mean: -43.729690\n",
      "ep 73: ep_len:740 episode reward: total was -97.650000. running mean: -44.268893\n",
      "ep 73: ep_len:720 episode reward: total was -72.030000. running mean: -44.546504\n",
      "ep 73: ep_len:3 episode reward: total was 0.000000. running mean: -44.101039\n",
      "ep 73: ep_len:250 episode reward: total was -9.940000. running mean: -43.759429\n",
      "ep 73: ep_len:540 episode reward: total was -57.760000. running mean: -43.899434\n",
      "epsilon:0.389899 episode_count: 518. steps_count: 242099.000000\n",
      "ep 74: ep_len:545 episode reward: total was -78.270000. running mean: -44.243140\n",
      "ep 74: ep_len:236 episode reward: total was -26.930000. running mean: -44.070009\n",
      "ep 74: ep_len:409 episode reward: total was -28.330000. running mean: -43.912609\n",
      "ep 74: ep_len:500 episode reward: total was -39.660000. running mean: -43.870082\n",
      "ep 74: ep_len:83 episode reward: total was -6.470000. running mean: -43.496082\n",
      "ep 74: ep_len:660 episode reward: total was -57.090000. running mean: -43.632021\n",
      "ep 74: ep_len:299 episode reward: total was -30.930000. running mean: -43.505001\n",
      "epsilon:0.389762 episode_count: 525. steps_count: 244831.000000\n",
      "ep 75: ep_len:600 episode reward: total was -72.730000. running mean: -43.797251\n",
      "ep 75: ep_len:540 episode reward: total was -57.250000. running mean: -43.931778\n",
      "ep 75: ep_len:396 episode reward: total was -42.900000. running mean: -43.921460\n",
      "ep 75: ep_len:520 episode reward: total was -43.190000. running mean: -43.914146\n",
      "ep 75: ep_len:3 episode reward: total was 0.000000. running mean: -43.475004\n",
      "ep 75: ep_len:500 episode reward: total was -34.360000. running mean: -43.383854\n",
      "ep 75: ep_len:595 episode reward: total was -59.150000. running mean: -43.541516\n",
      "epsilon:0.389626 episode_count: 532. steps_count: 247985.000000\n",
      "ep 76: ep_len:580 episode reward: total was -39.090000. running mean: -43.497001\n",
      "ep 76: ep_len:289 episode reward: total was -24.920000. running mean: -43.311231\n",
      "ep 76: ep_len:860 episode reward: total was -121.690000. running mean: -44.095018\n",
      "ep 76: ep_len:540 episode reward: total was -54.790000. running mean: -44.201968\n",
      "ep 76: ep_len:3 episode reward: total was 0.000000. running mean: -43.759948\n",
      "ep 76: ep_len:505 episode reward: total was -49.810000. running mean: -43.820449\n",
      "ep 76: ep_len:204 episode reward: total was -27.470000. running mean: -43.656944\n",
      "epsilon:0.389489 episode_count: 539. steps_count: 250966.000000\n",
      "ep 77: ep_len:805 episode reward: total was -85.710000. running mean: -44.077475\n",
      "ep 77: ep_len:343 episode reward: total was -23.380000. running mean: -43.870500\n",
      "ep 77: ep_len:471 episode reward: total was -40.380000. running mean: -43.835595\n",
      "ep 77: ep_len:510 episode reward: total was -50.690000. running mean: -43.904139\n",
      "ep 77: ep_len:3 episode reward: total was 0.000000. running mean: -43.465098\n",
      "ep 77: ep_len:670 episode reward: total was -67.600000. running mean: -43.706447\n",
      "ep 77: ep_len:580 episode reward: total was -51.660000. running mean: -43.785982\n",
      "epsilon:0.389353 episode_count: 546. steps_count: 254348.000000\n",
      "ep 78: ep_len:625 episode reward: total was -60.150000. running mean: -43.949623\n",
      "ep 78: ep_len:540 episode reward: total was -52.140000. running mean: -44.031526\n",
      "ep 78: ep_len:635 episode reward: total was -43.750000. running mean: -44.028711\n",
      "ep 78: ep_len:85 episode reward: total was -3.450000. running mean: -43.622924\n",
      "ep 78: ep_len:3 episode reward: total was 0.000000. running mean: -43.186695\n",
      "ep 78: ep_len:985 episode reward: total was -94.910000. running mean: -43.703928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 78: ep_len:990 episode reward: total was -119.940000. running mean: -44.466289\n",
      "epsilon:0.389216 episode_count: 553. steps_count: 258211.000000\n",
      "ep 79: ep_len:585 episode reward: total was -36.190000. running mean: -44.383526\n",
      "ep 79: ep_len:535 episode reward: total was -53.360000. running mean: -44.473290\n",
      "ep 79: ep_len:540 episode reward: total was -59.640000. running mean: -44.624957\n",
      "ep 79: ep_len:540 episode reward: total was -56.800000. running mean: -44.746708\n",
      "ep 79: ep_len:3 episode reward: total was 0.000000. running mean: -44.299241\n",
      "ep 79: ep_len:565 episode reward: total was -43.160000. running mean: -44.287848\n",
      "ep 79: ep_len:692 episode reward: total was -82.110000. running mean: -44.666070\n",
      "epsilon:0.389080 episode_count: 560. steps_count: 261671.000000\n",
      "ep 80: ep_len:500 episode reward: total was -44.260000. running mean: -44.662009\n",
      "ep 80: ep_len:555 episode reward: total was -38.250000. running mean: -44.597889\n",
      "ep 80: ep_len:78 episode reward: total was -8.990000. running mean: -44.241810\n",
      "ep 80: ep_len:505 episode reward: total was -52.170000. running mean: -44.321092\n",
      "ep 80: ep_len:48 episode reward: total was -6.000000. running mean: -43.937881\n",
      "ep 80: ep_len:555 episode reward: total was -54.270000. running mean: -44.041202\n",
      "ep 80: ep_len:610 episode reward: total was -70.660000. running mean: -44.307390\n",
      "epsilon:0.388943 episode_count: 567. steps_count: 264522.000000\n",
      "ep 81: ep_len:500 episode reward: total was -57.670000. running mean: -44.441016\n",
      "ep 81: ep_len:877 episode reward: total was -119.190000. running mean: -45.188506\n",
      "ep 81: ep_len:595 episode reward: total was -70.670000. running mean: -45.443321\n",
      "ep 81: ep_len:555 episode reward: total was -71.270000. running mean: -45.701588\n",
      "ep 81: ep_len:132 episode reward: total was -19.470000. running mean: -45.439272\n",
      "ep 81: ep_len:645 episode reward: total was -60.670000. running mean: -45.591579\n",
      "ep 81: ep_len:530 episode reward: total was -51.150000. running mean: -45.647164\n",
      "epsilon:0.388807 episode_count: 574. steps_count: 268356.000000\n",
      "ep 82: ep_len:620 episode reward: total was -67.230000. running mean: -45.862992\n",
      "ep 82: ep_len:500 episode reward: total was -27.340000. running mean: -45.677762\n",
      "ep 82: ep_len:765 episode reward: total was -70.030000. running mean: -45.921284\n",
      "ep 82: ep_len:575 episode reward: total was -57.680000. running mean: -46.038872\n",
      "ep 82: ep_len:87 episode reward: total was -13.970000. running mean: -45.718183\n",
      "ep 82: ep_len:510 episode reward: total was -65.360000. running mean: -45.914601\n",
      "ep 82: ep_len:515 episode reward: total was -51.230000. running mean: -45.967755\n",
      "epsilon:0.388670 episode_count: 581. steps_count: 271928.000000\n",
      "ep 83: ep_len:620 episode reward: total was -55.140000. running mean: -46.059478\n",
      "ep 83: ep_len:630 episode reward: total was -52.650000. running mean: -46.125383\n",
      "ep 83: ep_len:79 episode reward: total was -7.980000. running mean: -45.743929\n",
      "ep 83: ep_len:500 episode reward: total was -48.710000. running mean: -45.773590\n",
      "ep 83: ep_len:3 episode reward: total was 0.000000. running mean: -45.315854\n",
      "ep 83: ep_len:590 episode reward: total was -62.160000. running mean: -45.484295\n",
      "ep 83: ep_len:685 episode reward: total was -71.660000. running mean: -45.746052\n",
      "epsilon:0.388534 episode_count: 588. steps_count: 275035.000000\n",
      "ep 84: ep_len:580 episode reward: total was -67.140000. running mean: -45.959992\n",
      "ep 84: ep_len:720 episode reward: total was -92.810000. running mean: -46.428492\n",
      "ep 84: ep_len:505 episode reward: total was -63.280000. running mean: -46.597007\n",
      "ep 84: ep_len:520 episode reward: total was -61.310000. running mean: -46.744137\n",
      "ep 84: ep_len:49 episode reward: total was -7.500000. running mean: -46.351695\n",
      "ep 84: ep_len:590 episode reward: total was -64.210000. running mean: -46.530278\n",
      "ep 84: ep_len:293 episode reward: total was -35.440000. running mean: -46.419376\n",
      "epsilon:0.388397 episode_count: 595. steps_count: 278292.000000\n",
      "ep 85: ep_len:229 episode reward: total was -8.400000. running mean: -46.039182\n",
      "ep 85: ep_len:540 episode reward: total was -60.710000. running mean: -46.185890\n",
      "ep 85: ep_len:545 episode reward: total was -50.070000. running mean: -46.224731\n",
      "ep 85: ep_len:545 episode reward: total was -61.180000. running mean: -46.374284\n",
      "ep 85: ep_len:3 episode reward: total was 0.000000. running mean: -45.910541\n",
      "ep 85: ep_len:500 episode reward: total was -41.330000. running mean: -45.864736\n",
      "ep 85: ep_len:545 episode reward: total was -55.700000. running mean: -45.963088\n",
      "epsilon:0.388261 episode_count: 602. steps_count: 281199.000000\n",
      "ep 86: ep_len:515 episode reward: total was -33.310000. running mean: -45.836557\n",
      "ep 86: ep_len:515 episode reward: total was -54.780000. running mean: -45.925992\n",
      "ep 86: ep_len:500 episode reward: total was -51.230000. running mean: -45.979032\n",
      "ep 86: ep_len:575 episode reward: total was -40.820000. running mean: -45.927442\n",
      "ep 86: ep_len:3 episode reward: total was 0.000000. running mean: -45.468167\n",
      "ep 86: ep_len:620 episode reward: total was -59.140000. running mean: -45.604886\n",
      "ep 86: ep_len:500 episode reward: total was -52.930000. running mean: -45.678137\n",
      "epsilon:0.388124 episode_count: 609. steps_count: 284427.000000\n",
      "ep 87: ep_len:123 episode reward: total was -3.450000. running mean: -45.255855\n",
      "ep 87: ep_len:550 episode reward: total was -49.120000. running mean: -45.294497\n",
      "ep 87: ep_len:515 episode reward: total was -46.320000. running mean: -45.304752\n",
      "ep 87: ep_len:570 episode reward: total was -74.260000. running mean: -45.594304\n",
      "ep 87: ep_len:3 episode reward: total was 0.000000. running mean: -45.138361\n",
      "ep 87: ep_len:181 episode reward: total was -15.940000. running mean: -44.846378\n",
      "ep 87: ep_len:510 episode reward: total was -43.050000. running mean: -44.828414\n",
      "epsilon:0.387988 episode_count: 616. steps_count: 286879.000000\n",
      "ep 88: ep_len:635 episode reward: total was -62.590000. running mean: -45.006030\n",
      "ep 88: ep_len:555 episode reward: total was -68.170000. running mean: -45.237669\n",
      "ep 88: ep_len:500 episode reward: total was -57.360000. running mean: -45.358893\n",
      "ep 88: ep_len:500 episode reward: total was -47.240000. running mean: -45.377704\n",
      "ep 88: ep_len:3 episode reward: total was 0.000000. running mean: -44.923927\n",
      "ep 88: ep_len:217 episode reward: total was -6.460000. running mean: -44.539287\n",
      "ep 88: ep_len:570 episode reward: total was -65.910000. running mean: -44.752995\n",
      "epsilon:0.387851 episode_count: 623. steps_count: 289859.000000\n",
      "ep 89: ep_len:915 episode reward: total was -100.070000. running mean: -45.306165\n",
      "ep 89: ep_len:505 episode reward: total was -49.710000. running mean: -45.350203\n",
      "ep 89: ep_len:500 episode reward: total was -68.420000. running mean: -45.580901\n",
      "ep 89: ep_len:393 episode reward: total was -40.840000. running mean: -45.533492\n",
      "ep 89: ep_len:128 episode reward: total was -14.960000. running mean: -45.227757\n",
      "ep 89: ep_len:186 episode reward: total was -4.940000. running mean: -44.824879\n",
      "ep 89: ep_len:211 episode reward: total was -24.450000. running mean: -44.621131\n",
      "epsilon:0.387715 episode_count: 630. steps_count: 292697.000000\n",
      "ep 90: ep_len:224 episode reward: total was -8.410000. running mean: -44.259019\n",
      "ep 90: ep_len:545 episode reward: total was -50.710000. running mean: -44.323529\n",
      "ep 90: ep_len:665 episode reward: total was -53.010000. running mean: -44.410394\n",
      "ep 90: ep_len:515 episode reward: total was -35.240000. running mean: -44.318690\n",
      "ep 90: ep_len:3 episode reward: total was 0.000000. running mean: -43.875503\n",
      "ep 90: ep_len:560 episode reward: total was -35.650000. running mean: -43.793248\n",
      "ep 90: ep_len:276 episode reward: total was -29.910000. running mean: -43.654416\n",
      "epsilon:0.387578 episode_count: 637. steps_count: 295485.000000\n",
      "ep 91: ep_len:615 episode reward: total was -50.650000. running mean: -43.724371\n",
      "ep 91: ep_len:580 episode reward: total was -37.890000. running mean: -43.666028\n",
      "ep 91: ep_len:635 episode reward: total was -81.810000. running mean: -44.047467\n",
      "ep 91: ep_len:575 episode reward: total was -67.340000. running mean: -44.280393\n",
      "ep 91: ep_len:51 episode reward: total was -7.000000. running mean: -43.907589\n",
      "ep 91: ep_len:500 episode reward: total was -58.220000. running mean: -44.050713\n",
      "ep 91: ep_len:515 episode reward: total was -55.810000. running mean: -44.168306\n",
      "epsilon:0.387442 episode_count: 644. steps_count: 298956.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 92: ep_len:525 episode reward: total was -36.910000. running mean: -44.095723\n",
      "ep 92: ep_len:500 episode reward: total was -41.850000. running mean: -44.073265\n",
      "ep 92: ep_len:560 episode reward: total was -59.210000. running mean: -44.224633\n",
      "ep 92: ep_len:381 episode reward: total was -32.750000. running mean: -44.109886\n",
      "ep 92: ep_len:3 episode reward: total was 0.000000. running mean: -43.668788\n",
      "ep 92: ep_len:575 episode reward: total was -77.880000. running mean: -44.010900\n",
      "ep 92: ep_len:307 episode reward: total was -29.860000. running mean: -43.869391\n",
      "epsilon:0.387305 episode_count: 651. steps_count: 301807.000000\n",
      "ep 93: ep_len:615 episode reward: total was -61.830000. running mean: -44.048997\n",
      "ep 93: ep_len:560 episode reward: total was -56.720000. running mean: -44.175707\n",
      "ep 93: ep_len:510 episode reward: total was -48.790000. running mean: -44.221850\n",
      "ep 93: ep_len:398 episode reward: total was -19.260000. running mean: -43.972231\n",
      "ep 93: ep_len:3 episode reward: total was 0.000000. running mean: -43.532509\n",
      "ep 93: ep_len:500 episode reward: total was -46.820000. running mean: -43.565384\n",
      "ep 93: ep_len:575 episode reward: total was -62.160000. running mean: -43.751330\n",
      "epsilon:0.387169 episode_count: 658. steps_count: 304968.000000\n",
      "ep 94: ep_len:685 episode reward: total was -61.600000. running mean: -43.929817\n",
      "ep 94: ep_len:510 episode reward: total was -67.840000. running mean: -44.168919\n",
      "ep 94: ep_len:720 episode reward: total was -103.340000. running mean: -44.760629\n",
      "ep 94: ep_len:535 episode reward: total was -47.820000. running mean: -44.791223\n",
      "ep 94: ep_len:116 episode reward: total was -17.970000. running mean: -44.523011\n",
      "ep 94: ep_len:525 episode reward: total was -42.210000. running mean: -44.499881\n",
      "ep 94: ep_len:323 episode reward: total was -25.920000. running mean: -44.314082\n",
      "epsilon:0.387032 episode_count: 665. steps_count: 308382.000000\n",
      "ep 95: ep_len:630 episode reward: total was -65.700000. running mean: -44.527941\n",
      "ep 95: ep_len:500 episode reward: total was -38.830000. running mean: -44.470962\n",
      "ep 95: ep_len:374 episode reward: total was -30.930000. running mean: -44.335552\n",
      "ep 95: ep_len:515 episode reward: total was -40.670000. running mean: -44.298897\n",
      "ep 95: ep_len:3 episode reward: total was 0.000000. running mean: -43.855908\n",
      "ep 95: ep_len:640 episode reward: total was -65.260000. running mean: -44.069949\n",
      "ep 95: ep_len:505 episode reward: total was -52.150000. running mean: -44.150749\n",
      "epsilon:0.386896 episode_count: 672. steps_count: 311549.000000\n",
      "ep 96: ep_len:635 episode reward: total was -62.070000. running mean: -44.329942\n",
      "ep 96: ep_len:515 episode reward: total was -48.140000. running mean: -44.368042\n",
      "ep 96: ep_len:565 episode reward: total was -88.800000. running mean: -44.812362\n",
      "ep 96: ep_len:590 episode reward: total was -30.770000. running mean: -44.671938\n",
      "ep 96: ep_len:3 episode reward: total was 0.000000. running mean: -44.225219\n",
      "ep 96: ep_len:545 episode reward: total was -48.310000. running mean: -44.266067\n",
      "ep 96: ep_len:595 episode reward: total was -69.270000. running mean: -44.516106\n",
      "epsilon:0.386759 episode_count: 679. steps_count: 314997.000000\n",
      "ep 97: ep_len:530 episode reward: total was -75.350000. running mean: -44.824445\n",
      "ep 97: ep_len:510 episode reward: total was -16.370000. running mean: -44.539900\n",
      "ep 97: ep_len:79 episode reward: total was -0.450000. running mean: -44.099001\n",
      "ep 97: ep_len:575 episode reward: total was -35.750000. running mean: -44.015511\n",
      "ep 97: ep_len:85 episode reward: total was 3.550000. running mean: -43.539856\n",
      "ep 97: ep_len:740 episode reward: total was -81.140000. running mean: -43.915858\n",
      "ep 97: ep_len:620 episode reward: total was -66.170000. running mean: -44.138399\n",
      "epsilon:0.386623 episode_count: 686. steps_count: 318136.000000\n",
      "ep 98: ep_len:550 episode reward: total was -47.890000. running mean: -44.175915\n",
      "ep 98: ep_len:500 episode reward: total was -72.420000. running mean: -44.458356\n",
      "ep 98: ep_len:500 episode reward: total was -53.370000. running mean: -44.547472\n",
      "ep 98: ep_len:540 episode reward: total was -63.330000. running mean: -44.735298\n",
      "ep 98: ep_len:77 episode reward: total was -4.490000. running mean: -44.332845\n",
      "ep 98: ep_len:660 episode reward: total was -65.740000. running mean: -44.546916\n",
      "ep 98: ep_len:500 episode reward: total was -48.670000. running mean: -44.588147\n",
      "epsilon:0.386486 episode_count: 693. steps_count: 321463.000000\n",
      "ep 99: ep_len:540 episode reward: total was -59.740000. running mean: -44.739666\n",
      "ep 99: ep_len:184 episode reward: total was -24.420000. running mean: -44.536469\n",
      "ep 99: ep_len:670 episode reward: total was -52.120000. running mean: -44.612304\n",
      "ep 99: ep_len:132 episode reward: total was -6.480000. running mean: -44.230981\n",
      "ep 99: ep_len:3 episode reward: total was 0.000000. running mean: -43.788671\n",
      "ep 99: ep_len:535 episode reward: total was -40.890000. running mean: -43.759685\n",
      "ep 99: ep_len:605 episode reward: total was -61.670000. running mean: -43.938788\n",
      "epsilon:0.386350 episode_count: 700. steps_count: 324132.000000\n",
      "ep 100: ep_len:500 episode reward: total was -53.930000. running mean: -44.038700\n",
      "ep 100: ep_len:525 episode reward: total was -51.730000. running mean: -44.115613\n",
      "ep 100: ep_len:750 episode reward: total was -95.220000. running mean: -44.626657\n",
      "ep 100: ep_len:44 episode reward: total was -3.470000. running mean: -44.215090\n",
      "ep 100: ep_len:3 episode reward: total was 0.000000. running mean: -43.772939\n",
      "ep 100: ep_len:600 episode reward: total was -60.130000. running mean: -43.936510\n",
      "ep 100: ep_len:500 episode reward: total was -47.390000. running mean: -43.971045\n",
      "epsilon:0.386213 episode_count: 707. steps_count: 327054.000000\n",
      "ep 101: ep_len:650 episode reward: total was -40.170000. running mean: -43.933034\n",
      "ep 101: ep_len:605 episode reward: total was -56.160000. running mean: -44.055304\n",
      "ep 101: ep_len:78 episode reward: total was -7.000000. running mean: -43.684751\n",
      "ep 101: ep_len:575 episode reward: total was -39.760000. running mean: -43.645504\n",
      "ep 101: ep_len:104 episode reward: total was -13.480000. running mean: -43.343849\n",
      "ep 101: ep_len:500 episode reward: total was -53.890000. running mean: -43.449310\n",
      "ep 101: ep_len:545 episode reward: total was -50.300000. running mean: -43.517817\n",
      "epsilon:0.386077 episode_count: 714. steps_count: 330111.000000\n",
      "ep 102: ep_len:615 episode reward: total was -81.760000. running mean: -43.900239\n",
      "ep 102: ep_len:510 episode reward: total was -47.330000. running mean: -43.934536\n",
      "ep 102: ep_len:640 episode reward: total was -75.220000. running mean: -44.247391\n",
      "ep 102: ep_len:635 episode reward: total was -42.150000. running mean: -44.226417\n",
      "ep 102: ep_len:3 episode reward: total was 0.000000. running mean: -43.784153\n",
      "ep 102: ep_len:570 episode reward: total was -64.700000. running mean: -43.993311\n",
      "ep 102: ep_len:590 episode reward: total was -47.720000. running mean: -44.030578\n",
      "epsilon:0.385940 episode_count: 721. steps_count: 333674.000000\n",
      "ep 103: ep_len:570 episode reward: total was -45.700000. running mean: -44.047273\n",
      "ep 103: ep_len:182 episode reward: total was -23.480000. running mean: -43.841600\n",
      "ep 103: ep_len:388 episode reward: total was -25.390000. running mean: -43.657084\n",
      "ep 103: ep_len:56 episode reward: total was -4.470000. running mean: -43.265213\n",
      "ep 103: ep_len:106 episode reward: total was -17.480000. running mean: -43.007361\n",
      "ep 103: ep_len:615 episode reward: total was -43.700000. running mean: -43.014287\n",
      "ep 103: ep_len:540 episode reward: total was -50.650000. running mean: -43.090644\n",
      "epsilon:0.385804 episode_count: 728. steps_count: 336131.000000\n",
      "ep 104: ep_len:680 episode reward: total was -57.540000. running mean: -43.235138\n",
      "ep 104: ep_len:265 episode reward: total was -32.880000. running mean: -43.131587\n",
      "ep 104: ep_len:458 episode reward: total was -27.340000. running mean: -42.973671\n",
      "ep 104: ep_len:123 episode reward: total was -11.980000. running mean: -42.663734\n",
      "ep 104: ep_len:49 episode reward: total was 1.500000. running mean: -42.222097\n",
      "ep 104: ep_len:590 episode reward: total was -43.250000. running mean: -42.232376\n",
      "ep 104: ep_len:560 episode reward: total was -63.200000. running mean: -42.442052\n",
      "epsilon:0.385667 episode_count: 735. steps_count: 338856.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 105: ep_len:635 episode reward: total was -65.100000. running mean: -42.668631\n",
      "ep 105: ep_len:615 episode reward: total was -59.650000. running mean: -42.838445\n",
      "ep 105: ep_len:515 episode reward: total was -27.770000. running mean: -42.687761\n",
      "ep 105: ep_len:530 episode reward: total was -23.240000. running mean: -42.493283\n",
      "ep 105: ep_len:3 episode reward: total was 0.000000. running mean: -42.068350\n",
      "ep 105: ep_len:530 episode reward: total was -50.860000. running mean: -42.156267\n",
      "ep 105: ep_len:565 episode reward: total was -48.700000. running mean: -42.221704\n",
      "epsilon:0.385531 episode_count: 742. steps_count: 342249.000000\n",
      "ep 106: ep_len:585 episode reward: total was -54.220000. running mean: -42.341687\n",
      "ep 106: ep_len:590 episode reward: total was -52.700000. running mean: -42.445270\n",
      "ep 106: ep_len:570 episode reward: total was -46.180000. running mean: -42.482617\n",
      "ep 106: ep_len:590 episode reward: total was -45.210000. running mean: -42.509891\n",
      "ep 106: ep_len:89 episode reward: total was -11.950000. running mean: -42.204292\n",
      "ep 106: ep_len:575 episode reward: total was -41.240000. running mean: -42.194649\n",
      "ep 106: ep_len:630 episode reward: total was -61.550000. running mean: -42.388203\n",
      "epsilon:0.385394 episode_count: 749. steps_count: 345878.000000\n",
      "ep 107: ep_len:228 episode reward: total was -9.900000. running mean: -42.063321\n",
      "ep 107: ep_len:510 episode reward: total was -32.810000. running mean: -41.970788\n",
      "ep 107: ep_len:555 episode reward: total was -52.670000. running mean: -42.077780\n",
      "ep 107: ep_len:725 episode reward: total was -97.840000. running mean: -42.635402\n",
      "ep 107: ep_len:3 episode reward: total was 0.000000. running mean: -42.209048\n",
      "ep 107: ep_len:500 episode reward: total was -43.280000. running mean: -42.219757\n",
      "ep 107: ep_len:344 episode reward: total was -45.860000. running mean: -42.256160\n",
      "epsilon:0.385258 episode_count: 756. steps_count: 348743.000000\n",
      "ep 108: ep_len:130 episode reward: total was -6.430000. running mean: -41.897898\n",
      "ep 108: ep_len:500 episode reward: total was -42.260000. running mean: -41.901519\n",
      "ep 108: ep_len:655 episode reward: total was -62.830000. running mean: -42.110804\n",
      "ep 108: ep_len:575 episode reward: total was -67.790000. running mean: -42.367596\n",
      "ep 108: ep_len:88 episode reward: total was -3.460000. running mean: -41.978520\n",
      "ep 108: ep_len:600 episode reward: total was -64.290000. running mean: -42.201635\n",
      "ep 108: ep_len:620 episode reward: total was -54.080000. running mean: -42.320419\n",
      "epsilon:0.385121 episode_count: 763. steps_count: 351911.000000\n",
      "ep 109: ep_len:515 episode reward: total was -39.260000. running mean: -42.289814\n",
      "ep 109: ep_len:640 episode reward: total was -38.740000. running mean: -42.254316\n",
      "ep 109: ep_len:429 episode reward: total was -31.880000. running mean: -42.150573\n",
      "ep 109: ep_len:500 episode reward: total was -44.770000. running mean: -42.176767\n",
      "ep 109: ep_len:3 episode reward: total was 0.000000. running mean: -41.755000\n",
      "ep 109: ep_len:500 episode reward: total was -55.770000. running mean: -41.895150\n",
      "ep 109: ep_len:530 episode reward: total was -46.770000. running mean: -41.943898\n",
      "epsilon:0.384985 episode_count: 770. steps_count: 355028.000000\n",
      "ep 110: ep_len:217 episode reward: total was -17.390000. running mean: -41.698359\n",
      "ep 110: ep_len:284 episode reward: total was -17.910000. running mean: -41.460476\n",
      "ep 110: ep_len:810 episode reward: total was -69.580000. running mean: -41.741671\n",
      "ep 110: ep_len:500 episode reward: total was -62.810000. running mean: -41.952354\n",
      "ep 110: ep_len:92 episode reward: total was -10.460000. running mean: -41.637431\n",
      "ep 110: ep_len:585 episode reward: total was -57.090000. running mean: -41.791956\n",
      "ep 110: ep_len:575 episode reward: total was -39.540000. running mean: -41.769437\n",
      "epsilon:0.384848 episode_count: 777. steps_count: 358091.000000\n",
      "ep 111: ep_len:229 episode reward: total was -6.400000. running mean: -41.415742\n",
      "ep 111: ep_len:555 episode reward: total was -59.800000. running mean: -41.599585\n",
      "ep 111: ep_len:740 episode reward: total was -99.750000. running mean: -42.181089\n",
      "ep 111: ep_len:550 episode reward: total was -34.270000. running mean: -42.101978\n",
      "ep 111: ep_len:3 episode reward: total was 0.000000. running mean: -41.680958\n",
      "ep 111: ep_len:630 episode reward: total was -47.260000. running mean: -41.736749\n",
      "ep 111: ep_len:540 episode reward: total was -62.240000. running mean: -41.941781\n",
      "epsilon:0.384712 episode_count: 784. steps_count: 361338.000000\n",
      "ep 112: ep_len:570 episode reward: total was -34.610000. running mean: -41.868464\n",
      "ep 112: ep_len:505 episode reward: total was -33.050000. running mean: -41.780279\n",
      "ep 112: ep_len:394 episode reward: total was -26.900000. running mean: -41.631476\n",
      "ep 112: ep_len:525 episode reward: total was -58.250000. running mean: -41.797661\n",
      "ep 112: ep_len:3 episode reward: total was 0.000000. running mean: -41.379685\n",
      "ep 112: ep_len:540 episode reward: total was -50.880000. running mean: -41.474688\n",
      "ep 112: ep_len:590 episode reward: total was -60.170000. running mean: -41.661641\n",
      "epsilon:0.384575 episode_count: 791. steps_count: 364465.000000\n",
      "ep 113: ep_len:515 episode reward: total was -56.680000. running mean: -41.811825\n",
      "ep 113: ep_len:520 episode reward: total was -56.710000. running mean: -41.960806\n",
      "ep 113: ep_len:357 episode reward: total was -31.910000. running mean: -41.860298\n",
      "ep 113: ep_len:500 episode reward: total was -41.290000. running mean: -41.854595\n",
      "ep 113: ep_len:3 episode reward: total was 0.000000. running mean: -41.436049\n",
      "ep 113: ep_len:680 episode reward: total was -49.000000. running mean: -41.511689\n",
      "ep 113: ep_len:600 episode reward: total was -48.000000. running mean: -41.576572\n",
      "epsilon:0.384439 episode_count: 798. steps_count: 367640.000000\n",
      "ep 114: ep_len:520 episode reward: total was -49.090000. running mean: -41.651706\n",
      "ep 114: ep_len:705 episode reward: total was -69.710000. running mean: -41.932289\n",
      "ep 114: ep_len:670 episode reward: total was -65.080000. running mean: -42.163766\n",
      "ep 114: ep_len:500 episode reward: total was -35.260000. running mean: -42.094729\n",
      "ep 114: ep_len:73 episode reward: total was -12.500000. running mean: -41.798781\n",
      "ep 114: ep_len:250 episode reward: total was -20.930000. running mean: -41.590094\n",
      "ep 114: ep_len:328 episode reward: total was -37.340000. running mean: -41.547593\n",
      "epsilon:0.384302 episode_count: 805. steps_count: 370686.000000\n",
      "ep 115: ep_len:525 episode reward: total was -56.670000. running mean: -41.698817\n",
      "ep 115: ep_len:331 episode reward: total was -41.430000. running mean: -41.696129\n",
      "ep 115: ep_len:600 episode reward: total was -54.220000. running mean: -41.821367\n",
      "ep 115: ep_len:500 episode reward: total was -55.300000. running mean: -41.956154\n",
      "ep 115: ep_len:3 episode reward: total was 0.000000. running mean: -41.536592\n",
      "ep 115: ep_len:525 episode reward: total was -38.720000. running mean: -41.508426\n",
      "ep 115: ep_len:550 episode reward: total was -43.260000. running mean: -41.525942\n",
      "epsilon:0.384166 episode_count: 812. steps_count: 373720.000000\n",
      "ep 116: ep_len:540 episode reward: total was -55.160000. running mean: -41.662282\n",
      "ep 116: ep_len:630 episode reward: total was -68.810000. running mean: -41.933760\n",
      "ep 116: ep_len:640 episode reward: total was -61.670000. running mean: -42.131122\n",
      "ep 116: ep_len:394 episode reward: total was -34.230000. running mean: -42.052111\n",
      "ep 116: ep_len:3 episode reward: total was 0.000000. running mean: -41.631590\n",
      "ep 116: ep_len:545 episode reward: total was -28.630000. running mean: -41.501574\n",
      "ep 116: ep_len:620 episode reward: total was -59.190000. running mean: -41.678458\n",
      "epsilon:0.384029 episode_count: 819. steps_count: 377092.000000\n",
      "ep 117: ep_len:510 episode reward: total was -56.730000. running mean: -41.828973\n",
      "ep 117: ep_len:620 episode reward: total was -51.540000. running mean: -41.926084\n",
      "ep 117: ep_len:610 episode reward: total was -56.700000. running mean: -42.073823\n",
      "ep 117: ep_len:166 episode reward: total was -8.400000. running mean: -41.737085\n",
      "ep 117: ep_len:3 episode reward: total was 0.000000. running mean: -41.319714\n",
      "ep 117: ep_len:231 episode reward: total was -18.920000. running mean: -41.095717\n",
      "ep 117: ep_len:610 episode reward: total was -59.090000. running mean: -41.275659\n",
      "epsilon:0.383893 episode_count: 826. steps_count: 379842.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 118: ep_len:254 episode reward: total was -14.900000. running mean: -41.011903\n",
      "ep 118: ep_len:505 episode reward: total was -50.740000. running mean: -41.109184\n",
      "ep 118: ep_len:580 episode reward: total was -58.250000. running mean: -41.280592\n",
      "ep 118: ep_len:500 episode reward: total was -30.150000. running mean: -41.169286\n",
      "ep 118: ep_len:98 episode reward: total was -6.960000. running mean: -40.827193\n",
      "ep 118: ep_len:585 episode reward: total was -41.210000. running mean: -40.831021\n",
      "ep 118: ep_len:297 episode reward: total was -29.430000. running mean: -40.717011\n",
      "epsilon:0.383756 episode_count: 833. steps_count: 382661.000000\n",
      "ep 119: ep_len:565 episode reward: total was -50.240000. running mean: -40.812241\n",
      "ep 119: ep_len:515 episode reward: total was -15.890000. running mean: -40.563019\n",
      "ep 119: ep_len:555 episode reward: total was -34.760000. running mean: -40.504988\n",
      "ep 119: ep_len:615 episode reward: total was -64.150000. running mean: -40.741439\n",
      "ep 119: ep_len:115 episode reward: total was -12.950000. running mean: -40.463524\n",
      "ep 119: ep_len:540 episode reward: total was -62.850000. running mean: -40.687389\n",
      "ep 119: ep_len:600 episode reward: total was -37.600000. running mean: -40.656515\n",
      "epsilon:0.383620 episode_count: 840. steps_count: 386166.000000\n",
      "ep 120: ep_len:520 episode reward: total was -55.780000. running mean: -40.807750\n",
      "ep 120: ep_len:500 episode reward: total was -43.790000. running mean: -40.837572\n",
      "ep 120: ep_len:620 episode reward: total was -48.170000. running mean: -40.910897\n",
      "ep 120: ep_len:520 episode reward: total was -39.100000. running mean: -40.892788\n",
      "ep 120: ep_len:94 episode reward: total was -8.960000. running mean: -40.573460\n",
      "ep 120: ep_len:500 episode reward: total was -33.120000. running mean: -40.498925\n",
      "ep 120: ep_len:505 episode reward: total was -39.110000. running mean: -40.485036\n",
      "epsilon:0.383483 episode_count: 847. steps_count: 389425.000000\n",
      "ep 121: ep_len:535 episode reward: total was -47.730000. running mean: -40.557486\n",
      "ep 121: ep_len:580 episode reward: total was -37.230000. running mean: -40.524211\n",
      "ep 121: ep_len:620 episode reward: total was -77.730000. running mean: -40.896269\n",
      "ep 121: ep_len:355 episode reward: total was -30.800000. running mean: -40.795306\n",
      "ep 121: ep_len:112 episode reward: total was -16.490000. running mean: -40.552253\n",
      "ep 121: ep_len:685 episode reward: total was -60.590000. running mean: -40.752630\n",
      "ep 121: ep_len:610 episode reward: total was -57.090000. running mean: -40.916004\n",
      "epsilon:0.383347 episode_count: 854. steps_count: 392922.000000\n",
      "ep 122: ep_len:525 episode reward: total was -49.540000. running mean: -41.002244\n",
      "ep 122: ep_len:520 episode reward: total was -48.420000. running mean: -41.076422\n",
      "ep 122: ep_len:750 episode reward: total was -57.870000. running mean: -41.244357\n",
      "ep 122: ep_len:645 episode reward: total was -49.320000. running mean: -41.325114\n",
      "ep 122: ep_len:3 episode reward: total was 0.000000. running mean: -40.911863\n",
      "ep 122: ep_len:500 episode reward: total was -58.420000. running mean: -41.086944\n",
      "ep 122: ep_len:500 episode reward: total was -30.280000. running mean: -40.978875\n",
      "epsilon:0.383210 episode_count: 861. steps_count: 396365.000000\n",
      "ep 123: ep_len:101 episode reward: total was -1.960000. running mean: -40.588686\n",
      "ep 123: ep_len:615 episode reward: total was -59.630000. running mean: -40.779099\n",
      "ep 123: ep_len:615 episode reward: total was -53.640000. running mean: -40.907708\n",
      "ep 123: ep_len:545 episode reward: total was -23.250000. running mean: -40.731131\n",
      "ep 123: ep_len:3 episode reward: total was 0.000000. running mean: -40.323820\n",
      "ep 123: ep_len:555 episode reward: total was -59.670000. running mean: -40.517281\n",
      "ep 123: ep_len:540 episode reward: total was -37.040000. running mean: -40.482509\n",
      "epsilon:0.383074 episode_count: 868. steps_count: 399339.000000\n",
      "ep 124: ep_len:725 episode reward: total was -97.830000. running mean: -41.055983\n",
      "ep 124: ep_len:580 episode reward: total was -26.270000. running mean: -40.908124\n",
      "ep 124: ep_len:765 episode reward: total was -78.960000. running mean: -41.288642\n",
      "ep 124: ep_len:407 episode reward: total was -38.800000. running mean: -41.263756\n",
      "ep 124: ep_len:3 episode reward: total was 0.000000. running mean: -40.851118\n",
      "ep 124: ep_len:640 episode reward: total was -41.680000. running mean: -40.859407\n",
      "ep 124: ep_len:630 episode reward: total was -46.150000. running mean: -40.912313\n",
      "epsilon:0.382937 episode_count: 875. steps_count: 403089.000000\n",
      "ep 125: ep_len:505 episode reward: total was -40.200000. running mean: -40.905190\n",
      "ep 125: ep_len:510 episode reward: total was -65.350000. running mean: -41.149638\n",
      "ep 125: ep_len:665 episode reward: total was -63.000000. running mean: -41.368142\n",
      "ep 125: ep_len:515 episode reward: total was -46.690000. running mean: -41.421360\n",
      "ep 125: ep_len:3 episode reward: total was 0.000000. running mean: -41.007147\n",
      "ep 125: ep_len:590 episode reward: total was -55.040000. running mean: -41.147475\n",
      "ep 125: ep_len:500 episode reward: total was -51.400000. running mean: -41.250001\n",
      "epsilon:0.382801 episode_count: 882. steps_count: 406377.000000\n",
      "ep 126: ep_len:645 episode reward: total was -41.620000. running mean: -41.253701\n",
      "ep 126: ep_len:510 episode reward: total was -39.760000. running mean: -41.238764\n",
      "ep 126: ep_len:540 episode reward: total was -38.950000. running mean: -41.215876\n",
      "ep 126: ep_len:530 episode reward: total was -58.690000. running mean: -41.390617\n",
      "ep 126: ep_len:34 episode reward: total was 0.000000. running mean: -40.976711\n",
      "ep 126: ep_len:152 episode reward: total was -12.470000. running mean: -40.691644\n",
      "ep 126: ep_len:510 episode reward: total was -36.070000. running mean: -40.645427\n",
      "epsilon:0.382664 episode_count: 889. steps_count: 409298.000000\n",
      "ep 127: ep_len:635 episode reward: total was -62.620000. running mean: -40.865173\n",
      "ep 127: ep_len:590 episode reward: total was -56.750000. running mean: -41.024021\n",
      "ep 127: ep_len:785 episode reward: total was -112.800000. running mean: -41.741781\n",
      "ep 127: ep_len:535 episode reward: total was -63.190000. running mean: -41.956263\n",
      "ep 127: ep_len:3 episode reward: total was 0.000000. running mean: -41.536701\n",
      "ep 127: ep_len:815 episode reward: total was -119.310000. running mean: -42.314434\n",
      "ep 127: ep_len:560 episode reward: total was -67.280000. running mean: -42.564089\n",
      "epsilon:0.382528 episode_count: 896. steps_count: 413221.000000\n",
      "ep 128: ep_len:670 episode reward: total was -57.610000. running mean: -42.714548\n",
      "ep 128: ep_len:500 episode reward: total was -24.770000. running mean: -42.535103\n",
      "ep 128: ep_len:670 episode reward: total was -64.110000. running mean: -42.750852\n",
      "ep 128: ep_len:580 episode reward: total was -42.320000. running mean: -42.746543\n",
      "ep 128: ep_len:3 episode reward: total was 0.000000. running mean: -42.319078\n",
      "ep 128: ep_len:560 episode reward: total was -58.700000. running mean: -42.482887\n",
      "ep 128: ep_len:620 episode reward: total was -56.650000. running mean: -42.624558\n",
      "epsilon:0.382391 episode_count: 903. steps_count: 416824.000000\n",
      "ep 129: ep_len:550 episode reward: total was -38.700000. running mean: -42.585313\n",
      "ep 129: ep_len:575 episode reward: total was -33.240000. running mean: -42.491860\n",
      "ep 129: ep_len:515 episode reward: total was -32.240000. running mean: -42.389341\n",
      "ep 129: ep_len:520 episode reward: total was -31.710000. running mean: -42.282548\n",
      "ep 129: ep_len:3 episode reward: total was 0.000000. running mean: -41.859722\n",
      "ep 129: ep_len:500 episode reward: total was -34.130000. running mean: -41.782425\n",
      "ep 129: ep_len:555 episode reward: total was -66.250000. running mean: -42.027101\n",
      "epsilon:0.382255 episode_count: 910. steps_count: 420042.000000\n",
      "ep 130: ep_len:635 episode reward: total was -62.570000. running mean: -42.232530\n",
      "ep 130: ep_len:500 episode reward: total was -28.160000. running mean: -42.091804\n",
      "ep 130: ep_len:595 episode reward: total was -42.180000. running mean: -42.092686\n",
      "ep 130: ep_len:115 episode reward: total was -3.970000. running mean: -41.711459\n",
      "ep 130: ep_len:3 episode reward: total was 0.000000. running mean: -41.294345\n",
      "ep 130: ep_len:530 episode reward: total was -40.700000. running mean: -41.288401\n",
      "ep 130: ep_len:620 episode reward: total was -53.140000. running mean: -41.406917\n",
      "epsilon:0.382118 episode_count: 917. steps_count: 423040.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 131: ep_len:545 episode reward: total was -26.100000. running mean: -41.253848\n",
      "ep 131: ep_len:500 episode reward: total was -42.110000. running mean: -41.262410\n",
      "ep 131: ep_len:452 episode reward: total was -36.850000. running mean: -41.218286\n",
      "ep 131: ep_len:525 episode reward: total was -36.080000. running mean: -41.166903\n",
      "ep 131: ep_len:116 episode reward: total was -17.970000. running mean: -40.934934\n",
      "ep 131: ep_len:610 episode reward: total was -47.010000. running mean: -40.995684\n",
      "ep 131: ep_len:560 episode reward: total was -45.240000. running mean: -41.038128\n",
      "epsilon:0.381982 episode_count: 924. steps_count: 426348.000000\n",
      "ep 132: ep_len:500 episode reward: total was -59.320000. running mean: -41.220946\n",
      "ep 132: ep_len:545 episode reward: total was -54.330000. running mean: -41.352037\n",
      "ep 132: ep_len:440 episode reward: total was -47.390000. running mean: -41.412417\n",
      "ep 132: ep_len:545 episode reward: total was -47.210000. running mean: -41.470392\n",
      "ep 132: ep_len:3 episode reward: total was 0.000000. running mean: -41.055688\n",
      "ep 132: ep_len:605 episode reward: total was -43.580000. running mean: -41.080932\n",
      "ep 132: ep_len:333 episode reward: total was -34.830000. running mean: -41.018422\n",
      "epsilon:0.381845 episode_count: 931. steps_count: 429319.000000\n",
      "ep 133: ep_len:580 episode reward: total was -44.280000. running mean: -41.051038\n",
      "ep 133: ep_len:580 episode reward: total was -28.730000. running mean: -40.927828\n",
      "ep 133: ep_len:525 episode reward: total was -41.220000. running mean: -40.930749\n",
      "ep 133: ep_len:505 episode reward: total was -47.270000. running mean: -40.994142\n",
      "ep 133: ep_len:3 episode reward: total was 0.000000. running mean: -40.584200\n",
      "ep 133: ep_len:770 episode reward: total was -97.700000. running mean: -41.155358\n",
      "ep 133: ep_len:347 episode reward: total was -32.900000. running mean: -41.072805\n",
      "epsilon:0.381709 episode_count: 938. steps_count: 432629.000000\n",
      "ep 134: ep_len:500 episode reward: total was -26.350000. running mean: -40.925577\n",
      "ep 134: ep_len:285 episode reward: total was -23.910000. running mean: -40.755421\n",
      "ep 134: ep_len:565 episode reward: total was -41.140000. running mean: -40.759267\n",
      "ep 134: ep_len:580 episode reward: total was -30.730000. running mean: -40.658974\n",
      "ep 134: ep_len:3 episode reward: total was 0.000000. running mean: -40.252384\n",
      "ep 134: ep_len:635 episode reward: total was -36.670000. running mean: -40.216561\n",
      "ep 134: ep_len:198 episode reward: total was -22.460000. running mean: -40.038995\n",
      "epsilon:0.381572 episode_count: 945. steps_count: 435395.000000\n",
      "ep 135: ep_len:850 episode reward: total was -88.580000. running mean: -40.524405\n",
      "ep 135: ep_len:585 episode reward: total was -25.170000. running mean: -40.370861\n",
      "ep 135: ep_len:382 episode reward: total was -27.930000. running mean: -40.246452\n",
      "ep 135: ep_len:132 episode reward: total was -4.440000. running mean: -39.888388\n",
      "ep 135: ep_len:3 episode reward: total was 0.000000. running mean: -39.489504\n",
      "ep 135: ep_len:530 episode reward: total was -54.580000. running mean: -39.640409\n",
      "ep 135: ep_len:635 episode reward: total was -50.040000. running mean: -39.744405\n",
      "epsilon:0.381436 episode_count: 952. steps_count: 438512.000000\n",
      "ep 136: ep_len:515 episode reward: total was -45.110000. running mean: -39.798061\n",
      "ep 136: ep_len:540 episode reward: total was -44.700000. running mean: -39.847080\n",
      "ep 136: ep_len:500 episode reward: total was -31.260000. running mean: -39.761209\n",
      "ep 136: ep_len:510 episode reward: total was -29.250000. running mean: -39.656097\n",
      "ep 136: ep_len:3 episode reward: total was 0.000000. running mean: -39.259536\n",
      "ep 136: ep_len:630 episode reward: total was -71.680000. running mean: -39.583741\n",
      "ep 136: ep_len:650 episode reward: total was -62.540000. running mean: -39.813304\n",
      "epsilon:0.381299 episode_count: 959. steps_count: 441860.000000\n",
      "ep 137: ep_len:615 episode reward: total was -43.670000. running mean: -39.851870\n",
      "ep 137: ep_len:515 episode reward: total was -36.840000. running mean: -39.821752\n",
      "ep 137: ep_len:640 episode reward: total was -60.170000. running mean: -40.025234\n",
      "ep 137: ep_len:164 episode reward: total was -13.960000. running mean: -39.764582\n",
      "ep 137: ep_len:3 episode reward: total was 0.000000. running mean: -39.366936\n",
      "ep 137: ep_len:500 episode reward: total was -22.000000. running mean: -39.193267\n",
      "ep 137: ep_len:575 episode reward: total was -44.240000. running mean: -39.243734\n",
      "epsilon:0.381163 episode_count: 966. steps_count: 444872.000000\n",
      "ep 138: ep_len:640 episode reward: total was -62.090000. running mean: -39.472197\n",
      "ep 138: ep_len:500 episode reward: total was -51.850000. running mean: -39.595975\n",
      "ep 138: ep_len:770 episode reward: total was -77.450000. running mean: -39.974515\n",
      "ep 138: ep_len:56 episode reward: total was -1.960000. running mean: -39.594370\n",
      "ep 138: ep_len:3 episode reward: total was 0.000000. running mean: -39.198426\n",
      "ep 138: ep_len:500 episode reward: total was -42.200000. running mean: -39.228442\n",
      "ep 138: ep_len:560 episode reward: total was -59.250000. running mean: -39.428657\n",
      "epsilon:0.381026 episode_count: 973. steps_count: 447901.000000\n",
      "ep 139: ep_len:605 episode reward: total was -39.730000. running mean: -39.431671\n",
      "ep 139: ep_len:615 episode reward: total was -55.160000. running mean: -39.588954\n",
      "ep 139: ep_len:635 episode reward: total was -55.690000. running mean: -39.749965\n",
      "ep 139: ep_len:510 episode reward: total was -49.280000. running mean: -39.845265\n",
      "ep 139: ep_len:104 episode reward: total was -6.460000. running mean: -39.511412\n",
      "ep 139: ep_len:535 episode reward: total was -40.220000. running mean: -39.518498\n",
      "ep 139: ep_len:540 episode reward: total was -42.590000. running mean: -39.549213\n",
      "epsilon:0.380890 episode_count: 980. steps_count: 451445.000000\n",
      "ep 140: ep_len:595 episode reward: total was -57.650000. running mean: -39.730221\n",
      "ep 140: ep_len:500 episode reward: total was -60.420000. running mean: -39.937119\n",
      "ep 140: ep_len:655 episode reward: total was -59.300000. running mean: -40.130748\n",
      "ep 140: ep_len:505 episode reward: total was -41.780000. running mean: -40.147240\n",
      "ep 140: ep_len:3 episode reward: total was 0.000000. running mean: -39.745768\n",
      "ep 140: ep_len:500 episode reward: total was -47.850000. running mean: -39.826810\n",
      "ep 140: ep_len:181 episode reward: total was -20.400000. running mean: -39.632542\n",
      "epsilon:0.380753 episode_count: 987. steps_count: 454384.000000\n",
      "ep 141: ep_len:239 episode reward: total was -7.420000. running mean: -39.310417\n",
      "ep 141: ep_len:745 episode reward: total was -73.440000. running mean: -39.651712\n",
      "ep 141: ep_len:585 episode reward: total was -53.030000. running mean: -39.785495\n",
      "ep 141: ep_len:735 episode reward: total was -51.040000. running mean: -39.898040\n",
      "ep 141: ep_len:3 episode reward: total was 0.000000. running mean: -39.499060\n",
      "ep 141: ep_len:222 episode reward: total was -19.920000. running mean: -39.303269\n",
      "ep 141: ep_len:615 episode reward: total was -67.250000. running mean: -39.582737\n",
      "epsilon:0.380617 episode_count: 994. steps_count: 457528.000000\n",
      "ep 142: ep_len:500 episode reward: total was -20.370000. running mean: -39.390609\n",
      "ep 142: ep_len:615 episode reward: total was -19.110000. running mean: -39.187803\n",
      "ep 142: ep_len:615 episode reward: total was -45.070000. running mean: -39.246625\n",
      "ep 142: ep_len:550 episode reward: total was -48.310000. running mean: -39.337259\n",
      "ep 142: ep_len:51 episode reward: total was 3.500000. running mean: -38.908886\n",
      "ep 142: ep_len:815 episode reward: total was -94.060000. running mean: -39.460398\n",
      "ep 142: ep_len:520 episode reward: total was -64.410000. running mean: -39.709894\n",
      "epsilon:0.380480 episode_count: 1001. steps_count: 461194.000000\n",
      "ep 143: ep_len:520 episode reward: total was -47.150000. running mean: -39.784295\n",
      "ep 143: ep_len:620 episode reward: total was -65.920000. running mean: -40.045652\n",
      "ep 143: ep_len:595 episode reward: total was -72.700000. running mean: -40.372195\n",
      "ep 143: ep_len:500 episode reward: total was -42.690000. running mean: -40.395373\n",
      "ep 143: ep_len:3 episode reward: total was 0.000000. running mean: -39.991419\n",
      "ep 143: ep_len:560 episode reward: total was -53.630000. running mean: -40.127805\n",
      "ep 143: ep_len:355 episode reward: total was -30.370000. running mean: -40.030227\n",
      "epsilon:0.380344 episode_count: 1008. steps_count: 464347.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 144: ep_len:525 episode reward: total was -57.130000. running mean: -40.201225\n",
      "ep 144: ep_len:505 episode reward: total was -44.190000. running mean: -40.241113\n",
      "ep 144: ep_len:510 episode reward: total was -49.130000. running mean: -40.330002\n",
      "ep 144: ep_len:545 episode reward: total was -34.720000. running mean: -40.273902\n",
      "ep 144: ep_len:3 episode reward: total was 0.000000. running mean: -39.871163\n",
      "ep 144: ep_len:505 episode reward: total was -27.850000. running mean: -39.750951\n",
      "ep 144: ep_len:725 episode reward: total was -76.540000. running mean: -40.118841\n",
      "epsilon:0.380207 episode_count: 1015. steps_count: 467665.000000\n",
      "ep 145: ep_len:520 episode reward: total was -49.290000. running mean: -40.210553\n",
      "ep 145: ep_len:655 episode reward: total was -62.020000. running mean: -40.428647\n",
      "ep 145: ep_len:525 episode reward: total was -37.790000. running mean: -40.402261\n",
      "ep 145: ep_len:515 episode reward: total was -22.740000. running mean: -40.225638\n",
      "ep 145: ep_len:3 episode reward: total was 0.000000. running mean: -39.823382\n",
      "ep 145: ep_len:331 episode reward: total was -29.420000. running mean: -39.719348\n",
      "ep 145: ep_len:620 episode reward: total was -58.710000. running mean: -39.909255\n",
      "epsilon:0.380071 episode_count: 1022. steps_count: 470834.000000\n",
      "ep 146: ep_len:505 episode reward: total was -39.560000. running mean: -39.905762\n",
      "ep 146: ep_len:605 episode reward: total was -48.140000. running mean: -39.988105\n",
      "ep 146: ep_len:510 episode reward: total was -61.750000. running mean: -40.205723\n",
      "ep 146: ep_len:500 episode reward: total was -26.700000. running mean: -40.070666\n",
      "ep 146: ep_len:3 episode reward: total was 0.000000. running mean: -39.669960\n",
      "ep 146: ep_len:530 episode reward: total was -54.220000. running mean: -39.815460\n",
      "ep 146: ep_len:605 episode reward: total was -51.140000. running mean: -39.928705\n",
      "epsilon:0.379934 episode_count: 1029. steps_count: 474092.000000\n",
      "ep 147: ep_len:575 episode reward: total was -55.070000. running mean: -40.080118\n",
      "ep 147: ep_len:555 episode reward: total was -31.310000. running mean: -39.992417\n",
      "ep 147: ep_len:660 episode reward: total was -51.600000. running mean: -40.108493\n",
      "ep 147: ep_len:600 episode reward: total was -52.230000. running mean: -40.229708\n",
      "ep 147: ep_len:3 episode reward: total was 0.000000. running mean: -39.827411\n",
      "ep 147: ep_len:540 episode reward: total was -46.250000. running mean: -39.891637\n",
      "ep 147: ep_len:540 episode reward: total was -39.100000. running mean: -39.883720\n",
      "epsilon:0.379798 episode_count: 1036. steps_count: 477565.000000\n",
      "ep 148: ep_len:540 episode reward: total was -57.130000. running mean: -40.056183\n",
      "ep 148: ep_len:500 episode reward: total was -31.900000. running mean: -39.974621\n",
      "ep 148: ep_len:575 episode reward: total was -44.300000. running mean: -40.017875\n",
      "ep 148: ep_len:500 episode reward: total was -55.320000. running mean: -40.170896\n",
      "ep 148: ep_len:3 episode reward: total was 0.000000. running mean: -39.769188\n",
      "ep 148: ep_len:525 episode reward: total was -30.720000. running mean: -39.678696\n",
      "ep 148: ep_len:520 episode reward: total was -36.740000. running mean: -39.649309\n",
      "epsilon:0.379661 episode_count: 1043. steps_count: 480728.000000\n",
      "ep 149: ep_len:130 episode reward: total was -10.990000. running mean: -39.362716\n",
      "ep 149: ep_len:550 episode reward: total was -34.880000. running mean: -39.317888\n",
      "ep 149: ep_len:555 episode reward: total was -61.600000. running mean: -39.540710\n",
      "ep 149: ep_len:610 episode reward: total was -52.120000. running mean: -39.666502\n",
      "ep 149: ep_len:3 episode reward: total was 0.000000. running mean: -39.269837\n",
      "ep 149: ep_len:580 episode reward: total was -69.230000. running mean: -39.569439\n",
      "ep 149: ep_len:520 episode reward: total was -55.120000. running mean: -39.724945\n",
      "epsilon:0.379525 episode_count: 1050. steps_count: 483676.000000\n",
      "ep 150: ep_len:545 episode reward: total was -56.710000. running mean: -39.894795\n",
      "ep 150: ep_len:282 episode reward: total was -33.910000. running mean: -39.834947\n",
      "ep 150: ep_len:670 episode reward: total was -82.090000. running mean: -40.257498\n",
      "ep 150: ep_len:520 episode reward: total was -43.830000. running mean: -40.293223\n",
      "ep 150: ep_len:3 episode reward: total was 0.000000. running mean: -39.890291\n",
      "ep 150: ep_len:500 episode reward: total was -28.660000. running mean: -39.777988\n",
      "ep 150: ep_len:505 episode reward: total was -36.240000. running mean: -39.742608\n",
      "epsilon:0.379388 episode_count: 1057. steps_count: 486701.000000\n",
      "ep 151: ep_len:196 episode reward: total was -15.950000. running mean: -39.504682\n",
      "ep 151: ep_len:630 episode reward: total was -29.700000. running mean: -39.406635\n",
      "ep 151: ep_len:500 episode reward: total was -35.880000. running mean: -39.371369\n",
      "ep 151: ep_len:500 episode reward: total was -23.300000. running mean: -39.210655\n",
      "ep 151: ep_len:3 episode reward: total was 0.000000. running mean: -38.818548\n",
      "ep 151: ep_len:665 episode reward: total was -48.640000. running mean: -38.916763\n",
      "ep 151: ep_len:500 episode reward: total was -65.300000. running mean: -39.180595\n",
      "epsilon:0.379252 episode_count: 1064. steps_count: 489695.000000\n",
      "ep 152: ep_len:128 episode reward: total was -6.470000. running mean: -38.853489\n",
      "ep 152: ep_len:530 episode reward: total was -28.290000. running mean: -38.747854\n",
      "ep 152: ep_len:655 episode reward: total was -87.210000. running mean: -39.232476\n",
      "ep 152: ep_len:500 episode reward: total was -47.700000. running mean: -39.317151\n",
      "ep 152: ep_len:56 episode reward: total was -6.500000. running mean: -38.988980\n",
      "ep 152: ep_len:272 episode reward: total was -42.910000. running mean: -39.028190\n",
      "ep 152: ep_len:187 episode reward: total was -13.410000. running mean: -38.772008\n",
      "epsilon:0.379115 episode_count: 1071. steps_count: 492023.000000\n",
      "ep 153: ep_len:515 episode reward: total was -65.230000. running mean: -39.036588\n",
      "ep 153: ep_len:500 episode reward: total was -49.830000. running mean: -39.144522\n",
      "ep 153: ep_len:441 episode reward: total was -31.820000. running mean: -39.071277\n",
      "ep 153: ep_len:530 episode reward: total was -38.610000. running mean: -39.066664\n",
      "ep 153: ep_len:48 episode reward: total was -4.000000. running mean: -38.715997\n",
      "ep 153: ep_len:635 episode reward: total was -56.350000. running mean: -38.892337\n",
      "ep 153: ep_len:530 episode reward: total was -39.230000. running mean: -38.895714\n",
      "epsilon:0.378979 episode_count: 1078. steps_count: 495222.000000\n",
      "ep 154: ep_len:133 episode reward: total was -6.430000. running mean: -38.571057\n",
      "ep 154: ep_len:530 episode reward: total was -44.040000. running mean: -38.625746\n",
      "ep 154: ep_len:430 episode reward: total was -32.300000. running mean: -38.562489\n",
      "ep 154: ep_len:510 episode reward: total was -31.590000. running mean: -38.492764\n",
      "ep 154: ep_len:3 episode reward: total was 0.000000. running mean: -38.107836\n",
      "ep 154: ep_len:640 episode reward: total was -78.740000. running mean: -38.514158\n",
      "ep 154: ep_len:247 episode reward: total was -12.870000. running mean: -38.257716\n",
      "epsilon:0.378842 episode_count: 1085. steps_count: 497715.000000\n",
      "ep 155: ep_len:236 episode reward: total was -21.910000. running mean: -38.094239\n",
      "ep 155: ep_len:515 episode reward: total was -62.720000. running mean: -38.340497\n",
      "ep 155: ep_len:640 episode reward: total was -56.220000. running mean: -38.519292\n",
      "ep 155: ep_len:510 episode reward: total was -40.160000. running mean: -38.535699\n",
      "ep 155: ep_len:121 episode reward: total was -7.970000. running mean: -38.230042\n",
      "ep 155: ep_len:675 episode reward: total was -54.940000. running mean: -38.397141\n",
      "ep 155: ep_len:520 episode reward: total was -45.210000. running mean: -38.465270\n",
      "epsilon:0.378706 episode_count: 1092. steps_count: 500932.000000\n",
      "ep 156: ep_len:830 episode reward: total was -91.120000. running mean: -38.991817\n",
      "ep 156: ep_len:520 episode reward: total was -54.160000. running mean: -39.143499\n",
      "ep 156: ep_len:540 episode reward: total was -40.940000. running mean: -39.161464\n",
      "ep 156: ep_len:500 episode reward: total was -69.850000. running mean: -39.468350\n",
      "ep 156: ep_len:3 episode reward: total was 0.000000. running mean: -39.073666\n",
      "ep 156: ep_len:505 episode reward: total was -38.120000. running mean: -39.064129\n",
      "ep 156: ep_len:585 episode reward: total was -40.530000. running mean: -39.078788\n",
      "epsilon:0.378569 episode_count: 1099. steps_count: 504415.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 157: ep_len:600 episode reward: total was -53.180000. running mean: -39.219800\n",
      "ep 157: ep_len:500 episode reward: total was -30.740000. running mean: -39.135002\n",
      "ep 157: ep_len:620 episode reward: total was -37.110000. running mean: -39.114752\n",
      "ep 157: ep_len:500 episode reward: total was -32.710000. running mean: -39.050705\n",
      "ep 157: ep_len:3 episode reward: total was 0.000000. running mean: -38.660198\n",
      "ep 157: ep_len:525 episode reward: total was -61.710000. running mean: -38.890696\n",
      "ep 157: ep_len:605 episode reward: total was -61.760000. running mean: -39.119389\n",
      "epsilon:0.378433 episode_count: 1106. steps_count: 507768.000000\n",
      "ep 158: ep_len:625 episode reward: total was -61.570000. running mean: -39.343895\n",
      "ep 158: ep_len:500 episode reward: total was -35.390000. running mean: -39.304356\n",
      "ep 158: ep_len:580 episode reward: total was -42.050000. running mean: -39.331812\n",
      "ep 158: ep_len:520 episode reward: total was -21.640000. running mean: -39.154894\n",
      "ep 158: ep_len:3 episode reward: total was 0.000000. running mean: -38.763345\n",
      "ep 158: ep_len:525 episode reward: total was -43.620000. running mean: -38.811912\n",
      "ep 158: ep_len:505 episode reward: total was -52.820000. running mean: -38.951993\n",
      "epsilon:0.378296 episode_count: 1113. steps_count: 511026.000000\n",
      "ep 159: ep_len:540 episode reward: total was -43.710000. running mean: -38.999573\n",
      "ep 159: ep_len:585 episode reward: total was -49.320000. running mean: -39.102777\n",
      "ep 159: ep_len:585 episode reward: total was -50.030000. running mean: -39.212049\n",
      "ep 159: ep_len:590 episode reward: total was -60.220000. running mean: -39.422129\n",
      "ep 159: ep_len:99 episode reward: total was -14.470000. running mean: -39.172607\n",
      "ep 159: ep_len:178 episode reward: total was -11.960000. running mean: -38.900481\n",
      "ep 159: ep_len:600 episode reward: total was -36.060000. running mean: -38.872077\n",
      "epsilon:0.378160 episode_count: 1120. steps_count: 514203.000000\n",
      "ep 160: ep_len:560 episode reward: total was -29.290000. running mean: -38.776256\n",
      "ep 160: ep_len:650 episode reward: total was -55.200000. running mean: -38.940493\n",
      "ep 160: ep_len:71 episode reward: total was -4.990000. running mean: -38.600988\n",
      "ep 160: ep_len:565 episode reward: total was -68.270000. running mean: -38.897678\n",
      "ep 160: ep_len:3 episode reward: total was 0.000000. running mean: -38.508702\n",
      "ep 160: ep_len:545 episode reward: total was -66.210000. running mean: -38.785715\n",
      "ep 160: ep_len:525 episode reward: total was -45.790000. running mean: -38.855757\n",
      "epsilon:0.378023 episode_count: 1127. steps_count: 517122.000000\n",
      "ep 161: ep_len:705 episode reward: total was -80.160000. running mean: -39.268800\n",
      "ep 161: ep_len:615 episode reward: total was -33.780000. running mean: -39.213912\n",
      "ep 161: ep_len:79 episode reward: total was -7.980000. running mean: -38.901573\n",
      "ep 161: ep_len:535 episode reward: total was -27.240000. running mean: -38.784957\n",
      "ep 161: ep_len:3 episode reward: total was 0.000000. running mean: -38.397107\n",
      "ep 161: ep_len:500 episode reward: total was -34.320000. running mean: -38.356336\n",
      "ep 161: ep_len:282 episode reward: total was -48.930000. running mean: -38.462073\n",
      "epsilon:0.377887 episode_count: 1134. steps_count: 519841.000000\n",
      "ep 162: ep_len:640 episode reward: total was -59.130000. running mean: -38.668752\n",
      "ep 162: ep_len:595 episode reward: total was -75.230000. running mean: -39.034365\n",
      "ep 162: ep_len:500 episode reward: total was -39.720000. running mean: -39.041221\n",
      "ep 162: ep_len:515 episode reward: total was -46.710000. running mean: -39.117909\n",
      "ep 162: ep_len:3 episode reward: total was 0.000000. running mean: -38.726730\n",
      "ep 162: ep_len:500 episode reward: total was -33.690000. running mean: -38.676363\n",
      "ep 162: ep_len:530 episode reward: total was -39.490000. running mean: -38.684499\n",
      "epsilon:0.377750 episode_count: 1141. steps_count: 523124.000000\n",
      "ep 163: ep_len:500 episode reward: total was -28.780000. running mean: -38.585454\n",
      "ep 163: ep_len:555 episode reward: total was -24.730000. running mean: -38.446899\n",
      "ep 163: ep_len:510 episode reward: total was -36.780000. running mean: -38.430230\n",
      "ep 163: ep_len:500 episode reward: total was -56.840000. running mean: -38.614328\n",
      "ep 163: ep_len:3 episode reward: total was 0.000000. running mean: -38.228185\n",
      "ep 163: ep_len:575 episode reward: total was -66.120000. running mean: -38.507103\n",
      "ep 163: ep_len:520 episode reward: total was -55.680000. running mean: -38.678832\n",
      "epsilon:0.377614 episode_count: 1148. steps_count: 526287.000000\n",
      "ep 164: ep_len:600 episode reward: total was -38.450000. running mean: -38.676544\n",
      "ep 164: ep_len:595 episode reward: total was -43.270000. running mean: -38.722478\n",
      "ep 164: ep_len:680 episode reward: total was -91.860000. running mean: -39.253853\n",
      "ep 164: ep_len:545 episode reward: total was -40.740000. running mean: -39.268715\n",
      "ep 164: ep_len:3 episode reward: total was 0.000000. running mean: -38.876028\n",
      "ep 164: ep_len:500 episode reward: total was -37.920000. running mean: -38.866467\n",
      "ep 164: ep_len:630 episode reward: total was -85.290000. running mean: -39.330703\n",
      "epsilon:0.377477 episode_count: 1155. steps_count: 529840.000000\n",
      "ep 165: ep_len:610 episode reward: total was -56.710000. running mean: -39.504496\n",
      "ep 165: ep_len:500 episode reward: total was -18.870000. running mean: -39.298151\n",
      "ep 165: ep_len:510 episode reward: total was -54.390000. running mean: -39.449069\n",
      "ep 165: ep_len:515 episode reward: total was -41.300000. running mean: -39.467579\n",
      "ep 165: ep_len:93 episode reward: total was -13.470000. running mean: -39.207603\n",
      "ep 165: ep_len:601 episode reward: total was -66.700000. running mean: -39.482527\n",
      "ep 165: ep_len:297 episode reward: total was -25.930000. running mean: -39.347001\n",
      "epsilon:0.377341 episode_count: 1162. steps_count: 532966.000000\n",
      "ep 166: ep_len:530 episode reward: total was -32.290000. running mean: -39.276431\n",
      "ep 166: ep_len:261 episode reward: total was -47.960000. running mean: -39.363267\n",
      "ep 166: ep_len:560 episode reward: total was -56.280000. running mean: -39.532434\n",
      "ep 166: ep_len:405 episode reward: total was -35.290000. running mean: -39.490010\n",
      "ep 166: ep_len:3 episode reward: total was 0.000000. running mean: -39.095110\n",
      "ep 166: ep_len:580 episode reward: total was -35.270000. running mean: -39.056859\n",
      "ep 166: ep_len:620 episode reward: total was -42.690000. running mean: -39.093190\n",
      "epsilon:0.377204 episode_count: 1169. steps_count: 535925.000000\n",
      "ep 167: ep_len:630 episode reward: total was -60.760000. running mean: -39.309858\n",
      "ep 167: ep_len:555 episode reward: total was -38.230000. running mean: -39.299060\n",
      "ep 167: ep_len:510 episode reward: total was -50.610000. running mean: -39.412169\n",
      "ep 167: ep_len:170 episode reward: total was -11.430000. running mean: -39.132348\n",
      "ep 167: ep_len:3 episode reward: total was 0.000000. running mean: -38.741024\n",
      "ep 167: ep_len:510 episode reward: total was -46.200000. running mean: -38.815614\n",
      "ep 167: ep_len:500 episode reward: total was -60.680000. running mean: -39.034258\n",
      "epsilon:0.377068 episode_count: 1176. steps_count: 538803.000000\n",
      "ep 168: ep_len:670 episode reward: total was -68.610000. running mean: -39.330015\n",
      "ep 168: ep_len:530 episode reward: total was -47.170000. running mean: -39.408415\n",
      "ep 168: ep_len:443 episode reward: total was -30.880000. running mean: -39.323131\n",
      "ep 168: ep_len:500 episode reward: total was -31.070000. running mean: -39.240600\n",
      "ep 168: ep_len:3 episode reward: total was 0.000000. running mean: -38.848194\n",
      "ep 168: ep_len:620 episode reward: total was -51.700000. running mean: -38.976712\n",
      "ep 168: ep_len:610 episode reward: total was -49.180000. running mean: -39.078744\n",
      "epsilon:0.376931 episode_count: 1183. steps_count: 542179.000000\n",
      "ep 169: ep_len:660 episode reward: total was -36.610000. running mean: -39.054057\n",
      "ep 169: ep_len:570 episode reward: total was -59.230000. running mean: -39.255816\n",
      "ep 169: ep_len:695 episode reward: total was -61.680000. running mean: -39.480058\n",
      "ep 169: ep_len:410 episode reward: total was -34.850000. running mean: -39.433758\n",
      "ep 169: ep_len:130 episode reward: total was -10.470000. running mean: -39.144120\n",
      "ep 169: ep_len:515 episode reward: total was -38.820000. running mean: -39.140879\n",
      "ep 169: ep_len:331 episode reward: total was -24.330000. running mean: -38.992770\n",
      "epsilon:0.376795 episode_count: 1190. steps_count: 545490.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 170: ep_len:260 episode reward: total was -24.960000. running mean: -38.852442\n",
      "ep 170: ep_len:535 episode reward: total was -33.890000. running mean: -38.802818\n",
      "ep 170: ep_len:560 episode reward: total was -54.130000. running mean: -38.956090\n",
      "ep 170: ep_len:535 episode reward: total was -58.200000. running mean: -39.148529\n",
      "ep 170: ep_len:3 episode reward: total was 0.000000. running mean: -38.757044\n",
      "ep 170: ep_len:585 episode reward: total was -32.730000. running mean: -38.696773\n",
      "ep 170: ep_len:565 episode reward: total was -48.050000. running mean: -38.790305\n",
      "epsilon:0.376658 episode_count: 1197. steps_count: 548533.000000\n",
      "ep 171: ep_len:655 episode reward: total was -57.040000. running mean: -38.972802\n",
      "ep 171: ep_len:515 episode reward: total was -48.700000. running mean: -39.070074\n",
      "ep 171: ep_len:560 episode reward: total was -49.100000. running mean: -39.170374\n",
      "ep 171: ep_len:119 episode reward: total was -6.940000. running mean: -38.848070\n",
      "ep 171: ep_len:3 episode reward: total was 0.000000. running mean: -38.459589\n",
      "ep 171: ep_len:500 episode reward: total was -46.150000. running mean: -38.536493\n",
      "ep 171: ep_len:920 episode reward: total was -94.390000. running mean: -39.095028\n",
      "epsilon:0.376522 episode_count: 1204. steps_count: 551805.000000\n",
      "ep 172: ep_len:785 episode reward: total was -74.290000. running mean: -39.446978\n",
      "ep 172: ep_len:565 episode reward: total was -47.770000. running mean: -39.530208\n",
      "ep 172: ep_len:510 episode reward: total was -45.830000. running mean: -39.593206\n",
      "ep 172: ep_len:405 episode reward: total was -50.280000. running mean: -39.700074\n",
      "ep 172: ep_len:3 episode reward: total was 0.000000. running mean: -39.303073\n",
      "ep 172: ep_len:530 episode reward: total was -58.390000. running mean: -39.493943\n",
      "ep 172: ep_len:322 episode reward: total was -49.910000. running mean: -39.598103\n",
      "epsilon:0.376385 episode_count: 1211. steps_count: 554925.000000\n",
      "ep 173: ep_len:500 episode reward: total was -39.200000. running mean: -39.594122\n",
      "ep 173: ep_len:500 episode reward: total was -50.760000. running mean: -39.705781\n",
      "ep 173: ep_len:605 episode reward: total was -61.540000. running mean: -39.924123\n",
      "ep 173: ep_len:565 episode reward: total was -43.180000. running mean: -39.956682\n",
      "ep 173: ep_len:3 episode reward: total was 0.000000. running mean: -39.557115\n",
      "ep 173: ep_len:595 episode reward: total was -56.850000. running mean: -39.730044\n",
      "ep 173: ep_len:610 episode reward: total was -78.830000. running mean: -40.121044\n",
      "epsilon:0.376249 episode_count: 1218. steps_count: 558303.000000\n",
      "ep 174: ep_len:560 episode reward: total was -52.530000. running mean: -40.245133\n",
      "ep 174: ep_len:281 episode reward: total was -23.850000. running mean: -40.081182\n",
      "ep 174: ep_len:650 episode reward: total was -43.490000. running mean: -40.115270\n",
      "ep 174: ep_len:505 episode reward: total was -39.710000. running mean: -40.111217\n",
      "ep 174: ep_len:48 episode reward: total was -1.500000. running mean: -39.725105\n",
      "ep 174: ep_len:505 episode reward: total was -60.280000. running mean: -39.930654\n",
      "ep 174: ep_len:525 episode reward: total was -45.180000. running mean: -39.983148\n",
      "epsilon:0.376112 episode_count: 1225. steps_count: 561377.000000\n",
      "ep 175: ep_len:125 episode reward: total was -9.990000. running mean: -39.683216\n",
      "ep 175: ep_len:530 episode reward: total was -34.620000. running mean: -39.632584\n",
      "ep 175: ep_len:745 episode reward: total was -93.570000. running mean: -40.171958\n",
      "ep 175: ep_len:500 episode reward: total was -49.720000. running mean: -40.267438\n",
      "ep 175: ep_len:2 episode reward: total was 0.000000. running mean: -39.864764\n",
      "ep 175: ep_len:299 episode reward: total was -19.360000. running mean: -39.659716\n",
      "ep 175: ep_len:620 episode reward: total was -43.650000. running mean: -39.699619\n",
      "epsilon:0.375976 episode_count: 1232. steps_count: 564198.000000\n",
      "ep 176: ep_len:625 episode reward: total was -89.810000. running mean: -40.200723\n",
      "ep 176: ep_len:505 episode reward: total was -60.750000. running mean: -40.406216\n",
      "ep 176: ep_len:630 episode reward: total was -56.680000. running mean: -40.568954\n",
      "ep 176: ep_len:505 episode reward: total was -30.270000. running mean: -40.465964\n",
      "ep 176: ep_len:2 episode reward: total was 0.000000. running mean: -40.061305\n",
      "ep 176: ep_len:525 episode reward: total was -46.850000. running mean: -40.129191\n",
      "ep 176: ep_len:520 episode reward: total was -37.650000. running mean: -40.104400\n",
      "epsilon:0.375839 episode_count: 1239. steps_count: 567510.000000\n",
      "ep 177: ep_len:665 episode reward: total was -58.490000. running mean: -40.288256\n",
      "ep 177: ep_len:500 episode reward: total was -46.770000. running mean: -40.353073\n",
      "ep 177: ep_len:540 episode reward: total was -32.270000. running mean: -40.272242\n",
      "ep 177: ep_len:166 episode reward: total was -6.410000. running mean: -39.933620\n",
      "ep 177: ep_len:3 episode reward: total was 0.000000. running mean: -39.534284\n",
      "ep 177: ep_len:510 episode reward: total was -33.360000. running mean: -39.472541\n",
      "ep 177: ep_len:500 episode reward: total was -58.300000. running mean: -39.660815\n",
      "epsilon:0.375703 episode_count: 1246. steps_count: 570394.000000\n",
      "ep 178: ep_len:580 episode reward: total was -66.650000. running mean: -39.930707\n",
      "ep 178: ep_len:341 episode reward: total was -22.350000. running mean: -39.754900\n",
      "ep 178: ep_len:525 episode reward: total was -49.080000. running mean: -39.848151\n",
      "ep 178: ep_len:515 episode reward: total was -31.720000. running mean: -39.766870\n",
      "ep 178: ep_len:3 episode reward: total was 0.000000. running mean: -39.369201\n",
      "ep 178: ep_len:525 episode reward: total was -81.830000. running mean: -39.793809\n",
      "ep 178: ep_len:211 episode reward: total was -22.930000. running mean: -39.625171\n",
      "epsilon:0.375566 episode_count: 1253. steps_count: 573094.000000\n",
      "ep 179: ep_len:500 episode reward: total was -33.840000. running mean: -39.567319\n",
      "ep 179: ep_len:500 episode reward: total was -40.220000. running mean: -39.573846\n",
      "ep 179: ep_len:78 episode reward: total was -1.460000. running mean: -39.192708\n",
      "ep 179: ep_len:500 episode reward: total was -33.170000. running mean: -39.132480\n",
      "ep 179: ep_len:38 episode reward: total was -5.500000. running mean: -38.796156\n",
      "ep 179: ep_len:570 episode reward: total was -40.150000. running mean: -38.809694\n",
      "ep 179: ep_len:292 episode reward: total was -19.370000. running mean: -38.615297\n",
      "epsilon:0.375430 episode_count: 1260. steps_count: 575572.000000\n",
      "ep 180: ep_len:111 episode reward: total was -10.450000. running mean: -38.333644\n",
      "ep 180: ep_len:500 episode reward: total was -47.720000. running mean: -38.427508\n",
      "ep 180: ep_len:590 episode reward: total was -72.690000. running mean: -38.770133\n",
      "ep 180: ep_len:159 episode reward: total was -7.460000. running mean: -38.457031\n",
      "ep 180: ep_len:3 episode reward: total was 0.000000. running mean: -38.072461\n",
      "ep 180: ep_len:575 episode reward: total was -58.620000. running mean: -38.277936\n",
      "ep 180: ep_len:525 episode reward: total was -44.580000. running mean: -38.340957\n",
      "epsilon:0.375293 episode_count: 1267. steps_count: 578035.000000\n",
      "ep 181: ep_len:640 episode reward: total was -30.020000. running mean: -38.257747\n",
      "ep 181: ep_len:500 episode reward: total was -15.930000. running mean: -38.034470\n",
      "ep 181: ep_len:630 episode reward: total was -43.680000. running mean: -38.090925\n",
      "ep 181: ep_len:700 episode reward: total was -125.880000. running mean: -38.968816\n",
      "ep 181: ep_len:3 episode reward: total was 0.000000. running mean: -38.579128\n",
      "ep 181: ep_len:560 episode reward: total was -46.700000. running mean: -38.660337\n",
      "ep 181: ep_len:580 episode reward: total was -44.570000. running mean: -38.719433\n",
      "epsilon:0.375157 episode_count: 1274. steps_count: 581648.000000\n",
      "ep 182: ep_len:765 episode reward: total was -55.630000. running mean: -38.888539\n",
      "ep 182: ep_len:368 episode reward: total was -28.330000. running mean: -38.782954\n",
      "ep 182: ep_len:620 episode reward: total was -45.270000. running mean: -38.847824\n",
      "ep 182: ep_len:560 episode reward: total was -37.690000. running mean: -38.836246\n",
      "ep 182: ep_len:3 episode reward: total was 0.000000. running mean: -38.447883\n",
      "ep 182: ep_len:520 episode reward: total was -42.070000. running mean: -38.484104\n",
      "ep 182: ep_len:640 episode reward: total was -56.610000. running mean: -38.665363\n",
      "epsilon:0.375020 episode_count: 1281. steps_count: 585124.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 183: ep_len:520 episode reward: total was -61.150000. running mean: -38.890210\n",
      "ep 183: ep_len:560 episode reward: total was -58.230000. running mean: -39.083608\n",
      "ep 183: ep_len:600 episode reward: total was -60.750000. running mean: -39.300272\n",
      "ep 183: ep_len:620 episode reward: total was -53.760000. running mean: -39.444869\n",
      "ep 183: ep_len:3 episode reward: total was 0.000000. running mean: -39.050420\n",
      "ep 183: ep_len:530 episode reward: total was -35.790000. running mean: -39.017816\n",
      "ep 183: ep_len:500 episode reward: total was -44.730000. running mean: -39.074938\n",
      "epsilon:0.374884 episode_count: 1288. steps_count: 588457.000000\n",
      "ep 184: ep_len:535 episode reward: total was -45.600000. running mean: -39.140188\n",
      "ep 184: ep_len:595 episode reward: total was -42.640000. running mean: -39.175187\n",
      "ep 184: ep_len:580 episode reward: total was -56.550000. running mean: -39.348935\n",
      "ep 184: ep_len:730 episode reward: total was -108.840000. running mean: -40.043845\n",
      "ep 184: ep_len:40 episode reward: total was 2.500000. running mean: -39.618407\n",
      "ep 184: ep_len:680 episode reward: total was -46.940000. running mean: -39.691623\n",
      "ep 184: ep_len:670 episode reward: total was -82.190000. running mean: -40.116607\n",
      "epsilon:0.374747 episode_count: 1295. steps_count: 592287.000000\n",
      "ep 185: ep_len:505 episode reward: total was -36.560000. running mean: -40.081041\n",
      "ep 185: ep_len:500 episode reward: total was -41.670000. running mean: -40.096930\n",
      "ep 185: ep_len:500 episode reward: total was -55.280000. running mean: -40.248761\n",
      "ep 185: ep_len:41 episode reward: total was 0.030000. running mean: -39.845973\n",
      "ep 185: ep_len:3 episode reward: total was 0.000000. running mean: -39.447513\n",
      "ep 185: ep_len:500 episode reward: total was -44.620000. running mean: -39.499238\n",
      "ep 185: ep_len:510 episode reward: total was -46.660000. running mean: -39.570846\n",
      "epsilon:0.374611 episode_count: 1302. steps_count: 594846.000000\n",
      "ep 186: ep_len:545 episode reward: total was -35.750000. running mean: -39.532638\n",
      "ep 186: ep_len:505 episode reward: total was -43.840000. running mean: -39.575711\n",
      "ep 186: ep_len:535 episode reward: total was -60.150000. running mean: -39.781454\n",
      "ep 186: ep_len:610 episode reward: total was -75.210000. running mean: -40.135739\n",
      "ep 186: ep_len:106 episode reward: total was -16.960000. running mean: -39.903982\n",
      "ep 186: ep_len:314 episode reward: total was -30.340000. running mean: -39.808342\n",
      "ep 186: ep_len:500 episode reward: total was -45.310000. running mean: -39.863359\n",
      "epsilon:0.374474 episode_count: 1309. steps_count: 597961.000000\n",
      "ep 187: ep_len:660 episode reward: total was -67.560000. running mean: -40.140325\n",
      "ep 187: ep_len:500 episode reward: total was -41.690000. running mean: -40.155822\n",
      "ep 187: ep_len:67 episode reward: total was -2.490000. running mean: -39.779164\n",
      "ep 187: ep_len:530 episode reward: total was -55.270000. running mean: -39.934072\n",
      "ep 187: ep_len:78 episode reward: total was -9.460000. running mean: -39.629331\n",
      "ep 187: ep_len:535 episode reward: total was -58.150000. running mean: -39.814538\n",
      "ep 187: ep_len:545 episode reward: total was -42.040000. running mean: -39.836793\n",
      "epsilon:0.374338 episode_count: 1316. steps_count: 600876.000000\n",
      "ep 188: ep_len:665 episode reward: total was -54.440000. running mean: -39.982825\n",
      "ep 188: ep_len:500 episode reward: total was -50.690000. running mean: -40.089897\n",
      "ep 188: ep_len:790 episode reward: total was -65.950000. running mean: -40.348498\n",
      "ep 188: ep_len:555 episode reward: total was -57.210000. running mean: -40.517113\n",
      "ep 188: ep_len:3 episode reward: total was 0.000000. running mean: -40.111941\n",
      "ep 188: ep_len:555 episode reward: total was -44.910000. running mean: -40.159922\n",
      "ep 188: ep_len:293 episode reward: total was -21.910000. running mean: -39.977423\n",
      "epsilon:0.374201 episode_count: 1323. steps_count: 604237.000000\n",
      "ep 189: ep_len:500 episode reward: total was -60.460000. running mean: -40.182249\n",
      "ep 189: ep_len:555 episode reward: total was -54.090000. running mean: -40.321326\n",
      "ep 189: ep_len:580 episode reward: total was -48.720000. running mean: -40.405313\n",
      "ep 189: ep_len:520 episode reward: total was -41.630000. running mean: -40.417560\n",
      "ep 189: ep_len:2 episode reward: total was 0.000000. running mean: -40.013384\n",
      "ep 189: ep_len:660 episode reward: total was -71.160000. running mean: -40.324850\n",
      "ep 189: ep_len:530 episode reward: total was -67.270000. running mean: -40.594302\n",
      "epsilon:0.374065 episode_count: 1330. steps_count: 607584.000000\n",
      "ep 190: ep_len:224 episode reward: total was -5.930000. running mean: -40.247659\n",
      "ep 190: ep_len:590 episode reward: total was -47.820000. running mean: -40.323382\n",
      "ep 190: ep_len:79 episode reward: total was -0.970000. running mean: -39.929848\n",
      "ep 190: ep_len:560 episode reward: total was -37.740000. running mean: -39.907950\n",
      "ep 190: ep_len:85 episode reward: total was -12.960000. running mean: -39.638470\n",
      "ep 190: ep_len:500 episode reward: total was -46.920000. running mean: -39.711286\n",
      "ep 190: ep_len:590 episode reward: total was -60.660000. running mean: -39.920773\n",
      "epsilon:0.373928 episode_count: 1337. steps_count: 610212.000000\n",
      "ep 191: ep_len:644 episode reward: total was -70.650000. running mean: -40.228065\n",
      "ep 191: ep_len:525 episode reward: total was -45.210000. running mean: -40.277884\n",
      "ep 191: ep_len:600 episode reward: total was -67.550000. running mean: -40.550606\n",
      "ep 191: ep_len:169 episode reward: total was -4.940000. running mean: -40.194500\n",
      "ep 191: ep_len:3 episode reward: total was 0.000000. running mean: -39.792555\n",
      "ep 191: ep_len:575 episode reward: total was -42.170000. running mean: -39.816329\n",
      "ep 191: ep_len:585 episode reward: total was -69.350000. running mean: -40.111666\n",
      "epsilon:0.373792 episode_count: 1344. steps_count: 613313.000000\n",
      "ep 192: ep_len:600 episode reward: total was -63.670000. running mean: -40.347249\n",
      "ep 192: ep_len:301 episode reward: total was -28.440000. running mean: -40.228177\n",
      "ep 192: ep_len:770 episode reward: total was -96.770000. running mean: -40.793595\n",
      "ep 192: ep_len:520 episode reward: total was -28.160000. running mean: -40.667259\n",
      "ep 192: ep_len:3 episode reward: total was 0.000000. running mean: -40.260586\n",
      "ep 192: ep_len:660 episode reward: total was -35.420000. running mean: -40.212180\n",
      "ep 192: ep_len:620 episode reward: total was -65.160000. running mean: -40.461659\n",
      "epsilon:0.373655 episode_count: 1351. steps_count: 616787.000000\n",
      "ep 193: ep_len:520 episode reward: total was -42.650000. running mean: -40.483542\n",
      "ep 193: ep_len:540 episode reward: total was -34.920000. running mean: -40.427907\n",
      "ep 193: ep_len:655 episode reward: total was -63.740000. running mean: -40.661028\n",
      "ep 193: ep_len:535 episode reward: total was -34.210000. running mean: -40.596517\n",
      "ep 193: ep_len:3 episode reward: total was 0.000000. running mean: -40.190552\n",
      "ep 193: ep_len:520 episode reward: total was -35.610000. running mean: -40.144747\n",
      "ep 193: ep_len:610 episode reward: total was -51.260000. running mean: -40.255899\n",
      "epsilon:0.373519 episode_count: 1358. steps_count: 620170.000000\n",
      "ep 194: ep_len:525 episode reward: total was -36.260000. running mean: -40.215940\n",
      "ep 194: ep_len:264 episode reward: total was -31.420000. running mean: -40.127981\n",
      "ep 194: ep_len:670 episode reward: total was -87.200000. running mean: -40.598701\n",
      "ep 194: ep_len:505 episode reward: total was -45.230000. running mean: -40.645014\n",
      "ep 194: ep_len:79 episode reward: total was -6.480000. running mean: -40.303364\n",
      "ep 194: ep_len:505 episode reward: total was -31.820000. running mean: -40.218530\n",
      "ep 194: ep_len:595 episode reward: total was -57.240000. running mean: -40.388745\n",
      "epsilon:0.373382 episode_count: 1365. steps_count: 623313.000000\n",
      "ep 195: ep_len:600 episode reward: total was -45.700000. running mean: -40.441857\n",
      "ep 195: ep_len:520 episode reward: total was -51.160000. running mean: -40.549039\n",
      "ep 195: ep_len:575 episode reward: total was -40.640000. running mean: -40.549948\n",
      "ep 195: ep_len:110 episode reward: total was -6.470000. running mean: -40.209149\n",
      "ep 195: ep_len:3 episode reward: total was 0.000000. running mean: -39.807057\n",
      "ep 195: ep_len:500 episode reward: total was -61.370000. running mean: -40.022687\n",
      "ep 195: ep_len:540 episode reward: total was -43.240000. running mean: -40.054860\n",
      "epsilon:0.373246 episode_count: 1372. steps_count: 626161.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 196: ep_len:695 episode reward: total was -73.040000. running mean: -40.384711\n",
      "ep 196: ep_len:595 episode reward: total was -43.200000. running mean: -40.412864\n",
      "ep 196: ep_len:530 episode reward: total was -47.880000. running mean: -40.487536\n",
      "ep 196: ep_len:560 episode reward: total was -41.210000. running mean: -40.494760\n",
      "ep 196: ep_len:3 episode reward: total was 0.000000. running mean: -40.089813\n",
      "ep 196: ep_len:615 episode reward: total was -51.250000. running mean: -40.201415\n",
      "ep 196: ep_len:645 episode reward: total was -48.030000. running mean: -40.279700\n",
      "epsilon:0.373109 episode_count: 1379. steps_count: 629804.000000\n",
      "ep 197: ep_len:1010 episode reward: total was -135.710000. running mean: -41.234003\n",
      "ep 197: ep_len:640 episode reward: total was -43.280000. running mean: -41.254463\n",
      "ep 197: ep_len:655 episode reward: total was -61.220000. running mean: -41.454119\n",
      "ep 197: ep_len:500 episode reward: total was -35.690000. running mean: -41.396478\n",
      "ep 197: ep_len:3 episode reward: total was 0.000000. running mean: -40.982513\n",
      "ep 197: ep_len:565 episode reward: total was -42.680000. running mean: -40.999488\n",
      "ep 197: ep_len:301 episode reward: total was -32.350000. running mean: -40.912993\n",
      "epsilon:0.372973 episode_count: 1386. steps_count: 633478.000000\n",
      "ep 198: ep_len:605 episode reward: total was -35.650000. running mean: -40.860363\n",
      "ep 198: ep_len:560 episode reward: total was -17.310000. running mean: -40.624859\n",
      "ep 198: ep_len:620 episode reward: total was -89.730000. running mean: -41.115911\n",
      "ep 198: ep_len:56 episode reward: total was -6.990000. running mean: -40.774652\n",
      "ep 198: ep_len:3 episode reward: total was 0.000000. running mean: -40.366905\n",
      "ep 198: ep_len:600 episode reward: total was -45.100000. running mean: -40.414236\n",
      "ep 198: ep_len:525 episode reward: total was -44.580000. running mean: -40.455894\n",
      "epsilon:0.372836 episode_count: 1393. steps_count: 636447.000000\n",
      "ep 199: ep_len:129 episode reward: total was -5.950000. running mean: -40.110835\n",
      "ep 199: ep_len:585 episode reward: total was -54.090000. running mean: -40.250626\n",
      "ep 199: ep_len:560 episode reward: total was -53.080000. running mean: -40.378920\n",
      "ep 199: ep_len:510 episode reward: total was -31.260000. running mean: -40.287731\n",
      "ep 199: ep_len:84 episode reward: total was -5.470000. running mean: -39.939554\n",
      "ep 199: ep_len:520 episode reward: total was -41.890000. running mean: -39.959058\n",
      "ep 199: ep_len:530 episode reward: total was -44.730000. running mean: -40.006767\n",
      "epsilon:0.372700 episode_count: 1400. steps_count: 639365.000000\n",
      "ep 200: ep_len:660 episode reward: total was -73.650000. running mean: -40.343200\n",
      "ep 200: ep_len:505 episode reward: total was -54.720000. running mean: -40.486968\n",
      "ep 200: ep_len:605 episode reward: total was -57.220000. running mean: -40.654298\n",
      "ep 200: ep_len:56 episode reward: total was -7.970000. running mean: -40.327455\n",
      "ep 200: ep_len:3 episode reward: total was 0.000000. running mean: -39.924181\n",
      "ep 200: ep_len:575 episode reward: total was -51.800000. running mean: -40.042939\n",
      "ep 200: ep_len:595 episode reward: total was -47.120000. running mean: -40.113709\n",
      "epsilon:0.372563 episode_count: 1407. steps_count: 642364.000000\n",
      "ep 201: ep_len:545 episode reward: total was -40.670000. running mean: -40.119272\n",
      "ep 201: ep_len:700 episode reward: total was -49.160000. running mean: -40.209680\n",
      "ep 201: ep_len:640 episode reward: total was -54.120000. running mean: -40.348783\n",
      "ep 201: ep_len:47 episode reward: total was -1.960000. running mean: -39.964895\n",
      "ep 201: ep_len:3 episode reward: total was 0.000000. running mean: -39.565246\n",
      "ep 201: ep_len:520 episode reward: total was -49.290000. running mean: -39.662493\n",
      "ep 201: ep_len:570 episode reward: total was -42.220000. running mean: -39.688069\n",
      "epsilon:0.372427 episode_count: 1414. steps_count: 645389.000000\n",
      "ep 202: ep_len:660 episode reward: total was -55.180000. running mean: -39.842988\n",
      "ep 202: ep_len:535 episode reward: total was -42.820000. running mean: -39.872758\n",
      "ep 202: ep_len:580 episode reward: total was -38.110000. running mean: -39.855130\n",
      "ep 202: ep_len:525 episode reward: total was -36.700000. running mean: -39.823579\n",
      "ep 202: ep_len:97 episode reward: total was -12.450000. running mean: -39.549843\n",
      "ep 202: ep_len:990 episode reward: total was -87.110000. running mean: -40.025445\n",
      "ep 202: ep_len:500 episode reward: total was -38.190000. running mean: -40.007090\n",
      "epsilon:0.372290 episode_count: 1421. steps_count: 649276.000000\n",
      "ep 203: ep_len:645 episode reward: total was -60.550000. running mean: -40.212520\n",
      "ep 203: ep_len:615 episode reward: total was -53.590000. running mean: -40.346294\n",
      "ep 203: ep_len:610 episode reward: total was -61.500000. running mean: -40.557831\n",
      "ep 203: ep_len:560 episode reward: total was -45.640000. running mean: -40.608653\n",
      "ep 203: ep_len:3 episode reward: total was 0.000000. running mean: -40.202567\n",
      "ep 203: ep_len:595 episode reward: total was -51.290000. running mean: -40.313441\n",
      "ep 203: ep_len:202 episode reward: total was -16.470000. running mean: -40.075006\n",
      "epsilon:0.372154 episode_count: 1428. steps_count: 652506.000000\n",
      "ep 204: ep_len:595 episode reward: total was -12.690000. running mean: -39.801156\n",
      "ep 204: ep_len:510 episode reward: total was -34.310000. running mean: -39.746245\n",
      "ep 204: ep_len:500 episode reward: total was -37.210000. running mean: -39.720882\n",
      "ep 204: ep_len:500 episode reward: total was -36.150000. running mean: -39.685174\n",
      "ep 204: ep_len:94 episode reward: total was -8.990000. running mean: -39.378222\n",
      "ep 204: ep_len:510 episode reward: total was -38.350000. running mean: -39.367940\n",
      "ep 204: ep_len:590 episode reward: total was -62.650000. running mean: -39.600760\n",
      "epsilon:0.372017 episode_count: 1435. steps_count: 655805.000000\n",
      "ep 205: ep_len:500 episode reward: total was -43.820000. running mean: -39.642953\n",
      "ep 205: ep_len:223 episode reward: total was -19.900000. running mean: -39.445523\n",
      "ep 205: ep_len:655 episode reward: total was -71.760000. running mean: -39.768668\n",
      "ep 205: ep_len:43 episode reward: total was -2.480000. running mean: -39.395781\n",
      "ep 205: ep_len:3 episode reward: total was 0.000000. running mean: -39.001823\n",
      "ep 205: ep_len:670 episode reward: total was -76.260000. running mean: -39.374405\n",
      "ep 205: ep_len:615 episode reward: total was -74.260000. running mean: -39.723261\n",
      "epsilon:0.371881 episode_count: 1442. steps_count: 658514.000000\n",
      "ep 206: ep_len:530 episode reward: total was -56.740000. running mean: -39.893428\n",
      "ep 206: ep_len:296 episode reward: total was -11.340000. running mean: -39.607894\n",
      "ep 206: ep_len:500 episode reward: total was -36.260000. running mean: -39.574415\n",
      "ep 206: ep_len:530 episode reward: total was -27.220000. running mean: -39.450871\n",
      "ep 206: ep_len:64 episode reward: total was -5.970000. running mean: -39.116062\n",
      "ep 206: ep_len:610 episode reward: total was -60.570000. running mean: -39.330602\n",
      "ep 206: ep_len:585 episode reward: total was -42.610000. running mean: -39.363396\n",
      "epsilon:0.371744 episode_count: 1449. steps_count: 661629.000000\n",
      "ep 207: ep_len:535 episode reward: total was -50.830000. running mean: -39.478062\n",
      "ep 207: ep_len:535 episode reward: total was -31.080000. running mean: -39.394081\n",
      "ep 207: ep_len:500 episode reward: total was -51.810000. running mean: -39.518240\n",
      "ep 207: ep_len:500 episode reward: total was -36.740000. running mean: -39.490458\n",
      "ep 207: ep_len:3 episode reward: total was 0.000000. running mean: -39.095553\n",
      "ep 207: ep_len:685 episode reward: total was -67.170000. running mean: -39.376298\n",
      "ep 207: ep_len:515 episode reward: total was -34.210000. running mean: -39.324635\n",
      "epsilon:0.371608 episode_count: 1456. steps_count: 664902.000000\n",
      "ep 208: ep_len:730 episode reward: total was -63.460000. running mean: -39.565989\n",
      "ep 208: ep_len:810 episode reward: total was -96.840000. running mean: -40.138729\n",
      "ep 208: ep_len:570 episode reward: total was -53.140000. running mean: -40.268741\n",
      "ep 208: ep_len:655 episode reward: total was -82.870000. running mean: -40.694754\n",
      "ep 208: ep_len:3 episode reward: total was 0.000000. running mean: -40.287806\n",
      "ep 208: ep_len:590 episode reward: total was -56.200000. running mean: -40.446928\n",
      "ep 208: ep_len:675 episode reward: total was -63.110000. running mean: -40.673559\n",
      "epsilon:0.371471 episode_count: 1463. steps_count: 668935.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 209: ep_len:201 episode reward: total was -15.430000. running mean: -40.421123\n",
      "ep 209: ep_len:305 episode reward: total was -32.380000. running mean: -40.340712\n",
      "ep 209: ep_len:460 episode reward: total was -24.810000. running mean: -40.185405\n",
      "ep 209: ep_len:600 episode reward: total was -33.700000. running mean: -40.120551\n",
      "ep 209: ep_len:66 episode reward: total was -10.490000. running mean: -39.824246\n",
      "ep 209: ep_len:540 episode reward: total was -40.220000. running mean: -39.828203\n",
      "ep 209: ep_len:520 episode reward: total was -46.850000. running mean: -39.898421\n",
      "epsilon:0.371335 episode_count: 1470. steps_count: 671627.000000\n",
      "ep 210: ep_len:595 episode reward: total was -53.170000. running mean: -40.031137\n",
      "ep 210: ep_len:510 episode reward: total was -51.790000. running mean: -40.148725\n",
      "ep 210: ep_len:570 episode reward: total was -79.290000. running mean: -40.540138\n",
      "ep 210: ep_len:515 episode reward: total was -42.740000. running mean: -40.562137\n",
      "ep 210: ep_len:3 episode reward: total was 0.000000. running mean: -40.156515\n",
      "ep 210: ep_len:231 episode reward: total was -28.960000. running mean: -40.044550\n",
      "ep 210: ep_len:525 episode reward: total was -64.250000. running mean: -40.286605\n",
      "epsilon:0.371198 episode_count: 1477. steps_count: 674576.000000\n",
      "ep 211: ep_len:500 episode reward: total was -50.670000. running mean: -40.390439\n",
      "ep 211: ep_len:580 episode reward: total was -62.690000. running mean: -40.613434\n",
      "ep 211: ep_len:500 episode reward: total was -51.830000. running mean: -40.725600\n",
      "ep 211: ep_len:510 episode reward: total was -31.780000. running mean: -40.636144\n",
      "ep 211: ep_len:3 episode reward: total was 0.000000. running mean: -40.229783\n",
      "ep 211: ep_len:535 episode reward: total was -61.660000. running mean: -40.444085\n",
      "ep 211: ep_len:615 episode reward: total was -63.240000. running mean: -40.672044\n",
      "epsilon:0.371062 episode_count: 1484. steps_count: 677819.000000\n",
      "ep 212: ep_len:515 episode reward: total was -27.770000. running mean: -40.543023\n",
      "ep 212: ep_len:293 episode reward: total was -42.410000. running mean: -40.561693\n",
      "ep 212: ep_len:380 episode reward: total was -35.380000. running mean: -40.509876\n",
      "ep 212: ep_len:555 episode reward: total was -28.710000. running mean: -40.391878\n",
      "ep 212: ep_len:108 episode reward: total was -9.970000. running mean: -40.087659\n",
      "ep 212: ep_len:500 episode reward: total was -37.690000. running mean: -40.063682\n",
      "ep 212: ep_len:570 episode reward: total was -39.580000. running mean: -40.058845\n",
      "epsilon:0.370925 episode_count: 1491. steps_count: 680740.000000\n",
      "ep 213: ep_len:645 episode reward: total was -78.680000. running mean: -40.445057\n",
      "ep 213: ep_len:510 episode reward: total was -55.720000. running mean: -40.597806\n",
      "ep 213: ep_len:500 episode reward: total was -43.660000. running mean: -40.628428\n",
      "ep 213: ep_len:500 episode reward: total was -45.780000. running mean: -40.679944\n",
      "ep 213: ep_len:100 episode reward: total was -15.960000. running mean: -40.432745\n",
      "ep 213: ep_len:565 episode reward: total was -48.150000. running mean: -40.509917\n",
      "ep 213: ep_len:297 episode reward: total was -20.890000. running mean: -40.313718\n",
      "epsilon:0.370789 episode_count: 1498. steps_count: 683857.000000\n",
      "ep 214: ep_len:660 episode reward: total was -66.970000. running mean: -40.580281\n",
      "ep 214: ep_len:580 episode reward: total was -56.820000. running mean: -40.742678\n",
      "ep 214: ep_len:505 episode reward: total was -52.170000. running mean: -40.856951\n",
      "ep 214: ep_len:510 episode reward: total was -32.670000. running mean: -40.775082\n",
      "ep 214: ep_len:77 episode reward: total was -10.960000. running mean: -40.476931\n",
      "ep 214: ep_len:620 episode reward: total was -64.670000. running mean: -40.718862\n",
      "ep 214: ep_len:510 episode reward: total was -47.650000. running mean: -40.788173\n",
      "epsilon:0.370652 episode_count: 1505. steps_count: 687319.000000\n",
      "ep 215: ep_len:500 episode reward: total was -38.390000. running mean: -40.764191\n",
      "ep 215: ep_len:585 episode reward: total was -41.180000. running mean: -40.768349\n",
      "ep 215: ep_len:565 episode reward: total was -33.160000. running mean: -40.692266\n",
      "ep 215: ep_len:510 episode reward: total was -44.130000. running mean: -40.726643\n",
      "ep 215: ep_len:3 episode reward: total was 0.000000. running mean: -40.319377\n",
      "ep 215: ep_len:620 episode reward: total was -65.290000. running mean: -40.569083\n",
      "ep 215: ep_len:590 episode reward: total was -53.120000. running mean: -40.694592\n",
      "epsilon:0.370516 episode_count: 1512. steps_count: 690692.000000\n",
      "ep 216: ep_len:500 episode reward: total was -29.160000. running mean: -40.579246\n",
      "ep 216: ep_len:510 episode reward: total was -54.240000. running mean: -40.715854\n",
      "ep 216: ep_len:451 episode reward: total was -36.410000. running mean: -40.672795\n",
      "ep 216: ep_len:56 episode reward: total was -6.490000. running mean: -40.330967\n",
      "ep 216: ep_len:89 episode reward: total was -11.480000. running mean: -40.042458\n",
      "ep 216: ep_len:305 episode reward: total was -24.440000. running mean: -39.886433\n",
      "ep 216: ep_len:515 episode reward: total was -55.160000. running mean: -40.039169\n",
      "epsilon:0.370379 episode_count: 1519. steps_count: 693118.000000\n",
      "ep 217: ep_len:615 episode reward: total was -32.170000. running mean: -39.960477\n",
      "ep 217: ep_len:600 episode reward: total was -63.750000. running mean: -40.198372\n",
      "ep 217: ep_len:520 episode reward: total was -45.110000. running mean: -40.247488\n",
      "ep 217: ep_len:615 episode reward: total was -37.120000. running mean: -40.216214\n",
      "ep 217: ep_len:3 episode reward: total was 0.000000. running mean: -39.814051\n",
      "ep 217: ep_len:805 episode reward: total was -111.730000. running mean: -40.533211\n",
      "ep 217: ep_len:635 episode reward: total was -48.910000. running mean: -40.616979\n",
      "epsilon:0.370243 episode_count: 1526. steps_count: 696911.000000\n",
      "ep 218: ep_len:213 episode reward: total was -20.890000. running mean: -40.419709\n",
      "ep 218: ep_len:595 episode reward: total was -77.210000. running mean: -40.787612\n",
      "ep 218: ep_len:615 episode reward: total was -55.160000. running mean: -40.931336\n",
      "ep 218: ep_len:170 episode reward: total was -9.410000. running mean: -40.616122\n",
      "ep 218: ep_len:104 episode reward: total was -14.480000. running mean: -40.354761\n",
      "ep 218: ep_len:515 episode reward: total was -34.300000. running mean: -40.294214\n",
      "ep 218: ep_len:510 episode reward: total was -48.240000. running mean: -40.373671\n",
      "epsilon:0.370106 episode_count: 1533. steps_count: 699633.000000\n",
      "ep 219: ep_len:119 episode reward: total was -11.470000. running mean: -40.084635\n",
      "ep 219: ep_len:194 episode reward: total was -20.440000. running mean: -39.888188\n",
      "ep 219: ep_len:610 episode reward: total was -36.710000. running mean: -39.856407\n",
      "ep 219: ep_len:510 episode reward: total was -44.720000. running mean: -39.905042\n",
      "ep 219: ep_len:3 episode reward: total was 0.000000. running mean: -39.505992\n",
      "ep 219: ep_len:605 episode reward: total was -76.310000. running mean: -39.874032\n",
      "ep 219: ep_len:590 episode reward: total was -42.510000. running mean: -39.900392\n",
      "epsilon:0.369970 episode_count: 1540. steps_count: 702264.000000\n",
      "ep 220: ep_len:211 episode reward: total was -9.470000. running mean: -39.596088\n",
      "ep 220: ep_len:635 episode reward: total was -36.150000. running mean: -39.561627\n",
      "ep 220: ep_len:500 episode reward: total was -42.330000. running mean: -39.589311\n",
      "ep 220: ep_len:500 episode reward: total was -51.260000. running mean: -39.706018\n",
      "ep 220: ep_len:47 episode reward: total was 4.500000. running mean: -39.263957\n",
      "ep 220: ep_len:650 episode reward: total was -72.170000. running mean: -39.593018\n",
      "ep 220: ep_len:510 episode reward: total was -47.760000. running mean: -39.674688\n",
      "epsilon:0.369833 episode_count: 1547. steps_count: 705317.000000\n",
      "ep 221: ep_len:216 episode reward: total was -9.890000. running mean: -39.376841\n",
      "ep 221: ep_len:590 episode reward: total was -37.610000. running mean: -39.359172\n",
      "ep 221: ep_len:399 episode reward: total was -25.310000. running mean: -39.218681\n",
      "ep 221: ep_len:590 episode reward: total was -69.680000. running mean: -39.523294\n",
      "ep 221: ep_len:1 episode reward: total was 0.000000. running mean: -39.128061\n",
      "ep 221: ep_len:183 episode reward: total was -14.960000. running mean: -38.886380\n",
      "ep 221: ep_len:315 episode reward: total was -42.360000. running mean: -38.921117\n",
      "epsilon:0.369697 episode_count: 1554. steps_count: 707611.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 222: ep_len:545 episode reward: total was -47.680000. running mean: -39.008705\n",
      "ep 222: ep_len:500 episode reward: total was -12.420000. running mean: -38.742818\n",
      "ep 222: ep_len:555 episode reward: total was -45.580000. running mean: -38.811190\n",
      "ep 222: ep_len:510 episode reward: total was -33.590000. running mean: -38.758978\n",
      "ep 222: ep_len:3 episode reward: total was 0.000000. running mean: -38.371388\n",
      "ep 222: ep_len:545 episode reward: total was -62.370000. running mean: -38.611375\n",
      "ep 222: ep_len:560 episode reward: total was -39.220000. running mean: -38.617461\n",
      "epsilon:0.369560 episode_count: 1561. steps_count: 710829.000000\n",
      "ep 223: ep_len:109 episode reward: total was -10.970000. running mean: -38.340986\n",
      "ep 223: ep_len:650 episode reward: total was -43.300000. running mean: -38.390576\n",
      "ep 223: ep_len:555 episode reward: total was -58.190000. running mean: -38.588571\n",
      "ep 223: ep_len:394 episode reward: total was -32.860000. running mean: -38.531285\n",
      "ep 223: ep_len:114 episode reward: total was -4.970000. running mean: -38.195672\n",
      "ep 223: ep_len:246 episode reward: total was -24.460000. running mean: -38.058315\n",
      "ep 223: ep_len:625 episode reward: total was -53.640000. running mean: -38.214132\n",
      "epsilon:0.369424 episode_count: 1568. steps_count: 713522.000000\n",
      "ep 224: ep_len:650 episode reward: total was -58.500000. running mean: -38.416991\n",
      "ep 224: ep_len:530 episode reward: total was -54.810000. running mean: -38.580921\n",
      "ep 224: ep_len:420 episode reward: total was -26.890000. running mean: -38.464012\n",
      "ep 224: ep_len:555 episode reward: total was -34.810000. running mean: -38.427472\n",
      "ep 224: ep_len:134 episode reward: total was -8.940000. running mean: -38.132597\n",
      "ep 224: ep_len:615 episode reward: total was -44.190000. running mean: -38.193171\n",
      "ep 224: ep_len:500 episode reward: total was -38.770000. running mean: -38.198939\n",
      "epsilon:0.369287 episode_count: 1575. steps_count: 716926.000000\n",
      "ep 225: ep_len:790 episode reward: total was -102.270000. running mean: -38.839650\n",
      "ep 225: ep_len:530 episode reward: total was -38.180000. running mean: -38.833053\n",
      "ep 225: ep_len:590 episode reward: total was -42.230000. running mean: -38.867023\n",
      "ep 225: ep_len:530 episode reward: total was -30.620000. running mean: -38.784553\n",
      "ep 225: ep_len:130 episode reward: total was -10.950000. running mean: -38.506207\n",
      "ep 225: ep_len:625 episode reward: total was -55.270000. running mean: -38.673845\n",
      "ep 225: ep_len:580 episode reward: total was -55.230000. running mean: -38.839407\n",
      "epsilon:0.369151 episode_count: 1582. steps_count: 720701.000000\n",
      "ep 226: ep_len:600 episode reward: total was -33.230000. running mean: -38.783312\n",
      "ep 226: ep_len:500 episode reward: total was -17.360000. running mean: -38.569079\n",
      "ep 226: ep_len:585 episode reward: total was -46.790000. running mean: -38.651289\n",
      "ep 226: ep_len:610 episode reward: total was -62.050000. running mean: -38.885276\n",
      "ep 226: ep_len:3 episode reward: total was 0.000000. running mean: -38.496423\n",
      "ep 226: ep_len:535 episode reward: total was -45.620000. running mean: -38.567659\n",
      "ep 226: ep_len:311 episode reward: total was -30.410000. running mean: -38.486082\n",
      "epsilon:0.369014 episode_count: 1589. steps_count: 723845.000000\n",
      "ep 227: ep_len:670 episode reward: total was -79.550000. running mean: -38.896721\n",
      "ep 227: ep_len:670 episode reward: total was -67.130000. running mean: -39.179054\n",
      "ep 227: ep_len:545 episode reward: total was -56.140000. running mean: -39.348664\n",
      "ep 227: ep_len:143 episode reward: total was -9.440000. running mean: -39.049577\n",
      "ep 227: ep_len:3 episode reward: total was 0.000000. running mean: -38.659081\n",
      "ep 227: ep_len:550 episode reward: total was -34.680000. running mean: -38.619290\n",
      "ep 227: ep_len:605 episode reward: total was -43.160000. running mean: -38.664697\n",
      "epsilon:0.368878 episode_count: 1596. steps_count: 727031.000000\n",
      "ep 228: ep_len:570 episode reward: total was -32.210000. running mean: -38.600150\n",
      "ep 228: ep_len:500 episode reward: total was -45.210000. running mean: -38.666249\n",
      "ep 228: ep_len:635 episode reward: total was -73.620000. running mean: -39.015786\n",
      "ep 228: ep_len:545 episode reward: total was -37.830000. running mean: -39.003929\n",
      "ep 228: ep_len:3 episode reward: total was 0.000000. running mean: -38.613889\n",
      "ep 228: ep_len:525 episode reward: total was -47.420000. running mean: -38.701950\n",
      "ep 228: ep_len:182 episode reward: total was -13.960000. running mean: -38.454531\n",
      "epsilon:0.368741 episode_count: 1603. steps_count: 729991.000000\n",
      "ep 229: ep_len:610 episode reward: total was -43.060000. running mean: -38.500586\n",
      "ep 229: ep_len:500 episode reward: total was -60.300000. running mean: -38.718580\n",
      "ep 229: ep_len:660 episode reward: total was -50.650000. running mean: -38.837894\n",
      "ep 229: ep_len:392 episode reward: total was -28.800000. running mean: -38.737515\n",
      "ep 229: ep_len:3 episode reward: total was 0.000000. running mean: -38.350140\n",
      "ep 229: ep_len:685 episode reward: total was -56.550000. running mean: -38.532138\n",
      "ep 229: ep_len:640 episode reward: total was -41.400000. running mean: -38.560817\n",
      "epsilon:0.368605 episode_count: 1610. steps_count: 733481.000000\n",
      "ep 230: ep_len:665 episode reward: total was -72.070000. running mean: -38.895909\n",
      "ep 230: ep_len:635 episode reward: total was -53.710000. running mean: -39.044050\n",
      "ep 230: ep_len:750 episode reward: total was -76.010000. running mean: -39.413709\n",
      "ep 230: ep_len:505 episode reward: total was -35.300000. running mean: -39.372572\n",
      "ep 230: ep_len:3 episode reward: total was 0.000000. running mean: -38.978846\n",
      "ep 230: ep_len:505 episode reward: total was -29.310000. running mean: -38.882158\n",
      "ep 230: ep_len:209 episode reward: total was -22.920000. running mean: -38.722536\n",
      "epsilon:0.368468 episode_count: 1617. steps_count: 736753.000000\n",
      "ep 231: ep_len:670 episode reward: total was -52.690000. running mean: -38.862211\n",
      "ep 231: ep_len:535 episode reward: total was -26.270000. running mean: -38.736289\n",
      "ep 231: ep_len:585 episode reward: total was -47.050000. running mean: -38.819426\n",
      "ep 231: ep_len:151 episode reward: total was -18.450000. running mean: -38.615732\n",
      "ep 231: ep_len:106 episode reward: total was -11.470000. running mean: -38.344274\n",
      "ep 231: ep_len:685 episode reward: total was -60.530000. running mean: -38.566132\n",
      "ep 231: ep_len:620 episode reward: total was -76.730000. running mean: -38.947770\n",
      "epsilon:0.368332 episode_count: 1624. steps_count: 740105.000000\n",
      "ep 232: ep_len:570 episode reward: total was -36.790000. running mean: -38.926193\n",
      "ep 232: ep_len:500 episode reward: total was -35.750000. running mean: -38.894431\n",
      "ep 232: ep_len:550 episode reward: total was -34.790000. running mean: -38.853386\n",
      "ep 232: ep_len:56 episode reward: total was -3.960000. running mean: -38.504453\n",
      "ep 232: ep_len:86 episode reward: total was -13.480000. running mean: -38.254208\n",
      "ep 232: ep_len:515 episode reward: total was -42.840000. running mean: -38.300066\n",
      "ep 232: ep_len:615 episode reward: total was -36.570000. running mean: -38.282765\n",
      "epsilon:0.368195 episode_count: 1631. steps_count: 742997.000000\n",
      "ep 233: ep_len:565 episode reward: total was -55.610000. running mean: -38.456038\n",
      "ep 233: ep_len:565 episode reward: total was -52.170000. running mean: -38.593177\n",
      "ep 233: ep_len:570 episode reward: total was -41.030000. running mean: -38.617546\n",
      "ep 233: ep_len:500 episode reward: total was -39.140000. running mean: -38.622770\n",
      "ep 233: ep_len:85 episode reward: total was -10.970000. running mean: -38.346242\n",
      "ep 233: ep_len:505 episode reward: total was -44.750000. running mean: -38.410280\n",
      "ep 233: ep_len:565 episode reward: total was -64.150000. running mean: -38.667677\n",
      "epsilon:0.368059 episode_count: 1638. steps_count: 746352.000000\n",
      "ep 234: ep_len:630 episode reward: total was -33.970000. running mean: -38.620700\n",
      "ep 234: ep_len:550 episode reward: total was -46.580000. running mean: -38.700293\n",
      "ep 234: ep_len:645 episode reward: total was -61.040000. running mean: -38.923690\n",
      "ep 234: ep_len:550 episode reward: total was -40.720000. running mean: -38.941654\n",
      "ep 234: ep_len:3 episode reward: total was 0.000000. running mean: -38.552237\n",
      "ep 234: ep_len:565 episode reward: total was -34.730000. running mean: -38.514015\n",
      "ep 234: ep_len:190 episode reward: total was -29.970000. running mean: -38.428574\n",
      "epsilon:0.367922 episode_count: 1645. steps_count: 749485.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 235: ep_len:950 episode reward: total was -107.490000. running mean: -39.119189\n",
      "ep 235: ep_len:515 episode reward: total was -38.260000. running mean: -39.110597\n",
      "ep 235: ep_len:584 episode reward: total was -66.690000. running mean: -39.386391\n",
      "ep 235: ep_len:565 episode reward: total was -44.270000. running mean: -39.435227\n",
      "ep 235: ep_len:88 episode reward: total was 0.520000. running mean: -39.035675\n",
      "ep 235: ep_len:685 episode reward: total was -105.850000. running mean: -39.703818\n",
      "ep 235: ep_len:520 episode reward: total was -52.090000. running mean: -39.827680\n",
      "epsilon:0.367786 episode_count: 1652. steps_count: 753392.000000\n",
      "ep 236: ep_len:124 episode reward: total was -11.450000. running mean: -39.543903\n",
      "ep 236: ep_len:327 episode reward: total was -43.370000. running mean: -39.582164\n",
      "ep 236: ep_len:64 episode reward: total was -1.980000. running mean: -39.206142\n",
      "ep 236: ep_len:500 episode reward: total was -37.620000. running mean: -39.190281\n",
      "ep 236: ep_len:79 episode reward: total was 0.530000. running mean: -38.793078\n",
      "ep 236: ep_len:595 episode reward: total was -62.770000. running mean: -39.032847\n",
      "ep 236: ep_len:505 episode reward: total was -39.300000. running mean: -39.035519\n",
      "epsilon:0.367649 episode_count: 1659. steps_count: 755586.000000\n",
      "ep 237: ep_len:530 episode reward: total was -43.760000. running mean: -39.082764\n",
      "ep 237: ep_len:289 episode reward: total was -22.380000. running mean: -38.915736\n",
      "ep 237: ep_len:625 episode reward: total was -57.680000. running mean: -39.103379\n",
      "ep 237: ep_len:540 episode reward: total was -54.210000. running mean: -39.254445\n",
      "ep 237: ep_len:3 episode reward: total was 0.000000. running mean: -38.861900\n",
      "ep 237: ep_len:500 episode reward: total was -26.740000. running mean: -38.740681\n",
      "ep 237: ep_len:291 episode reward: total was -35.400000. running mean: -38.707275\n",
      "epsilon:0.367513 episode_count: 1666. steps_count: 758364.000000\n",
      "ep 238: ep_len:640 episode reward: total was -75.620000. running mean: -39.076402\n",
      "ep 238: ep_len:369 episode reward: total was -50.390000. running mean: -39.189538\n",
      "ep 238: ep_len:438 episode reward: total was -29.830000. running mean: -39.095942\n",
      "ep 238: ep_len:500 episode reward: total was -57.790000. running mean: -39.282883\n",
      "ep 238: ep_len:3 episode reward: total was 0.000000. running mean: -38.890054\n",
      "ep 238: ep_len:565 episode reward: total was -57.170000. running mean: -39.072854\n",
      "ep 238: ep_len:580 episode reward: total was -31.670000. running mean: -38.998825\n",
      "epsilon:0.367376 episode_count: 1673. steps_count: 761459.000000\n",
      "ep 239: ep_len:530 episode reward: total was -41.020000. running mean: -39.019037\n",
      "ep 239: ep_len:500 episode reward: total was -36.370000. running mean: -38.992547\n",
      "ep 239: ep_len:565 episode reward: total was -55.220000. running mean: -39.154821\n",
      "ep 239: ep_len:640 episode reward: total was -56.200000. running mean: -39.325273\n",
      "ep 239: ep_len:3 episode reward: total was 0.000000. running mean: -38.932020\n",
      "ep 239: ep_len:660 episode reward: total was -49.260000. running mean: -39.035300\n",
      "ep 239: ep_len:510 episode reward: total was -50.710000. running mean: -39.152047\n",
      "epsilon:0.367240 episode_count: 1680. steps_count: 764867.000000\n",
      "ep 240: ep_len:255 episode reward: total was -23.880000. running mean: -38.999326\n",
      "ep 240: ep_len:505 episode reward: total was -36.350000. running mean: -38.972833\n",
      "ep 240: ep_len:79 episode reward: total was -5.470000. running mean: -38.637805\n",
      "ep 240: ep_len:515 episode reward: total was -69.890000. running mean: -38.950327\n",
      "ep 240: ep_len:84 episode reward: total was -2.460000. running mean: -38.585424\n",
      "ep 240: ep_len:625 episode reward: total was -72.130000. running mean: -38.920869\n",
      "ep 240: ep_len:575 episode reward: total was -48.740000. running mean: -39.019061\n",
      "epsilon:0.367103 episode_count: 1687. steps_count: 767505.000000\n",
      "ep 241: ep_len:600 episode reward: total was -32.190000. running mean: -38.950770\n",
      "ep 241: ep_len:610 episode reward: total was -52.270000. running mean: -39.083962\n",
      "ep 241: ep_len:575 episode reward: total was -66.630000. running mean: -39.359423\n",
      "ep 241: ep_len:500 episode reward: total was -43.220000. running mean: -39.398028\n",
      "ep 241: ep_len:3 episode reward: total was 0.000000. running mean: -39.004048\n",
      "ep 241: ep_len:505 episode reward: total was -32.810000. running mean: -38.942108\n",
      "ep 241: ep_len:565 episode reward: total was -54.650000. running mean: -39.099187\n",
      "epsilon:0.366967 episode_count: 1694. steps_count: 770863.000000\n",
      "ep 242: ep_len:660 episode reward: total was -62.050000. running mean: -39.328695\n",
      "ep 242: ep_len:585 episode reward: total was -46.520000. running mean: -39.400608\n",
      "ep 242: ep_len:590 episode reward: total was -43.640000. running mean: -39.443002\n",
      "ep 242: ep_len:392 episode reward: total was -26.280000. running mean: -39.311372\n",
      "ep 242: ep_len:3 episode reward: total was 0.000000. running mean: -38.918258\n",
      "ep 242: ep_len:515 episode reward: total was -43.760000. running mean: -38.966675\n",
      "ep 242: ep_len:500 episode reward: total was -58.350000. running mean: -39.160509\n",
      "epsilon:0.366830 episode_count: 1701. steps_count: 774108.000000\n",
      "ep 243: ep_len:112 episode reward: total was -12.960000. running mean: -38.898504\n",
      "ep 243: ep_len:575 episode reward: total was -57.640000. running mean: -39.085919\n",
      "ep 243: ep_len:740 episode reward: total was -84.410000. running mean: -39.539159\n",
      "ep 243: ep_len:145 episode reward: total was -10.410000. running mean: -39.247868\n",
      "ep 243: ep_len:3 episode reward: total was 0.000000. running mean: -38.855389\n",
      "ep 243: ep_len:645 episode reward: total was -51.560000. running mean: -38.982435\n",
      "ep 243: ep_len:191 episode reward: total was -17.900000. running mean: -38.771611\n",
      "epsilon:0.366694 episode_count: 1708. steps_count: 776519.000000\n",
      "ep 244: ep_len:635 episode reward: total was -61.060000. running mean: -38.994495\n",
      "ep 244: ep_len:500 episode reward: total was -45.240000. running mean: -39.056950\n",
      "ep 244: ep_len:605 episode reward: total was -43.930000. running mean: -39.105680\n",
      "ep 244: ep_len:530 episode reward: total was -54.180000. running mean: -39.256423\n",
      "ep 244: ep_len:3 episode reward: total was 0.000000. running mean: -38.863859\n",
      "ep 244: ep_len:510 episode reward: total was -28.730000. running mean: -38.762521\n",
      "ep 244: ep_len:620 episode reward: total was -64.230000. running mean: -39.017195\n",
      "epsilon:0.366557 episode_count: 1715. steps_count: 779922.000000\n",
      "ep 245: ep_len:590 episode reward: total was -45.600000. running mean: -39.083023\n",
      "ep 245: ep_len:655 episode reward: total was -64.740000. running mean: -39.339593\n",
      "ep 245: ep_len:600 episode reward: total was -69.050000. running mean: -39.636697\n",
      "ep 245: ep_len:560 episode reward: total was -59.230000. running mean: -39.832630\n",
      "ep 245: ep_len:85 episode reward: total was -6.000000. running mean: -39.494304\n",
      "ep 245: ep_len:595 episode reward: total was -51.310000. running mean: -39.612461\n",
      "ep 245: ep_len:580 episode reward: total was -49.180000. running mean: -39.708136\n",
      "epsilon:0.366421 episode_count: 1722. steps_count: 783587.000000\n",
      "ep 246: ep_len:198 episode reward: total was -13.910000. running mean: -39.450155\n",
      "ep 246: ep_len:738 episode reward: total was -87.030000. running mean: -39.925953\n",
      "ep 246: ep_len:525 episode reward: total was -31.750000. running mean: -39.844194\n",
      "ep 246: ep_len:500 episode reward: total was -39.200000. running mean: -39.837752\n",
      "ep 246: ep_len:3 episode reward: total was 0.000000. running mean: -39.439374\n",
      "ep 246: ep_len:261 episode reward: total was -23.900000. running mean: -39.283981\n",
      "ep 246: ep_len:520 episode reward: total was -64.890000. running mean: -39.540041\n",
      "epsilon:0.366284 episode_count: 1729. steps_count: 786332.000000\n",
      "ep 247: ep_len:500 episode reward: total was -34.860000. running mean: -39.493241\n",
      "ep 247: ep_len:575 episode reward: total was -45.260000. running mean: -39.550908\n",
      "ep 247: ep_len:550 episode reward: total was -37.920000. running mean: -39.534599\n",
      "ep 247: ep_len:385 episode reward: total was -21.210000. running mean: -39.351353\n",
      "ep 247: ep_len:53 episode reward: total was 3.500000. running mean: -38.922840\n",
      "ep 247: ep_len:525 episode reward: total was -38.700000. running mean: -38.920611\n",
      "ep 247: ep_len:620 episode reward: total was -47.200000. running mean: -39.003405\n",
      "epsilon:0.366148 episode_count: 1736. steps_count: 789540.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 248: ep_len:675 episode reward: total was -67.590000. running mean: -39.289271\n",
      "ep 248: ep_len:200 episode reward: total was -13.930000. running mean: -39.035678\n",
      "ep 248: ep_len:840 episode reward: total was -109.660000. running mean: -39.741921\n",
      "ep 248: ep_len:500 episode reward: total was -37.310000. running mean: -39.717602\n",
      "ep 248: ep_len:3 episode reward: total was 0.000000. running mean: -39.320426\n",
      "ep 248: ep_len:530 episode reward: total was -43.270000. running mean: -39.359922\n",
      "ep 248: ep_len:590 episode reward: total was -45.040000. running mean: -39.416723\n",
      "epsilon:0.366011 episode_count: 1743. steps_count: 792878.000000\n",
      "ep 249: ep_len:570 episode reward: total was -71.260000. running mean: -39.735156\n",
      "ep 249: ep_len:600 episode reward: total was -35.750000. running mean: -39.695304\n",
      "ep 249: ep_len:56 episode reward: total was -1.990000. running mean: -39.318251\n",
      "ep 249: ep_len:520 episode reward: total was -43.120000. running mean: -39.356268\n",
      "ep 249: ep_len:95 episode reward: total was -3.990000. running mean: -39.002606\n",
      "ep 249: ep_len:575 episode reward: total was -49.810000. running mean: -39.110680\n",
      "ep 249: ep_len:279 episode reward: total was -28.930000. running mean: -39.008873\n",
      "epsilon:0.365875 episode_count: 1750. steps_count: 795573.000000\n",
      "ep 250: ep_len:815 episode reward: total was -91.610000. running mean: -39.534884\n",
      "ep 250: ep_len:500 episode reward: total was -40.810000. running mean: -39.547635\n",
      "ep 250: ep_len:600 episode reward: total was -44.280000. running mean: -39.594959\n",
      "ep 250: ep_len:423 episode reward: total was -23.250000. running mean: -39.431509\n",
      "ep 250: ep_len:3 episode reward: total was 0.000000. running mean: -39.037194\n",
      "ep 250: ep_len:500 episode reward: total was -31.110000. running mean: -38.957922\n",
      "ep 250: ep_len:525 episode reward: total was -47.640000. running mean: -39.044743\n",
      "epsilon:0.365738 episode_count: 1757. steps_count: 798939.000000\n",
      "ep 251: ep_len:500 episode reward: total was -52.860000. running mean: -39.182896\n",
      "ep 251: ep_len:282 episode reward: total was -20.890000. running mean: -38.999967\n",
      "ep 251: ep_len:555 episode reward: total was -38.270000. running mean: -38.992667\n",
      "ep 251: ep_len:520 episode reward: total was -34.820000. running mean: -38.950940\n",
      "ep 251: ep_len:87 episode reward: total was -12.960000. running mean: -38.691031\n",
      "ep 251: ep_len:595 episode reward: total was -47.710000. running mean: -38.781221\n",
      "ep 251: ep_len:615 episode reward: total was -60.150000. running mean: -38.994908\n",
      "epsilon:0.365602 episode_count: 1764. steps_count: 802093.000000\n",
      "ep 252: ep_len:123 episode reward: total was -9.960000. running mean: -38.704559\n",
      "ep 252: ep_len:500 episode reward: total was -46.270000. running mean: -38.780214\n",
      "ep 252: ep_len:61 episode reward: total was -7.980000. running mean: -38.472212\n",
      "ep 252: ep_len:700 episode reward: total was -64.050000. running mean: -38.727990\n",
      "ep 252: ep_len:50 episode reward: total was -7.490000. running mean: -38.415610\n",
      "ep 252: ep_len:530 episode reward: total was -54.670000. running mean: -38.578154\n",
      "ep 252: ep_len:545 episode reward: total was -49.090000. running mean: -38.683272\n",
      "epsilon:0.365465 episode_count: 1771. steps_count: 804602.000000\n",
      "ep 253: ep_len:960 episode reward: total was -176.930000. running mean: -40.065739\n",
      "ep 253: ep_len:545 episode reward: total was -37.700000. running mean: -40.042082\n",
      "ep 253: ep_len:645 episode reward: total was -63.810000. running mean: -40.279761\n",
      "ep 253: ep_len:555 episode reward: total was -14.670000. running mean: -40.023663\n",
      "ep 253: ep_len:84 episode reward: total was -0.960000. running mean: -39.633027\n",
      "ep 253: ep_len:175 episode reward: total was -5.450000. running mean: -39.291197\n",
      "ep 253: ep_len:585 episode reward: total was -61.280000. running mean: -39.511085\n",
      "epsilon:0.365329 episode_count: 1778. steps_count: 808151.000000\n",
      "ep 254: ep_len:565 episode reward: total was -68.660000. running mean: -39.802574\n",
      "ep 254: ep_len:500 episode reward: total was -51.190000. running mean: -39.916448\n",
      "ep 254: ep_len:500 episode reward: total was -54.890000. running mean: -40.066184\n",
      "ep 254: ep_len:565 episode reward: total was -40.680000. running mean: -40.072322\n",
      "ep 254: ep_len:38 episode reward: total was -2.500000. running mean: -39.696598\n",
      "ep 254: ep_len:555 episode reward: total was -41.170000. running mean: -39.711332\n",
      "ep 254: ep_len:308 episode reward: total was -32.960000. running mean: -39.643819\n",
      "epsilon:0.365192 episode_count: 1785. steps_count: 811182.000000\n",
      "ep 255: ep_len:620 episode reward: total was -81.730000. running mean: -40.064681\n",
      "ep 255: ep_len:505 episode reward: total was -53.270000. running mean: -40.196734\n",
      "ep 255: ep_len:510 episode reward: total was -40.160000. running mean: -40.196367\n",
      "ep 255: ep_len:500 episode reward: total was -47.220000. running mean: -40.266603\n",
      "ep 255: ep_len:3 episode reward: total was 0.000000. running mean: -39.863937\n",
      "ep 255: ep_len:630 episode reward: total was -45.990000. running mean: -39.925198\n",
      "ep 255: ep_len:515 episode reward: total was -37.070000. running mean: -39.896646\n",
      "epsilon:0.365056 episode_count: 1792. steps_count: 814465.000000\n",
      "ep 256: ep_len:665 episode reward: total was -66.080000. running mean: -40.158479\n",
      "ep 256: ep_len:605 episode reward: total was -28.300000. running mean: -40.039895\n",
      "ep 256: ep_len:53 episode reward: total was -1.970000. running mean: -39.659196\n",
      "ep 256: ep_len:620 episode reward: total was -73.180000. running mean: -39.994404\n",
      "ep 256: ep_len:3 episode reward: total was 0.000000. running mean: -39.594460\n",
      "ep 256: ep_len:143 episode reward: total was -7.470000. running mean: -39.273215\n",
      "ep 256: ep_len:595 episode reward: total was -57.270000. running mean: -39.453183\n",
      "epsilon:0.364919 episode_count: 1799. steps_count: 817149.000000\n",
      "ep 257: ep_len:505 episode reward: total was -44.210000. running mean: -39.500751\n",
      "ep 257: ep_len:276 episode reward: total was -25.970000. running mean: -39.365443\n",
      "ep 257: ep_len:600 episode reward: total was -67.140000. running mean: -39.643189\n",
      "ep 257: ep_len:505 episode reward: total was -18.090000. running mean: -39.427657\n",
      "ep 257: ep_len:80 episode reward: total was -6.960000. running mean: -39.102981\n",
      "ep 257: ep_len:535 episode reward: total was -62.760000. running mean: -39.339551\n",
      "ep 257: ep_len:206 episode reward: total was -19.460000. running mean: -39.140755\n",
      "epsilon:0.364783 episode_count: 1806. steps_count: 819856.000000\n",
      "ep 258: ep_len:550 episode reward: total was -55.180000. running mean: -39.301148\n",
      "ep 258: ep_len:550 episode reward: total was -67.270000. running mean: -39.580836\n",
      "ep 258: ep_len:540 episode reward: total was -44.790000. running mean: -39.632928\n",
      "ep 258: ep_len:575 episode reward: total was -20.740000. running mean: -39.443999\n",
      "ep 258: ep_len:36 episode reward: total was -5.500000. running mean: -39.104559\n",
      "ep 258: ep_len:635 episode reward: total was -67.770000. running mean: -39.391213\n",
      "ep 258: ep_len:500 episode reward: total was -44.740000. running mean: -39.444701\n",
      "epsilon:0.364646 episode_count: 1813. steps_count: 823242.000000\n",
      "ep 259: ep_len:615 episode reward: total was -39.730000. running mean: -39.447554\n",
      "ep 259: ep_len:555 episode reward: total was -44.710000. running mean: -39.500178\n",
      "ep 259: ep_len:600 episode reward: total was -69.160000. running mean: -39.796777\n",
      "ep 259: ep_len:170 episode reward: total was -10.930000. running mean: -39.508109\n",
      "ep 259: ep_len:3 episode reward: total was 0.000000. running mean: -39.113028\n",
      "ep 259: ep_len:550 episode reward: total was -31.650000. running mean: -39.038397\n",
      "ep 259: ep_len:580 episode reward: total was -62.170000. running mean: -39.269713\n",
      "epsilon:0.364510 episode_count: 1820. steps_count: 826315.000000\n",
      "ep 260: ep_len:510 episode reward: total was -32.700000. running mean: -39.204016\n",
      "ep 260: ep_len:500 episode reward: total was -61.870000. running mean: -39.430676\n",
      "ep 260: ep_len:575 episode reward: total was -40.750000. running mean: -39.443869\n",
      "ep 260: ep_len:505 episode reward: total was -33.760000. running mean: -39.387031\n",
      "ep 260: ep_len:95 episode reward: total was -14.470000. running mean: -39.137860\n",
      "ep 260: ep_len:237 episode reward: total was -8.410000. running mean: -38.830582\n",
      "ep 260: ep_len:322 episode reward: total was -29.400000. running mean: -38.736276\n",
      "epsilon:0.364373 episode_count: 1827. steps_count: 829059.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 261: ep_len:565 episode reward: total was -52.170000. running mean: -38.870613\n",
      "ep 261: ep_len:175 episode reward: total was -23.990000. running mean: -38.721807\n",
      "ep 261: ep_len:570 episode reward: total was -62.150000. running mean: -38.956089\n",
      "ep 261: ep_len:735 episode reward: total was -88.130000. running mean: -39.447828\n",
      "ep 261: ep_len:3 episode reward: total was 0.000000. running mean: -39.053350\n",
      "ep 261: ep_len:500 episode reward: total was -33.200000. running mean: -38.994816\n",
      "ep 261: ep_len:520 episode reward: total was -27.450000. running mean: -38.879368\n",
      "epsilon:0.364237 episode_count: 1834. steps_count: 832127.000000\n",
      "ep 262: ep_len:610 episode reward: total was -36.200000. running mean: -38.852575\n",
      "ep 262: ep_len:540 episode reward: total was -32.370000. running mean: -38.787749\n",
      "ep 262: ep_len:505 episode reward: total was -38.200000. running mean: -38.781871\n",
      "ep 262: ep_len:610 episode reward: total was -40.210000. running mean: -38.796153\n",
      "ep 262: ep_len:3 episode reward: total was 0.000000. running mean: -38.408191\n",
      "ep 262: ep_len:640 episode reward: total was -43.510000. running mean: -38.459209\n",
      "ep 262: ep_len:306 episode reward: total was -26.400000. running mean: -38.338617\n",
      "epsilon:0.364100 episode_count: 1841. steps_count: 835341.000000\n",
      "ep 263: ep_len:610 episode reward: total was -71.320000. running mean: -38.668431\n",
      "ep 263: ep_len:575 episode reward: total was -50.800000. running mean: -38.789747\n",
      "ep 263: ep_len:500 episode reward: total was -49.740000. running mean: -38.899249\n",
      "ep 263: ep_len:515 episode reward: total was -19.640000. running mean: -38.706657\n",
      "ep 263: ep_len:3 episode reward: total was 0.000000. running mean: -38.319590\n",
      "ep 263: ep_len:157 episode reward: total was -5.420000. running mean: -37.990594\n",
      "ep 263: ep_len:505 episode reward: total was -35.760000. running mean: -37.968288\n",
      "epsilon:0.363964 episode_count: 1848. steps_count: 838206.000000\n",
      "ep 264: ep_len:694 episode reward: total was -75.990000. running mean: -38.348505\n",
      "ep 264: ep_len:142 episode reward: total was -23.980000. running mean: -38.204820\n",
      "ep 264: ep_len:446 episode reward: total was -42.890000. running mean: -38.251672\n",
      "ep 264: ep_len:131 episode reward: total was 0.590000. running mean: -37.863255\n",
      "ep 264: ep_len:3 episode reward: total was 0.000000. running mean: -37.484623\n",
      "ep 264: ep_len:515 episode reward: total was -41.590000. running mean: -37.525677\n",
      "ep 264: ep_len:505 episode reward: total was -38.030000. running mean: -37.530720\n",
      "epsilon:0.363827 episode_count: 1855. steps_count: 840642.000000\n",
      "ep 265: ep_len:565 episode reward: total was -52.520000. running mean: -37.680613\n",
      "ep 265: ep_len:515 episode reward: total was -43.270000. running mean: -37.736506\n",
      "ep 265: ep_len:710 episode reward: total was -40.920000. running mean: -37.768341\n",
      "ep 265: ep_len:500 episode reward: total was -44.730000. running mean: -37.837958\n",
      "ep 265: ep_len:3 episode reward: total was 0.000000. running mean: -37.459578\n",
      "ep 265: ep_len:635 episode reward: total was -64.990000. running mean: -37.734883\n",
      "ep 265: ep_len:500 episode reward: total was -32.450000. running mean: -37.682034\n",
      "epsilon:0.363691 episode_count: 1862. steps_count: 844070.000000\n",
      "ep 266: ep_len:575 episode reward: total was -43.310000. running mean: -37.738313\n",
      "ep 266: ep_len:505 episode reward: total was -25.380000. running mean: -37.614730\n",
      "ep 266: ep_len:665 episode reward: total was -59.630000. running mean: -37.834883\n",
      "ep 266: ep_len:383 episode reward: total was -26.250000. running mean: -37.719034\n",
      "ep 266: ep_len:3 episode reward: total was 0.000000. running mean: -37.341844\n",
      "ep 266: ep_len:575 episode reward: total was -48.980000. running mean: -37.458225\n",
      "ep 266: ep_len:540 episode reward: total was -31.180000. running mean: -37.395443\n",
      "epsilon:0.363554 episode_count: 1869. steps_count: 847316.000000\n",
      "ep 267: ep_len:505 episode reward: total was -43.110000. running mean: -37.452589\n",
      "ep 267: ep_len:505 episode reward: total was -44.240000. running mean: -37.520463\n",
      "ep 267: ep_len:655 episode reward: total was -45.630000. running mean: -37.601558\n",
      "ep 267: ep_len:510 episode reward: total was -39.730000. running mean: -37.622843\n",
      "ep 267: ep_len:49 episode reward: total was 1.500000. running mean: -37.231614\n",
      "ep 267: ep_len:605 episode reward: total was -53.250000. running mean: -37.391798\n",
      "ep 267: ep_len:565 episode reward: total was -47.610000. running mean: -37.493980\n",
      "epsilon:0.363418 episode_count: 1876. steps_count: 850710.000000\n",
      "ep 268: ep_len:205 episode reward: total was -12.420000. running mean: -37.243240\n",
      "ep 268: ep_len:500 episode reward: total was -20.430000. running mean: -37.075108\n",
      "ep 268: ep_len:695 episode reward: total was -50.440000. running mean: -37.208757\n",
      "ep 268: ep_len:375 episode reward: total was -38.750000. running mean: -37.224169\n",
      "ep 268: ep_len:3 episode reward: total was 0.000000. running mean: -36.851928\n",
      "ep 268: ep_len:288 episode reward: total was -18.460000. running mean: -36.668008\n",
      "ep 268: ep_len:297 episode reward: total was -15.910000. running mean: -36.460428\n",
      "epsilon:0.363281 episode_count: 1883. steps_count: 853073.000000\n",
      "ep 269: ep_len:206 episode reward: total was -22.950000. running mean: -36.325324\n",
      "ep 269: ep_len:540 episode reward: total was -59.670000. running mean: -36.558771\n",
      "ep 269: ep_len:590 episode reward: total was -51.670000. running mean: -36.709883\n",
      "ep 269: ep_len:500 episode reward: total was -19.720000. running mean: -36.539984\n",
      "ep 269: ep_len:3 episode reward: total was 0.000000. running mean: -36.174584\n",
      "ep 269: ep_len:500 episode reward: total was -45.860000. running mean: -36.271438\n",
      "ep 269: ep_len:500 episode reward: total was -36.520000. running mean: -36.273924\n",
      "epsilon:0.363145 episode_count: 1890. steps_count: 855912.000000\n",
      "ep 270: ep_len:650 episode reward: total was -104.850000. running mean: -36.959685\n",
      "ep 270: ep_len:660 episode reward: total was -33.640000. running mean: -36.926488\n",
      "ep 270: ep_len:505 episode reward: total was -32.210000. running mean: -36.879323\n",
      "ep 270: ep_len:530 episode reward: total was -39.610000. running mean: -36.906630\n",
      "ep 270: ep_len:95 episode reward: total was -7.480000. running mean: -36.612364\n",
      "ep 270: ep_len:505 episode reward: total was -54.780000. running mean: -36.794040\n",
      "ep 270: ep_len:530 episode reward: total was -54.290000. running mean: -36.969000\n",
      "epsilon:0.363008 episode_count: 1897. steps_count: 859387.000000\n",
      "ep 271: ep_len:129 episode reward: total was -4.440000. running mean: -36.643710\n",
      "ep 271: ep_len:645 episode reward: total was -30.640000. running mean: -36.583672\n",
      "ep 271: ep_len:575 episode reward: total was -41.990000. running mean: -36.637736\n",
      "ep 271: ep_len:500 episode reward: total was -28.300000. running mean: -36.554358\n",
      "ep 271: ep_len:100 episode reward: total was -14.480000. running mean: -36.333615\n",
      "ep 271: ep_len:610 episode reward: total was -37.450000. running mean: -36.344779\n",
      "ep 271: ep_len:560 episode reward: total was -44.320000. running mean: -36.424531\n",
      "epsilon:0.362872 episode_count: 1904. steps_count: 862506.000000\n",
      "ep 272: ep_len:500 episode reward: total was -61.820000. running mean: -36.678486\n",
      "ep 272: ep_len:179 episode reward: total was -25.930000. running mean: -36.571001\n",
      "ep 272: ep_len:640 episode reward: total was -41.220000. running mean: -36.617491\n",
      "ep 272: ep_len:520 episode reward: total was -48.330000. running mean: -36.734616\n",
      "ep 272: ep_len:51 episode reward: total was -5.500000. running mean: -36.422270\n",
      "ep 272: ep_len:640 episode reward: total was -53.030000. running mean: -36.588347\n",
      "ep 272: ep_len:500 episode reward: total was -44.870000. running mean: -36.671163\n",
      "epsilon:0.362735 episode_count: 1911. steps_count: 865536.000000\n",
      "ep 273: ep_len:211 episode reward: total was -7.410000. running mean: -36.378552\n",
      "ep 273: ep_len:277 episode reward: total was -23.920000. running mean: -36.253966\n",
      "ep 273: ep_len:411 episode reward: total was -35.930000. running mean: -36.250727\n",
      "ep 273: ep_len:515 episode reward: total was -55.790000. running mean: -36.446119\n",
      "ep 273: ep_len:3 episode reward: total was 0.000000. running mean: -36.081658\n",
      "ep 273: ep_len:169 episode reward: total was -3.940000. running mean: -35.760242\n",
      "ep 273: ep_len:515 episode reward: total was -37.150000. running mean: -35.774139\n",
      "epsilon:0.362599 episode_count: 1918. steps_count: 867637.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 274: ep_len:665 episode reward: total was -54.680000. running mean: -35.963198\n",
      "ep 274: ep_len:1105 episode reward: total was -131.280000. running mean: -36.916366\n",
      "ep 274: ep_len:545 episode reward: total was -41.120000. running mean: -36.958402\n",
      "ep 274: ep_len:510 episode reward: total was -35.820000. running mean: -36.947018\n",
      "ep 274: ep_len:95 episode reward: total was 0.540000. running mean: -36.572148\n",
      "ep 274: ep_len:510 episode reward: total was -43.900000. running mean: -36.645426\n",
      "ep 274: ep_len:267 episode reward: total was -32.410000. running mean: -36.603072\n",
      "epsilon:0.362462 episode_count: 1925. steps_count: 871334.000000\n",
      "ep 275: ep_len:540 episode reward: total was -49.650000. running mean: -36.733541\n",
      "ep 275: ep_len:600 episode reward: total was -65.730000. running mean: -37.023506\n",
      "ep 275: ep_len:530 episode reward: total was -91.880000. running mean: -37.572071\n",
      "ep 275: ep_len:535 episode reward: total was -28.180000. running mean: -37.478150\n",
      "ep 275: ep_len:3 episode reward: total was 0.000000. running mean: -37.103369\n",
      "ep 275: ep_len:500 episode reward: total was -38.880000. running mean: -37.121135\n",
      "ep 275: ep_len:210 episode reward: total was -20.430000. running mean: -36.954224\n",
      "epsilon:0.362326 episode_count: 1932. steps_count: 874252.000000\n",
      "ep 276: ep_len:228 episode reward: total was -19.920000. running mean: -36.783881\n",
      "ep 276: ep_len:500 episode reward: total was -51.150000. running mean: -36.927543\n",
      "ep 276: ep_len:462 episode reward: total was -38.860000. running mean: -36.946867\n",
      "ep 276: ep_len:505 episode reward: total was -60.310000. running mean: -37.180499\n",
      "ep 276: ep_len:125 episode reward: total was -6.960000. running mean: -36.878294\n",
      "ep 276: ep_len:500 episode reward: total was -48.870000. running mean: -36.998211\n",
      "ep 276: ep_len:211 episode reward: total was -19.940000. running mean: -36.827629\n",
      "epsilon:0.362189 episode_count: 1939. steps_count: 876783.000000\n",
      "ep 277: ep_len:500 episode reward: total was -32.830000. running mean: -36.787652\n",
      "ep 277: ep_len:515 episode reward: total was -45.680000. running mean: -36.876576\n",
      "ep 277: ep_len:565 episode reward: total was -74.240000. running mean: -37.250210\n",
      "ep 277: ep_len:535 episode reward: total was -11.120000. running mean: -36.988908\n",
      "ep 277: ep_len:3 episode reward: total was 0.000000. running mean: -36.619019\n",
      "ep 277: ep_len:510 episode reward: total was -40.130000. running mean: -36.654129\n",
      "ep 277: ep_len:205 episode reward: total was -19.890000. running mean: -36.486487\n",
      "epsilon:0.362053 episode_count: 1946. steps_count: 879616.000000\n",
      "ep 278: ep_len:500 episode reward: total was -32.700000. running mean: -36.448622\n",
      "ep 278: ep_len:600 episode reward: total was -68.150000. running mean: -36.765636\n",
      "ep 278: ep_len:635 episode reward: total was -69.690000. running mean: -37.094880\n",
      "ep 278: ep_len:505 episode reward: total was -30.260000. running mean: -37.026531\n",
      "ep 278: ep_len:3 episode reward: total was 0.000000. running mean: -36.656266\n",
      "ep 278: ep_len:261 episode reward: total was -17.940000. running mean: -36.469103\n",
      "ep 278: ep_len:570 episode reward: total was -50.820000. running mean: -36.612612\n",
      "epsilon:0.361916 episode_count: 1953. steps_count: 882690.000000\n",
      "ep 279: ep_len:650 episode reward: total was -54.020000. running mean: -36.786686\n",
      "ep 279: ep_len:510 episode reward: total was -41.260000. running mean: -36.831419\n",
      "ep 279: ep_len:625 episode reward: total was -41.730000. running mean: -36.880405\n",
      "ep 279: ep_len:500 episode reward: total was -44.770000. running mean: -36.959301\n",
      "ep 279: ep_len:3 episode reward: total was 0.000000. running mean: -36.589708\n",
      "ep 279: ep_len:690 episode reward: total was -39.890000. running mean: -36.622711\n",
      "ep 279: ep_len:650 episode reward: total was -57.270000. running mean: -36.829184\n",
      "epsilon:0.361780 episode_count: 1960. steps_count: 886318.000000\n",
      "ep 280: ep_len:655 episode reward: total was -52.240000. running mean: -36.983292\n",
      "ep 280: ep_len:670 episode reward: total was -72.760000. running mean: -37.341059\n",
      "ep 280: ep_len:610 episode reward: total was -60.620000. running mean: -37.573848\n",
      "ep 280: ep_len:500 episode reward: total was -38.180000. running mean: -37.579910\n",
      "ep 280: ep_len:3 episode reward: total was 0.000000. running mean: -37.204111\n",
      "ep 280: ep_len:510 episode reward: total was -36.610000. running mean: -37.198170\n",
      "ep 280: ep_len:530 episode reward: total was -45.580000. running mean: -37.281988\n",
      "epsilon:0.361643 episode_count: 1967. steps_count: 889796.000000\n",
      "ep 281: ep_len:515 episode reward: total was -57.150000. running mean: -37.480668\n",
      "ep 281: ep_len:580 episode reward: total was -42.760000. running mean: -37.533461\n",
      "ep 281: ep_len:550 episode reward: total was -33.710000. running mean: -37.495227\n",
      "ep 281: ep_len:542 episode reward: total was -40.630000. running mean: -37.526574\n",
      "ep 281: ep_len:95 episode reward: total was -1.480000. running mean: -37.166109\n",
      "ep 281: ep_len:535 episode reward: total was -39.130000. running mean: -37.185748\n",
      "ep 281: ep_len:306 episode reward: total was -43.900000. running mean: -37.252890\n",
      "epsilon:0.361507 episode_count: 1974. steps_count: 892919.000000\n",
      "ep 282: ep_len:585 episode reward: total was -63.270000. running mean: -37.513061\n",
      "ep 282: ep_len:500 episode reward: total was -51.700000. running mean: -37.654931\n",
      "ep 282: ep_len:595 episode reward: total was -51.230000. running mean: -37.790681\n",
      "ep 282: ep_len:500 episode reward: total was -25.620000. running mean: -37.668975\n",
      "ep 282: ep_len:53 episode reward: total was -5.500000. running mean: -37.347285\n",
      "ep 282: ep_len:605 episode reward: total was -48.670000. running mean: -37.460512\n",
      "ep 282: ep_len:640 episode reward: total was -64.170000. running mean: -37.727607\n",
      "epsilon:0.361370 episode_count: 1981. steps_count: 896397.000000\n",
      "ep 283: ep_len:585 episode reward: total was -46.160000. running mean: -37.811931\n",
      "ep 283: ep_len:500 episode reward: total was -53.770000. running mean: -37.971511\n",
      "ep 283: ep_len:555 episode reward: total was -78.390000. running mean: -38.375696\n",
      "ep 283: ep_len:609 episode reward: total was -57.150000. running mean: -38.563439\n",
      "ep 283: ep_len:73 episode reward: total was -11.980000. running mean: -38.297605\n",
      "ep 283: ep_len:665 episode reward: total was -57.480000. running mean: -38.489429\n",
      "ep 283: ep_len:645 episode reward: total was -62.060000. running mean: -38.725135\n",
      "epsilon:0.361234 episode_count: 1988. steps_count: 900029.000000\n",
      "ep 284: ep_len:520 episode reward: total was -46.110000. running mean: -38.798983\n",
      "ep 284: ep_len:640 episode reward: total was -44.210000. running mean: -38.853093\n",
      "ep 284: ep_len:452 episode reward: total was -29.880000. running mean: -38.763363\n",
      "ep 284: ep_len:560 episode reward: total was -42.730000. running mean: -38.803029\n",
      "ep 284: ep_len:51 episode reward: total was -3.500000. running mean: -38.449999\n",
      "ep 284: ep_len:560 episode reward: total was -50.280000. running mean: -38.568299\n",
      "ep 284: ep_len:333 episode reward: total was -20.890000. running mean: -38.391516\n",
      "epsilon:0.361097 episode_count: 1995. steps_count: 903145.000000\n",
      "ep 285: ep_len:610 episode reward: total was -44.720000. running mean: -38.454800\n",
      "ep 285: ep_len:620 episode reward: total was -38.690000. running mean: -38.457152\n",
      "ep 285: ep_len:436 episode reward: total was -36.340000. running mean: -38.435981\n",
      "ep 285: ep_len:545 episode reward: total was -25.220000. running mean: -38.303821\n",
      "ep 285: ep_len:3 episode reward: total was 0.000000. running mean: -37.920783\n",
      "ep 285: ep_len:180 episode reward: total was -13.460000. running mean: -37.676175\n",
      "ep 285: ep_len:575 episode reward: total was -55.350000. running mean: -37.852913\n",
      "epsilon:0.360961 episode_count: 2002. steps_count: 906114.000000\n",
      "ep 286: ep_len:580 episode reward: total was -44.840000. running mean: -37.922784\n",
      "ep 286: ep_len:595 episode reward: total was -38.550000. running mean: -37.929056\n",
      "ep 286: ep_len:394 episode reward: total was -30.460000. running mean: -37.854366\n",
      "ep 286: ep_len:585 episode reward: total was -77.420000. running mean: -38.250022\n",
      "ep 286: ep_len:3 episode reward: total was 0.000000. running mean: -37.867522\n",
      "ep 286: ep_len:690 episode reward: total was -68.620000. running mean: -38.175047\n",
      "ep 286: ep_len:590 episode reward: total was -60.260000. running mean: -38.395896\n",
      "epsilon:0.360824 episode_count: 2009. steps_count: 909551.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 287: ep_len:510 episode reward: total was -27.830000. running mean: -38.290237\n",
      "ep 287: ep_len:313 episode reward: total was -40.460000. running mean: -38.311935\n",
      "ep 287: ep_len:500 episode reward: total was -54.400000. running mean: -38.472816\n",
      "ep 287: ep_len:535 episode reward: total was -52.710000. running mean: -38.615187\n",
      "ep 287: ep_len:83 episode reward: total was -8.960000. running mean: -38.318636\n",
      "ep 287: ep_len:680 episode reward: total was -81.280000. running mean: -38.748249\n",
      "ep 287: ep_len:500 episode reward: total was -31.720000. running mean: -38.677967\n",
      "epsilon:0.360688 episode_count: 2016. steps_count: 912672.000000\n",
      "ep 288: ep_len:530 episode reward: total was -27.790000. running mean: -38.569087\n",
      "ep 288: ep_len:1455 episode reward: total was -170.380000. running mean: -39.887196\n",
      "ep 288: ep_len:655 episode reward: total was -55.250000. running mean: -40.040824\n",
      "ep 288: ep_len:505 episode reward: total was -26.780000. running mean: -39.908216\n",
      "ep 288: ep_len:97 episode reward: total was -7.970000. running mean: -39.588834\n",
      "ep 288: ep_len:565 episode reward: total was -35.570000. running mean: -39.548645\n",
      "ep 288: ep_len:570 episode reward: total was -36.590000. running mean: -39.519059\n",
      "epsilon:0.360551 episode_count: 2023. steps_count: 917049.000000\n",
      "ep 289: ep_len:800 episode reward: total was -69.680000. running mean: -39.820668\n",
      "ep 289: ep_len:595 episode reward: total was -44.320000. running mean: -39.865662\n",
      "ep 289: ep_len:695 episode reward: total was -76.060000. running mean: -40.227605\n",
      "ep 289: ep_len:500 episode reward: total was -29.180000. running mean: -40.117129\n",
      "ep 289: ep_len:56 episode reward: total was -4.500000. running mean: -39.760958\n",
      "ep 289: ep_len:500 episode reward: total was -45.780000. running mean: -39.821148\n",
      "ep 289: ep_len:505 episode reward: total was -36.270000. running mean: -39.785637\n",
      "epsilon:0.360415 episode_count: 2030. steps_count: 920700.000000\n",
      "ep 290: ep_len:665 episode reward: total was -63.080000. running mean: -40.018580\n",
      "ep 290: ep_len:189 episode reward: total was -16.440000. running mean: -39.782795\n",
      "ep 290: ep_len:68 episode reward: total was -1.480000. running mean: -39.399767\n",
      "ep 290: ep_len:545 episode reward: total was -23.150000. running mean: -39.237269\n",
      "ep 290: ep_len:3 episode reward: total was 0.000000. running mean: -38.844896\n",
      "ep 290: ep_len:500 episode reward: total was -35.820000. running mean: -38.814647\n",
      "ep 290: ep_len:342 episode reward: total was -20.360000. running mean: -38.630101\n",
      "epsilon:0.360278 episode_count: 2037. steps_count: 923012.000000\n",
      "ep 291: ep_len:500 episode reward: total was -24.710000. running mean: -38.490900\n",
      "ep 291: ep_len:560 episode reward: total was -30.140000. running mean: -38.407391\n",
      "ep 291: ep_len:625 episode reward: total was -52.150000. running mean: -38.544817\n",
      "ep 291: ep_len:525 episode reward: total was -28.270000. running mean: -38.442069\n",
      "ep 291: ep_len:3 episode reward: total was 0.000000. running mean: -38.057648\n",
      "ep 291: ep_len:520 episode reward: total was -32.230000. running mean: -37.999372\n",
      "ep 291: ep_len:163 episode reward: total was -15.940000. running mean: -37.778778\n",
      "epsilon:0.360142 episode_count: 2044. steps_count: 925908.000000\n",
      "ep 292: ep_len:500 episode reward: total was -45.870000. running mean: -37.859690\n",
      "ep 292: ep_len:540 episode reward: total was -52.140000. running mean: -38.002493\n",
      "ep 292: ep_len:555 episode reward: total was -54.590000. running mean: -38.168368\n",
      "ep 292: ep_len:610 episode reward: total was -42.630000. running mean: -38.212985\n",
      "ep 292: ep_len:3 episode reward: total was 0.000000. running mean: -37.830855\n",
      "ep 292: ep_len:540 episode reward: total was -51.640000. running mean: -37.968946\n",
      "ep 292: ep_len:500 episode reward: total was -40.310000. running mean: -37.992357\n",
      "epsilon:0.360005 episode_count: 2051. steps_count: 929156.000000\n",
      "ep 293: ep_len:570 episode reward: total was -68.170000. running mean: -38.294133\n",
      "ep 293: ep_len:585 episode reward: total was -22.320000. running mean: -38.134392\n",
      "ep 293: ep_len:540 episode reward: total was -56.150000. running mean: -38.314548\n",
      "ep 293: ep_len:170 episode reward: total was -11.940000. running mean: -38.050802\n",
      "ep 293: ep_len:99 episode reward: total was -1.480000. running mean: -37.685094\n",
      "ep 293: ep_len:565 episode reward: total was -51.570000. running mean: -37.823943\n",
      "ep 293: ep_len:520 episode reward: total was -44.010000. running mean: -37.885804\n",
      "epsilon:0.359869 episode_count: 2058. steps_count: 932205.000000\n",
      "ep 294: ep_len:545 episode reward: total was -40.750000. running mean: -37.914446\n",
      "ep 294: ep_len:590 episode reward: total was -66.670000. running mean: -38.202001\n",
      "ep 294: ep_len:635 episode reward: total was -51.020000. running mean: -38.330181\n",
      "ep 294: ep_len:159 episode reward: total was -1.960000. running mean: -37.966480\n",
      "ep 294: ep_len:3 episode reward: total was 0.000000. running mean: -37.586815\n",
      "ep 294: ep_len:230 episode reward: total was -11.970000. running mean: -37.330647\n",
      "ep 294: ep_len:565 episode reward: total was -40.630000. running mean: -37.363640\n",
      "epsilon:0.359732 episode_count: 2065. steps_count: 934932.000000\n",
      "ep 295: ep_len:655 episode reward: total was -53.540000. running mean: -37.525404\n",
      "ep 295: ep_len:289 episode reward: total was -22.410000. running mean: -37.374250\n",
      "ep 295: ep_len:585 episode reward: total was -43.610000. running mean: -37.436607\n",
      "ep 295: ep_len:610 episode reward: total was -68.610000. running mean: -37.748341\n",
      "ep 295: ep_len:3 episode reward: total was 0.000000. running mean: -37.370858\n",
      "ep 295: ep_len:605 episode reward: total was -56.570000. running mean: -37.562849\n",
      "ep 295: ep_len:545 episode reward: total was -51.850000. running mean: -37.705721\n",
      "epsilon:0.359596 episode_count: 2072. steps_count: 938224.000000\n",
      "ep 296: ep_len:610 episode reward: total was -46.280000. running mean: -37.791464\n",
      "ep 296: ep_len:301 episode reward: total was -21.910000. running mean: -37.632649\n",
      "ep 296: ep_len:500 episode reward: total was -47.340000. running mean: -37.729722\n",
      "ep 296: ep_len:555 episode reward: total was -63.300000. running mean: -37.985425\n",
      "ep 296: ep_len:3 episode reward: total was 0.000000. running mean: -37.605571\n",
      "ep 296: ep_len:535 episode reward: total was -33.560000. running mean: -37.565115\n",
      "ep 296: ep_len:540 episode reward: total was -43.690000. running mean: -37.626364\n",
      "epsilon:0.359459 episode_count: 2079. steps_count: 941268.000000\n",
      "ep 297: ep_len:665 episode reward: total was -59.990000. running mean: -37.850000\n",
      "ep 297: ep_len:605 episode reward: total was -63.780000. running mean: -38.109300\n",
      "ep 297: ep_len:540 episode reward: total was -79.830000. running mean: -38.526507\n",
      "ep 297: ep_len:500 episode reward: total was -15.070000. running mean: -38.291942\n",
      "ep 297: ep_len:3 episode reward: total was 0.000000. running mean: -37.909023\n",
      "ep 297: ep_len:515 episode reward: total was -52.230000. running mean: -38.052233\n",
      "ep 297: ep_len:580 episode reward: total was -25.450000. running mean: -37.926210\n",
      "epsilon:0.359323 episode_count: 2086. steps_count: 944676.000000\n",
      "ep 298: ep_len:560 episode reward: total was -34.790000. running mean: -37.894848\n",
      "ep 298: ep_len:675 episode reward: total was -50.950000. running mean: -38.025400\n",
      "ep 298: ep_len:368 episode reward: total was -24.360000. running mean: -37.888746\n",
      "ep 298: ep_len:745 episode reward: total was -75.180000. running mean: -38.261658\n",
      "ep 298: ep_len:3 episode reward: total was 0.000000. running mean: -37.879042\n",
      "ep 298: ep_len:635 episode reward: total was -46.740000. running mean: -37.967651\n",
      "ep 298: ep_len:520 episode reward: total was -49.110000. running mean: -38.079075\n",
      "epsilon:0.359186 episode_count: 2093. steps_count: 948182.000000\n",
      "ep 299: ep_len:505 episode reward: total was -42.850000. running mean: -38.126784\n",
      "ep 299: ep_len:500 episode reward: total was -22.180000. running mean: -37.967316\n",
      "ep 299: ep_len:585 episode reward: total was -57.070000. running mean: -38.158343\n",
      "ep 299: ep_len:545 episode reward: total was -50.650000. running mean: -38.283260\n",
      "ep 299: ep_len:3 episode reward: total was 0.000000. running mean: -37.900427\n",
      "ep 299: ep_len:233 episode reward: total was -5.910000. running mean: -37.580523\n",
      "ep 299: ep_len:284 episode reward: total was -29.870000. running mean: -37.503418\n",
      "epsilon:0.359050 episode_count: 2100. steps_count: 950837.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 300: ep_len:500 episode reward: total was -45.260000. running mean: -37.580983\n",
      "ep 300: ep_len:615 episode reward: total was -43.670000. running mean: -37.641874\n",
      "ep 300: ep_len:580 episode reward: total was -41.210000. running mean: -37.677555\n",
      "ep 300: ep_len:369 episode reward: total was -29.360000. running mean: -37.594379\n",
      "ep 300: ep_len:3 episode reward: total was 0.000000. running mean: -37.218435\n",
      "ep 300: ep_len:590 episode reward: total was -52.500000. running mean: -37.371251\n",
      "ep 300: ep_len:530 episode reward: total was -42.200000. running mean: -37.419539\n",
      "epsilon:0.358913 episode_count: 2107. steps_count: 954024.000000\n",
      "ep 301: ep_len:118 episode reward: total was -10.000000. running mean: -37.145343\n",
      "ep 301: ep_len:277 episode reward: total was -20.920000. running mean: -36.983090\n",
      "ep 301: ep_len:525 episode reward: total was -31.770000. running mean: -36.930959\n",
      "ep 301: ep_len:555 episode reward: total was -17.670000. running mean: -36.738349\n",
      "ep 301: ep_len:92 episode reward: total was -4.950000. running mean: -36.420466\n",
      "ep 301: ep_len:530 episode reward: total was -52.350000. running mean: -36.579761\n",
      "ep 301: ep_len:185 episode reward: total was -18.430000. running mean: -36.398264\n",
      "epsilon:0.358777 episode_count: 2114. steps_count: 956306.000000\n",
      "ep 302: ep_len:580 episode reward: total was -46.430000. running mean: -36.498581\n",
      "ep 302: ep_len:500 episode reward: total was -28.840000. running mean: -36.421995\n",
      "ep 302: ep_len:560 episode reward: total was -58.110000. running mean: -36.638875\n",
      "ep 302: ep_len:132 episode reward: total was -0.940000. running mean: -36.281886\n",
      "ep 302: ep_len:3 episode reward: total was 0.000000. running mean: -35.919068\n",
      "ep 302: ep_len:825 episode reward: total was -103.670000. running mean: -36.596577\n",
      "ep 302: ep_len:191 episode reward: total was -12.450000. running mean: -36.355111\n",
      "epsilon:0.358640 episode_count: 2121. steps_count: 959097.000000\n",
      "ep 303: ep_len:555 episode reward: total was -58.060000. running mean: -36.572160\n",
      "ep 303: ep_len:525 episode reward: total was -34.680000. running mean: -36.553238\n",
      "ep 303: ep_len:685 episode reward: total was -61.550000. running mean: -36.803206\n",
      "ep 303: ep_len:500 episode reward: total was -49.170000. running mean: -36.926874\n",
      "ep 303: ep_len:3 episode reward: total was 0.000000. running mean: -36.557605\n",
      "ep 303: ep_len:630 episode reward: total was -28.080000. running mean: -36.472829\n",
      "ep 303: ep_len:635 episode reward: total was -42.200000. running mean: -36.530101\n",
      "epsilon:0.358504 episode_count: 2128. steps_count: 962630.000000\n",
      "ep 304: ep_len:695 episode reward: total was -55.490000. running mean: -36.719700\n",
      "ep 304: ep_len:645 episode reward: total was -61.530000. running mean: -36.967803\n",
      "ep 304: ep_len:1040 episode reward: total was -101.210000. running mean: -37.610225\n",
      "ep 304: ep_len:615 episode reward: total was -45.600000. running mean: -37.690123\n",
      "ep 304: ep_len:3 episode reward: total was 0.000000. running mean: -37.313221\n",
      "ep 304: ep_len:635 episode reward: total was -48.630000. running mean: -37.426389\n",
      "ep 304: ep_len:575 episode reward: total was -36.980000. running mean: -37.421925\n",
      "epsilon:0.358367 episode_count: 2135. steps_count: 966838.000000\n",
      "ep 305: ep_len:500 episode reward: total was -23.810000. running mean: -37.285806\n",
      "ep 305: ep_len:525 episode reward: total was -31.330000. running mean: -37.226248\n",
      "ep 305: ep_len:404 episode reward: total was -35.400000. running mean: -37.207985\n",
      "ep 305: ep_len:605 episode reward: total was -43.650000. running mean: -37.272406\n",
      "ep 305: ep_len:3 episode reward: total was 0.000000. running mean: -36.899682\n",
      "ep 305: ep_len:525 episode reward: total was -54.190000. running mean: -37.072585\n",
      "ep 305: ep_len:298 episode reward: total was -25.380000. running mean: -36.955659\n",
      "epsilon:0.358231 episode_count: 2142. steps_count: 969698.000000\n",
      "ep 306: ep_len:650 episode reward: total was -60.080000. running mean: -37.186902\n",
      "ep 306: ep_len:575 episode reward: total was -53.710000. running mean: -37.352133\n",
      "ep 306: ep_len:640 episode reward: total was -64.080000. running mean: -37.619412\n",
      "ep 306: ep_len:600 episode reward: total was -59.760000. running mean: -37.840818\n",
      "ep 306: ep_len:118 episode reward: total was -13.470000. running mean: -37.597110\n",
      "ep 306: ep_len:570 episode reward: total was -50.070000. running mean: -37.721839\n",
      "ep 306: ep_len:500 episode reward: total was -36.580000. running mean: -37.710420\n",
      "epsilon:0.358094 episode_count: 2149. steps_count: 973351.000000\n",
      "ep 307: ep_len:595 episode reward: total was -35.120000. running mean: -37.684516\n",
      "ep 307: ep_len:510 episode reward: total was -44.670000. running mean: -37.754371\n",
      "ep 307: ep_len:373 episode reward: total was -12.370000. running mean: -37.500527\n",
      "ep 307: ep_len:505 episode reward: total was -25.110000. running mean: -37.376622\n",
      "ep 307: ep_len:77 episode reward: total was -9.460000. running mean: -37.097456\n",
      "ep 307: ep_len:500 episode reward: total was -35.790000. running mean: -37.084381\n",
      "ep 307: ep_len:595 episode reward: total was -45.700000. running mean: -37.170537\n",
      "epsilon:0.357958 episode_count: 2156. steps_count: 976506.000000\n",
      "ep 308: ep_len:134 episode reward: total was -8.450000. running mean: -36.883332\n",
      "ep 308: ep_len:655 episode reward: total was -30.160000. running mean: -36.816099\n",
      "ep 308: ep_len:605 episode reward: total was -36.750000. running mean: -36.815438\n",
      "ep 308: ep_len:545 episode reward: total was -36.620000. running mean: -36.813483\n",
      "ep 308: ep_len:3 episode reward: total was 0.000000. running mean: -36.445348\n",
      "ep 308: ep_len:545 episode reward: total was -68.700000. running mean: -36.767895\n",
      "ep 308: ep_len:570 episode reward: total was -41.570000. running mean: -36.815916\n",
      "epsilon:0.357821 episode_count: 2163. steps_count: 979563.000000\n",
      "ep 309: ep_len:570 episode reward: total was -38.760000. running mean: -36.835357\n",
      "ep 309: ep_len:630 episode reward: total was -72.740000. running mean: -37.194403\n",
      "ep 309: ep_len:595 episode reward: total was -56.790000. running mean: -37.390359\n",
      "ep 309: ep_len:510 episode reward: total was -44.660000. running mean: -37.463056\n",
      "ep 309: ep_len:87 episode reward: total was -11.950000. running mean: -37.207925\n",
      "ep 309: ep_len:585 episode reward: total was -44.550000. running mean: -37.281346\n",
      "ep 309: ep_len:300 episode reward: total was -35.370000. running mean: -37.262232\n",
      "epsilon:0.357685 episode_count: 2170. steps_count: 982840.000000\n",
      "ep 310: ep_len:540 episode reward: total was -34.270000. running mean: -37.232310\n",
      "ep 310: ep_len:251 episode reward: total was -24.400000. running mean: -37.103987\n",
      "ep 310: ep_len:765 episode reward: total was -80.920000. running mean: -37.542147\n",
      "ep 310: ep_len:500 episode reward: total was -35.650000. running mean: -37.523226\n",
      "ep 310: ep_len:92 episode reward: total was -8.470000. running mean: -37.232693\n",
      "ep 310: ep_len:505 episode reward: total was -63.420000. running mean: -37.494566\n",
      "ep 310: ep_len:595 episode reward: total was -54.210000. running mean: -37.661721\n",
      "epsilon:0.357548 episode_count: 2177. steps_count: 986088.000000\n",
      "ep 311: ep_len:675 episode reward: total was -70.560000. running mean: -37.990703\n",
      "ep 311: ep_len:351 episode reward: total was -44.900000. running mean: -38.059796\n",
      "ep 311: ep_len:540 episode reward: total was -38.570000. running mean: -38.064898\n",
      "ep 311: ep_len:565 episode reward: total was -26.620000. running mean: -37.950449\n",
      "ep 311: ep_len:107 episode reward: total was -7.460000. running mean: -37.645545\n",
      "ep 311: ep_len:232 episode reward: total was -25.360000. running mean: -37.522690\n",
      "ep 311: ep_len:500 episode reward: total was -45.910000. running mean: -37.606563\n",
      "epsilon:0.357412 episode_count: 2184. steps_count: 989058.000000\n",
      "ep 312: ep_len:520 episode reward: total was -37.770000. running mean: -37.608197\n",
      "ep 312: ep_len:665 episode reward: total was -76.230000. running mean: -37.994415\n",
      "ep 312: ep_len:409 episode reward: total was -34.440000. running mean: -37.958871\n",
      "ep 312: ep_len:510 episode reward: total was -30.200000. running mean: -37.881282\n",
      "ep 312: ep_len:3 episode reward: total was 0.000000. running mean: -37.502469\n",
      "ep 312: ep_len:710 episode reward: total was -72.080000. running mean: -37.848245\n",
      "ep 312: ep_len:580 episode reward: total was -57.700000. running mean: -38.046762\n",
      "epsilon:0.357275 episode_count: 2191. steps_count: 992455.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 313: ep_len:254 episode reward: total was -13.850000. running mean: -37.804795\n",
      "ep 313: ep_len:605 episode reward: total was -50.620000. running mean: -37.932947\n",
      "ep 313: ep_len:570 episode reward: total was -50.080000. running mean: -38.054417\n",
      "ep 313: ep_len:510 episode reward: total was -34.730000. running mean: -38.021173\n",
      "ep 313: ep_len:117 episode reward: total was -3.450000. running mean: -37.675461\n",
      "ep 313: ep_len:520 episode reward: total was -52.900000. running mean: -37.827707\n",
      "ep 313: ep_len:615 episode reward: total was -57.550000. running mean: -38.024930\n",
      "epsilon:0.357139 episode_count: 2198. steps_count: 995646.000000\n",
      "ep 314: ep_len:1085 episode reward: total was -107.280000. running mean: -38.717480\n",
      "ep 314: ep_len:585 episode reward: total was -44.660000. running mean: -38.776906\n",
      "ep 314: ep_len:500 episode reward: total was -43.730000. running mean: -38.826436\n",
      "ep 314: ep_len:56 episode reward: total was -3.980000. running mean: -38.477972\n",
      "ep 314: ep_len:3 episode reward: total was 0.000000. running mean: -38.093192\n",
      "ep 314: ep_len:540 episode reward: total was -46.080000. running mean: -38.173060\n",
      "ep 314: ep_len:520 episode reward: total was -48.350000. running mean: -38.274830\n",
      "epsilon:0.357002 episode_count: 2205. steps_count: 998935.000000\n",
      "ep 315: ep_len:525 episode reward: total was -32.350000. running mean: -38.215582\n",
      "ep 315: ep_len:590 episode reward: total was -47.860000. running mean: -38.312026\n",
      "ep 315: ep_len:455 episode reward: total was -29.280000. running mean: -38.221705\n",
      "ep 315: ep_len:500 episode reward: total was -27.690000. running mean: -38.116388\n",
      "ep 315: ep_len:3 episode reward: total was 0.000000. running mean: -37.735225\n",
      "ep 315: ep_len:670 episode reward: total was -48.030000. running mean: -37.838172\n",
      "ep 315: ep_len:190 episode reward: total was -22.950000. running mean: -37.689291\n",
      "epsilon:0.356866 episode_count: 2212. steps_count: 1001868.000000\n",
      "ep 316: ep_len:530 episode reward: total was -55.240000. running mean: -37.864798\n",
      "ep 316: ep_len:505 episode reward: total was -47.320000. running mean: -37.959350\n",
      "ep 316: ep_len:570 episode reward: total was -57.210000. running mean: -38.151856\n",
      "ep 316: ep_len:45 episode reward: total was -3.460000. running mean: -37.804938\n",
      "ep 316: ep_len:3 episode reward: total was 0.000000. running mean: -37.426888\n",
      "ep 316: ep_len:630 episode reward: total was -50.780000. running mean: -37.560419\n",
      "ep 316: ep_len:545 episode reward: total was -29.560000. running mean: -37.480415\n",
      "epsilon:0.356729 episode_count: 2219. steps_count: 1004696.000000\n",
      "ep 317: ep_len:580 episode reward: total was -59.530000. running mean: -37.700911\n",
      "ep 317: ep_len:590 episode reward: total was -65.690000. running mean: -37.980802\n",
      "ep 317: ep_len:525 episode reward: total was -64.280000. running mean: -38.243794\n",
      "ep 317: ep_len:505 episode reward: total was -37.700000. running mean: -38.238356\n",
      "ep 317: ep_len:3 episode reward: total was 0.000000. running mean: -37.855972\n",
      "ep 317: ep_len:510 episode reward: total was -64.290000. running mean: -38.120313\n",
      "ep 317: ep_len:580 episode reward: total was -56.650000. running mean: -38.305610\n",
      "epsilon:0.356593 episode_count: 2226. steps_count: 1007989.000000\n",
      "ep 318: ep_len:229 episode reward: total was -9.440000. running mean: -38.016953\n",
      "ep 318: ep_len:500 episode reward: total was -49.770000. running mean: -38.134484\n",
      "ep 318: ep_len:620 episode reward: total was -40.670000. running mean: -38.159839\n",
      "ep 318: ep_len:126 episode reward: total was -8.420000. running mean: -37.862441\n",
      "ep 318: ep_len:3 episode reward: total was 0.000000. running mean: -37.483816\n",
      "ep 318: ep_len:500 episode reward: total was -29.360000. running mean: -37.402578\n",
      "ep 318: ep_len:570 episode reward: total was -58.700000. running mean: -37.615552\n",
      "epsilon:0.356456 episode_count: 2233. steps_count: 1010537.000000\n",
      "ep 319: ep_len:595 episode reward: total was -48.290000. running mean: -37.722297\n",
      "ep 319: ep_len:605 episode reward: total was -47.200000. running mean: -37.817074\n",
      "ep 319: ep_len:500 episode reward: total was -50.200000. running mean: -37.940903\n",
      "ep 319: ep_len:515 episode reward: total was -30.720000. running mean: -37.868694\n",
      "ep 319: ep_len:3 episode reward: total was 0.000000. running mean: -37.490007\n",
      "ep 319: ep_len:580 episode reward: total was -54.010000. running mean: -37.655207\n",
      "ep 319: ep_len:331 episode reward: total was -22.360000. running mean: -37.502255\n",
      "epsilon:0.356320 episode_count: 2240. steps_count: 1013666.000000\n",
      "ep 320: ep_len:535 episode reward: total was -27.760000. running mean: -37.404832\n",
      "ep 320: ep_len:500 episode reward: total was -57.430000. running mean: -37.605084\n",
      "ep 320: ep_len:655 episode reward: total was -68.230000. running mean: -37.911333\n",
      "ep 320: ep_len:500 episode reward: total was -18.770000. running mean: -37.719920\n",
      "ep 320: ep_len:85 episode reward: total was -11.490000. running mean: -37.457621\n",
      "ep 320: ep_len:500 episode reward: total was -17.200000. running mean: -37.255045\n",
      "ep 320: ep_len:500 episode reward: total was -42.120000. running mean: -37.303694\n",
      "epsilon:0.356183 episode_count: 2247. steps_count: 1016941.000000\n",
      "ep 321: ep_len:222 episode reward: total was -11.410000. running mean: -37.044757\n",
      "ep 321: ep_len:565 episode reward: total was -47.240000. running mean: -37.146710\n",
      "ep 321: ep_len:550 episode reward: total was -53.090000. running mean: -37.306142\n",
      "ep 321: ep_len:510 episode reward: total was -40.710000. running mean: -37.340181\n",
      "ep 321: ep_len:3 episode reward: total was 0.000000. running mean: -36.966779\n",
      "ep 321: ep_len:535 episode reward: total was -65.880000. running mean: -37.255911\n",
      "ep 321: ep_len:500 episode reward: total was -47.160000. running mean: -37.354952\n",
      "epsilon:0.356047 episode_count: 2254. steps_count: 1019826.000000\n",
      "ep 322: ep_len:600 episode reward: total was -40.770000. running mean: -37.389103\n",
      "ep 322: ep_len:505 episode reward: total was -54.260000. running mean: -37.557812\n",
      "ep 322: ep_len:590 episode reward: total was -41.640000. running mean: -37.598634\n",
      "ep 322: ep_len:620 episode reward: total was -65.170000. running mean: -37.874347\n",
      "ep 322: ep_len:3 episode reward: total was 0.000000. running mean: -37.495604\n",
      "ep 322: ep_len:530 episode reward: total was -35.620000. running mean: -37.476848\n",
      "ep 322: ep_len:605 episode reward: total was -30.570000. running mean: -37.407779\n",
      "epsilon:0.355910 episode_count: 2261. steps_count: 1023279.000000\n",
      "ep 323: ep_len:520 episode reward: total was -37.810000. running mean: -37.411802\n",
      "ep 323: ep_len:500 episode reward: total was -30.330000. running mean: -37.340984\n",
      "ep 323: ep_len:600 episode reward: total was -40.120000. running mean: -37.368774\n",
      "ep 323: ep_len:610 episode reward: total was -43.190000. running mean: -37.426986\n",
      "ep 323: ep_len:88 episode reward: total was -4.470000. running mean: -37.097416\n",
      "ep 323: ep_len:173 episode reward: total was -15.980000. running mean: -36.886242\n",
      "ep 323: ep_len:590 episode reward: total was -49.800000. running mean: -37.015380\n",
      "epsilon:0.355774 episode_count: 2268. steps_count: 1026360.000000\n",
      "ep 324: ep_len:570 episode reward: total was -43.220000. running mean: -37.077426\n",
      "ep 324: ep_len:530 episode reward: total was -17.320000. running mean: -36.879851\n",
      "ep 324: ep_len:710 episode reward: total was -84.170000. running mean: -37.352753\n",
      "ep 324: ep_len:56 episode reward: total was -2.970000. running mean: -37.008925\n",
      "ep 324: ep_len:70 episode reward: total was -0.490000. running mean: -36.643736\n",
      "ep 324: ep_len:218 episode reward: total was -15.460000. running mean: -36.431899\n",
      "ep 324: ep_len:550 episode reward: total was -38.200000. running mean: -36.449580\n",
      "epsilon:0.355637 episode_count: 2275. steps_count: 1029064.000000\n",
      "ep 325: ep_len:249 episode reward: total was -15.440000. running mean: -36.239484\n",
      "ep 325: ep_len:505 episode reward: total was -68.390000. running mean: -36.560989\n",
      "ep 325: ep_len:535 episode reward: total was -52.880000. running mean: -36.724179\n",
      "ep 325: ep_len:520 episode reward: total was -49.710000. running mean: -36.854037\n",
      "ep 325: ep_len:126 episode reward: total was -7.960000. running mean: -36.565097\n",
      "ep 325: ep_len:500 episode reward: total was -35.760000. running mean: -36.557046\n",
      "ep 325: ep_len:580 episode reward: total was -60.160000. running mean: -36.793076\n",
      "epsilon:0.355501 episode_count: 2282. steps_count: 1032079.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 326: ep_len:605 episode reward: total was -63.630000. running mean: -37.061445\n",
      "ep 326: ep_len:650 episode reward: total was -35.120000. running mean: -37.042030\n",
      "ep 326: ep_len:1000 episode reward: total was -160.750000. running mean: -38.279110\n",
      "ep 326: ep_len:500 episode reward: total was -33.800000. running mean: -38.234319\n",
      "ep 326: ep_len:112 episode reward: total was -8.460000. running mean: -37.936576\n",
      "ep 326: ep_len:741 episode reward: total was -80.100000. running mean: -38.358210\n",
      "ep 326: ep_len:211 episode reward: total was -17.460000. running mean: -38.149228\n",
      "epsilon:0.355364 episode_count: 2289. steps_count: 1035898.000000\n",
      "ep 327: ep_len:127 episode reward: total was -5.980000. running mean: -37.827536\n",
      "ep 327: ep_len:505 episode reward: total was -41.280000. running mean: -37.862060\n",
      "ep 327: ep_len:775 episode reward: total was -90.530000. running mean: -38.388740\n",
      "ep 327: ep_len:500 episode reward: total was -24.670000. running mean: -38.251552\n",
      "ep 327: ep_len:47 episode reward: total was -2.500000. running mean: -37.894037\n",
      "ep 327: ep_len:505 episode reward: total was -56.280000. running mean: -38.077896\n",
      "ep 327: ep_len:600 episode reward: total was -53.110000. running mean: -38.228218\n",
      "epsilon:0.355228 episode_count: 2296. steps_count: 1038957.000000\n",
      "ep 328: ep_len:223 episode reward: total was -27.400000. running mean: -38.119935\n",
      "ep 328: ep_len:525 episode reward: total was -38.200000. running mean: -38.120736\n",
      "ep 328: ep_len:425 episode reward: total was -40.880000. running mean: -38.148329\n",
      "ep 328: ep_len:580 episode reward: total was -50.380000. running mean: -38.270645\n",
      "ep 328: ep_len:3 episode reward: total was 0.000000. running mean: -37.887939\n",
      "ep 328: ep_len:600 episode reward: total was -29.220000. running mean: -37.801260\n",
      "ep 328: ep_len:525 episode reward: total was -36.260000. running mean: -37.785847\n",
      "epsilon:0.355091 episode_count: 2303. steps_count: 1041838.000000\n",
      "ep 329: ep_len:735 episode reward: total was -65.990000. running mean: -38.067888\n",
      "ep 329: ep_len:500 episode reward: total was -21.360000. running mean: -37.900810\n",
      "ep 329: ep_len:387 episode reward: total was -32.860000. running mean: -37.850401\n",
      "ep 329: ep_len:515 episode reward: total was -38.160000. running mean: -37.853497\n",
      "ep 329: ep_len:96 episode reward: total was 2.530000. running mean: -37.449662\n",
      "ep 329: ep_len:176 episode reward: total was -8.940000. running mean: -37.164566\n",
      "ep 329: ep_len:610 episode reward: total was -47.570000. running mean: -37.268620\n",
      "epsilon:0.354955 episode_count: 2310. steps_count: 1044857.000000\n",
      "ep 330: ep_len:795 episode reward: total was -81.130000. running mean: -37.707234\n",
      "ep 330: ep_len:645 episode reward: total was -84.760000. running mean: -38.177762\n",
      "ep 330: ep_len:500 episode reward: total was -59.760000. running mean: -38.393584\n",
      "ep 330: ep_len:540 episode reward: total was -34.720000. running mean: -38.356848\n",
      "ep 330: ep_len:96 episode reward: total was -1.970000. running mean: -37.992980\n",
      "ep 330: ep_len:525 episode reward: total was -42.790000. running mean: -38.040950\n",
      "ep 330: ep_len:520 episode reward: total was -44.850000. running mean: -38.109040\n",
      "epsilon:0.354818 episode_count: 2317. steps_count: 1048478.000000\n",
      "ep 331: ep_len:615 episode reward: total was -45.720000. running mean: -38.185150\n",
      "ep 331: ep_len:500 episode reward: total was -21.920000. running mean: -38.022499\n",
      "ep 331: ep_len:660 episode reward: total was -60.720000. running mean: -38.249474\n",
      "ep 331: ep_len:560 episode reward: total was -41.260000. running mean: -38.279579\n",
      "ep 331: ep_len:111 episode reward: total was -17.470000. running mean: -38.071483\n",
      "ep 331: ep_len:186 episode reward: total was -10.480000. running mean: -37.795568\n",
      "ep 331: ep_len:515 episode reward: total was -41.250000. running mean: -37.830112\n",
      "epsilon:0.354682 episode_count: 2324. steps_count: 1051625.000000\n",
      "ep 332: ep_len:265 episode reward: total was -23.940000. running mean: -37.691211\n",
      "ep 332: ep_len:635 episode reward: total was -47.220000. running mean: -37.786499\n",
      "ep 332: ep_len:404 episode reward: total was -50.430000. running mean: -37.912934\n",
      "ep 332: ep_len:368 episode reward: total was -17.280000. running mean: -37.706605\n",
      "ep 332: ep_len:99 episode reward: total was -17.990000. running mean: -37.509439\n",
      "ep 332: ep_len:500 episode reward: total was -57.840000. running mean: -37.712744\n",
      "ep 332: ep_len:625 episode reward: total was -54.620000. running mean: -37.881817\n",
      "epsilon:0.354545 episode_count: 2331. steps_count: 1054521.000000\n",
      "ep 333: ep_len:134 episode reward: total was -3.430000. running mean: -37.537299\n",
      "ep 333: ep_len:580 episode reward: total was -51.190000. running mean: -37.673826\n",
      "ep 333: ep_len:605 episode reward: total was -30.750000. running mean: -37.604588\n",
      "ep 333: ep_len:152 episode reward: total was -6.460000. running mean: -37.293142\n",
      "ep 333: ep_len:3 episode reward: total was 0.000000. running mean: -36.920210\n",
      "ep 333: ep_len:505 episode reward: total was -63.900000. running mean: -37.190008\n",
      "ep 333: ep_len:530 episode reward: total was -49.130000. running mean: -37.309408\n",
      "epsilon:0.354409 episode_count: 2338. steps_count: 1057030.000000\n",
      "ep 334: ep_len:665 episode reward: total was -50.410000. running mean: -37.440414\n",
      "ep 334: ep_len:600 episode reward: total was -75.160000. running mean: -37.817610\n",
      "ep 334: ep_len:625 episode reward: total was -50.530000. running mean: -37.944734\n",
      "ep 334: ep_len:565 episode reward: total was -34.240000. running mean: -37.907686\n",
      "ep 334: ep_len:102 episode reward: total was -15.990000. running mean: -37.688510\n",
      "ep 334: ep_len:550 episode reward: total was -61.170000. running mean: -37.923325\n",
      "ep 334: ep_len:294 episode reward: total was -26.400000. running mean: -37.808091\n",
      "epsilon:0.354272 episode_count: 2345. steps_count: 1060431.000000\n",
      "ep 335: ep_len:520 episode reward: total was -52.290000. running mean: -37.952910\n",
      "ep 335: ep_len:675 episode reward: total was -64.630000. running mean: -38.219681\n",
      "ep 335: ep_len:650 episode reward: total was -56.750000. running mean: -38.404984\n",
      "ep 335: ep_len:113 episode reward: total was 0.590000. running mean: -38.015035\n",
      "ep 335: ep_len:3 episode reward: total was 0.000000. running mean: -37.634884\n",
      "ep 335: ep_len:665 episode reward: total was -90.690000. running mean: -38.165435\n",
      "ep 335: ep_len:515 episode reward: total was -49.130000. running mean: -38.275081\n",
      "epsilon:0.354136 episode_count: 2352. steps_count: 1063572.000000\n",
      "ep 336: ep_len:510 episode reward: total was -51.300000. running mean: -38.405330\n",
      "ep 336: ep_len:500 episode reward: total was -38.180000. running mean: -38.403077\n",
      "ep 336: ep_len:580 episode reward: total was -50.030000. running mean: -38.519346\n",
      "ep 336: ep_len:510 episode reward: total was -46.160000. running mean: -38.595753\n",
      "ep 336: ep_len:3 episode reward: total was 0.000000. running mean: -38.209795\n",
      "ep 336: ep_len:515 episode reward: total was -36.870000. running mean: -38.196397\n",
      "ep 336: ep_len:297 episode reward: total was -17.890000. running mean: -37.993333\n",
      "epsilon:0.353999 episode_count: 2359. steps_count: 1066487.000000\n",
      "ep 337: ep_len:500 episode reward: total was -32.290000. running mean: -37.936300\n",
      "ep 337: ep_len:570 episode reward: total was -46.230000. running mean: -38.019237\n",
      "ep 337: ep_len:625 episode reward: total was -63.330000. running mean: -38.272345\n",
      "ep 337: ep_len:510 episode reward: total was -34.620000. running mean: -38.235821\n",
      "ep 337: ep_len:55 episode reward: total was -1.990000. running mean: -37.873363\n",
      "ep 337: ep_len:570 episode reward: total was -47.800000. running mean: -37.972629\n",
      "ep 337: ep_len:580 episode reward: total was -47.010000. running mean: -38.063003\n",
      "epsilon:0.353863 episode_count: 2366. steps_count: 1069897.000000\n",
      "ep 338: ep_len:605 episode reward: total was -47.650000. running mean: -38.158873\n",
      "ep 338: ep_len:695 episode reward: total was -28.620000. running mean: -38.063484\n",
      "ep 338: ep_len:625 episode reward: total was -54.170000. running mean: -38.224549\n",
      "ep 338: ep_len:625 episode reward: total was -44.230000. running mean: -38.284604\n",
      "ep 338: ep_len:3 episode reward: total was 0.000000. running mean: -37.901758\n",
      "ep 338: ep_len:530 episode reward: total was -57.700000. running mean: -38.099740\n",
      "ep 338: ep_len:635 episode reward: total was -40.030000. running mean: -38.119043\n",
      "epsilon:0.353726 episode_count: 2373. steps_count: 1073615.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 339: ep_len:535 episode reward: total was -39.150000. running mean: -38.129352\n",
      "ep 339: ep_len:505 episode reward: total was -21.430000. running mean: -37.962359\n",
      "ep 339: ep_len:615 episode reward: total was -41.230000. running mean: -37.995035\n",
      "ep 339: ep_len:530 episode reward: total was -26.230000. running mean: -37.877385\n",
      "ep 339: ep_len:98 episode reward: total was -9.960000. running mean: -37.598211\n",
      "ep 339: ep_len:570 episode reward: total was -53.660000. running mean: -37.758829\n",
      "ep 339: ep_len:570 episode reward: total was -48.720000. running mean: -37.868441\n",
      "epsilon:0.353590 episode_count: 2380. steps_count: 1077038.000000\n",
      "ep 340: ep_len:585 episode reward: total was -26.770000. running mean: -37.757456\n",
      "ep 340: ep_len:515 episode reward: total was -25.920000. running mean: -37.639082\n",
      "ep 340: ep_len:585 episode reward: total was -47.140000. running mean: -37.734091\n",
      "ep 340: ep_len:505 episode reward: total was -38.650000. running mean: -37.743250\n",
      "ep 340: ep_len:3 episode reward: total was 0.000000. running mean: -37.365818\n",
      "ep 340: ep_len:500 episode reward: total was -31.880000. running mean: -37.310959\n",
      "ep 340: ep_len:640 episode reward: total was -59.230000. running mean: -37.530150\n",
      "epsilon:0.353453 episode_count: 2387. steps_count: 1080371.000000\n",
      "ep 341: ep_len:590 episode reward: total was -83.270000. running mean: -37.987548\n",
      "ep 341: ep_len:510 episode reward: total was -52.430000. running mean: -38.131973\n",
      "ep 341: ep_len:515 episode reward: total was -43.620000. running mean: -38.186853\n",
      "ep 341: ep_len:500 episode reward: total was -29.760000. running mean: -38.102585\n",
      "ep 341: ep_len:3 episode reward: total was 0.000000. running mean: -37.721559\n",
      "ep 341: ep_len:515 episode reward: total was -34.160000. running mean: -37.685943\n",
      "ep 341: ep_len:555 episode reward: total was -46.700000. running mean: -37.776084\n",
      "epsilon:0.353317 episode_count: 2394. steps_count: 1083559.000000\n",
      "ep 342: ep_len:505 episode reward: total was -28.710000. running mean: -37.685423\n",
      "ep 342: ep_len:176 episode reward: total was -18.470000. running mean: -37.493269\n",
      "ep 342: ep_len:680 episode reward: total was -92.770000. running mean: -38.046036\n",
      "ep 342: ep_len:530 episode reward: total was -34.260000. running mean: -38.008176\n",
      "ep 342: ep_len:94 episode reward: total was -4.970000. running mean: -37.677794\n",
      "ep 342: ep_len:520 episode reward: total was -62.440000. running mean: -37.925416\n",
      "ep 342: ep_len:585 episode reward: total was -33.620000. running mean: -37.882362\n",
      "epsilon:0.353180 episode_count: 2401. steps_count: 1086649.000000\n",
      "ep 343: ep_len:212 episode reward: total was -8.920000. running mean: -37.592738\n",
      "ep 343: ep_len:510 episode reward: total was -57.710000. running mean: -37.793911\n",
      "ep 343: ep_len:560 episode reward: total was -63.210000. running mean: -38.048072\n",
      "ep 343: ep_len:505 episode reward: total was -28.800000. running mean: -37.955591\n",
      "ep 343: ep_len:3 episode reward: total was 0.000000. running mean: -37.576035\n",
      "ep 343: ep_len:550 episode reward: total was -62.240000. running mean: -37.822675\n",
      "ep 343: ep_len:500 episode reward: total was -58.320000. running mean: -38.027648\n",
      "epsilon:0.353044 episode_count: 2408. steps_count: 1089489.000000\n",
      "ep 344: ep_len:500 episode reward: total was -28.730000. running mean: -37.934671\n",
      "ep 344: ep_len:525 episode reward: total was -23.900000. running mean: -37.794325\n",
      "ep 344: ep_len:500 episode reward: total was -39.680000. running mean: -37.813181\n",
      "ep 344: ep_len:387 episode reward: total was -21.770000. running mean: -37.652750\n",
      "ep 344: ep_len:90 episode reward: total was -1.480000. running mean: -37.291022\n",
      "ep 344: ep_len:530 episode reward: total was -52.820000. running mean: -37.446312\n",
      "ep 344: ep_len:555 episode reward: total was -43.240000. running mean: -37.504249\n",
      "epsilon:0.352907 episode_count: 2415. steps_count: 1092576.000000\n",
      "ep 345: ep_len:500 episode reward: total was -33.200000. running mean: -37.461206\n",
      "ep 345: ep_len:500 episode reward: total was -17.930000. running mean: -37.265894\n",
      "ep 345: ep_len:630 episode reward: total was -52.060000. running mean: -37.413835\n",
      "ep 345: ep_len:379 episode reward: total was -33.220000. running mean: -37.371897\n",
      "ep 345: ep_len:3 episode reward: total was 0.000000. running mean: -36.998178\n",
      "ep 345: ep_len:650 episode reward: total was -52.100000. running mean: -37.149196\n",
      "ep 345: ep_len:585 episode reward: total was -55.100000. running mean: -37.328704\n",
      "epsilon:0.352771 episode_count: 2422. steps_count: 1095823.000000\n",
      "ep 346: ep_len:188 episode reward: total was -6.910000. running mean: -37.024517\n",
      "ep 346: ep_len:615 episode reward: total was -45.720000. running mean: -37.111472\n",
      "ep 346: ep_len:79 episode reward: total was -2.470000. running mean: -36.765057\n",
      "ep 346: ep_len:560 episode reward: total was -30.730000. running mean: -36.704707\n",
      "ep 346: ep_len:112 episode reward: total was -3.990000. running mean: -36.377560\n",
      "ep 346: ep_len:550 episode reward: total was -27.240000. running mean: -36.286184\n",
      "ep 346: ep_len:500 episode reward: total was -46.650000. running mean: -36.389822\n",
      "epsilon:0.352634 episode_count: 2429. steps_count: 1098427.000000\n",
      "ep 347: ep_len:570 episode reward: total was -25.790000. running mean: -36.283824\n",
      "ep 347: ep_len:362 episode reward: total was -34.380000. running mean: -36.264786\n",
      "ep 347: ep_len:585 episode reward: total was -71.190000. running mean: -36.614038\n",
      "ep 347: ep_len:530 episode reward: total was -71.740000. running mean: -36.965298\n",
      "ep 347: ep_len:3 episode reward: total was 0.000000. running mean: -36.595645\n",
      "ep 347: ep_len:500 episode reward: total was -18.750000. running mean: -36.417188\n",
      "ep 347: ep_len:500 episode reward: total was -29.540000. running mean: -36.348416\n",
      "epsilon:0.352498 episode_count: 2436. steps_count: 1101477.000000\n",
      "ep 348: ep_len:222 episode reward: total was -10.400000. running mean: -36.088932\n",
      "ep 348: ep_len:600 episode reward: total was -40.190000. running mean: -36.129943\n",
      "ep 348: ep_len:610 episode reward: total was -59.060000. running mean: -36.359243\n",
      "ep 348: ep_len:535 episode reward: total was -45.150000. running mean: -36.447151\n",
      "ep 348: ep_len:3 episode reward: total was 0.000000. running mean: -36.082679\n",
      "ep 348: ep_len:545 episode reward: total was -38.130000. running mean: -36.103153\n",
      "ep 348: ep_len:520 episode reward: total was -44.660000. running mean: -36.188721\n",
      "epsilon:0.352361 episode_count: 2443. steps_count: 1104512.000000\n",
      "ep 349: ep_len:126 episode reward: total was -6.950000. running mean: -35.896334\n",
      "ep 349: ep_len:680 episode reward: total was -64.120000. running mean: -36.178570\n",
      "ep 349: ep_len:555 episode reward: total was -49.510000. running mean: -36.311885\n",
      "ep 349: ep_len:610 episode reward: total was -59.210000. running mean: -36.540866\n",
      "ep 349: ep_len:3 episode reward: total was 0.000000. running mean: -36.175457\n",
      "ep 349: ep_len:500 episode reward: total was -32.400000. running mean: -36.137703\n",
      "ep 349: ep_len:505 episode reward: total was -25.820000. running mean: -36.034526\n",
      "epsilon:0.352225 episode_count: 2450. steps_count: 1107491.000000\n",
      "ep 350: ep_len:575 episode reward: total was -34.120000. running mean: -36.015380\n",
      "ep 350: ep_len:505 episode reward: total was -37.200000. running mean: -36.027227\n",
      "ep 350: ep_len:585 episode reward: total was -32.940000. running mean: -35.996354\n",
      "ep 350: ep_len:559 episode reward: total was -47.130000. running mean: -36.107691\n",
      "ep 350: ep_len:3 episode reward: total was 0.000000. running mean: -35.746614\n",
      "ep 350: ep_len:500 episode reward: total was -32.780000. running mean: -35.716948\n",
      "ep 350: ep_len:675 episode reward: total was -56.480000. running mean: -35.924578\n",
      "epsilon:0.352088 episode_count: 2457. steps_count: 1110893.000000\n",
      "ep 351: ep_len:550 episode reward: total was -40.080000. running mean: -35.966133\n",
      "ep 351: ep_len:555 episode reward: total was -38.200000. running mean: -35.988471\n",
      "ep 351: ep_len:421 episode reward: total was -29.390000. running mean: -35.922486\n",
      "ep 351: ep_len:140 episode reward: total was -8.400000. running mean: -35.647262\n",
      "ep 351: ep_len:3 episode reward: total was 0.000000. running mean: -35.290789\n",
      "ep 351: ep_len:600 episode reward: total was -56.790000. running mean: -35.505781\n",
      "ep 351: ep_len:625 episode reward: total was -40.480000. running mean: -35.555523\n",
      "epsilon:0.351952 episode_count: 2464. steps_count: 1113787.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 352: ep_len:505 episode reward: total was -35.230000. running mean: -35.552268\n",
      "ep 352: ep_len:301 episode reward: total was -36.840000. running mean: -35.565145\n",
      "ep 352: ep_len:520 episode reward: total was -32.280000. running mean: -35.532294\n",
      "ep 352: ep_len:116 episode reward: total was -15.960000. running mean: -35.336571\n",
      "ep 352: ep_len:80 episode reward: total was -0.470000. running mean: -34.987905\n",
      "ep 352: ep_len:525 episode reward: total was -16.040000. running mean: -34.798426\n",
      "ep 352: ep_len:585 episode reward: total was -34.470000. running mean: -34.795142\n",
      "epsilon:0.351815 episode_count: 2471. steps_count: 1116419.000000\n",
      "ep 353: ep_len:500 episode reward: total was -40.780000. running mean: -34.854991\n",
      "ep 353: ep_len:500 episode reward: total was -41.280000. running mean: -34.919241\n",
      "ep 353: ep_len:680 episode reward: total was -36.850000. running mean: -34.938548\n",
      "ep 353: ep_len:540 episode reward: total was -43.640000. running mean: -35.025563\n",
      "ep 353: ep_len:3 episode reward: total was 0.000000. running mean: -34.675307\n",
      "ep 353: ep_len:600 episode reward: total was -68.240000. running mean: -35.010954\n",
      "ep 353: ep_len:630 episode reward: total was -43.990000. running mean: -35.100744\n",
      "epsilon:0.351679 episode_count: 2478. steps_count: 1119872.000000\n",
      "ep 354: ep_len:620 episode reward: total was -40.080000. running mean: -35.150537\n",
      "ep 354: ep_len:500 episode reward: total was -19.400000. running mean: -34.993032\n",
      "ep 354: ep_len:650 episode reward: total was -47.700000. running mean: -35.120101\n",
      "ep 354: ep_len:500 episode reward: total was -43.260000. running mean: -35.201500\n",
      "ep 354: ep_len:3 episode reward: total was 0.000000. running mean: -34.849485\n",
      "ep 354: ep_len:585 episode reward: total was -46.450000. running mean: -34.965490\n",
      "ep 354: ep_len:515 episode reward: total was -33.970000. running mean: -34.955536\n",
      "epsilon:0.351542 episode_count: 2485. steps_count: 1123245.000000\n",
      "ep 355: ep_len:525 episode reward: total was -25.590000. running mean: -34.861880\n",
      "ep 355: ep_len:610 episode reward: total was -34.260000. running mean: -34.855861\n",
      "ep 355: ep_len:515 episode reward: total was -49.070000. running mean: -34.998003\n",
      "ep 355: ep_len:595 episode reward: total was -30.760000. running mean: -34.955623\n",
      "ep 355: ep_len:3 episode reward: total was 0.000000. running mean: -34.606067\n",
      "ep 355: ep_len:585 episode reward: total was -24.180000. running mean: -34.501806\n",
      "ep 355: ep_len:525 episode reward: total was -35.140000. running mean: -34.508188\n",
      "epsilon:0.351406 episode_count: 2492. steps_count: 1126603.000000\n",
      "ep 356: ep_len:500 episode reward: total was -46.400000. running mean: -34.627106\n",
      "ep 356: ep_len:500 episode reward: total was -24.410000. running mean: -34.524935\n",
      "ep 356: ep_len:79 episode reward: total was -0.970000. running mean: -34.189386\n",
      "ep 356: ep_len:500 episode reward: total was -23.760000. running mean: -34.085092\n",
      "ep 356: ep_len:3 episode reward: total was 0.000000. running mean: -33.744241\n",
      "ep 356: ep_len:161 episode reward: total was -5.940000. running mean: -33.466198\n",
      "ep 356: ep_len:352 episode reward: total was -19.360000. running mean: -33.325136\n",
      "epsilon:0.351269 episode_count: 2499. steps_count: 1128698.000000\n",
      "ep 357: ep_len:535 episode reward: total was -23.290000. running mean: -33.224785\n",
      "ep 357: ep_len:600 episode reward: total was -24.690000. running mean: -33.139437\n",
      "ep 357: ep_len:585 episode reward: total was -47.770000. running mean: -33.285743\n",
      "ep 357: ep_len:590 episode reward: total was -56.700000. running mean: -33.519885\n",
      "ep 357: ep_len:3 episode reward: total was 0.000000. running mean: -33.184687\n",
      "ep 357: ep_len:610 episode reward: total was -66.050000. running mean: -33.513340\n",
      "ep 357: ep_len:545 episode reward: total was -57.230000. running mean: -33.750506\n",
      "epsilon:0.351133 episode_count: 2506. steps_count: 1132166.000000\n",
      "ep 358: ep_len:500 episode reward: total was -28.760000. running mean: -33.700601\n",
      "ep 358: ep_len:505 episode reward: total was -35.200000. running mean: -33.715595\n",
      "ep 358: ep_len:505 episode reward: total was -42.910000. running mean: -33.807539\n",
      "ep 358: ep_len:565 episode reward: total was -59.800000. running mean: -34.067464\n",
      "ep 358: ep_len:3 episode reward: total was 0.000000. running mean: -33.726789\n",
      "ep 358: ep_len:570 episode reward: total was -52.210000. running mean: -33.911621\n",
      "ep 358: ep_len:615 episode reward: total was -30.920000. running mean: -33.881705\n",
      "epsilon:0.350996 episode_count: 2513. steps_count: 1135429.000000\n",
      "ep 359: ep_len:500 episode reward: total was -19.300000. running mean: -33.735888\n",
      "ep 359: ep_len:520 episode reward: total was -49.650000. running mean: -33.895029\n",
      "ep 359: ep_len:540 episode reward: total was -35.430000. running mean: -33.910379\n",
      "ep 359: ep_len:595 episode reward: total was -58.600000. running mean: -34.157275\n",
      "ep 359: ep_len:97 episode reward: total was 2.040000. running mean: -33.795302\n",
      "ep 359: ep_len:535 episode reward: total was -38.720000. running mean: -33.844549\n",
      "ep 359: ep_len:530 episode reward: total was -21.400000. running mean: -33.720104\n",
      "epsilon:0.350860 episode_count: 2520. steps_count: 1138746.000000\n",
      "ep 360: ep_len:605 episode reward: total was -32.380000. running mean: -33.706703\n",
      "ep 360: ep_len:590 episode reward: total was -62.740000. running mean: -33.997036\n",
      "ep 360: ep_len:500 episode reward: total was -59.200000. running mean: -34.249065\n",
      "ep 360: ep_len:56 episode reward: total was -4.970000. running mean: -33.956275\n",
      "ep 360: ep_len:3 episode reward: total was 0.000000. running mean: -33.616712\n",
      "ep 360: ep_len:500 episode reward: total was -51.310000. running mean: -33.793645\n",
      "ep 360: ep_len:610 episode reward: total was -56.210000. running mean: -34.017808\n",
      "epsilon:0.350723 episode_count: 2527. steps_count: 1141610.000000\n",
      "ep 361: ep_len:500 episode reward: total was -26.740000. running mean: -33.945030\n",
      "ep 361: ep_len:580 episode reward: total was -51.620000. running mean: -34.121780\n",
      "ep 361: ep_len:525 episode reward: total was -48.640000. running mean: -34.266962\n",
      "ep 361: ep_len:570 episode reward: total was -30.630000. running mean: -34.230593\n",
      "ep 361: ep_len:3 episode reward: total was 0.000000. running mean: -33.888287\n",
      "ep 361: ep_len:510 episode reward: total was -46.590000. running mean: -34.015304\n",
      "ep 361: ep_len:580 episode reward: total was -68.720000. running mean: -34.362351\n",
      "epsilon:0.350587 episode_count: 2534. steps_count: 1144878.000000\n",
      "ep 362: ep_len:560 episode reward: total was -50.450000. running mean: -34.523227\n",
      "ep 362: ep_len:166 episode reward: total was -11.490000. running mean: -34.292895\n",
      "ep 362: ep_len:565 episode reward: total was -40.940000. running mean: -34.359366\n",
      "ep 362: ep_len:575 episode reward: total was -33.230000. running mean: -34.348072\n",
      "ep 362: ep_len:3 episode reward: total was 0.000000. running mean: -34.004592\n",
      "ep 362: ep_len:515 episode reward: total was -38.820000. running mean: -34.052746\n",
      "ep 362: ep_len:615 episode reward: total was -47.210000. running mean: -34.184318\n",
      "epsilon:0.350450 episode_count: 2541. steps_count: 1147877.000000\n",
      "ep 363: ep_len:515 episode reward: total was -53.280000. running mean: -34.375275\n",
      "ep 363: ep_len:565 episode reward: total was -49.060000. running mean: -34.522122\n",
      "ep 363: ep_len:605 episode reward: total was -39.600000. running mean: -34.572901\n",
      "ep 363: ep_len:535 episode reward: total was -44.670000. running mean: -34.673872\n",
      "ep 363: ep_len:3 episode reward: total was 0.000000. running mean: -34.327133\n",
      "ep 363: ep_len:650 episode reward: total was -59.740000. running mean: -34.581262\n",
      "ep 363: ep_len:560 episode reward: total was -55.720000. running mean: -34.792649\n",
      "epsilon:0.350314 episode_count: 2548. steps_count: 1151310.000000\n",
      "ep 364: ep_len:585 episode reward: total was -39.720000. running mean: -34.841923\n",
      "ep 364: ep_len:179 episode reward: total was -16.980000. running mean: -34.663304\n",
      "ep 364: ep_len:565 episode reward: total was -50.690000. running mean: -34.823571\n",
      "ep 364: ep_len:128 episode reward: total was -3.430000. running mean: -34.509635\n",
      "ep 364: ep_len:80 episode reward: total was -7.950000. running mean: -34.244039\n",
      "ep 364: ep_len:625 episode reward: total was -36.100000. running mean: -34.262598\n",
      "ep 364: ep_len:203 episode reward: total was -19.440000. running mean: -34.114372\n",
      "epsilon:0.350177 episode_count: 2555. steps_count: 1153675.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 365: ep_len:650 episode reward: total was -49.700000. running mean: -34.270229\n",
      "ep 365: ep_len:515 episode reward: total was -54.380000. running mean: -34.471326\n",
      "ep 365: ep_len:560 episode reward: total was -46.060000. running mean: -34.587213\n",
      "ep 365: ep_len:580 episode reward: total was -47.210000. running mean: -34.713441\n",
      "ep 365: ep_len:50 episode reward: total was -3.500000. running mean: -34.401306\n",
      "ep 365: ep_len:575 episode reward: total was -39.460000. running mean: -34.451893\n",
      "ep 365: ep_len:271 episode reward: total was -23.880000. running mean: -34.346174\n",
      "epsilon:0.350041 episode_count: 2562. steps_count: 1156876.000000\n",
      "ep 366: ep_len:192 episode reward: total was -13.460000. running mean: -34.137313\n",
      "ep 366: ep_len:505 episode reward: total was -47.890000. running mean: -34.274840\n",
      "ep 366: ep_len:423 episode reward: total was -27.390000. running mean: -34.205991\n",
      "ep 366: ep_len:505 episode reward: total was -50.250000. running mean: -34.366431\n",
      "ep 366: ep_len:3 episode reward: total was 0.000000. running mean: -34.022767\n",
      "ep 366: ep_len:279 episode reward: total was -25.930000. running mean: -33.941839\n",
      "ep 366: ep_len:580 episode reward: total was -62.150000. running mean: -34.223921\n",
      "epsilon:0.349904 episode_count: 2569. steps_count: 1159363.000000\n",
      "ep 367: ep_len:605 episode reward: total was -41.210000. running mean: -34.293782\n",
      "ep 367: ep_len:565 episode reward: total was -19.300000. running mean: -34.143844\n",
      "ep 367: ep_len:515 episode reward: total was -38.530000. running mean: -34.187705\n",
      "ep 367: ep_len:545 episode reward: total was -29.030000. running mean: -34.136128\n",
      "ep 367: ep_len:49 episode reward: total was 1.500000. running mean: -33.779767\n",
      "ep 367: ep_len:565 episode reward: total was -32.850000. running mean: -33.770469\n",
      "ep 367: ep_len:580 episode reward: total was -49.100000. running mean: -33.923765\n",
      "epsilon:0.349768 episode_count: 2576. steps_count: 1162787.000000\n",
      "ep 368: ep_len:208 episode reward: total was -7.340000. running mean: -33.657927\n",
      "ep 368: ep_len:366 episode reward: total was -59.870000. running mean: -33.920048\n",
      "ep 368: ep_len:600 episode reward: total was -46.710000. running mean: -34.047947\n",
      "ep 368: ep_len:500 episode reward: total was -46.710000. running mean: -34.174568\n",
      "ep 368: ep_len:112 episode reward: total was -2.970000. running mean: -33.862522\n",
      "ep 368: ep_len:545 episode reward: total was -31.610000. running mean: -33.839997\n",
      "ep 368: ep_len:500 episode reward: total was -40.560000. running mean: -33.907197\n",
      "epsilon:0.349631 episode_count: 2583. steps_count: 1165618.000000\n",
      "ep 369: ep_len:259 episode reward: total was -14.950000. running mean: -33.717625\n",
      "ep 369: ep_len:545 episode reward: total was -18.290000. running mean: -33.563349\n",
      "ep 369: ep_len:675 episode reward: total was -77.110000. running mean: -33.998815\n",
      "ep 369: ep_len:560 episode reward: total was -37.310000. running mean: -34.031927\n",
      "ep 369: ep_len:45 episode reward: total was -7.500000. running mean: -33.766608\n",
      "ep 369: ep_len:610 episode reward: total was -44.400000. running mean: -33.872942\n",
      "ep 369: ep_len:329 episode reward: total was -32.840000. running mean: -33.862612\n",
      "epsilon:0.349495 episode_count: 2590. steps_count: 1168641.000000\n",
      "ep 370: ep_len:765 episode reward: total was -77.150000. running mean: -34.295486\n",
      "ep 370: ep_len:500 episode reward: total was -17.890000. running mean: -34.131431\n",
      "ep 370: ep_len:645 episode reward: total was -58.040000. running mean: -34.370517\n",
      "ep 370: ep_len:387 episode reward: total was -42.220000. running mean: -34.449012\n",
      "ep 370: ep_len:51 episode reward: total was -3.500000. running mean: -34.139522\n",
      "ep 370: ep_len:770 episode reward: total was -79.720000. running mean: -34.595327\n",
      "ep 370: ep_len:595 episode reward: total was -44.690000. running mean: -34.696273\n",
      "epsilon:0.349358 episode_count: 2597. steps_count: 1172354.000000\n",
      "ep 371: ep_len:510 episode reward: total was -28.760000. running mean: -34.636911\n",
      "ep 371: ep_len:525 episode reward: total was -69.360000. running mean: -34.984141\n",
      "ep 371: ep_len:515 episode reward: total was -27.460000. running mean: -34.908900\n",
      "ep 371: ep_len:160 episode reward: total was -6.940000. running mean: -34.629211\n",
      "ep 371: ep_len:96 episode reward: total was -1.970000. running mean: -34.302619\n",
      "ep 371: ep_len:570 episode reward: total was -41.540000. running mean: -34.374993\n",
      "ep 371: ep_len:585 episode reward: total was -46.940000. running mean: -34.500643\n",
      "epsilon:0.349222 episode_count: 2604. steps_count: 1175315.000000\n",
      "ep 372: ep_len:207 episode reward: total was -15.480000. running mean: -34.310436\n",
      "ep 372: ep_len:510 episode reward: total was -22.800000. running mean: -34.195332\n",
      "ep 372: ep_len:550 episode reward: total was -47.150000. running mean: -34.324879\n",
      "ep 372: ep_len:695 episode reward: total was -53.650000. running mean: -34.518130\n",
      "ep 372: ep_len:102 episode reward: total was -12.470000. running mean: -34.297649\n",
      "ep 372: ep_len:535 episode reward: total was -31.600000. running mean: -34.270672\n",
      "ep 372: ep_len:505 episode reward: total was -42.110000. running mean: -34.349065\n",
      "epsilon:0.349085 episode_count: 2611. steps_count: 1178419.000000\n",
      "ep 373: ep_len:630 episode reward: total was -54.000000. running mean: -34.545575\n",
      "ep 373: ep_len:690 episode reward: total was -24.150000. running mean: -34.441619\n",
      "ep 373: ep_len:540 episode reward: total was -44.680000. running mean: -34.544003\n",
      "ep 373: ep_len:515 episode reward: total was -40.240000. running mean: -34.600963\n",
      "ep 373: ep_len:3 episode reward: total was 0.000000. running mean: -34.254953\n",
      "ep 373: ep_len:750 episode reward: total was -112.700000. running mean: -35.039404\n",
      "ep 373: ep_len:530 episode reward: total was -42.770000. running mean: -35.116710\n",
      "epsilon:0.348949 episode_count: 2618. steps_count: 1182077.000000\n",
      "ep 374: ep_len:565 episode reward: total was -42.180000. running mean: -35.187342\n",
      "ep 374: ep_len:610 episode reward: total was -24.100000. running mean: -35.076469\n",
      "ep 374: ep_len:500 episode reward: total was -37.260000. running mean: -35.098304\n",
      "ep 374: ep_len:525 episode reward: total was -24.140000. running mean: -34.988721\n",
      "ep 374: ep_len:113 episode reward: total was -12.450000. running mean: -34.763334\n",
      "ep 374: ep_len:520 episode reward: total was -33.130000. running mean: -34.747001\n",
      "ep 374: ep_len:580 episode reward: total was -45.100000. running mean: -34.850531\n",
      "epsilon:0.348812 episode_count: 2625. steps_count: 1185490.000000\n",
      "ep 375: ep_len:590 episode reward: total was -60.140000. running mean: -35.103425\n",
      "ep 375: ep_len:555 episode reward: total was -63.170000. running mean: -35.384091\n",
      "ep 375: ep_len:625 episode reward: total was -95.240000. running mean: -35.982650\n",
      "ep 375: ep_len:526 episode reward: total was -50.070000. running mean: -36.123524\n",
      "ep 375: ep_len:3 episode reward: total was 0.000000. running mean: -35.762289\n",
      "ep 375: ep_len:575 episode reward: total was -43.000000. running mean: -35.834666\n",
      "ep 375: ep_len:560 episode reward: total was -36.670000. running mean: -35.843019\n",
      "epsilon:0.348676 episode_count: 2632. steps_count: 1188924.000000\n",
      "ep 376: ep_len:510 episode reward: total was -42.120000. running mean: -35.905789\n",
      "ep 376: ep_len:186 episode reward: total was -9.950000. running mean: -35.646231\n",
      "ep 376: ep_len:625 episode reward: total was -37.590000. running mean: -35.665669\n",
      "ep 376: ep_len:500 episode reward: total was -27.690000. running mean: -35.585912\n",
      "ep 376: ep_len:3 episode reward: total was 0.000000. running mean: -35.230053\n",
      "ep 376: ep_len:535 episode reward: total was -45.170000. running mean: -35.329452\n",
      "ep 376: ep_len:288 episode reward: total was -23.900000. running mean: -35.215158\n",
      "epsilon:0.348539 episode_count: 2639. steps_count: 1191571.000000\n",
      "ep 377: ep_len:640 episode reward: total was -38.370000. running mean: -35.246706\n",
      "ep 377: ep_len:545 episode reward: total was -60.220000. running mean: -35.496439\n",
      "ep 377: ep_len:850 episode reward: total was -57.720000. running mean: -35.718675\n",
      "ep 377: ep_len:535 episode reward: total was -40.790000. running mean: -35.769388\n",
      "ep 377: ep_len:3 episode reward: total was 0.000000. running mean: -35.411694\n",
      "ep 377: ep_len:700 episode reward: total was -46.880000. running mean: -35.526377\n",
      "ep 377: ep_len:550 episode reward: total was -32.500000. running mean: -35.496113\n",
      "epsilon:0.348403 episode_count: 2646. steps_count: 1195394.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 378: ep_len:600 episode reward: total was -36.340000. running mean: -35.504552\n",
      "ep 378: ep_len:500 episode reward: total was -52.830000. running mean: -35.677807\n",
      "ep 378: ep_len:505 episode reward: total was -24.210000. running mean: -35.563129\n",
      "ep 378: ep_len:535 episode reward: total was -30.720000. running mean: -35.514697\n",
      "ep 378: ep_len:3 episode reward: total was 0.000000. running mean: -35.159550\n",
      "ep 378: ep_len:635 episode reward: total was -41.250000. running mean: -35.220455\n",
      "ep 378: ep_len:660 episode reward: total was -42.410000. running mean: -35.292350\n",
      "epsilon:0.348266 episode_count: 2653. steps_count: 1198832.000000\n",
      "ep 379: ep_len:595 episode reward: total was -38.650000. running mean: -35.325927\n",
      "ep 379: ep_len:201 episode reward: total was -15.460000. running mean: -35.127268\n",
      "ep 379: ep_len:610 episode reward: total was -44.680000. running mean: -35.222795\n",
      "ep 379: ep_len:500 episode reward: total was -42.180000. running mean: -35.292367\n",
      "ep 379: ep_len:3 episode reward: total was 0.000000. running mean: -34.939443\n",
      "ep 379: ep_len:237 episode reward: total was -29.880000. running mean: -34.888849\n",
      "ep 379: ep_len:520 episode reward: total was -32.530000. running mean: -34.865260\n",
      "epsilon:0.348130 episode_count: 2660. steps_count: 1201498.000000\n",
      "ep 380: ep_len:500 episode reward: total was -27.250000. running mean: -34.789108\n",
      "ep 380: ep_len:530 episode reward: total was -27.810000. running mean: -34.719317\n",
      "ep 380: ep_len:615 episode reward: total was -34.750000. running mean: -34.719624\n",
      "ep 380: ep_len:545 episode reward: total was -29.090000. running mean: -34.663327\n",
      "ep 380: ep_len:3 episode reward: total was 0.000000. running mean: -34.316694\n",
      "ep 380: ep_len:323 episode reward: total was -22.460000. running mean: -34.198127\n",
      "ep 380: ep_len:695 episode reward: total was -55.460000. running mean: -34.410746\n",
      "epsilon:0.347993 episode_count: 2667. steps_count: 1204709.000000\n",
      "ep 381: ep_len:595 episode reward: total was -41.630000. running mean: -34.482938\n",
      "ep 381: ep_len:505 episode reward: total was -43.810000. running mean: -34.576209\n",
      "ep 381: ep_len:825 episode reward: total was -89.660000. running mean: -35.127047\n",
      "ep 381: ep_len:530 episode reward: total was -41.760000. running mean: -35.193376\n",
      "ep 381: ep_len:3 episode reward: total was 0.000000. running mean: -34.841443\n",
      "ep 381: ep_len:515 episode reward: total was -31.260000. running mean: -34.805628\n",
      "ep 381: ep_len:560 episode reward: total was -27.990000. running mean: -34.737472\n",
      "epsilon:0.347857 episode_count: 2674. steps_count: 1208242.000000\n",
      "ep 382: ep_len:520 episode reward: total was -48.600000. running mean: -34.876097\n",
      "ep 382: ep_len:500 episode reward: total was -14.920000. running mean: -34.676536\n",
      "ep 382: ep_len:505 episode reward: total was -39.020000. running mean: -34.719971\n",
      "ep 382: ep_len:560 episode reward: total was -39.780000. running mean: -34.770571\n",
      "ep 382: ep_len:48 episode reward: total was 3.000000. running mean: -34.392865\n",
      "ep 382: ep_len:535 episode reward: total was -48.570000. running mean: -34.534637\n",
      "ep 382: ep_len:189 episode reward: total was -13.410000. running mean: -34.323390\n",
      "epsilon:0.347720 episode_count: 2681. steps_count: 1211099.000000\n",
      "ep 383: ep_len:540 episode reward: total was -34.130000. running mean: -34.321457\n",
      "ep 383: ep_len:635 episode reward: total was -22.640000. running mean: -34.204642\n",
      "ep 383: ep_len:705 episode reward: total was -56.510000. running mean: -34.427696\n",
      "ep 383: ep_len:510 episode reward: total was -33.600000. running mean: -34.419419\n",
      "ep 383: ep_len:3 episode reward: total was 0.000000. running mean: -34.075224\n",
      "ep 383: ep_len:515 episode reward: total was -36.170000. running mean: -34.096172\n",
      "ep 383: ep_len:342 episode reward: total was -25.410000. running mean: -34.009310\n",
      "epsilon:0.347584 episode_count: 2688. steps_count: 1214349.000000\n",
      "ep 384: ep_len:515 episode reward: total was -47.060000. running mean: -34.139817\n",
      "ep 384: ep_len:359 episode reward: total was -45.360000. running mean: -34.252019\n",
      "ep 384: ep_len:570 episode reward: total was -28.690000. running mean: -34.196399\n",
      "ep 384: ep_len:500 episode reward: total was -18.680000. running mean: -34.041235\n",
      "ep 384: ep_len:3 episode reward: total was 0.000000. running mean: -33.700823\n",
      "ep 384: ep_len:520 episode reward: total was -39.280000. running mean: -33.756614\n",
      "ep 384: ep_len:565 episode reward: total was -46.770000. running mean: -33.886748\n",
      "epsilon:0.347447 episode_count: 2695. steps_count: 1217381.000000\n",
      "ep 385: ep_len:500 episode reward: total was -51.820000. running mean: -34.066081\n",
      "ep 385: ep_len:540 episode reward: total was -47.850000. running mean: -34.203920\n",
      "ep 385: ep_len:368 episode reward: total was -23.870000. running mean: -34.100581\n",
      "ep 385: ep_len:500 episode reward: total was -24.150000. running mean: -34.001075\n",
      "ep 385: ep_len:56 episode reward: total was -4.500000. running mean: -33.706064\n",
      "ep 385: ep_len:630 episode reward: total was -40.230000. running mean: -33.771304\n",
      "ep 385: ep_len:595 episode reward: total was -60.020000. running mean: -34.033791\n",
      "epsilon:0.347311 episode_count: 2702. steps_count: 1220570.000000\n",
      "ep 386: ep_len:530 episode reward: total was -38.680000. running mean: -34.080253\n",
      "ep 386: ep_len:520 episode reward: total was -38.610000. running mean: -34.125550\n",
      "ep 386: ep_len:590 episode reward: total was -37.430000. running mean: -34.158595\n",
      "ep 386: ep_len:394 episode reward: total was -31.200000. running mean: -34.129009\n",
      "ep 386: ep_len:3 episode reward: total was 0.000000. running mean: -33.787719\n",
      "ep 386: ep_len:500 episode reward: total was -24.260000. running mean: -33.692441\n",
      "ep 386: ep_len:635 episode reward: total was -58.620000. running mean: -33.941717\n",
      "epsilon:0.347174 episode_count: 2709. steps_count: 1223742.000000\n",
      "ep 387: ep_len:1055 episode reward: total was -126.700000. running mean: -34.869300\n",
      "ep 387: ep_len:194 episode reward: total was -16.920000. running mean: -34.689807\n",
      "ep 387: ep_len:580 episode reward: total was -31.760000. running mean: -34.660509\n",
      "ep 387: ep_len:402 episode reward: total was -23.730000. running mean: -34.551204\n",
      "ep 387: ep_len:95 episode reward: total was -13.950000. running mean: -34.345192\n",
      "ep 387: ep_len:735 episode reward: total was -50.910000. running mean: -34.510840\n",
      "ep 387: ep_len:525 episode reward: total was -37.220000. running mean: -34.537931\n",
      "epsilon:0.347038 episode_count: 2716. steps_count: 1227328.000000\n",
      "ep 388: ep_len:570 episode reward: total was -43.690000. running mean: -34.629452\n",
      "ep 388: ep_len:500 episode reward: total was -44.920000. running mean: -34.732357\n",
      "ep 388: ep_len:565 episode reward: total was -29.110000. running mean: -34.676134\n",
      "ep 388: ep_len:414 episode reward: total was -35.260000. running mean: -34.681973\n",
      "ep 388: ep_len:53 episode reward: total was -5.000000. running mean: -34.385153\n",
      "ep 388: ep_len:505 episode reward: total was -25.750000. running mean: -34.298801\n",
      "ep 388: ep_len:590 episode reward: total was -47.750000. running mean: -34.433313\n",
      "epsilon:0.346901 episode_count: 2723. steps_count: 1230525.000000\n",
      "ep 389: ep_len:505 episode reward: total was -41.320000. running mean: -34.502180\n",
      "ep 389: ep_len:595 episode reward: total was -30.750000. running mean: -34.464658\n",
      "ep 389: ep_len:585 episode reward: total was -31.130000. running mean: -34.431312\n",
      "ep 389: ep_len:525 episode reward: total was -18.770000. running mean: -34.274699\n",
      "ep 389: ep_len:95 episode reward: total was -13.950000. running mean: -34.071452\n",
      "ep 389: ep_len:620 episode reward: total was -50.650000. running mean: -34.237237\n",
      "ep 389: ep_len:500 episode reward: total was -58.880000. running mean: -34.483665\n",
      "epsilon:0.346765 episode_count: 2730. steps_count: 1233950.000000\n",
      "ep 390: ep_len:505 episode reward: total was -18.210000. running mean: -34.320928\n",
      "ep 390: ep_len:575 episode reward: total was -61.560000. running mean: -34.593319\n",
      "ep 390: ep_len:418 episode reward: total was -18.820000. running mean: -34.435586\n",
      "ep 390: ep_len:510 episode reward: total was -43.840000. running mean: -34.529630\n",
      "ep 390: ep_len:94 episode reward: total was -3.970000. running mean: -34.224034\n",
      "ep 390: ep_len:157 episode reward: total was -2.950000. running mean: -33.911293\n",
      "ep 390: ep_len:555 episode reward: total was -57.130000. running mean: -34.143480\n",
      "epsilon:0.346628 episode_count: 2737. steps_count: 1236764.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 391: ep_len:197 episode reward: total was -14.460000. running mean: -33.946645\n",
      "ep 391: ep_len:590 episode reward: total was -50.730000. running mean: -34.114479\n",
      "ep 391: ep_len:550 episode reward: total was -37.760000. running mean: -34.150934\n",
      "ep 391: ep_len:56 episode reward: total was -5.480000. running mean: -33.864225\n",
      "ep 391: ep_len:3 episode reward: total was 0.000000. running mean: -33.525583\n",
      "ep 391: ep_len:725 episode reward: total was -55.520000. running mean: -33.745527\n",
      "ep 391: ep_len:341 episode reward: total was -24.850000. running mean: -33.656572\n",
      "epsilon:0.346492 episode_count: 2744. steps_count: 1239226.000000\n",
      "ep 392: ep_len:224 episode reward: total was -5.430000. running mean: -33.374306\n",
      "ep 392: ep_len:540 episode reward: total was -32.880000. running mean: -33.369363\n",
      "ep 392: ep_len:575 episode reward: total was -34.600000. running mean: -33.381669\n",
      "ep 392: ep_len:56 episode reward: total was -0.950000. running mean: -33.057352\n",
      "ep 392: ep_len:3 episode reward: total was 0.000000. running mean: -32.726779\n",
      "ep 392: ep_len:595 episode reward: total was -52.230000. running mean: -32.921811\n",
      "ep 392: ep_len:515 episode reward: total was -54.120000. running mean: -33.133793\n",
      "epsilon:0.346355 episode_count: 2751. steps_count: 1241734.000000\n",
      "ep 393: ep_len:605 episode reward: total was -58.500000. running mean: -33.387455\n",
      "ep 393: ep_len:500 episode reward: total was -25.400000. running mean: -33.307581\n",
      "ep 393: ep_len:550 episode reward: total was -23.220000. running mean: -33.206705\n",
      "ep 393: ep_len:520 episode reward: total was -26.750000. running mean: -33.142138\n",
      "ep 393: ep_len:3 episode reward: total was 0.000000. running mean: -32.810716\n",
      "ep 393: ep_len:530 episode reward: total was -57.180000. running mean: -33.054409\n",
      "ep 393: ep_len:625 episode reward: total was -42.470000. running mean: -33.148565\n",
      "epsilon:0.346219 episode_count: 2758. steps_count: 1245067.000000\n",
      "ep 394: ep_len:500 episode reward: total was -34.780000. running mean: -33.164879\n",
      "ep 394: ep_len:500 episode reward: total was -36.750000. running mean: -33.200731\n",
      "ep 394: ep_len:595 episode reward: total was -31.620000. running mean: -33.184923\n",
      "ep 394: ep_len:56 episode reward: total was -0.460000. running mean: -32.857674\n",
      "ep 394: ep_len:87 episode reward: total was -12.960000. running mean: -32.658697\n",
      "ep 394: ep_len:620 episode reward: total was -49.200000. running mean: -32.824110\n",
      "ep 394: ep_len:535 episode reward: total was -32.470000. running mean: -32.820569\n",
      "epsilon:0.346082 episode_count: 2765. steps_count: 1247960.000000\n",
      "ep 395: ep_len:213 episode reward: total was -12.380000. running mean: -32.616164\n",
      "ep 395: ep_len:505 episode reward: total was -39.190000. running mean: -32.681902\n",
      "ep 395: ep_len:535 episode reward: total was -37.830000. running mean: -32.733383\n",
      "ep 395: ep_len:860 episode reward: total was -98.070000. running mean: -33.386749\n",
      "ep 395: ep_len:3 episode reward: total was 0.000000. running mean: -33.052882\n",
      "ep 395: ep_len:760 episode reward: total was -60.870000. running mean: -33.331053\n",
      "ep 395: ep_len:565 episode reward: total was -41.150000. running mean: -33.409242\n",
      "epsilon:0.345946 episode_count: 2772. steps_count: 1251401.000000\n",
      "ep 396: ep_len:540 episode reward: total was -29.110000. running mean: -33.366250\n",
      "ep 396: ep_len:595 episode reward: total was -40.290000. running mean: -33.435487\n",
      "ep 396: ep_len:395 episode reward: total was -42.930000. running mean: -33.530432\n",
      "ep 396: ep_len:515 episode reward: total was -23.730000. running mean: -33.432428\n",
      "ep 396: ep_len:3 episode reward: total was 0.000000. running mean: -33.098104\n",
      "ep 396: ep_len:610 episode reward: total was -66.020000. running mean: -33.427323\n",
      "ep 396: ep_len:630 episode reward: total was -22.380000. running mean: -33.316850\n",
      "epsilon:0.345809 episode_count: 2779. steps_count: 1254689.000000\n",
      "ep 397: ep_len:665 episode reward: total was -58.870000. running mean: -33.572381\n",
      "ep 397: ep_len:550 episode reward: total was -38.750000. running mean: -33.624157\n",
      "ep 397: ep_len:545 episode reward: total was -42.060000. running mean: -33.708516\n",
      "ep 397: ep_len:505 episode reward: total was -39.760000. running mean: -33.769031\n",
      "ep 397: ep_len:88 episode reward: total was -6.950000. running mean: -33.500840\n",
      "ep 397: ep_len:500 episode reward: total was -54.810000. running mean: -33.713932\n",
      "ep 397: ep_len:515 episode reward: total was -36.000000. running mean: -33.736792\n",
      "epsilon:0.345673 episode_count: 2786. steps_count: 1258057.000000\n",
      "ep 398: ep_len:525 episode reward: total was -46.130000. running mean: -33.860725\n",
      "ep 398: ep_len:550 episode reward: total was -55.630000. running mean: -34.078417\n",
      "ep 398: ep_len:645 episode reward: total was -55.640000. running mean: -34.294033\n",
      "ep 398: ep_len:500 episode reward: total was -19.240000. running mean: -34.143493\n",
      "ep 398: ep_len:3 episode reward: total was 0.000000. running mean: -33.802058\n",
      "ep 398: ep_len:605 episode reward: total was -53.050000. running mean: -33.994537\n",
      "ep 398: ep_len:595 episode reward: total was -59.580000. running mean: -34.250392\n",
      "epsilon:0.345536 episode_count: 2793. steps_count: 1261480.000000\n",
      "ep 399: ep_len:625 episode reward: total was -71.590000. running mean: -34.623788\n",
      "ep 399: ep_len:605 episode reward: total was -47.260000. running mean: -34.750150\n",
      "ep 399: ep_len:590 episode reward: total was -39.430000. running mean: -34.796949\n",
      "ep 399: ep_len:520 episode reward: total was -28.680000. running mean: -34.735779\n",
      "ep 399: ep_len:3 episode reward: total was 0.000000. running mean: -34.388421\n",
      "ep 399: ep_len:206 episode reward: total was -5.440000. running mean: -34.098937\n",
      "ep 399: ep_len:605 episode reward: total was -43.980000. running mean: -34.197748\n",
      "epsilon:0.345400 episode_count: 2800. steps_count: 1264634.000000\n",
      "ep 400: ep_len:735 episode reward: total was -67.200000. running mean: -34.527770\n",
      "ep 400: ep_len:710 episode reward: total was -59.040000. running mean: -34.772893\n",
      "ep 400: ep_len:500 episode reward: total was -28.890000. running mean: -34.714064\n",
      "ep 400: ep_len:540 episode reward: total was -24.720000. running mean: -34.614123\n",
      "ep 400: ep_len:3 episode reward: total was 0.000000. running mean: -34.267982\n",
      "ep 400: ep_len:580 episode reward: total was -58.320000. running mean: -34.508502\n",
      "ep 400: ep_len:630 episode reward: total was -51.160000. running mean: -34.675017\n",
      "epsilon:0.345263 episode_count: 2807. steps_count: 1268332.000000\n",
      "ep 401: ep_len:525 episode reward: total was -48.790000. running mean: -34.816167\n",
      "ep 401: ep_len:515 episode reward: total was -18.740000. running mean: -34.655405\n",
      "ep 401: ep_len:65 episode reward: total was -3.960000. running mean: -34.348451\n",
      "ep 401: ep_len:615 episode reward: total was -67.740000. running mean: -34.682367\n",
      "ep 401: ep_len:92 episode reward: total was -4.980000. running mean: -34.385343\n",
      "ep 401: ep_len:520 episode reward: total was -54.620000. running mean: -34.587689\n",
      "ep 401: ep_len:193 episode reward: total was -19.950000. running mean: -34.441313\n",
      "epsilon:0.345127 episode_count: 2814. steps_count: 1270857.000000\n",
      "ep 402: ep_len:515 episode reward: total was -39.170000. running mean: -34.488599\n",
      "ep 402: ep_len:640 episode reward: total was -21.100000. running mean: -34.354713\n",
      "ep 402: ep_len:580 episode reward: total was -51.570000. running mean: -34.526866\n",
      "ep 402: ep_len:638 episode reward: total was -62.570000. running mean: -34.807298\n",
      "ep 402: ep_len:3 episode reward: total was 0.000000. running mean: -34.459225\n",
      "ep 402: ep_len:500 episode reward: total was -36.380000. running mean: -34.478432\n",
      "ep 402: ep_len:186 episode reward: total was -17.950000. running mean: -34.313148\n",
      "epsilon:0.344990 episode_count: 2821. steps_count: 1273919.000000\n",
      "ep 403: ep_len:580 episode reward: total was -25.210000. running mean: -34.222117\n",
      "ep 403: ep_len:500 episode reward: total was -33.430000. running mean: -34.214195\n",
      "ep 403: ep_len:540 episode reward: total was -31.220000. running mean: -34.184254\n",
      "ep 403: ep_len:520 episode reward: total was -23.660000. running mean: -34.079011\n",
      "ep 403: ep_len:3 episode reward: total was 0.000000. running mean: -33.738221\n",
      "ep 403: ep_len:500 episode reward: total was -23.080000. running mean: -33.631639\n",
      "ep 403: ep_len:545 episode reward: total was -48.820000. running mean: -33.783522\n",
      "epsilon:0.344854 episode_count: 2828. steps_count: 1277107.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 404: ep_len:595 episode reward: total was -31.730000. running mean: -33.762987\n",
      "ep 404: ep_len:500 episode reward: total was -27.440000. running mean: -33.699757\n",
      "ep 404: ep_len:625 episode reward: total was -33.100000. running mean: -33.693760\n",
      "ep 404: ep_len:570 episode reward: total was -58.710000. running mean: -33.943922\n",
      "ep 404: ep_len:126 episode reward: total was -5.970000. running mean: -33.664183\n",
      "ep 404: ep_len:555 episode reward: total was -61.760000. running mean: -33.945141\n",
      "ep 404: ep_len:605 episode reward: total was -39.570000. running mean: -34.001390\n",
      "epsilon:0.344717 episode_count: 2835. steps_count: 1280683.000000\n",
      "ep 405: ep_len:215 episode reward: total was -20.940000. running mean: -33.870776\n",
      "ep 405: ep_len:500 episode reward: total was -50.240000. running mean: -34.034468\n",
      "ep 405: ep_len:675 episode reward: total was -57.580000. running mean: -34.269923\n",
      "ep 405: ep_len:106 episode reward: total was -3.960000. running mean: -33.966824\n",
      "ep 405: ep_len:3 episode reward: total was 0.000000. running mean: -33.627156\n",
      "ep 405: ep_len:500 episode reward: total was -38.220000. running mean: -33.673084\n",
      "ep 405: ep_len:284 episode reward: total was -27.900000. running mean: -33.615353\n",
      "epsilon:0.344581 episode_count: 2842. steps_count: 1282966.000000\n",
      "ep 406: ep_len:590 episode reward: total was -41.260000. running mean: -33.691800\n",
      "ep 406: ep_len:896 episode reward: total was -96.040000. running mean: -34.315282\n",
      "ep 406: ep_len:426 episode reward: total was -30.350000. running mean: -34.275629\n",
      "ep 406: ep_len:550 episode reward: total was -35.520000. running mean: -34.288073\n",
      "ep 406: ep_len:3 episode reward: total was 0.000000. running mean: -33.945192\n",
      "ep 406: ep_len:500 episode reward: total was -66.880000. running mean: -34.274540\n",
      "ep 406: ep_len:187 episode reward: total was -24.920000. running mean: -34.180995\n",
      "epsilon:0.344444 episode_count: 2849. steps_count: 1286118.000000\n",
      "ep 407: ep_len:575 episode reward: total was -39.250000. running mean: -34.231685\n",
      "ep 407: ep_len:500 episode reward: total was -47.700000. running mean: -34.366368\n",
      "ep 407: ep_len:720 episode reward: total was -81.760000. running mean: -34.840304\n",
      "ep 407: ep_len:555 episode reward: total was -25.220000. running mean: -34.744101\n",
      "ep 407: ep_len:3 episode reward: total was 0.000000. running mean: -34.396660\n",
      "ep 407: ep_len:500 episode reward: total was -46.740000. running mean: -34.520094\n",
      "ep 407: ep_len:600 episode reward: total was -70.730000. running mean: -34.882193\n",
      "epsilon:0.344308 episode_count: 2856. steps_count: 1289571.000000\n",
      "ep 408: ep_len:515 episode reward: total was -40.260000. running mean: -34.935971\n",
      "ep 408: ep_len:565 episode reward: total was -33.230000. running mean: -34.918911\n",
      "ep 408: ep_len:382 episode reward: total was -8.330000. running mean: -34.653022\n",
      "ep 408: ep_len:875 episode reward: total was -131.780000. running mean: -35.624292\n",
      "ep 408: ep_len:104 episode reward: total was -10.450000. running mean: -35.372549\n",
      "ep 408: ep_len:540 episode reward: total was -39.630000. running mean: -35.415123\n",
      "ep 408: ep_len:575 episode reward: total was -32.600000. running mean: -35.386972\n",
      "epsilon:0.344171 episode_count: 2863. steps_count: 1293127.000000\n",
      "ep 409: ep_len:750 episode reward: total was -56.410000. running mean: -35.597202\n",
      "ep 409: ep_len:635 episode reward: total was -58.610000. running mean: -35.827330\n",
      "ep 409: ep_len:545 episode reward: total was -55.370000. running mean: -36.022757\n",
      "ep 409: ep_len:49 episode reward: total was 2.550000. running mean: -35.637029\n",
      "ep 409: ep_len:3 episode reward: total was 0.000000. running mean: -35.280659\n",
      "ep 409: ep_len:530 episode reward: total was -27.510000. running mean: -35.202952\n",
      "ep 409: ep_len:610 episode reward: total was -45.990000. running mean: -35.310823\n",
      "epsilon:0.344035 episode_count: 2870. steps_count: 1296249.000000\n",
      "ep 410: ep_len:120 episode reward: total was -7.460000. running mean: -35.032315\n",
      "ep 410: ep_len:540 episode reward: total was -27.750000. running mean: -34.959492\n",
      "ep 410: ep_len:540 episode reward: total was -28.750000. running mean: -34.897397\n",
      "ep 410: ep_len:530 episode reward: total was -30.660000. running mean: -34.855023\n",
      "ep 410: ep_len:3 episode reward: total was 0.000000. running mean: -34.506472\n",
      "ep 410: ep_len:545 episode reward: total was -29.100000. running mean: -34.452408\n",
      "ep 410: ep_len:575 episode reward: total was -43.280000. running mean: -34.540684\n",
      "epsilon:0.343898 episode_count: 2877. steps_count: 1299102.000000\n",
      "ep 411: ep_len:580 episode reward: total was -52.490000. running mean: -34.720177\n",
      "ep 411: ep_len:500 episode reward: total was -29.680000. running mean: -34.669775\n",
      "ep 411: ep_len:610 episode reward: total was -71.390000. running mean: -35.036977\n",
      "ep 411: ep_len:525 episode reward: total was -46.190000. running mean: -35.148508\n",
      "ep 411: ep_len:112 episode reward: total was -11.460000. running mean: -34.911622\n",
      "ep 411: ep_len:610 episode reward: total was -42.230000. running mean: -34.984806\n",
      "ep 411: ep_len:291 episode reward: total was -25.330000. running mean: -34.888258\n",
      "epsilon:0.343762 episode_count: 2884. steps_count: 1302330.000000\n",
      "ep 412: ep_len:545 episode reward: total was -32.650000. running mean: -34.865876\n",
      "ep 412: ep_len:515 episode reward: total was -36.290000. running mean: -34.880117\n",
      "ep 412: ep_len:545 episode reward: total was -27.180000. running mean: -34.803116\n",
      "ep 412: ep_len:536 episode reward: total was -49.140000. running mean: -34.946485\n",
      "ep 412: ep_len:63 episode reward: total was -10.990000. running mean: -34.706920\n",
      "ep 412: ep_len:675 episode reward: total was -22.900000. running mean: -34.588850\n",
      "ep 412: ep_len:297 episode reward: total was -26.340000. running mean: -34.506362\n",
      "epsilon:0.343625 episode_count: 2891. steps_count: 1305506.000000\n",
      "ep 413: ep_len:550 episode reward: total was -58.720000. running mean: -34.748498\n",
      "ep 413: ep_len:500 episode reward: total was -40.240000. running mean: -34.803413\n",
      "ep 413: ep_len:590 episode reward: total was -63.560000. running mean: -35.090979\n",
      "ep 413: ep_len:116 episode reward: total was -6.420000. running mean: -34.804269\n",
      "ep 413: ep_len:3 episode reward: total was 0.000000. running mean: -34.456227\n",
      "ep 413: ep_len:560 episode reward: total was -58.700000. running mean: -34.698664\n",
      "ep 413: ep_len:520 episode reward: total was -43.060000. running mean: -34.782278\n",
      "epsilon:0.343489 episode_count: 2898. steps_count: 1308345.000000\n",
      "ep 414: ep_len:600 episode reward: total was -36.700000. running mean: -34.801455\n",
      "ep 414: ep_len:650 episode reward: total was -39.410000. running mean: -34.847541\n",
      "ep 414: ep_len:555 episode reward: total was -40.750000. running mean: -34.906565\n",
      "ep 414: ep_len:540 episode reward: total was -52.220000. running mean: -35.079699\n",
      "ep 414: ep_len:3 episode reward: total was 0.000000. running mean: -34.728902\n",
      "ep 414: ep_len:545 episode reward: total was -42.190000. running mean: -34.803513\n",
      "ep 414: ep_len:610 episode reward: total was -39.880000. running mean: -34.854278\n",
      "epsilon:0.343352 episode_count: 2905. steps_count: 1311848.000000\n",
      "ep 415: ep_len:560 episode reward: total was -55.380000. running mean: -35.059536\n",
      "ep 415: ep_len:194 episode reward: total was -19.460000. running mean: -34.903540\n",
      "ep 415: ep_len:835 episode reward: total was -78.870000. running mean: -35.343205\n",
      "ep 415: ep_len:500 episode reward: total was -27.240000. running mean: -35.262173\n",
      "ep 415: ep_len:100 episode reward: total was -0.950000. running mean: -34.919051\n",
      "ep 415: ep_len:750 episode reward: total was -73.440000. running mean: -35.304260\n",
      "ep 415: ep_len:300 episode reward: total was -19.380000. running mean: -35.145018\n",
      "epsilon:0.343216 episode_count: 2912. steps_count: 1315087.000000\n",
      "ep 416: ep_len:520 episode reward: total was -33.650000. running mean: -35.130068\n",
      "ep 416: ep_len:620 episode reward: total was -40.510000. running mean: -35.183867\n",
      "ep 416: ep_len:685 episode reward: total was -72.040000. running mean: -35.552428\n",
      "ep 416: ep_len:510 episode reward: total was -31.350000. running mean: -35.510404\n",
      "ep 416: ep_len:3 episode reward: total was 0.000000. running mean: -35.155300\n",
      "ep 416: ep_len:560 episode reward: total was -55.300000. running mean: -35.356747\n",
      "ep 416: ep_len:820 episode reward: total was -77.410000. running mean: -35.777280\n",
      "epsilon:0.343079 episode_count: 2919. steps_count: 1318805.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 417: ep_len:535 episode reward: total was -43.720000. running mean: -35.856707\n",
      "ep 417: ep_len:520 episode reward: total was -55.200000. running mean: -36.050140\n",
      "ep 417: ep_len:585 episode reward: total was -49.630000. running mean: -36.185938\n",
      "ep 417: ep_len:590 episode reward: total was -40.140000. running mean: -36.225479\n",
      "ep 417: ep_len:3 episode reward: total was 0.000000. running mean: -35.863224\n",
      "ep 417: ep_len:228 episode reward: total was -10.450000. running mean: -35.609092\n",
      "ep 417: ep_len:500 episode reward: total was -29.530000. running mean: -35.548301\n",
      "epsilon:0.342943 episode_count: 2926. steps_count: 1321766.000000\n",
      "ep 418: ep_len:650 episode reward: total was -61.060000. running mean: -35.803418\n",
      "ep 418: ep_len:525 episode reward: total was -33.610000. running mean: -35.781484\n",
      "ep 418: ep_len:510 episode reward: total was -36.770000. running mean: -35.791369\n",
      "ep 418: ep_len:560 episode reward: total was -28.240000. running mean: -35.715855\n",
      "ep 418: ep_len:3 episode reward: total was 0.000000. running mean: -35.358697\n",
      "ep 418: ep_len:500 episode reward: total was -25.880000. running mean: -35.263910\n",
      "ep 418: ep_len:560 episode reward: total was -31.720000. running mean: -35.228471\n",
      "epsilon:0.342806 episode_count: 2933. steps_count: 1325074.000000\n",
      "ep 419: ep_len:585 episode reward: total was -27.200000. running mean: -35.148186\n",
      "ep 419: ep_len:625 episode reward: total was -65.130000. running mean: -35.448004\n",
      "ep 419: ep_len:595 episode reward: total was -50.960000. running mean: -35.603124\n",
      "ep 419: ep_len:600 episode reward: total was -26.590000. running mean: -35.512993\n",
      "ep 419: ep_len:106 episode reward: total was -2.960000. running mean: -35.187463\n",
      "ep 419: ep_len:326 episode reward: total was -28.310000. running mean: -35.118688\n",
      "ep 419: ep_len:194 episode reward: total was -13.950000. running mean: -34.907001\n",
      "epsilon:0.342670 episode_count: 2940. steps_count: 1328105.000000\n",
      "ep 420: ep_len:122 episode reward: total was -3.940000. running mean: -34.597331\n",
      "ep 420: ep_len:500 episode reward: total was -37.890000. running mean: -34.630258\n",
      "ep 420: ep_len:500 episode reward: total was -44.310000. running mean: -34.727055\n",
      "ep 420: ep_len:510 episode reward: total was -21.090000. running mean: -34.590685\n",
      "ep 420: ep_len:99 episode reward: total was -6.960000. running mean: -34.314378\n",
      "ep 420: ep_len:690 episode reward: total was -31.290000. running mean: -34.284134\n",
      "ep 420: ep_len:349 episode reward: total was -31.350000. running mean: -34.254793\n",
      "epsilon:0.342533 episode_count: 2947. steps_count: 1330875.000000\n",
      "ep 421: ep_len:670 episode reward: total was -80.040000. running mean: -34.712645\n",
      "ep 421: ep_len:575 episode reward: total was -37.660000. running mean: -34.742119\n",
      "ep 421: ep_len:69 episode reward: total was -4.480000. running mean: -34.439497\n",
      "ep 421: ep_len:590 episode reward: total was -35.140000. running mean: -34.446502\n",
      "ep 421: ep_len:3 episode reward: total was 0.000000. running mean: -34.102037\n",
      "ep 421: ep_len:650 episode reward: total was -72.550000. running mean: -34.486517\n",
      "ep 421: ep_len:545 episode reward: total was -40.990000. running mean: -34.551552\n",
      "epsilon:0.342397 episode_count: 2954. steps_count: 1333977.000000\n",
      "ep 422: ep_len:500 episode reward: total was -43.290000. running mean: -34.638936\n",
      "ep 422: ep_len:505 episode reward: total was -17.990000. running mean: -34.472447\n",
      "ep 422: ep_len:605 episode reward: total was -53.070000. running mean: -34.658422\n",
      "ep 422: ep_len:500 episode reward: total was -17.640000. running mean: -34.488238\n",
      "ep 422: ep_len:132 episode reward: total was -15.920000. running mean: -34.302556\n",
      "ep 422: ep_len:500 episode reward: total was -35.110000. running mean: -34.310630\n",
      "ep 422: ep_len:505 episode reward: total was -32.550000. running mean: -34.293024\n",
      "epsilon:0.342260 episode_count: 2961. steps_count: 1337224.000000\n",
      "ep 423: ep_len:770 episode reward: total was -85.570000. running mean: -34.805794\n",
      "ep 423: ep_len:500 episode reward: total was -35.730000. running mean: -34.815036\n",
      "ep 423: ep_len:660 episode reward: total was -43.120000. running mean: -34.898085\n",
      "ep 423: ep_len:510 episode reward: total was -43.140000. running mean: -34.980505\n",
      "ep 423: ep_len:3 episode reward: total was 0.000000. running mean: -34.630700\n",
      "ep 423: ep_len:625 episode reward: total was -34.190000. running mean: -34.626293\n",
      "ep 423: ep_len:620 episode reward: total was -65.200000. running mean: -34.932030\n",
      "epsilon:0.342124 episode_count: 2968. steps_count: 1340912.000000\n",
      "ep 424: ep_len:590 episode reward: total was -57.630000. running mean: -35.159009\n",
      "ep 424: ep_len:515 episode reward: total was -61.660000. running mean: -35.424019\n",
      "ep 424: ep_len:500 episode reward: total was -37.670000. running mean: -35.446479\n",
      "ep 424: ep_len:116 episode reward: total was -7.410000. running mean: -35.166114\n",
      "ep 424: ep_len:95 episode reward: total was -8.460000. running mean: -34.899053\n",
      "ep 424: ep_len:500 episode reward: total was -9.030000. running mean: -34.640363\n",
      "ep 424: ep_len:298 episode reward: total was -28.330000. running mean: -34.577259\n",
      "epsilon:0.341987 episode_count: 2975. steps_count: 1343526.000000\n",
      "ep 425: ep_len:565 episode reward: total was -43.110000. running mean: -34.662586\n",
      "ep 425: ep_len:575 episode reward: total was -51.140000. running mean: -34.827361\n",
      "ep 425: ep_len:615 episode reward: total was -61.520000. running mean: -35.094287\n",
      "ep 425: ep_len:510 episode reward: total was -48.070000. running mean: -35.224044\n",
      "ep 425: ep_len:3 episode reward: total was 0.000000. running mean: -34.871804\n",
      "ep 425: ep_len:785 episode reward: total was -82.520000. running mean: -35.348286\n",
      "ep 425: ep_len:535 episode reward: total was -32.990000. running mean: -35.324703\n",
      "epsilon:0.341851 episode_count: 2982. steps_count: 1347114.000000\n",
      "ep 426: ep_len:595 episode reward: total was -33.170000. running mean: -35.303156\n",
      "ep 426: ep_len:505 episode reward: total was -18.900000. running mean: -35.139124\n",
      "ep 426: ep_len:610 episode reward: total was -39.710000. running mean: -35.184833\n",
      "ep 426: ep_len:615 episode reward: total was -50.260000. running mean: -35.335585\n",
      "ep 426: ep_len:3 episode reward: total was 0.000000. running mean: -34.982229\n",
      "ep 426: ep_len:650 episode reward: total was -67.760000. running mean: -35.310006\n",
      "ep 426: ep_len:314 episode reward: total was -27.420000. running mean: -35.231106\n",
      "epsilon:0.341714 episode_count: 2989. steps_count: 1350406.000000\n",
      "ep 427: ep_len:610 episode reward: total was -55.780000. running mean: -35.436595\n",
      "ep 427: ep_len:660 episode reward: total was -74.140000. running mean: -35.823629\n",
      "ep 427: ep_len:550 episode reward: total was -56.530000. running mean: -36.030693\n",
      "ep 427: ep_len:545 episode reward: total was -40.060000. running mean: -36.070986\n",
      "ep 427: ep_len:3 episode reward: total was 0.000000. running mean: -35.710276\n",
      "ep 427: ep_len:655 episode reward: total was -44.190000. running mean: -35.795073\n",
      "ep 427: ep_len:650 episode reward: total was -69.550000. running mean: -36.132623\n",
      "epsilon:0.341578 episode_count: 2996. steps_count: 1354079.000000\n",
      "ep 428: ep_len:585 episode reward: total was -36.750000. running mean: -36.138797\n",
      "ep 428: ep_len:505 episode reward: total was -45.660000. running mean: -36.234009\n",
      "ep 428: ep_len:505 episode reward: total was -31.550000. running mean: -36.187168\n",
      "ep 428: ep_len:610 episode reward: total was -39.730000. running mean: -36.222597\n",
      "ep 428: ep_len:3 episode reward: total was 0.000000. running mean: -35.860371\n",
      "ep 428: ep_len:500 episode reward: total was -27.250000. running mean: -35.774267\n",
      "ep 428: ep_len:500 episode reward: total was -55.260000. running mean: -35.969124\n",
      "epsilon:0.341441 episode_count: 3003. steps_count: 1357287.000000\n",
      "ep 429: ep_len:505 episode reward: total was -32.400000. running mean: -35.933433\n",
      "ep 429: ep_len:265 episode reward: total was -32.850000. running mean: -35.902599\n",
      "ep 429: ep_len:457 episode reward: total was -20.330000. running mean: -35.746873\n",
      "ep 429: ep_len:525 episode reward: total was -49.810000. running mean: -35.887504\n",
      "ep 429: ep_len:55 episode reward: total was 2.010000. running mean: -35.508529\n",
      "ep 429: ep_len:690 episode reward: total was -46.440000. running mean: -35.617844\n",
      "ep 429: ep_len:605 episode reward: total was -40.180000. running mean: -35.663465\n",
      "epsilon:0.341305 episode_count: 3010. steps_count: 1360389.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 430: ep_len:605 episode reward: total was -26.750000. running mean: -35.574331\n",
      "ep 430: ep_len:520 episode reward: total was -36.490000. running mean: -35.583487\n",
      "ep 430: ep_len:404 episode reward: total was -17.790000. running mean: -35.405553\n",
      "ep 430: ep_len:565 episode reward: total was -32.220000. running mean: -35.373697\n",
      "ep 430: ep_len:3 episode reward: total was 0.000000. running mean: -35.019960\n",
      "ep 430: ep_len:560 episode reward: total was -57.810000. running mean: -35.247860\n",
      "ep 430: ep_len:540 episode reward: total was -60.630000. running mean: -35.501682\n",
      "epsilon:0.341168 episode_count: 3017. steps_count: 1363586.000000\n",
      "ep 431: ep_len:505 episode reward: total was -49.090000. running mean: -35.637565\n",
      "ep 431: ep_len:645 episode reward: total was -37.930000. running mean: -35.660489\n",
      "ep 431: ep_len:620 episode reward: total was -38.940000. running mean: -35.693284\n",
      "ep 431: ep_len:570 episode reward: total was -27.740000. running mean: -35.613752\n",
      "ep 431: ep_len:53 episode reward: total was -5.000000. running mean: -35.307614\n",
      "ep 431: ep_len:500 episode reward: total was -30.900000. running mean: -35.263538\n",
      "ep 431: ep_len:510 episode reward: total was -52.280000. running mean: -35.433703\n",
      "epsilon:0.341032 episode_count: 3024. steps_count: 1366989.000000\n",
      "ep 432: ep_len:500 episode reward: total was -30.730000. running mean: -35.386666\n",
      "ep 432: ep_len:555 episode reward: total was -66.220000. running mean: -35.694999\n",
      "ep 432: ep_len:640 episode reward: total was -36.560000. running mean: -35.703649\n",
      "ep 432: ep_len:535 episode reward: total was -37.020000. running mean: -35.716812\n",
      "ep 432: ep_len:3 episode reward: total was 0.000000. running mean: -35.359644\n",
      "ep 432: ep_len:515 episode reward: total was -51.350000. running mean: -35.519548\n",
      "ep 432: ep_len:590 episode reward: total was -59.530000. running mean: -35.759652\n",
      "epsilon:0.340895 episode_count: 3031. steps_count: 1370327.000000\n",
      "ep 433: ep_len:265 episode reward: total was -14.920000. running mean: -35.551256\n",
      "ep 433: ep_len:510 episode reward: total was -38.200000. running mean: -35.577743\n",
      "ep 433: ep_len:560 episode reward: total was -42.270000. running mean: -35.644666\n",
      "ep 433: ep_len:500 episode reward: total was -34.660000. running mean: -35.634819\n",
      "ep 433: ep_len:47 episode reward: total was 3.000000. running mean: -35.248471\n",
      "ep 433: ep_len:535 episode reward: total was -38.590000. running mean: -35.281886\n",
      "ep 433: ep_len:585 episode reward: total was -33.600000. running mean: -35.265067\n",
      "epsilon:0.340759 episode_count: 3038. steps_count: 1373329.000000\n",
      "ep 434: ep_len:550 episode reward: total was -27.130000. running mean: -35.183717\n",
      "ep 434: ep_len:510 episode reward: total was -18.930000. running mean: -35.021180\n",
      "ep 434: ep_len:550 episode reward: total was -28.020000. running mean: -34.951168\n",
      "ep 434: ep_len:376 episode reward: total was -45.790000. running mean: -35.059556\n",
      "ep 434: ep_len:105 episode reward: total was -3.480000. running mean: -34.743761\n",
      "ep 434: ep_len:520 episode reward: total was -35.780000. running mean: -34.754123\n",
      "ep 434: ep_len:605 episode reward: total was -34.460000. running mean: -34.751182\n",
      "epsilon:0.340622 episode_count: 3045. steps_count: 1376545.000000\n",
      "ep 435: ep_len:500 episode reward: total was -28.170000. running mean: -34.685370\n",
      "ep 435: ep_len:339 episode reward: total was -31.380000. running mean: -34.652316\n",
      "ep 435: ep_len:640 episode reward: total was -38.730000. running mean: -34.693093\n",
      "ep 435: ep_len:500 episode reward: total was -36.700000. running mean: -34.713162\n",
      "ep 435: ep_len:85 episode reward: total was -0.460000. running mean: -34.370631\n",
      "ep 435: ep_len:510 episode reward: total was -31.150000. running mean: -34.338424\n",
      "ep 435: ep_len:590 episode reward: total was -31.490000. running mean: -34.309940\n",
      "epsilon:0.340486 episode_count: 3052. steps_count: 1379709.000000\n",
      "ep 436: ep_len:605 episode reward: total was -21.180000. running mean: -34.178641\n",
      "ep 436: ep_len:161 episode reward: total was -4.920000. running mean: -33.886054\n",
      "ep 436: ep_len:375 episode reward: total was -4.340000. running mean: -33.590594\n",
      "ep 436: ep_len:500 episode reward: total was -29.660000. running mean: -33.551288\n",
      "ep 436: ep_len:3 episode reward: total was 0.000000. running mean: -33.215775\n",
      "ep 436: ep_len:610 episode reward: total was -40.250000. running mean: -33.286117\n",
      "ep 436: ep_len:510 episode reward: total was -49.810000. running mean: -33.451356\n",
      "epsilon:0.340349 episode_count: 3059. steps_count: 1382473.000000\n",
      "ep 437: ep_len:625 episode reward: total was -40.710000. running mean: -33.523942\n",
      "ep 437: ep_len:192 episode reward: total was -17.960000. running mean: -33.368303\n",
      "ep 437: ep_len:800 episode reward: total was -66.880000. running mean: -33.703420\n",
      "ep 437: ep_len:500 episode reward: total was -33.150000. running mean: -33.697886\n",
      "ep 437: ep_len:48 episode reward: total was 1.500000. running mean: -33.345907\n",
      "ep 437: ep_len:530 episode reward: total was -47.090000. running mean: -33.483348\n",
      "ep 437: ep_len:247 episode reward: total was -20.430000. running mean: -33.352814\n",
      "epsilon:0.340213 episode_count: 3066. steps_count: 1385415.000000\n",
      "ep 438: ep_len:130 episode reward: total was -13.930000. running mean: -33.158586\n",
      "ep 438: ep_len:359 episode reward: total was -34.430000. running mean: -33.171300\n",
      "ep 438: ep_len:590 episode reward: total was -39.740000. running mean: -33.236987\n",
      "ep 438: ep_len:510 episode reward: total was -21.140000. running mean: -33.116017\n",
      "ep 438: ep_len:3 episode reward: total was 0.000000. running mean: -32.784857\n",
      "ep 438: ep_len:560 episode reward: total was -50.210000. running mean: -32.959109\n",
      "ep 438: ep_len:610 episode reward: total was -32.460000. running mean: -32.954118\n",
      "epsilon:0.340076 episode_count: 3073. steps_count: 1388177.000000\n",
      "ep 439: ep_len:525 episode reward: total was -40.820000. running mean: -33.032776\n",
      "ep 439: ep_len:580 episode reward: total was -45.110000. running mean: -33.153549\n",
      "ep 439: ep_len:500 episode reward: total was -26.870000. running mean: -33.090713\n",
      "ep 439: ep_len:625 episode reward: total was -50.250000. running mean: -33.262306\n",
      "ep 439: ep_len:3 episode reward: total was 0.000000. running mean: -32.929683\n",
      "ep 439: ep_len:510 episode reward: total was -40.770000. running mean: -33.008086\n",
      "ep 439: ep_len:530 episode reward: total was -46.730000. running mean: -33.145305\n",
      "epsilon:0.339940 episode_count: 3080. steps_count: 1391450.000000\n",
      "ep 440: ep_len:218 episode reward: total was -13.900000. running mean: -32.952852\n",
      "ep 440: ep_len:500 episode reward: total was -41.300000. running mean: -33.036324\n",
      "ep 440: ep_len:875 episode reward: total was -98.500000. running mean: -33.690960\n",
      "ep 440: ep_len:605 episode reward: total was -38.180000. running mean: -33.735851\n",
      "ep 440: ep_len:3 episode reward: total was 0.000000. running mean: -33.398492\n",
      "ep 440: ep_len:580 episode reward: total was -57.570000. running mean: -33.640207\n",
      "ep 440: ep_len:520 episode reward: total was -40.170000. running mean: -33.705505\n",
      "epsilon:0.339803 episode_count: 3087. steps_count: 1394751.000000\n",
      "ep 441: ep_len:535 episode reward: total was -54.690000. running mean: -33.915350\n",
      "ep 441: ep_len:500 episode reward: total was -47.220000. running mean: -34.048397\n",
      "ep 441: ep_len:615 episode reward: total was -75.740000. running mean: -34.465313\n",
      "ep 441: ep_len:500 episode reward: total was -16.260000. running mean: -34.283260\n",
      "ep 441: ep_len:85 episode reward: total was -11.990000. running mean: -34.060327\n",
      "ep 441: ep_len:550 episode reward: total was -36.740000. running mean: -34.087124\n",
      "ep 441: ep_len:600 episode reward: total was -89.850000. running mean: -34.644753\n",
      "epsilon:0.339667 episode_count: 3094. steps_count: 1398136.000000\n",
      "ep 442: ep_len:590 episode reward: total was -28.630000. running mean: -34.584605\n",
      "ep 442: ep_len:605 episode reward: total was -34.290000. running mean: -34.581659\n",
      "ep 442: ep_len:79 episode reward: total was -2.990000. running mean: -34.265742\n",
      "ep 442: ep_len:515 episode reward: total was -58.800000. running mean: -34.511085\n",
      "ep 442: ep_len:3 episode reward: total was 0.000000. running mean: -34.165974\n",
      "ep 442: ep_len:318 episode reward: total was -27.430000. running mean: -34.098614\n",
      "ep 442: ep_len:282 episode reward: total was -19.910000. running mean: -33.956728\n",
      "epsilon:0.339530 episode_count: 3101. steps_count: 1400528.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 443: ep_len:500 episode reward: total was -30.560000. running mean: -33.922761\n",
      "ep 443: ep_len:505 episode reward: total was -42.650000. running mean: -34.010033\n",
      "ep 443: ep_len:590 episode reward: total was -44.720000. running mean: -34.117133\n",
      "ep 443: ep_len:500 episode reward: total was -46.680000. running mean: -34.242762\n",
      "ep 443: ep_len:113 episode reward: total was -6.470000. running mean: -33.965034\n",
      "ep 443: ep_len:750 episode reward: total was -64.500000. running mean: -34.270384\n",
      "ep 443: ep_len:515 episode reward: total was -15.920000. running mean: -34.086880\n",
      "epsilon:0.339394 episode_count: 3108. steps_count: 1404001.000000\n",
      "ep 444: ep_len:260 episode reward: total was -11.420000. running mean: -33.860211\n",
      "ep 444: ep_len:500 episode reward: total was -39.350000. running mean: -33.915109\n",
      "ep 444: ep_len:620 episode reward: total was -73.230000. running mean: -34.308258\n",
      "ep 444: ep_len:540 episode reward: total was -30.220000. running mean: -34.267375\n",
      "ep 444: ep_len:3 episode reward: total was 0.000000. running mean: -33.924702\n",
      "ep 444: ep_len:615 episode reward: total was -61.150000. running mean: -34.196955\n",
      "ep 444: ep_len:625 episode reward: total was -49.730000. running mean: -34.352285\n",
      "epsilon:0.339257 episode_count: 3115. steps_count: 1407164.000000\n",
      "ep 445: ep_len:108 episode reward: total was -2.980000. running mean: -34.038562\n",
      "ep 445: ep_len:620 episode reward: total was -40.520000. running mean: -34.103377\n",
      "ep 445: ep_len:500 episode reward: total was -28.790000. running mean: -34.050243\n",
      "ep 445: ep_len:500 episode reward: total was -39.710000. running mean: -34.106840\n",
      "ep 445: ep_len:118 episode reward: total was -2.970000. running mean: -33.795472\n",
      "ep 445: ep_len:575 episode reward: total was -48.130000. running mean: -33.938817\n",
      "ep 445: ep_len:560 episode reward: total was -55.730000. running mean: -34.156729\n",
      "epsilon:0.339121 episode_count: 3122. steps_count: 1410145.000000\n",
      "ep 446: ep_len:575 episode reward: total was -49.500000. running mean: -34.310162\n",
      "ep 446: ep_len:535 episode reward: total was -46.030000. running mean: -34.427360\n",
      "ep 446: ep_len:550 episode reward: total was -44.240000. running mean: -34.525487\n",
      "ep 446: ep_len:535 episode reward: total was -25.640000. running mean: -34.436632\n",
      "ep 446: ep_len:3 episode reward: total was 0.000000. running mean: -34.092265\n",
      "ep 446: ep_len:555 episode reward: total was -44.550000. running mean: -34.196843\n",
      "ep 446: ep_len:585 episode reward: total was -45.650000. running mean: -34.311374\n",
      "epsilon:0.338984 episode_count: 3129. steps_count: 1413483.000000\n",
      "ep 447: ep_len:500 episode reward: total was -38.050000. running mean: -34.348761\n",
      "ep 447: ep_len:615 episode reward: total was -38.050000. running mean: -34.385773\n",
      "ep 447: ep_len:570 episode reward: total was -30.030000. running mean: -34.342215\n",
      "ep 447: ep_len:540 episode reward: total was -28.790000. running mean: -34.286693\n",
      "ep 447: ep_len:3 episode reward: total was 0.000000. running mean: -33.943826\n",
      "ep 447: ep_len:575 episode reward: total was -45.550000. running mean: -34.059888\n",
      "ep 447: ep_len:520 episode reward: total was -42.680000. running mean: -34.146089\n",
      "epsilon:0.338848 episode_count: 3136. steps_count: 1416806.000000\n",
      "ep 448: ep_len:265 episode reward: total was -12.450000. running mean: -33.929128\n",
      "ep 448: ep_len:545 episode reward: total was -53.240000. running mean: -34.122237\n",
      "ep 448: ep_len:377 episode reward: total was -30.450000. running mean: -34.085514\n",
      "ep 448: ep_len:735 episode reward: total was -95.340000. running mean: -34.698059\n",
      "ep 448: ep_len:3 episode reward: total was 0.000000. running mean: -34.351079\n",
      "ep 448: ep_len:555 episode reward: total was -40.650000. running mean: -34.414068\n",
      "ep 448: ep_len:750 episode reward: total was -88.630000. running mean: -34.956227\n",
      "epsilon:0.338711 episode_count: 3143. steps_count: 1420036.000000\n",
      "ep 449: ep_len:535 episode reward: total was -32.050000. running mean: -34.927165\n",
      "ep 449: ep_len:505 episode reward: total was -18.290000. running mean: -34.760793\n",
      "ep 449: ep_len:560 episode reward: total was -37.780000. running mean: -34.790985\n",
      "ep 449: ep_len:530 episode reward: total was -31.100000. running mean: -34.754076\n",
      "ep 449: ep_len:68 episode reward: total was -6.480000. running mean: -34.471335\n",
      "ep 449: ep_len:690 episode reward: total was -64.560000. running mean: -34.772221\n",
      "ep 449: ep_len:595 episode reward: total was -33.170000. running mean: -34.756199\n",
      "epsilon:0.338575 episode_count: 3150. steps_count: 1423519.000000\n",
      "ep 450: ep_len:249 episode reward: total was -29.390000. running mean: -34.702537\n",
      "ep 450: ep_len:545 episode reward: total was -39.680000. running mean: -34.752312\n",
      "ep 450: ep_len:505 episode reward: total was -54.670000. running mean: -34.951489\n",
      "ep 450: ep_len:41 episode reward: total was -3.960000. running mean: -34.641574\n",
      "ep 450: ep_len:1 episode reward: total was 0.000000. running mean: -34.295158\n",
      "ep 450: ep_len:570 episode reward: total was -46.010000. running mean: -34.412307\n",
      "ep 450: ep_len:298 episode reward: total was -36.390000. running mean: -34.432083\n",
      "epsilon:0.338438 episode_count: 3157. steps_count: 1425728.000000\n",
      "ep 451: ep_len:560 episode reward: total was -33.780000. running mean: -34.425563\n",
      "ep 451: ep_len:595 episode reward: total was -44.580000. running mean: -34.527107\n",
      "ep 451: ep_len:580 episode reward: total was -32.620000. running mean: -34.508036\n",
      "ep 451: ep_len:540 episode reward: total was -34.200000. running mean: -34.504956\n",
      "ep 451: ep_len:73 episode reward: total was -6.000000. running mean: -34.219906\n",
      "ep 451: ep_len:232 episode reward: total was -21.370000. running mean: -34.091407\n",
      "ep 451: ep_len:610 episode reward: total was -39.630000. running mean: -34.146793\n",
      "epsilon:0.338302 episode_count: 3164. steps_count: 1428918.000000\n",
      "ep 452: ep_len:575 episode reward: total was -60.230000. running mean: -34.407625\n",
      "ep 452: ep_len:530 episode reward: total was -32.410000. running mean: -34.387649\n",
      "ep 452: ep_len:595 episode reward: total was -36.440000. running mean: -34.408172\n",
      "ep 452: ep_len:510 episode reward: total was -26.610000. running mean: -34.330190\n",
      "ep 452: ep_len:2 episode reward: total was 0.000000. running mean: -33.986889\n",
      "ep 452: ep_len:970 episode reward: total was -101.000000. running mean: -34.657020\n",
      "ep 452: ep_len:545 episode reward: total was -26.100000. running mean: -34.571450\n",
      "epsilon:0.338165 episode_count: 3171. steps_count: 1432645.000000\n",
      "ep 453: ep_len:630 episode reward: total was -38.550000. running mean: -34.611235\n",
      "ep 453: ep_len:251 episode reward: total was -30.400000. running mean: -34.569123\n",
      "ep 453: ep_len:600 episode reward: total was -53.050000. running mean: -34.753931\n",
      "ep 453: ep_len:520 episode reward: total was -31.700000. running mean: -34.723392\n",
      "ep 453: ep_len:133 episode reward: total was -5.450000. running mean: -34.430658\n",
      "ep 453: ep_len:635 episode reward: total was -52.460000. running mean: -34.610952\n",
      "ep 453: ep_len:505 episode reward: total was -45.080000. running mean: -34.715642\n",
      "epsilon:0.338029 episode_count: 3178. steps_count: 1435919.000000\n",
      "ep 454: ep_len:660 episode reward: total was -74.550000. running mean: -35.113986\n",
      "ep 454: ep_len:625 episode reward: total was -58.570000. running mean: -35.348546\n",
      "ep 454: ep_len:73 episode reward: total was -3.980000. running mean: -35.034860\n",
      "ep 454: ep_len:770 episode reward: total was -64.320000. running mean: -35.327712\n",
      "ep 454: ep_len:3 episode reward: total was 0.000000. running mean: -34.974435\n",
      "ep 454: ep_len:505 episode reward: total was -35.060000. running mean: -34.975290\n",
      "ep 454: ep_len:680 episode reward: total was -44.440000. running mean: -35.069937\n",
      "epsilon:0.337892 episode_count: 3185. steps_count: 1439235.000000\n",
      "ep 455: ep_len:505 episode reward: total was -52.880000. running mean: -35.248038\n",
      "ep 455: ep_len:630 episode reward: total was -21.120000. running mean: -35.106758\n",
      "ep 455: ep_len:560 episode reward: total was -36.420000. running mean: -35.119890\n",
      "ep 455: ep_len:565 episode reward: total was -35.760000. running mean: -35.126291\n",
      "ep 455: ep_len:97 episode reward: total was -4.970000. running mean: -34.824728\n",
      "ep 455: ep_len:645 episode reward: total was -52.710000. running mean: -35.003581\n",
      "ep 455: ep_len:575 episode reward: total was -42.190000. running mean: -35.075445\n",
      "epsilon:0.337756 episode_count: 3192. steps_count: 1442812.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 456: ep_len:500 episode reward: total was -48.900000. running mean: -35.213691\n",
      "ep 456: ep_len:500 episode reward: total was -22.130000. running mean: -35.082854\n",
      "ep 456: ep_len:590 episode reward: total was -46.980000. running mean: -35.201825\n",
      "ep 456: ep_len:505 episode reward: total was -51.680000. running mean: -35.366607\n",
      "ep 456: ep_len:3 episode reward: total was 0.000000. running mean: -35.012941\n",
      "ep 456: ep_len:585 episode reward: total was -46.750000. running mean: -35.130312\n",
      "ep 456: ep_len:520 episode reward: total was -48.820000. running mean: -35.267208\n",
      "epsilon:0.337619 episode_count: 3199. steps_count: 1446015.000000\n",
      "ep 457: ep_len:530 episode reward: total was -43.500000. running mean: -35.349536\n",
      "ep 457: ep_len:510 episode reward: total was -30.750000. running mean: -35.303541\n",
      "ep 457: ep_len:428 episode reward: total was -14.230000. running mean: -35.092806\n",
      "ep 457: ep_len:505 episode reward: total was -42.130000. running mean: -35.163177\n",
      "ep 457: ep_len:3 episode reward: total was 0.000000. running mean: -34.811546\n",
      "ep 457: ep_len:685 episode reward: total was -43.910000. running mean: -34.902530\n",
      "ep 457: ep_len:535 episode reward: total was -33.670000. running mean: -34.890205\n",
      "epsilon:0.337483 episode_count: 3206. steps_count: 1449211.000000\n",
      "ep 458: ep_len:580 episode reward: total was -60.050000. running mean: -35.141803\n",
      "ep 458: ep_len:565 episode reward: total was -49.250000. running mean: -35.282885\n",
      "ep 458: ep_len:397 episode reward: total was -32.850000. running mean: -35.258556\n",
      "ep 458: ep_len:560 episode reward: total was -47.040000. running mean: -35.376370\n",
      "ep 458: ep_len:89 episode reward: total was -5.450000. running mean: -35.077107\n",
      "ep 458: ep_len:590 episode reward: total was -39.200000. running mean: -35.118336\n",
      "ep 458: ep_len:610 episode reward: total was -40.510000. running mean: -35.172252\n",
      "epsilon:0.337346 episode_count: 3213. steps_count: 1452602.000000\n",
      "ep 459: ep_len:615 episode reward: total was -43.100000. running mean: -35.251530\n",
      "ep 459: ep_len:505 episode reward: total was -12.880000. running mean: -35.027815\n",
      "ep 459: ep_len:560 episode reward: total was -62.090000. running mean: -35.298436\n",
      "ep 459: ep_len:500 episode reward: total was -32.720000. running mean: -35.272652\n",
      "ep 459: ep_len:102 episode reward: total was -6.980000. running mean: -34.989725\n",
      "ep 459: ep_len:540 episode reward: total was -40.590000. running mean: -35.045728\n",
      "ep 459: ep_len:665 episode reward: total was -86.760000. running mean: -35.562871\n",
      "epsilon:0.337210 episode_count: 3220. steps_count: 1456089.000000\n",
      "ep 460: ep_len:209 episode reward: total was -9.410000. running mean: -35.301342\n",
      "ep 460: ep_len:620 episode reward: total was -38.180000. running mean: -35.330129\n",
      "ep 460: ep_len:630 episode reward: total was -39.640000. running mean: -35.373228\n",
      "ep 460: ep_len:590 episode reward: total was -27.210000. running mean: -35.291595\n",
      "ep 460: ep_len:3 episode reward: total was 0.000000. running mean: -34.938679\n",
      "ep 460: ep_len:500 episode reward: total was -29.840000. running mean: -34.887693\n",
      "ep 460: ep_len:565 episode reward: total was -67.300000. running mean: -35.211816\n",
      "epsilon:0.337073 episode_count: 3227. steps_count: 1459206.000000\n",
      "ep 461: ep_len:630 episode reward: total was -50.620000. running mean: -35.365897\n",
      "ep 461: ep_len:505 episode reward: total was -44.130000. running mean: -35.453538\n",
      "ep 461: ep_len:525 episode reward: total was -51.260000. running mean: -35.611603\n",
      "ep 461: ep_len:510 episode reward: total was -36.120000. running mean: -35.616687\n",
      "ep 461: ep_len:3 episode reward: total was 0.000000. running mean: -35.260520\n",
      "ep 461: ep_len:515 episode reward: total was -35.160000. running mean: -35.259515\n",
      "ep 461: ep_len:555 episode reward: total was -56.740000. running mean: -35.474320\n",
      "epsilon:0.336937 episode_count: 3234. steps_count: 1462449.000000\n",
      "ep 462: ep_len:530 episode reward: total was -47.570000. running mean: -35.595277\n",
      "ep 462: ep_len:500 episode reward: total was -50.270000. running mean: -35.742024\n",
      "ep 462: ep_len:730 episode reward: total was -57.410000. running mean: -35.958704\n",
      "ep 462: ep_len:515 episode reward: total was -40.200000. running mean: -36.001117\n",
      "ep 462: ep_len:93 episode reward: total was -12.460000. running mean: -35.765705\n",
      "ep 462: ep_len:655 episode reward: total was -65.900000. running mean: -36.067048\n",
      "ep 462: ep_len:565 episode reward: total was -53.340000. running mean: -36.239778\n",
      "epsilon:0.336800 episode_count: 3241. steps_count: 1466037.000000\n",
      "ep 463: ep_len:575 episode reward: total was -29.220000. running mean: -36.169580\n",
      "ep 463: ep_len:615 episode reward: total was -53.000000. running mean: -36.337884\n",
      "ep 463: ep_len:443 episode reward: total was -33.810000. running mean: -36.312605\n",
      "ep 463: ep_len:575 episode reward: total was -25.210000. running mean: -36.201579\n",
      "ep 463: ep_len:3 episode reward: total was 0.000000. running mean: -35.839564\n",
      "ep 463: ep_len:555 episode reward: total was -33.050000. running mean: -35.811668\n",
      "ep 463: ep_len:535 episode reward: total was -44.620000. running mean: -35.899751\n",
      "epsilon:0.336664 episode_count: 3248. steps_count: 1469338.000000\n",
      "ep 464: ep_len:570 episode reward: total was -54.500000. running mean: -36.085754\n",
      "ep 464: ep_len:500 episode reward: total was -54.240000. running mean: -36.267296\n",
      "ep 464: ep_len:655 episode reward: total was -61.020000. running mean: -36.514823\n",
      "ep 464: ep_len:535 episode reward: total was -47.230000. running mean: -36.621975\n",
      "ep 464: ep_len:3 episode reward: total was 0.000000. running mean: -36.255755\n",
      "ep 464: ep_len:520 episode reward: total was -40.120000. running mean: -36.294398\n",
      "ep 464: ep_len:500 episode reward: total was -31.960000. running mean: -36.251054\n",
      "epsilon:0.336527 episode_count: 3255. steps_count: 1472621.000000\n",
      "ep 465: ep_len:550 episode reward: total was -36.690000. running mean: -36.255443\n",
      "ep 465: ep_len:500 episode reward: total was -35.890000. running mean: -36.251789\n",
      "ep 465: ep_len:590 episode reward: total was -32.760000. running mean: -36.216871\n",
      "ep 465: ep_len:500 episode reward: total was -42.180000. running mean: -36.276502\n",
      "ep 465: ep_len:3 episode reward: total was 0.000000. running mean: -35.913737\n",
      "ep 465: ep_len:229 episode reward: total was -14.430000. running mean: -35.698900\n",
      "ep 465: ep_len:565 episode reward: total was -38.990000. running mean: -35.731811\n",
      "epsilon:0.336391 episode_count: 3262. steps_count: 1475558.000000\n",
      "ep 466: ep_len:183 episode reward: total was -13.950000. running mean: -35.513993\n",
      "ep 466: ep_len:515 episode reward: total was -44.250000. running mean: -35.601353\n",
      "ep 466: ep_len:545 episode reward: total was -27.190000. running mean: -35.517239\n",
      "ep 466: ep_len:530 episode reward: total was -32.260000. running mean: -35.484667\n",
      "ep 466: ep_len:3 episode reward: total was 0.000000. running mean: -35.129820\n",
      "ep 466: ep_len:515 episode reward: total was -39.690000. running mean: -35.175422\n",
      "ep 466: ep_len:610 episode reward: total was -73.760000. running mean: -35.561268\n",
      "epsilon:0.336254 episode_count: 3269. steps_count: 1478459.000000\n",
      "ep 467: ep_len:265 episode reward: total was -7.390000. running mean: -35.279555\n",
      "ep 467: ep_len:615 episode reward: total was -57.530000. running mean: -35.502060\n",
      "ep 467: ep_len:675 episode reward: total was -38.330000. running mean: -35.530339\n",
      "ep 467: ep_len:540 episode reward: total was -50.100000. running mean: -35.676036\n",
      "ep 467: ep_len:3 episode reward: total was 0.000000. running mean: -35.319275\n",
      "ep 467: ep_len:535 episode reward: total was -31.600000. running mean: -35.282082\n",
      "ep 467: ep_len:510 episode reward: total was -35.720000. running mean: -35.286462\n",
      "epsilon:0.336118 episode_count: 3276. steps_count: 1481602.000000\n",
      "ep 468: ep_len:540 episode reward: total was -45.510000. running mean: -35.388697\n",
      "ep 468: ep_len:500 episode reward: total was -47.660000. running mean: -35.511410\n",
      "ep 468: ep_len:655 episode reward: total was -45.180000. running mean: -35.608096\n",
      "ep 468: ep_len:530 episode reward: total was -38.200000. running mean: -35.634015\n",
      "ep 468: ep_len:83 episode reward: total was -0.960000. running mean: -35.287275\n",
      "ep 468: ep_len:560 episode reward: total was -30.740000. running mean: -35.241802\n",
      "ep 468: ep_len:600 episode reward: total was -42.490000. running mean: -35.314284\n",
      "epsilon:0.335981 episode_count: 3283. steps_count: 1485070.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 469: ep_len:116 episode reward: total was -6.930000. running mean: -35.030441\n",
      "ep 469: ep_len:500 episode reward: total was -28.840000. running mean: -34.968537\n",
      "ep 469: ep_len:600 episode reward: total was -44.250000. running mean: -35.061351\n",
      "ep 469: ep_len:530 episode reward: total was -28.120000. running mean: -34.991938\n",
      "ep 469: ep_len:3 episode reward: total was 0.000000. running mean: -34.642019\n",
      "ep 469: ep_len:253 episode reward: total was -16.890000. running mean: -34.464498\n",
      "ep 469: ep_len:510 episode reward: total was -43.710000. running mean: -34.556953\n",
      "epsilon:0.335845 episode_count: 3290. steps_count: 1487582.000000\n",
      "ep 470: ep_len:515 episode reward: total was -40.650000. running mean: -34.617884\n",
      "ep 470: ep_len:505 episode reward: total was -52.700000. running mean: -34.798705\n",
      "ep 470: ep_len:635 episode reward: total was -53.970000. running mean: -34.990418\n",
      "ep 470: ep_len:595 episode reward: total was -47.120000. running mean: -35.111714\n",
      "ep 470: ep_len:3 episode reward: total was 0.000000. running mean: -34.760597\n",
      "ep 470: ep_len:600 episode reward: total was -46.200000. running mean: -34.874991\n",
      "ep 470: ep_len:261 episode reward: total was -20.380000. running mean: -34.730041\n",
      "epsilon:0.335708 episode_count: 3297. steps_count: 1490696.000000\n",
      "ep 471: ep_len:580 episode reward: total was -63.850000. running mean: -35.021240\n",
      "ep 471: ep_len:590 episode reward: total was -49.180000. running mean: -35.162828\n",
      "ep 471: ep_len:79 episode reward: total was -5.470000. running mean: -34.865900\n",
      "ep 471: ep_len:164 episode reward: total was -13.940000. running mean: -34.656641\n",
      "ep 471: ep_len:85 episode reward: total was -6.460000. running mean: -34.374674\n",
      "ep 471: ep_len:307 episode reward: total was -20.930000. running mean: -34.240228\n",
      "ep 471: ep_len:590 episode reward: total was -44.300000. running mean: -34.340825\n",
      "epsilon:0.335572 episode_count: 3304. steps_count: 1493091.000000\n",
      "ep 472: ep_len:525 episode reward: total was -33.290000. running mean: -34.330317\n",
      "ep 472: ep_len:500 episode reward: total was -44.850000. running mean: -34.435514\n",
      "ep 472: ep_len:665 episode reward: total was -40.430000. running mean: -34.495459\n",
      "ep 472: ep_len:510 episode reward: total was -46.710000. running mean: -34.617604\n",
      "ep 472: ep_len:3 episode reward: total was 0.000000. running mean: -34.271428\n",
      "ep 472: ep_len:530 episode reward: total was -30.660000. running mean: -34.235314\n",
      "ep 472: ep_len:625 episode reward: total was -50.580000. running mean: -34.398761\n",
      "epsilon:0.335435 episode_count: 3311. steps_count: 1496449.000000\n",
      "ep 473: ep_len:635 episode reward: total was -39.090000. running mean: -34.445673\n",
      "ep 473: ep_len:610 episode reward: total was -73.710000. running mean: -34.838316\n",
      "ep 473: ep_len:650 episode reward: total was -67.580000. running mean: -35.165733\n",
      "ep 473: ep_len:500 episode reward: total was -38.820000. running mean: -35.202276\n",
      "ep 473: ep_len:3 episode reward: total was 0.000000. running mean: -34.850253\n",
      "ep 473: ep_len:520 episode reward: total was -30.270000. running mean: -34.804451\n",
      "ep 473: ep_len:294 episode reward: total was -21.900000. running mean: -34.675406\n",
      "epsilon:0.335299 episode_count: 3318. steps_count: 1499661.000000\n",
      "ep 474: ep_len:201 episode reward: total was -12.930000. running mean: -34.457952\n",
      "ep 474: ep_len:520 episode reward: total was -22.280000. running mean: -34.336172\n",
      "ep 474: ep_len:585 episode reward: total was -28.670000. running mean: -34.279511\n",
      "ep 474: ep_len:117 episode reward: total was -6.920000. running mean: -34.005916\n",
      "ep 474: ep_len:3 episode reward: total was 0.000000. running mean: -33.665856\n",
      "ep 474: ep_len:500 episode reward: total was -33.870000. running mean: -33.667898\n",
      "ep 474: ep_len:350 episode reward: total was -31.880000. running mean: -33.650019\n",
      "epsilon:0.335162 episode_count: 3325. steps_count: 1501937.000000\n",
      "ep 475: ep_len:510 episode reward: total was -33.880000. running mean: -33.652319\n",
      "ep 475: ep_len:520 episode reward: total was -42.240000. running mean: -33.738196\n",
      "ep 475: ep_len:439 episode reward: total was -31.810000. running mean: -33.718914\n",
      "ep 475: ep_len:560 episode reward: total was -28.720000. running mean: -33.668924\n",
      "ep 475: ep_len:3 episode reward: total was 0.000000. running mean: -33.332235\n",
      "ep 475: ep_len:575 episode reward: total was -28.200000. running mean: -33.280913\n",
      "ep 475: ep_len:570 episode reward: total was -48.020000. running mean: -33.428304\n",
      "epsilon:0.335026 episode_count: 3332. steps_count: 1505114.000000\n",
      "ep 476: ep_len:500 episode reward: total was -30.290000. running mean: -33.396921\n",
      "ep 476: ep_len:565 episode reward: total was -55.170000. running mean: -33.614651\n",
      "ep 476: ep_len:565 episode reward: total was -47.790000. running mean: -33.756405\n",
      "ep 476: ep_len:538 episode reward: total was -52.210000. running mean: -33.940941\n",
      "ep 476: ep_len:3 episode reward: total was 0.000000. running mean: -33.601531\n",
      "ep 476: ep_len:615 episode reward: total was -43.730000. running mean: -33.702816\n",
      "ep 476: ep_len:341 episode reward: total was -32.330000. running mean: -33.689088\n",
      "epsilon:0.334889 episode_count: 3339. steps_count: 1508241.000000\n",
      "ep 477: ep_len:134 episode reward: total was -6.460000. running mean: -33.416797\n",
      "ep 477: ep_len:500 episode reward: total was -53.770000. running mean: -33.620329\n",
      "ep 477: ep_len:545 episode reward: total was -41.750000. running mean: -33.701626\n",
      "ep 477: ep_len:500 episode reward: total was -33.080000. running mean: -33.695410\n",
      "ep 477: ep_len:102 episode reward: total was -2.450000. running mean: -33.382956\n",
      "ep 477: ep_len:555 episode reward: total was -46.820000. running mean: -33.517326\n",
      "ep 477: ep_len:590 episode reward: total was -35.150000. running mean: -33.533653\n",
      "epsilon:0.334753 episode_count: 3346. steps_count: 1511167.000000\n",
      "ep 478: ep_len:530 episode reward: total was -51.580000. running mean: -33.714116\n",
      "ep 478: ep_len:505 episode reward: total was -39.690000. running mean: -33.773875\n",
      "ep 478: ep_len:625 episode reward: total was -28.190000. running mean: -33.718036\n",
      "ep 478: ep_len:135 episode reward: total was -6.470000. running mean: -33.445556\n",
      "ep 478: ep_len:3 episode reward: total was 0.000000. running mean: -33.111100\n",
      "ep 478: ep_len:620 episode reward: total was -42.300000. running mean: -33.202989\n",
      "ep 478: ep_len:530 episode reward: total was -42.090000. running mean: -33.291859\n",
      "epsilon:0.334616 episode_count: 3353. steps_count: 1514115.000000\n",
      "ep 479: ep_len:265 episode reward: total was -11.440000. running mean: -33.073341\n",
      "ep 479: ep_len:550 episode reward: total was -35.090000. running mean: -33.093507\n",
      "ep 479: ep_len:510 episode reward: total was -63.290000. running mean: -33.395472\n",
      "ep 479: ep_len:530 episode reward: total was -52.250000. running mean: -33.584018\n",
      "ep 479: ep_len:3 episode reward: total was 0.000000. running mean: -33.248177\n",
      "ep 479: ep_len:625 episode reward: total was -42.120000. running mean: -33.336896\n",
      "ep 479: ep_len:294 episode reward: total was -33.450000. running mean: -33.338027\n",
      "epsilon:0.334480 episode_count: 3360. steps_count: 1516892.000000\n",
      "ep 480: ep_len:187 episode reward: total was -15.420000. running mean: -33.158846\n",
      "ep 480: ep_len:173 episode reward: total was -28.480000. running mean: -33.112058\n",
      "ep 480: ep_len:560 episode reward: total was -58.130000. running mean: -33.362237\n",
      "ep 480: ep_len:166 episode reward: total was -3.410000. running mean: -33.062715\n",
      "ep 480: ep_len:3 episode reward: total was 0.000000. running mean: -32.732088\n",
      "ep 480: ep_len:505 episode reward: total was -33.790000. running mean: -32.742667\n",
      "ep 480: ep_len:500 episode reward: total was -39.350000. running mean: -32.808740\n",
      "epsilon:0.334343 episode_count: 3367. steps_count: 1518986.000000\n",
      "ep 481: ep_len:600 episode reward: total was -34.770000. running mean: -32.828353\n",
      "ep 481: ep_len:540 episode reward: total was -32.030000. running mean: -32.820369\n",
      "ep 481: ep_len:500 episode reward: total was -44.710000. running mean: -32.939266\n",
      "ep 481: ep_len:515 episode reward: total was -38.140000. running mean: -32.991273\n",
      "ep 481: ep_len:88 episode reward: total was 3.550000. running mean: -32.625860\n",
      "ep 481: ep_len:650 episode reward: total was -74.190000. running mean: -33.041502\n",
      "ep 481: ep_len:600 episode reward: total was -43.550000. running mean: -33.146587\n",
      "epsilon:0.334207 episode_count: 3374. steps_count: 1522479.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 482: ep_len:206 episode reward: total was -11.400000. running mean: -32.929121\n",
      "ep 482: ep_len:345 episode reward: total was -31.400000. running mean: -32.913830\n",
      "ep 482: ep_len:400 episode reward: total was -14.820000. running mean: -32.732891\n",
      "ep 482: ep_len:500 episode reward: total was -23.580000. running mean: -32.641362\n",
      "ep 482: ep_len:3 episode reward: total was 0.000000. running mean: -32.314949\n",
      "ep 482: ep_len:179 episode reward: total was -20.910000. running mean: -32.200899\n",
      "ep 482: ep_len:600 episode reward: total was -35.510000. running mean: -32.233990\n",
      "epsilon:0.334070 episode_count: 3381. steps_count: 1524712.000000\n",
      "ep 483: ep_len:960 episode reward: total was -80.250000. running mean: -32.714150\n",
      "ep 483: ep_len:725 episode reward: total was -53.160000. running mean: -32.918609\n",
      "ep 483: ep_len:500 episode reward: total was -50.180000. running mean: -33.091223\n",
      "ep 483: ep_len:505 episode reward: total was -26.220000. running mean: -33.022511\n",
      "ep 483: ep_len:3 episode reward: total was 0.000000. running mean: -32.692285\n",
      "ep 483: ep_len:680 episode reward: total was -18.790000. running mean: -32.553263\n",
      "ep 483: ep_len:211 episode reward: total was -17.920000. running mean: -32.406930\n",
      "epsilon:0.333934 episode_count: 3388. steps_count: 1528296.000000\n",
      "ep 484: ep_len:640 episode reward: total was -47.950000. running mean: -32.562361\n",
      "ep 484: ep_len:555 episode reward: total was -28.620000. running mean: -32.522937\n",
      "ep 484: ep_len:590 episode reward: total was -41.930000. running mean: -32.617008\n",
      "ep 484: ep_len:540 episode reward: total was -46.070000. running mean: -32.751538\n",
      "ep 484: ep_len:3 episode reward: total was 0.000000. running mean: -32.424022\n",
      "ep 484: ep_len:605 episode reward: total was -30.220000. running mean: -32.401982\n",
      "ep 484: ep_len:530 episode reward: total was -30.760000. running mean: -32.385562\n",
      "epsilon:0.333797 episode_count: 3395. steps_count: 1531759.000000\n",
      "ep 485: ep_len:570 episode reward: total was -30.580000. running mean: -32.367507\n",
      "ep 485: ep_len:590 episode reward: total was -26.790000. running mean: -32.311732\n",
      "ep 485: ep_len:500 episode reward: total was -28.580000. running mean: -32.274414\n",
      "ep 485: ep_len:500 episode reward: total was -32.650000. running mean: -32.278170\n",
      "ep 485: ep_len:3 episode reward: total was 0.000000. running mean: -31.955388\n",
      "ep 485: ep_len:520 episode reward: total was -41.790000. running mean: -32.053735\n",
      "ep 485: ep_len:645 episode reward: total was -66.530000. running mean: -32.398497\n",
      "epsilon:0.333661 episode_count: 3402. steps_count: 1535087.000000\n",
      "ep 486: ep_len:615 episode reward: total was -46.630000. running mean: -32.540812\n",
      "ep 486: ep_len:670 episode reward: total was -42.150000. running mean: -32.636904\n",
      "ep 486: ep_len:575 episode reward: total was -37.980000. running mean: -32.690335\n",
      "ep 486: ep_len:520 episode reward: total was -40.750000. running mean: -32.770932\n",
      "ep 486: ep_len:3 episode reward: total was 0.000000. running mean: -32.443222\n",
      "ep 486: ep_len:665 episode reward: total was -50.380000. running mean: -32.622590\n",
      "ep 486: ep_len:510 episode reward: total was -40.380000. running mean: -32.700164\n",
      "epsilon:0.333524 episode_count: 3409. steps_count: 1538645.000000\n",
      "ep 487: ep_len:133 episode reward: total was -6.460000. running mean: -32.437763\n",
      "ep 487: ep_len:500 episode reward: total was -9.870000. running mean: -32.212085\n",
      "ep 487: ep_len:630 episode reward: total was -42.650000. running mean: -32.316464\n",
      "ep 487: ep_len:605 episode reward: total was -53.080000. running mean: -32.524099\n",
      "ep 487: ep_len:3 episode reward: total was 0.000000. running mean: -32.198858\n",
      "ep 487: ep_len:535 episode reward: total was -37.740000. running mean: -32.254270\n",
      "ep 487: ep_len:570 episode reward: total was -34.140000. running mean: -32.273127\n",
      "epsilon:0.333388 episode_count: 3416. steps_count: 1541621.000000\n",
      "ep 488: ep_len:610 episode reward: total was -58.110000. running mean: -32.531496\n",
      "ep 488: ep_len:605 episode reward: total was -31.700000. running mean: -32.523181\n",
      "ep 488: ep_len:530 episode reward: total was -26.720000. running mean: -32.465149\n",
      "ep 488: ep_len:500 episode reward: total was -31.640000. running mean: -32.456898\n",
      "ep 488: ep_len:65 episode reward: total was -5.000000. running mean: -32.182329\n",
      "ep 488: ep_len:500 episode reward: total was -43.380000. running mean: -32.294305\n",
      "ep 488: ep_len:590 episode reward: total was -48.630000. running mean: -32.457662\n",
      "epsilon:0.333251 episode_count: 3423. steps_count: 1545021.000000\n",
      "ep 489: ep_len:500 episode reward: total was -76.340000. running mean: -32.896486\n",
      "ep 489: ep_len:625 episode reward: total was -57.150000. running mean: -33.139021\n",
      "ep 489: ep_len:575 episode reward: total was -48.210000. running mean: -33.289731\n",
      "ep 489: ep_len:510 episode reward: total was -26.230000. running mean: -33.219133\n",
      "ep 489: ep_len:120 episode reward: total was -2.470000. running mean: -32.911642\n",
      "ep 489: ep_len:510 episode reward: total was -46.640000. running mean: -33.048926\n",
      "ep 489: ep_len:302 episode reward: total was -15.370000. running mean: -32.872136\n",
      "epsilon:0.333115 episode_count: 3430. steps_count: 1548163.000000\n",
      "ep 490: ep_len:635 episode reward: total was -26.550000. running mean: -32.808915\n",
      "ep 490: ep_len:620 episode reward: total was -82.270000. running mean: -33.303526\n",
      "ep 490: ep_len:615 episode reward: total was -61.100000. running mean: -33.581491\n",
      "ep 490: ep_len:351 episode reward: total was -45.780000. running mean: -33.703476\n",
      "ep 490: ep_len:3 episode reward: total was 0.000000. running mean: -33.366441\n",
      "ep 490: ep_len:525 episode reward: total was -48.770000. running mean: -33.520476\n",
      "ep 490: ep_len:630 episode reward: total was -72.700000. running mean: -33.912272\n",
      "epsilon:0.332978 episode_count: 3437. steps_count: 1551542.000000\n",
      "ep 491: ep_len:545 episode reward: total was -18.250000. running mean: -33.755649\n",
      "ep 491: ep_len:520 episode reward: total was -16.820000. running mean: -33.586293\n",
      "ep 491: ep_len:565 episode reward: total was -31.610000. running mean: -33.566530\n",
      "ep 491: ep_len:56 episode reward: total was -6.000000. running mean: -33.290864\n",
      "ep 491: ep_len:3 episode reward: total was 0.000000. running mean: -32.957956\n",
      "ep 491: ep_len:500 episode reward: total was -42.250000. running mean: -33.050876\n",
      "ep 491: ep_len:530 episode reward: total was -54.250000. running mean: -33.262867\n",
      "epsilon:0.332842 episode_count: 3444. steps_count: 1554261.000000\n",
      "ep 492: ep_len:630 episode reward: total was -64.480000. running mean: -33.575039\n",
      "ep 492: ep_len:510 episode reward: total was -19.420000. running mean: -33.433488\n",
      "ep 492: ep_len:575 episode reward: total was -34.470000. running mean: -33.443853\n",
      "ep 492: ep_len:500 episode reward: total was -35.070000. running mean: -33.460115\n",
      "ep 492: ep_len:3 episode reward: total was 0.000000. running mean: -33.125514\n",
      "ep 492: ep_len:595 episode reward: total was -43.740000. running mean: -33.231659\n",
      "ep 492: ep_len:500 episode reward: total was -25.410000. running mean: -33.153442\n",
      "epsilon:0.332705 episode_count: 3451. steps_count: 1557574.000000\n",
      "ep 493: ep_len:605 episode reward: total was -27.760000. running mean: -33.099508\n",
      "ep 493: ep_len:288 episode reward: total was -19.380000. running mean: -32.962312\n",
      "ep 493: ep_len:500 episode reward: total was -34.590000. running mean: -32.978589\n",
      "ep 493: ep_len:585 episode reward: total was -30.740000. running mean: -32.956203\n",
      "ep 493: ep_len:124 episode reward: total was -8.970000. running mean: -32.716341\n",
      "ep 493: ep_len:500 episode reward: total was -36.320000. running mean: -32.752378\n",
      "ep 493: ep_len:275 episode reward: total was -25.380000. running mean: -32.678654\n",
      "epsilon:0.332569 episode_count: 3458. steps_count: 1560451.000000\n",
      "ep 494: ep_len:560 episode reward: total was -36.110000. running mean: -32.712968\n",
      "ep 494: ep_len:590 episode reward: total was -25.560000. running mean: -32.641438\n",
      "ep 494: ep_len:665 episode reward: total was -42.380000. running mean: -32.738824\n",
      "ep 494: ep_len:540 episode reward: total was -38.690000. running mean: -32.798335\n",
      "ep 494: ep_len:3 episode reward: total was 0.000000. running mean: -32.470352\n",
      "ep 494: ep_len:172 episode reward: total was -9.930000. running mean: -32.244949\n",
      "ep 494: ep_len:570 episode reward: total was -46.010000. running mean: -32.382599\n",
      "epsilon:0.332432 episode_count: 3465. steps_count: 1563551.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 495: ep_len:505 episode reward: total was -49.440000. running mean: -32.553173\n",
      "ep 495: ep_len:565 episode reward: total was -28.730000. running mean: -32.514941\n",
      "ep 495: ep_len:665 episode reward: total was -42.390000. running mean: -32.613692\n",
      "ep 495: ep_len:595 episode reward: total was -29.730000. running mean: -32.584855\n",
      "ep 495: ep_len:3 episode reward: total was 0.000000. running mean: -32.259006\n",
      "ep 495: ep_len:550 episode reward: total was -37.490000. running mean: -32.311316\n",
      "ep 495: ep_len:207 episode reward: total was -11.410000. running mean: -32.102303\n",
      "epsilon:0.332296 episode_count: 3472. steps_count: 1566641.000000\n",
      "ep 496: ep_len:625 episode reward: total was -68.650000. running mean: -32.467780\n",
      "ep 496: ep_len:302 episode reward: total was -21.870000. running mean: -32.361802\n",
      "ep 496: ep_len:820 episode reward: total was -80.490000. running mean: -32.843084\n",
      "ep 496: ep_len:170 episode reward: total was -10.450000. running mean: -32.619154\n",
      "ep 496: ep_len:46 episode reward: total was -1.000000. running mean: -32.302962\n",
      "ep 496: ep_len:255 episode reward: total was -4.880000. running mean: -32.028732\n",
      "ep 496: ep_len:520 episode reward: total was -28.980000. running mean: -31.998245\n",
      "epsilon:0.332159 episode_count: 3479. steps_count: 1569379.000000\n",
      "ep 497: ep_len:500 episode reward: total was -30.390000. running mean: -31.982163\n",
      "ep 497: ep_len:585 episode reward: total was -39.590000. running mean: -32.058241\n",
      "ep 497: ep_len:56 episode reward: total was -3.000000. running mean: -31.767659\n",
      "ep 497: ep_len:372 episode reward: total was -26.720000. running mean: -31.717182\n",
      "ep 497: ep_len:32 episode reward: total was -1.500000. running mean: -31.415010\n",
      "ep 497: ep_len:287 episode reward: total was 0.210000. running mean: -31.098760\n",
      "ep 497: ep_len:550 episode reward: total was -36.710000. running mean: -31.154872\n",
      "epsilon:0.332023 episode_count: 3486. steps_count: 1571761.000000\n",
      "ep 498: ep_len:535 episode reward: total was -55.650000. running mean: -31.399824\n",
      "ep 498: ep_len:515 episode reward: total was -44.360000. running mean: -31.529425\n",
      "ep 498: ep_len:79 episode reward: total was -6.480000. running mean: -31.278931\n",
      "ep 498: ep_len:1020 episode reward: total was -130.700000. running mean: -32.273142\n",
      "ep 498: ep_len:51 episode reward: total was -4.000000. running mean: -31.990411\n",
      "ep 498: ep_len:313 episode reward: total was -12.380000. running mean: -31.794306\n",
      "ep 498: ep_len:520 episode reward: total was -36.210000. running mean: -31.838463\n",
      "epsilon:0.331886 episode_count: 3493. steps_count: 1574794.000000\n",
      "ep 499: ep_len:500 episode reward: total was -29.890000. running mean: -31.818979\n",
      "ep 499: ep_len:510 episode reward: total was -29.170000. running mean: -31.792489\n",
      "ep 499: ep_len:79 episode reward: total was -2.470000. running mean: -31.499264\n",
      "ep 499: ep_len:620 episode reward: total was -43.250000. running mean: -31.616771\n",
      "ep 499: ep_len:3 episode reward: total was 0.000000. running mean: -31.300604\n",
      "ep 499: ep_len:690 episode reward: total was -89.180000. running mean: -31.879398\n",
      "ep 499: ep_len:205 episode reward: total was -13.890000. running mean: -31.699504\n",
      "epsilon:0.331750 episode_count: 3500. steps_count: 1577401.000000\n",
      "ep 500: ep_len:565 episode reward: total was -29.230000. running mean: -31.674809\n",
      "ep 500: ep_len:565 episode reward: total was -24.680000. running mean: -31.604861\n",
      "ep 500: ep_len:359 episode reward: total was -17.360000. running mean: -31.462412\n",
      "ep 500: ep_len:500 episode reward: total was -31.590000. running mean: -31.463688\n",
      "ep 500: ep_len:3 episode reward: total was 0.000000. running mean: -31.149051\n",
      "ep 500: ep_len:590 episode reward: total was -41.940000. running mean: -31.256960\n",
      "ep 500: ep_len:580 episode reward: total was -39.990000. running mean: -31.344291\n",
      "epsilon:0.331613 episode_count: 3507. steps_count: 1580563.000000\n",
      "ep 501: ep_len:590 episode reward: total was -30.020000. running mean: -31.331048\n",
      "ep 501: ep_len:535 episode reward: total was -55.920000. running mean: -31.576937\n",
      "ep 501: ep_len:695 episode reward: total was -57.070000. running mean: -31.831868\n",
      "ep 501: ep_len:500 episode reward: total was -21.190000. running mean: -31.725449\n",
      "ep 501: ep_len:3 episode reward: total was 0.000000. running mean: -31.408195\n",
      "ep 501: ep_len:575 episode reward: total was -52.750000. running mean: -31.621613\n",
      "ep 501: ep_len:201 episode reward: total was -13.900000. running mean: -31.444397\n",
      "epsilon:0.331477 episode_count: 3514. steps_count: 1583662.000000\n",
      "ep 502: ep_len:615 episode reward: total was -78.290000. running mean: -31.912853\n",
      "ep 502: ep_len:580 episode reward: total was -38.140000. running mean: -31.975124\n",
      "ep 502: ep_len:580 episode reward: total was -31.690000. running mean: -31.972273\n",
      "ep 502: ep_len:535 episode reward: total was -30.720000. running mean: -31.959750\n",
      "ep 502: ep_len:3 episode reward: total was 0.000000. running mean: -31.640153\n",
      "ep 502: ep_len:640 episode reward: total was -40.740000. running mean: -31.731151\n",
      "ep 502: ep_len:326 episode reward: total was -21.860000. running mean: -31.632440\n",
      "epsilon:0.331340 episode_count: 3521. steps_count: 1586941.000000\n",
      "ep 503: ep_len:650 episode reward: total was -48.970000. running mean: -31.805815\n",
      "ep 503: ep_len:620 episode reward: total was -40.760000. running mean: -31.895357\n",
      "ep 503: ep_len:600 episode reward: total was -44.920000. running mean: -32.025604\n",
      "ep 503: ep_len:530 episode reward: total was -36.980000. running mean: -32.075148\n",
      "ep 503: ep_len:80 episode reward: total was -3.470000. running mean: -31.789096\n",
      "ep 503: ep_len:720 episode reward: total was -65.500000. running mean: -32.126205\n",
      "ep 503: ep_len:625 episode reward: total was -42.160000. running mean: -32.226543\n",
      "epsilon:0.331204 episode_count: 3528. steps_count: 1590766.000000\n",
      "ep 504: ep_len:595 episode reward: total was -25.220000. running mean: -32.156478\n",
      "ep 504: ep_len:630 episode reward: total was -50.030000. running mean: -32.335213\n",
      "ep 504: ep_len:780 episode reward: total was -77.950000. running mean: -32.791361\n",
      "ep 504: ep_len:515 episode reward: total was -59.310000. running mean: -33.056547\n",
      "ep 504: ep_len:3 episode reward: total was 0.000000. running mean: -32.725982\n",
      "ep 504: ep_len:560 episode reward: total was -30.070000. running mean: -32.699422\n",
      "ep 504: ep_len:515 episode reward: total was -37.080000. running mean: -32.743228\n",
      "epsilon:0.331067 episode_count: 3535. steps_count: 1594364.000000\n",
      "ep 505: ep_len:595 episode reward: total was -57.020000. running mean: -32.985995\n",
      "ep 505: ep_len:183 episode reward: total was -13.960000. running mean: -32.795735\n",
      "ep 505: ep_len:655 episode reward: total was -34.030000. running mean: -32.808078\n",
      "ep 505: ep_len:505 episode reward: total was -33.150000. running mean: -32.811497\n",
      "ep 505: ep_len:125 episode reward: total was -10.960000. running mean: -32.592982\n",
      "ep 505: ep_len:515 episode reward: total was -30.170000. running mean: -32.568753\n",
      "ep 505: ep_len:510 episode reward: total was -38.250000. running mean: -32.625565\n",
      "epsilon:0.330931 episode_count: 3542. steps_count: 1597452.000000\n",
      "ep 506: ep_len:127 episode reward: total was -5.460000. running mean: -32.353909\n",
      "ep 506: ep_len:645 episode reward: total was -32.790000. running mean: -32.358270\n",
      "ep 506: ep_len:500 episode reward: total was -41.620000. running mean: -32.450888\n",
      "ep 506: ep_len:585 episode reward: total was -30.110000. running mean: -32.427479\n",
      "ep 506: ep_len:3 episode reward: total was 0.000000. running mean: -32.103204\n",
      "ep 506: ep_len:550 episode reward: total was -70.270000. running mean: -32.484872\n",
      "ep 506: ep_len:335 episode reward: total was -27.270000. running mean: -32.432723\n",
      "epsilon:0.330794 episode_count: 3549. steps_count: 1600197.000000\n",
      "ep 507: ep_len:665 episode reward: total was -94.220000. running mean: -33.050596\n",
      "ep 507: ep_len:545 episode reward: total was -28.470000. running mean: -33.004790\n",
      "ep 507: ep_len:525 episode reward: total was -34.730000. running mean: -33.022042\n",
      "ep 507: ep_len:505 episode reward: total was -23.260000. running mean: -32.924422\n",
      "ep 507: ep_len:97 episode reward: total was -1.450000. running mean: -32.609677\n",
      "ep 507: ep_len:177 episode reward: total was -8.930000. running mean: -32.372881\n",
      "ep 507: ep_len:340 episode reward: total was -31.880000. running mean: -32.367952\n",
      "epsilon:0.330658 episode_count: 3556. steps_count: 1603051.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 508: ep_len:500 episode reward: total was -29.910000. running mean: -32.343372\n",
      "ep 508: ep_len:585 episode reward: total was -30.150000. running mean: -32.321439\n",
      "ep 508: ep_len:590 episode reward: total was -26.180000. running mean: -32.260024\n",
      "ep 508: ep_len:505 episode reward: total was -21.660000. running mean: -32.154024\n",
      "ep 508: ep_len:90 episode reward: total was -9.460000. running mean: -31.927084\n",
      "ep 508: ep_len:610 episode reward: total was -49.770000. running mean: -32.105513\n",
      "ep 508: ep_len:280 episode reward: total was -14.380000. running mean: -31.928258\n",
      "epsilon:0.330521 episode_count: 3563. steps_count: 1606211.000000\n",
      "ep 509: ep_len:660 episode reward: total was -62.080000. running mean: -32.229775\n",
      "ep 509: ep_len:615 episode reward: total was -57.080000. running mean: -32.478277\n",
      "ep 509: ep_len:535 episode reward: total was -26.440000. running mean: -32.417895\n",
      "ep 509: ep_len:610 episode reward: total was -45.640000. running mean: -32.550116\n",
      "ep 509: ep_len:3 episode reward: total was 0.000000. running mean: -32.224615\n",
      "ep 509: ep_len:655 episode reward: total was -37.540000. running mean: -32.277768\n",
      "ep 509: ep_len:500 episode reward: total was -42.300000. running mean: -32.377991\n",
      "epsilon:0.330385 episode_count: 3570. steps_count: 1609789.000000\n",
      "ep 510: ep_len:500 episode reward: total was -35.850000. running mean: -32.412711\n",
      "ep 510: ep_len:505 episode reward: total was -50.700000. running mean: -32.595584\n",
      "ep 510: ep_len:515 episode reward: total was -41.180000. running mean: -32.681428\n",
      "ep 510: ep_len:122 episode reward: total was -2.430000. running mean: -32.378914\n",
      "ep 510: ep_len:3 episode reward: total was 0.000000. running mean: -32.055124\n",
      "ep 510: ep_len:500 episode reward: total was -35.680000. running mean: -32.091373\n",
      "ep 510: ep_len:645 episode reward: total was -50.240000. running mean: -32.272859\n",
      "epsilon:0.330248 episode_count: 3577. steps_count: 1612579.000000\n",
      "ep 511: ep_len:595 episode reward: total was -38.250000. running mean: -32.332631\n",
      "ep 511: ep_len:550 episode reward: total was -11.280000. running mean: -32.122105\n",
      "ep 511: ep_len:560 episode reward: total was -24.690000. running mean: -32.047784\n",
      "ep 511: ep_len:500 episode reward: total was -41.180000. running mean: -32.139106\n",
      "ep 511: ep_len:3 episode reward: total was 0.000000. running mean: -31.817715\n",
      "ep 511: ep_len:173 episode reward: total was -4.910000. running mean: -31.548637\n",
      "ep 511: ep_len:620 episode reward: total was -41.010000. running mean: -31.643251\n",
      "epsilon:0.330112 episode_count: 3584. steps_count: 1615580.000000\n",
      "ep 512: ep_len:128 episode reward: total was -6.950000. running mean: -31.396319\n",
      "ep 512: ep_len:525 episode reward: total was -36.740000. running mean: -31.449755\n",
      "ep 512: ep_len:555 episode reward: total was -48.120000. running mean: -31.616458\n",
      "ep 512: ep_len:500 episode reward: total was -38.190000. running mean: -31.682193\n",
      "ep 512: ep_len:108 episode reward: total was -0.480000. running mean: -31.370171\n",
      "ep 512: ep_len:186 episode reward: total was -7.450000. running mean: -31.130970\n",
      "ep 512: ep_len:295 episode reward: total was -16.340000. running mean: -30.983060\n",
      "epsilon:0.329975 episode_count: 3591. steps_count: 1617877.000000\n",
      "ep 513: ep_len:500 episode reward: total was -24.290000. running mean: -30.916129\n",
      "ep 513: ep_len:515 episode reward: total was -32.050000. running mean: -30.927468\n",
      "ep 513: ep_len:535 episode reward: total was -33.730000. running mean: -30.955493\n",
      "ep 513: ep_len:535 episode reward: total was -67.280000. running mean: -31.318738\n",
      "ep 513: ep_len:121 episode reward: total was -12.950000. running mean: -31.135051\n",
      "ep 513: ep_len:515 episode reward: total was -31.060000. running mean: -31.134301\n",
      "ep 513: ep_len:500 episode reward: total was -22.120000. running mean: -31.044158\n",
      "epsilon:0.329839 episode_count: 3598. steps_count: 1621098.000000\n",
      "ep 514: ep_len:505 episode reward: total was -19.740000. running mean: -30.931116\n",
      "ep 514: ep_len:520 episode reward: total was -48.100000. running mean: -31.102805\n",
      "ep 514: ep_len:500 episode reward: total was -26.680000. running mean: -31.058577\n",
      "ep 514: ep_len:595 episode reward: total was -34.160000. running mean: -31.089591\n",
      "ep 514: ep_len:81 episode reward: total was -0.470000. running mean: -30.783395\n",
      "ep 514: ep_len:505 episode reward: total was -35.230000. running mean: -30.827861\n",
      "ep 514: ep_len:555 episode reward: total was -37.760000. running mean: -30.897183\n",
      "epsilon:0.329702 episode_count: 3605. steps_count: 1624359.000000\n",
      "ep 515: ep_len:570 episode reward: total was -45.440000. running mean: -31.042611\n",
      "ep 515: ep_len:505 episode reward: total was -52.860000. running mean: -31.260785\n",
      "ep 515: ep_len:565 episode reward: total was -76.290000. running mean: -31.711077\n",
      "ep 515: ep_len:590 episode reward: total was -32.650000. running mean: -31.720466\n",
      "ep 515: ep_len:108 episode reward: total was -14.470000. running mean: -31.547961\n",
      "ep 515: ep_len:565 episode reward: total was -53.070000. running mean: -31.763182\n",
      "ep 515: ep_len:500 episode reward: total was -35.620000. running mean: -31.801750\n",
      "epsilon:0.329566 episode_count: 3612. steps_count: 1627762.000000\n",
      "ep 516: ep_len:218 episode reward: total was -2.420000. running mean: -31.507932\n",
      "ep 516: ep_len:366 episode reward: total was -24.380000. running mean: -31.436653\n",
      "ep 516: ep_len:500 episode reward: total was -29.970000. running mean: -31.421987\n",
      "ep 516: ep_len:505 episode reward: total was -34.770000. running mean: -31.455467\n",
      "ep 516: ep_len:3 episode reward: total was 0.000000. running mean: -31.140912\n",
      "ep 516: ep_len:240 episode reward: total was -21.350000. running mean: -31.043003\n",
      "ep 516: ep_len:515 episode reward: total was -36.050000. running mean: -31.093073\n",
      "epsilon:0.329429 episode_count: 3619. steps_count: 1630109.000000\n",
      "ep 517: ep_len:610 episode reward: total was -44.950000. running mean: -31.231642\n",
      "ep 517: ep_len:505 episode reward: total was -20.920000. running mean: -31.128526\n",
      "ep 517: ep_len:79 episode reward: total was -2.470000. running mean: -30.841940\n",
      "ep 517: ep_len:610 episode reward: total was -47.130000. running mean: -31.004821\n",
      "ep 517: ep_len:3 episode reward: total was 0.000000. running mean: -30.694773\n",
      "ep 517: ep_len:317 episode reward: total was -36.410000. running mean: -30.751925\n",
      "ep 517: ep_len:535 episode reward: total was -33.040000. running mean: -30.774806\n",
      "epsilon:0.329293 episode_count: 3626. steps_count: 1632768.000000\n",
      "ep 518: ep_len:535 episode reward: total was -37.100000. running mean: -30.838058\n",
      "ep 518: ep_len:505 episode reward: total was -27.910000. running mean: -30.808777\n",
      "ep 518: ep_len:630 episode reward: total was -39.350000. running mean: -30.894189\n",
      "ep 518: ep_len:515 episode reward: total was -85.930000. running mean: -31.444548\n",
      "ep 518: ep_len:87 episode reward: total was -12.960000. running mean: -31.259702\n",
      "ep 518: ep_len:248 episode reward: total was -6.400000. running mean: -31.011105\n",
      "ep 518: ep_len:595 episode reward: total was -46.510000. running mean: -31.166094\n",
      "epsilon:0.329156 episode_count: 3633. steps_count: 1635883.000000\n",
      "ep 519: ep_len:580 episode reward: total was -41.220000. running mean: -31.266633\n",
      "ep 519: ep_len:545 episode reward: total was -21.290000. running mean: -31.166867\n",
      "ep 519: ep_len:530 episode reward: total was -31.580000. running mean: -31.170998\n",
      "ep 519: ep_len:56 episode reward: total was -7.480000. running mean: -30.934088\n",
      "ep 519: ep_len:3 episode reward: total was 0.000000. running mean: -30.624747\n",
      "ep 519: ep_len:515 episode reward: total was -47.170000. running mean: -30.790200\n",
      "ep 519: ep_len:565 episode reward: total was -35.650000. running mean: -30.838798\n",
      "epsilon:0.329020 episode_count: 3640. steps_count: 1638677.000000\n",
      "ep 520: ep_len:540 episode reward: total was -34.620000. running mean: -30.876610\n",
      "ep 520: ep_len:565 episode reward: total was -32.720000. running mean: -30.895044\n",
      "ep 520: ep_len:500 episode reward: total was -46.700000. running mean: -31.053093\n",
      "ep 520: ep_len:515 episode reward: total was -38.700000. running mean: -31.129562\n",
      "ep 520: ep_len:3 episode reward: total was 0.000000. running mean: -30.818267\n",
      "ep 520: ep_len:550 episode reward: total was -22.890000. running mean: -30.738984\n",
      "ep 520: ep_len:565 episode reward: total was -47.220000. running mean: -30.903794\n",
      "epsilon:0.328883 episode_count: 3647. steps_count: 1641915.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 521: ep_len:570 episode reward: total was -33.350000. running mean: -30.928256\n",
      "ep 521: ep_len:525 episode reward: total was -38.220000. running mean: -31.001174\n",
      "ep 521: ep_len:775 episode reward: total was -68.950000. running mean: -31.380662\n",
      "ep 521: ep_len:615 episode reward: total was -55.710000. running mean: -31.623955\n",
      "ep 521: ep_len:55 episode reward: total was -8.490000. running mean: -31.392616\n",
      "ep 521: ep_len:500 episode reward: total was -38.330000. running mean: -31.461990\n",
      "ep 521: ep_len:292 episode reward: total was -21.890000. running mean: -31.366270\n",
      "epsilon:0.328747 episode_count: 3654. steps_count: 1645247.000000\n",
      "ep 522: ep_len:605 episode reward: total was -48.400000. running mean: -31.536607\n",
      "ep 522: ep_len:500 episode reward: total was -17.930000. running mean: -31.400541\n",
      "ep 522: ep_len:565 episode reward: total was -27.230000. running mean: -31.358836\n",
      "ep 522: ep_len:590 episode reward: total was -31.660000. running mean: -31.361847\n",
      "ep 522: ep_len:73 episode reward: total was -0.490000. running mean: -31.053129\n",
      "ep 522: ep_len:650 episode reward: total was -47.470000. running mean: -31.217297\n",
      "ep 522: ep_len:845 episode reward: total was -81.880000. running mean: -31.723924\n",
      "epsilon:0.328610 episode_count: 3661. steps_count: 1649075.000000\n",
      "ep 523: ep_len:520 episode reward: total was -49.090000. running mean: -31.897585\n",
      "ep 523: ep_len:630 episode reward: total was -42.480000. running mean: -32.003409\n",
      "ep 523: ep_len:565 episode reward: total was -47.010000. running mean: -32.153475\n",
      "ep 523: ep_len:520 episode reward: total was -20.690000. running mean: -32.038840\n",
      "ep 523: ep_len:84 episode reward: total was -7.480000. running mean: -31.793252\n",
      "ep 523: ep_len:500 episode reward: total was -26.160000. running mean: -31.736920\n",
      "ep 523: ep_len:580 episode reward: total was -77.720000. running mean: -32.196750\n",
      "epsilon:0.328474 episode_count: 3668. steps_count: 1652474.000000\n",
      "ep 524: ep_len:240 episode reward: total was -19.430000. running mean: -32.069083\n",
      "ep 524: ep_len:335 episode reward: total was -23.430000. running mean: -31.982692\n",
      "ep 524: ep_len:505 episode reward: total was -47.070000. running mean: -32.133565\n",
      "ep 524: ep_len:550 episode reward: total was -34.690000. running mean: -32.159129\n",
      "ep 524: ep_len:3 episode reward: total was 0.000000. running mean: -31.837538\n",
      "ep 524: ep_len:510 episode reward: total was -47.890000. running mean: -31.998063\n",
      "ep 524: ep_len:540 episode reward: total was -23.120000. running mean: -31.909282\n",
      "epsilon:0.328337 episode_count: 3675. steps_count: 1655157.000000\n",
      "ep 525: ep_len:515 episode reward: total was -20.770000. running mean: -31.797889\n",
      "ep 525: ep_len:594 episode reward: total was -56.600000. running mean: -32.045910\n",
      "ep 525: ep_len:545 episode reward: total was -46.560000. running mean: -32.191051\n",
      "ep 525: ep_len:565 episode reward: total was -43.290000. running mean: -32.302041\n",
      "ep 525: ep_len:111 episode reward: total was -15.470000. running mean: -32.133720\n",
      "ep 525: ep_len:590 episode reward: total was -60.120000. running mean: -32.413583\n",
      "ep 525: ep_len:550 episode reward: total was -62.770000. running mean: -32.717147\n",
      "epsilon:0.328201 episode_count: 3682. steps_count: 1658627.000000\n",
      "ep 526: ep_len:500 episode reward: total was -25.200000. running mean: -32.641976\n",
      "ep 526: ep_len:530 episode reward: total was -18.150000. running mean: -32.497056\n",
      "ep 526: ep_len:565 episode reward: total was -45.640000. running mean: -32.628486\n",
      "ep 526: ep_len:750 episode reward: total was -69.030000. running mean: -32.992501\n",
      "ep 526: ep_len:85 episode reward: total was -7.470000. running mean: -32.737276\n",
      "ep 526: ep_len:530 episode reward: total was -42.080000. running mean: -32.830703\n",
      "ep 526: ep_len:625 episode reward: total was -57.150000. running mean: -33.073896\n",
      "epsilon:0.328064 episode_count: 3689. steps_count: 1662212.000000\n",
      "ep 527: ep_len:605 episode reward: total was -94.270000. running mean: -33.685857\n",
      "ep 527: ep_len:530 episode reward: total was -46.750000. running mean: -33.816498\n",
      "ep 527: ep_len:366 episode reward: total was -26.880000. running mean: -33.747133\n",
      "ep 527: ep_len:500 episode reward: total was -25.610000. running mean: -33.665762\n",
      "ep 527: ep_len:3 episode reward: total was 0.000000. running mean: -33.329104\n",
      "ep 527: ep_len:645 episode reward: total was -53.580000. running mean: -33.531613\n",
      "ep 527: ep_len:515 episode reward: total was -30.170000. running mean: -33.497997\n",
      "epsilon:0.327928 episode_count: 3696. steps_count: 1665376.000000\n",
      "ep 528: ep_len:615 episode reward: total was -58.040000. running mean: -33.743417\n",
      "ep 528: ep_len:550 episode reward: total was -57.600000. running mean: -33.981983\n",
      "ep 528: ep_len:550 episode reward: total was -26.920000. running mean: -33.911363\n",
      "ep 528: ep_len:103 episode reward: total was -9.460000. running mean: -33.666850\n",
      "ep 528: ep_len:51 episode reward: total was -1.000000. running mean: -33.340181\n",
      "ep 528: ep_len:520 episode reward: total was -45.140000. running mean: -33.458179\n",
      "ep 528: ep_len:530 episode reward: total was -21.890000. running mean: -33.342498\n",
      "epsilon:0.327791 episode_count: 3703. steps_count: 1668295.000000\n",
      "ep 529: ep_len:570 episode reward: total was -42.930000. running mean: -33.438373\n",
      "ep 529: ep_len:560 episode reward: total was -49.130000. running mean: -33.595289\n",
      "ep 529: ep_len:760 episode reward: total was -82.490000. running mean: -34.084236\n",
      "ep 529: ep_len:505 episode reward: total was -41.690000. running mean: -34.160294\n",
      "ep 529: ep_len:3 episode reward: total was 0.000000. running mean: -33.818691\n",
      "ep 529: ep_len:520 episode reward: total was -35.670000. running mean: -33.837204\n",
      "ep 529: ep_len:595 episode reward: total was -43.170000. running mean: -33.930532\n",
      "epsilon:0.327655 episode_count: 3710. steps_count: 1671808.000000\n",
      "ep 530: ep_len:505 episode reward: total was -54.330000. running mean: -34.134526\n",
      "ep 530: ep_len:630 episode reward: total was -36.650000. running mean: -34.159681\n",
      "ep 530: ep_len:600 episode reward: total was -49.830000. running mean: -34.316384\n",
      "ep 530: ep_len:565 episode reward: total was -37.150000. running mean: -34.344721\n",
      "ep 530: ep_len:3 episode reward: total was 0.000000. running mean: -34.001273\n",
      "ep 530: ep_len:500 episode reward: total was -33.890000. running mean: -34.000161\n",
      "ep 530: ep_len:285 episode reward: total was -28.410000. running mean: -33.944259\n",
      "epsilon:0.327518 episode_count: 3717. steps_count: 1674896.000000\n",
      "ep 531: ep_len:650 episode reward: total was -71.230000. running mean: -34.317116\n",
      "ep 531: ep_len:605 episode reward: total was -40.470000. running mean: -34.378645\n",
      "ep 531: ep_len:515 episode reward: total was -29.840000. running mean: -34.333259\n",
      "ep 531: ep_len:170 episode reward: total was -8.420000. running mean: -34.074126\n",
      "ep 531: ep_len:96 episode reward: total was 3.050000. running mean: -33.702885\n",
      "ep 531: ep_len:610 episode reward: total was -38.210000. running mean: -33.747956\n",
      "ep 531: ep_len:332 episode reward: total was -27.820000. running mean: -33.688676\n",
      "epsilon:0.327382 episode_count: 3724. steps_count: 1677874.000000\n",
      "ep 532: ep_len:595 episode reward: total was -31.030000. running mean: -33.662090\n",
      "ep 532: ep_len:334 episode reward: total was -17.380000. running mean: -33.499269\n",
      "ep 532: ep_len:535 episode reward: total was -20.670000. running mean: -33.370976\n",
      "ep 532: ep_len:515 episode reward: total was -8.730000. running mean: -33.124566\n",
      "ep 532: ep_len:3 episode reward: total was 0.000000. running mean: -32.793321\n",
      "ep 532: ep_len:535 episode reward: total was -44.150000. running mean: -32.906888\n",
      "ep 532: ep_len:505 episode reward: total was -40.760000. running mean: -32.985419\n",
      "epsilon:0.327245 episode_count: 3731. steps_count: 1680896.000000\n",
      "ep 533: ep_len:520 episode reward: total was -33.210000. running mean: -32.987664\n",
      "ep 533: ep_len:500 episode reward: total was -27.610000. running mean: -32.933888\n",
      "ep 533: ep_len:570 episode reward: total was -38.210000. running mean: -32.986649\n",
      "ep 533: ep_len:505 episode reward: total was -22.760000. running mean: -32.884382\n",
      "ep 533: ep_len:3 episode reward: total was 0.000000. running mean: -32.555539\n",
      "ep 533: ep_len:515 episode reward: total was -31.260000. running mean: -32.542583\n",
      "ep 533: ep_len:500 episode reward: total was -22.580000. running mean: -32.442957\n",
      "epsilon:0.327109 episode_count: 3738. steps_count: 1684009.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 534: ep_len:625 episode reward: total was -34.200000. running mean: -32.460528\n",
      "ep 534: ep_len:560 episode reward: total was -41.270000. running mean: -32.548623\n",
      "ep 534: ep_len:540 episode reward: total was -27.720000. running mean: -32.500336\n",
      "ep 534: ep_len:121 episode reward: total was -5.460000. running mean: -32.229933\n",
      "ep 534: ep_len:106 episode reward: total was -8.990000. running mean: -31.997534\n",
      "ep 534: ep_len:261 episode reward: total was -7.900000. running mean: -31.756558\n",
      "ep 534: ep_len:620 episode reward: total was -37.460000. running mean: -31.813593\n",
      "epsilon:0.326972 episode_count: 3745. steps_count: 1686842.000000\n",
      "ep 535: ep_len:234 episode reward: total was -8.970000. running mean: -31.585157\n",
      "ep 535: ep_len:640 episode reward: total was -47.960000. running mean: -31.748905\n",
      "ep 535: ep_len:382 episode reward: total was -24.850000. running mean: -31.679916\n",
      "ep 535: ep_len:520 episode reward: total was -29.740000. running mean: -31.660517\n",
      "ep 535: ep_len:75 episode reward: total was 1.020000. running mean: -31.333712\n",
      "ep 535: ep_len:610 episode reward: total was -51.120000. running mean: -31.531575\n",
      "ep 535: ep_len:580 episode reward: total was -38.030000. running mean: -31.596559\n",
      "epsilon:0.326836 episode_count: 3752. steps_count: 1689883.000000\n",
      "ep 536: ep_len:555 episode reward: total was -36.670000. running mean: -31.647293\n",
      "ep 536: ep_len:525 episode reward: total was -35.150000. running mean: -31.682320\n",
      "ep 536: ep_len:454 episode reward: total was -19.810000. running mean: -31.563597\n",
      "ep 536: ep_len:750 episode reward: total was -84.860000. running mean: -32.096561\n",
      "ep 536: ep_len:3 episode reward: total was 0.000000. running mean: -31.775596\n",
      "ep 536: ep_len:545 episode reward: total was -56.110000. running mean: -32.018940\n",
      "ep 536: ep_len:515 episode reward: total was -33.050000. running mean: -32.029250\n",
      "epsilon:0.326699 episode_count: 3759. steps_count: 1693230.000000\n",
      "ep 537: ep_len:500 episode reward: total was -30.400000. running mean: -32.012958\n",
      "ep 537: ep_len:580 episode reward: total was -7.750000. running mean: -31.770328\n",
      "ep 537: ep_len:540 episode reward: total was -35.220000. running mean: -31.804825\n",
      "ep 537: ep_len:520 episode reward: total was -43.170000. running mean: -31.918477\n",
      "ep 537: ep_len:3 episode reward: total was 0.000000. running mean: -31.599292\n",
      "ep 537: ep_len:515 episode reward: total was -53.160000. running mean: -31.814899\n",
      "ep 537: ep_len:585 episode reward: total was -55.200000. running mean: -32.048750\n",
      "epsilon:0.326563 episode_count: 3766. steps_count: 1696473.000000\n",
      "ep 538: ep_len:530 episode reward: total was -28.840000. running mean: -32.016663\n",
      "ep 538: ep_len:540 episode reward: total was -40.050000. running mean: -32.096996\n",
      "ep 538: ep_len:615 episode reward: total was -79.660000. running mean: -32.572626\n",
      "ep 538: ep_len:500 episode reward: total was -32.240000. running mean: -32.569300\n",
      "ep 538: ep_len:3 episode reward: total was 0.000000. running mean: -32.243607\n",
      "ep 538: ep_len:560 episode reward: total was -20.990000. running mean: -32.131071\n",
      "ep 538: ep_len:600 episode reward: total was -29.440000. running mean: -32.104160\n",
      "epsilon:0.326426 episode_count: 3773. steps_count: 1699821.000000\n",
      "ep 539: ep_len:243 episode reward: total was -22.440000. running mean: -32.007518\n",
      "ep 539: ep_len:337 episode reward: total was -24.870000. running mean: -31.936143\n",
      "ep 539: ep_len:620 episode reward: total was -50.170000. running mean: -32.118482\n",
      "ep 539: ep_len:515 episode reward: total was -26.160000. running mean: -32.058897\n",
      "ep 539: ep_len:3 episode reward: total was 0.000000. running mean: -31.738308\n",
      "ep 539: ep_len:655 episode reward: total was -45.180000. running mean: -31.872725\n",
      "ep 539: ep_len:515 episode reward: total was -37.040000. running mean: -31.924398\n",
      "epsilon:0.326290 episode_count: 3780. steps_count: 1702709.000000\n",
      "ep 540: ep_len:625 episode reward: total was -57.800000. running mean: -32.183154\n",
      "ep 540: ep_len:520 episode reward: total was -50.680000. running mean: -32.368122\n",
      "ep 540: ep_len:765 episode reward: total was -66.430000. running mean: -32.708741\n",
      "ep 540: ep_len:550 episode reward: total was -38.700000. running mean: -32.768653\n",
      "ep 540: ep_len:3 episode reward: total was 0.000000. running mean: -32.440967\n",
      "ep 540: ep_len:500 episode reward: total was -14.700000. running mean: -32.263557\n",
      "ep 540: ep_len:530 episode reward: total was -85.380000. running mean: -32.794722\n",
      "epsilon:0.326153 episode_count: 3787. steps_count: 1706202.000000\n",
      "ep 541: ep_len:640 episode reward: total was -49.120000. running mean: -32.957974\n",
      "ep 541: ep_len:790 episode reward: total was -115.180000. running mean: -33.780195\n",
      "ep 541: ep_len:79 episode reward: total was -1.980000. running mean: -33.462193\n",
      "ep 541: ep_len:515 episode reward: total was -36.660000. running mean: -33.494171\n",
      "ep 541: ep_len:3 episode reward: total was 0.000000. running mean: -33.159229\n",
      "ep 541: ep_len:500 episode reward: total was -38.140000. running mean: -33.209037\n",
      "ep 541: ep_len:590 episode reward: total was -39.650000. running mean: -33.273446\n",
      "epsilon:0.326017 episode_count: 3794. steps_count: 1709319.000000\n",
      "ep 542: ep_len:500 episode reward: total was -25.930000. running mean: -33.200012\n",
      "ep 542: ep_len:500 episode reward: total was -44.150000. running mean: -33.309512\n",
      "ep 542: ep_len:72 episode reward: total was -0.980000. running mean: -32.986217\n",
      "ep 542: ep_len:500 episode reward: total was -27.650000. running mean: -32.932855\n",
      "ep 542: ep_len:3 episode reward: total was 0.000000. running mean: -32.603526\n",
      "ep 542: ep_len:570 episode reward: total was -36.960000. running mean: -32.647091\n",
      "ep 542: ep_len:510 episode reward: total was -42.590000. running mean: -32.746520\n",
      "epsilon:0.325880 episode_count: 3801. steps_count: 1711974.000000\n",
      "ep 543: ep_len:635 episode reward: total was -49.140000. running mean: -32.910455\n",
      "ep 543: ep_len:176 episode reward: total was -25.440000. running mean: -32.835750\n",
      "ep 543: ep_len:79 episode reward: total was 0.040000. running mean: -32.506993\n",
      "ep 543: ep_len:575 episode reward: total was -41.190000. running mean: -32.593823\n",
      "ep 543: ep_len:110 episode reward: total was 4.030000. running mean: -32.227584\n",
      "ep 543: ep_len:500 episode reward: total was -52.880000. running mean: -32.434109\n",
      "ep 543: ep_len:525 episode reward: total was -34.040000. running mean: -32.450168\n",
      "epsilon:0.325744 episode_count: 3808. steps_count: 1714574.000000\n",
      "ep 544: ep_len:505 episode reward: total was -64.760000. running mean: -32.773266\n",
      "ep 544: ep_len:500 episode reward: total was -22.680000. running mean: -32.672333\n",
      "ep 544: ep_len:530 episode reward: total was -32.010000. running mean: -32.665710\n",
      "ep 544: ep_len:545 episode reward: total was -29.680000. running mean: -32.635853\n",
      "ep 544: ep_len:3 episode reward: total was 0.000000. running mean: -32.309494\n",
      "ep 544: ep_len:500 episode reward: total was -20.140000. running mean: -32.187799\n",
      "ep 544: ep_len:625 episode reward: total was -33.920000. running mean: -32.205121\n",
      "epsilon:0.325607 episode_count: 3815. steps_count: 1717782.000000\n",
      "ep 545: ep_len:505 episode reward: total was -30.240000. running mean: -32.185470\n",
      "ep 545: ep_len:500 episode reward: total was -33.670000. running mean: -32.200315\n",
      "ep 545: ep_len:75 episode reward: total was -7.490000. running mean: -31.953212\n",
      "ep 545: ep_len:56 episode reward: total was -6.490000. running mean: -31.698580\n",
      "ep 545: ep_len:3 episode reward: total was 0.000000. running mean: -31.381594\n",
      "ep 545: ep_len:510 episode reward: total was -59.190000. running mean: -31.659678\n",
      "ep 545: ep_len:615 episode reward: total was -36.150000. running mean: -31.704582\n",
      "epsilon:0.325471 episode_count: 3822. steps_count: 1720046.000000\n",
      "ep 546: ep_len:635 episode reward: total was -44.900000. running mean: -31.836536\n",
      "ep 546: ep_len:680 episode reward: total was -70.040000. running mean: -32.218570\n",
      "ep 546: ep_len:500 episode reward: total was -19.270000. running mean: -32.089085\n",
      "ep 546: ep_len:500 episode reward: total was -23.210000. running mean: -32.000294\n",
      "ep 546: ep_len:3 episode reward: total was 0.000000. running mean: -31.680291\n",
      "ep 546: ep_len:520 episode reward: total was -36.610000. running mean: -31.729588\n",
      "ep 546: ep_len:580 episode reward: total was -45.590000. running mean: -31.868192\n",
      "epsilon:0.325334 episode_count: 3829. steps_count: 1723464.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 547: ep_len:225 episode reward: total was -16.880000. running mean: -31.718310\n",
      "ep 547: ep_len:500 episode reward: total was -24.600000. running mean: -31.647127\n",
      "ep 547: ep_len:550 episode reward: total was -61.610000. running mean: -31.946756\n",
      "ep 547: ep_len:505 episode reward: total was -26.750000. running mean: -31.894788\n",
      "ep 547: ep_len:3 episode reward: total was 0.000000. running mean: -31.575840\n",
      "ep 547: ep_len:620 episode reward: total was -50.130000. running mean: -31.761382\n",
      "ep 547: ep_len:500 episode reward: total was -40.850000. running mean: -31.852268\n",
      "epsilon:0.325198 episode_count: 3836. steps_count: 1726367.000000\n",
      "ep 548: ep_len:122 episode reward: total was -16.980000. running mean: -31.703545\n",
      "ep 548: ep_len:635 episode reward: total was -31.700000. running mean: -31.703510\n",
      "ep 548: ep_len:79 episode reward: total was -4.980000. running mean: -31.436275\n",
      "ep 548: ep_len:530 episode reward: total was -21.610000. running mean: -31.338012\n",
      "ep 548: ep_len:87 episode reward: total was -9.980000. running mean: -31.124432\n",
      "ep 548: ep_len:186 episode reward: total was -14.970000. running mean: -30.962888\n",
      "ep 548: ep_len:570 episode reward: total was -50.710000. running mean: -31.160359\n",
      "epsilon:0.325061 episode_count: 3843. steps_count: 1728576.000000\n",
      "ep 549: ep_len:500 episode reward: total was -52.660000. running mean: -31.375355\n",
      "ep 549: ep_len:500 episode reward: total was -32.820000. running mean: -31.389802\n",
      "ep 549: ep_len:535 episode reward: total was -37.770000. running mean: -31.453604\n",
      "ep 549: ep_len:500 episode reward: total was -31.140000. running mean: -31.450468\n",
      "ep 549: ep_len:3 episode reward: total was 0.000000. running mean: -31.135963\n",
      "ep 549: ep_len:500 episode reward: total was -26.340000. running mean: -31.088003\n",
      "ep 549: ep_len:358 episode reward: total was -25.340000. running mean: -31.030523\n",
      "epsilon:0.324925 episode_count: 3850. steps_count: 1731472.000000\n",
      "ep 550: ep_len:247 episode reward: total was -14.930000. running mean: -30.869518\n",
      "ep 550: ep_len:580 episode reward: total was -36.160000. running mean: -30.922423\n",
      "ep 550: ep_len:500 episode reward: total was -24.820000. running mean: -30.861399\n",
      "ep 550: ep_len:530 episode reward: total was -23.560000. running mean: -30.788385\n",
      "ep 550: ep_len:3 episode reward: total was 0.000000. running mean: -30.480501\n",
      "ep 550: ep_len:510 episode reward: total was -35.660000. running mean: -30.532296\n",
      "ep 550: ep_len:515 episode reward: total was -37.180000. running mean: -30.598773\n",
      "epsilon:0.324788 episode_count: 3857. steps_count: 1734357.000000\n",
      "ep 551: ep_len:525 episode reward: total was -34.070000. running mean: -30.633485\n",
      "ep 551: ep_len:505 episode reward: total was -44.920000. running mean: -30.776350\n",
      "ep 551: ep_len:515 episode reward: total was -39.720000. running mean: -30.865787\n",
      "ep 551: ep_len:525 episode reward: total was -38.620000. running mean: -30.943329\n",
      "ep 551: ep_len:114 episode reward: total was 2.530000. running mean: -30.608596\n",
      "ep 551: ep_len:575 episode reward: total was -26.910000. running mean: -30.571610\n",
      "ep 551: ep_len:545 episode reward: total was -34.000000. running mean: -30.605894\n",
      "epsilon:0.324652 episode_count: 3864. steps_count: 1737661.000000\n",
      "ep 552: ep_len:131 episode reward: total was -17.970000. running mean: -30.479535\n",
      "ep 552: ep_len:515 episode reward: total was -38.190000. running mean: -30.556639\n",
      "ep 552: ep_len:585 episode reward: total was -55.130000. running mean: -30.802373\n",
      "ep 552: ep_len:500 episode reward: total was -60.360000. running mean: -31.097949\n",
      "ep 552: ep_len:77 episode reward: total was -3.960000. running mean: -30.826570\n",
      "ep 552: ep_len:535 episode reward: total was -50.790000. running mean: -31.026204\n",
      "ep 552: ep_len:555 episode reward: total was -38.510000. running mean: -31.101042\n",
      "epsilon:0.324515 episode_count: 3871. steps_count: 1740559.000000\n",
      "ep 553: ep_len:590 episode reward: total was -39.730000. running mean: -31.187332\n",
      "ep 553: ep_len:500 episode reward: total was -24.870000. running mean: -31.124158\n",
      "ep 553: ep_len:79 episode reward: total was -4.980000. running mean: -30.862717\n",
      "ep 553: ep_len:56 episode reward: total was -4.970000. running mean: -30.603789\n",
      "ep 553: ep_len:97 episode reward: total was -11.960000. running mean: -30.417352\n",
      "ep 553: ep_len:253 episode reward: total was -5.900000. running mean: -30.172178\n",
      "ep 553: ep_len:560 episode reward: total was -35.150000. running mean: -30.221956\n",
      "epsilon:0.324379 episode_count: 3878. steps_count: 1742694.000000\n",
      "ep 554: ep_len:555 episode reward: total was -18.720000. running mean: -30.106937\n",
      "ep 554: ep_len:715 episode reward: total was -57.500000. running mean: -30.380867\n",
      "ep 554: ep_len:620 episode reward: total was -28.220000. running mean: -30.359259\n",
      "ep 554: ep_len:585 episode reward: total was -32.820000. running mean: -30.383866\n",
      "ep 554: ep_len:3 episode reward: total was 0.000000. running mean: -30.080027\n",
      "ep 554: ep_len:605 episode reward: total was -37.350000. running mean: -30.152727\n",
      "ep 554: ep_len:570 episode reward: total was -35.050000. running mean: -30.201700\n",
      "epsilon:0.324242 episode_count: 3885. steps_count: 1746347.000000\n",
      "ep 555: ep_len:500 episode reward: total was -25.180000. running mean: -30.151483\n",
      "ep 555: ep_len:575 episode reward: total was -38.620000. running mean: -30.236168\n",
      "ep 555: ep_len:540 episode reward: total was -36.190000. running mean: -30.295706\n",
      "ep 555: ep_len:555 episode reward: total was -66.810000. running mean: -30.660849\n",
      "ep 555: ep_len:89 episode reward: total was -7.990000. running mean: -30.434141\n",
      "ep 555: ep_len:295 episode reward: total was -20.430000. running mean: -30.334099\n",
      "ep 555: ep_len:590 episode reward: total was -54.170000. running mean: -30.572458\n",
      "epsilon:0.324106 episode_count: 3892. steps_count: 1749491.000000\n",
      "ep 556: ep_len:645 episode reward: total was -30.050000. running mean: -30.567234\n",
      "ep 556: ep_len:545 episode reward: total was -11.720000. running mean: -30.378761\n",
      "ep 556: ep_len:555 episode reward: total was -42.550000. running mean: -30.500474\n",
      "ep 556: ep_len:560 episode reward: total was -30.210000. running mean: -30.497569\n",
      "ep 556: ep_len:29 episode reward: total was -3.500000. running mean: -30.227593\n",
      "ep 556: ep_len:570 episode reward: total was -25.690000. running mean: -30.182218\n",
      "ep 556: ep_len:540 episode reward: total was -24.990000. running mean: -30.130295\n",
      "epsilon:0.323969 episode_count: 3899. steps_count: 1752935.000000\n",
      "ep 557: ep_len:260 episode reward: total was -10.400000. running mean: -29.932992\n",
      "ep 557: ep_len:530 episode reward: total was -18.290000. running mean: -29.816562\n",
      "ep 557: ep_len:580 episode reward: total was -51.560000. running mean: -30.033997\n",
      "ep 557: ep_len:570 episode reward: total was -55.660000. running mean: -30.290257\n",
      "ep 557: ep_len:103 episode reward: total was 5.020000. running mean: -29.937154\n",
      "ep 557: ep_len:645 episode reward: total was -56.820000. running mean: -30.205983\n",
      "ep 557: ep_len:525 episode reward: total was -50.750000. running mean: -30.411423\n",
      "epsilon:0.323833 episode_count: 3906. steps_count: 1756148.000000\n",
      "ep 558: ep_len:123 episode reward: total was -17.470000. running mean: -30.282009\n",
      "ep 558: ep_len:580 episode reward: total was -61.230000. running mean: -30.591489\n",
      "ep 558: ep_len:595 episode reward: total was -34.050000. running mean: -30.626074\n",
      "ep 558: ep_len:605 episode reward: total was -54.110000. running mean: -30.860913\n",
      "ep 558: ep_len:3 episode reward: total was 0.000000. running mean: -30.552304\n",
      "ep 558: ep_len:640 episode reward: total was -30.380000. running mean: -30.550581\n",
      "ep 558: ep_len:575 episode reward: total was -37.130000. running mean: -30.616375\n",
      "epsilon:0.323696 episode_count: 3913. steps_count: 1759269.000000\n",
      "ep 559: ep_len:565 episode reward: total was -30.580000. running mean: -30.616011\n",
      "ep 559: ep_len:164 episode reward: total was -11.440000. running mean: -30.424251\n",
      "ep 559: ep_len:575 episode reward: total was -48.070000. running mean: -30.600709\n",
      "ep 559: ep_len:560 episode reward: total was -38.610000. running mean: -30.680802\n",
      "ep 559: ep_len:3 episode reward: total was 0.000000. running mean: -30.373994\n",
      "ep 559: ep_len:675 episode reward: total was -81.610000. running mean: -30.886354\n",
      "ep 559: ep_len:320 episode reward: total was -23.340000. running mean: -30.810890\n",
      "epsilon:0.323560 episode_count: 3920. steps_count: 1762131.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 560: ep_len:575 episode reward: total was -17.150000. running mean: -30.674281\n",
      "ep 560: ep_len:269 episode reward: total was -36.370000. running mean: -30.731238\n",
      "ep 560: ep_len:560 episode reward: total was -26.660000. running mean: -30.690526\n",
      "ep 560: ep_len:552 episode reward: total was -56.580000. running mean: -30.949421\n",
      "ep 560: ep_len:3 episode reward: total was 0.000000. running mean: -30.639927\n",
      "ep 560: ep_len:705 episode reward: total was -67.020000. running mean: -31.003727\n",
      "ep 560: ep_len:339 episode reward: total was -24.330000. running mean: -30.936990\n",
      "epsilon:0.323423 episode_count: 3927. steps_count: 1765134.000000\n",
      "ep 561: ep_len:226 episode reward: total was -22.860000. running mean: -30.856220\n",
      "ep 561: ep_len:505 episode reward: total was -54.930000. running mean: -31.096958\n",
      "ep 561: ep_len:865 episode reward: total was -86.430000. running mean: -31.650288\n",
      "ep 561: ep_len:530 episode reward: total was -56.800000. running mean: -31.901785\n",
      "ep 561: ep_len:3 episode reward: total was 0.000000. running mean: -31.582768\n",
      "ep 561: ep_len:645 episode reward: total was -54.490000. running mean: -31.811840\n",
      "ep 561: ep_len:179 episode reward: total was -13.950000. running mean: -31.633221\n",
      "epsilon:0.323287 episode_count: 3934. steps_count: 1768087.000000\n",
      "ep 562: ep_len:575 episode reward: total was -36.280000. running mean: -31.679689\n",
      "ep 562: ep_len:500 episode reward: total was -19.430000. running mean: -31.557192\n",
      "ep 562: ep_len:376 episode reward: total was -30.880000. running mean: -31.550420\n",
      "ep 562: ep_len:590 episode reward: total was -35.530000. running mean: -31.590216\n",
      "ep 562: ep_len:89 episode reward: total was 1.040000. running mean: -31.263914\n",
      "ep 562: ep_len:295 episode reward: total was -36.420000. running mean: -31.315475\n",
      "ep 562: ep_len:615 episode reward: total was -27.670000. running mean: -31.279020\n",
      "epsilon:0.323150 episode_count: 3941. steps_count: 1771127.000000\n",
      "ep 563: ep_len:635 episode reward: total was -58.550000. running mean: -31.551730\n",
      "ep 563: ep_len:500 episode reward: total was -51.910000. running mean: -31.755313\n",
      "ep 563: ep_len:510 episode reward: total was -59.810000. running mean: -32.035860\n",
      "ep 563: ep_len:500 episode reward: total was -46.820000. running mean: -32.183701\n",
      "ep 563: ep_len:3 episode reward: total was 0.000000. running mean: -31.861864\n",
      "ep 563: ep_len:620 episode reward: total was -59.110000. running mean: -32.134345\n",
      "ep 563: ep_len:186 episode reward: total was -14.420000. running mean: -31.957202\n",
      "epsilon:0.323014 episode_count: 3948. steps_count: 1774081.000000\n",
      "ep 564: ep_len:635 episode reward: total was -55.980000. running mean: -32.197430\n",
      "ep 564: ep_len:352 episode reward: total was -24.900000. running mean: -32.124456\n",
      "ep 564: ep_len:500 episode reward: total was -35.150000. running mean: -32.154711\n",
      "ep 564: ep_len:540 episode reward: total was -42.290000. running mean: -32.256064\n",
      "ep 564: ep_len:3 episode reward: total was 0.000000. running mean: -31.933503\n",
      "ep 564: ep_len:500 episode reward: total was -55.300000. running mean: -32.167168\n",
      "ep 564: ep_len:565 episode reward: total was -33.670000. running mean: -32.182197\n",
      "epsilon:0.322877 episode_count: 3955. steps_count: 1777176.000000\n",
      "ep 565: ep_len:213 episode reward: total was -24.470000. running mean: -32.105075\n",
      "ep 565: ep_len:690 episode reward: total was -56.080000. running mean: -32.344824\n",
      "ep 565: ep_len:450 episode reward: total was -29.330000. running mean: -32.314676\n",
      "ep 565: ep_len:500 episode reward: total was -44.250000. running mean: -32.434029\n",
      "ep 565: ep_len:3 episode reward: total was 0.000000. running mean: -32.109689\n",
      "ep 565: ep_len:555 episode reward: total was -58.400000. running mean: -32.372592\n",
      "ep 565: ep_len:570 episode reward: total was -64.770000. running mean: -32.696566\n",
      "epsilon:0.322741 episode_count: 3962. steps_count: 1780157.000000\n",
      "ep 566: ep_len:235 episode reward: total was -33.940000. running mean: -32.709000\n",
      "ep 566: ep_len:500 episode reward: total was -29.360000. running mean: -32.675510\n",
      "ep 566: ep_len:510 episode reward: total was -73.360000. running mean: -33.082355\n",
      "ep 566: ep_len:500 episode reward: total was -36.780000. running mean: -33.119331\n",
      "ep 566: ep_len:3 episode reward: total was 0.000000. running mean: -32.788138\n",
      "ep 566: ep_len:505 episode reward: total was -31.140000. running mean: -32.771657\n",
      "ep 566: ep_len:555 episode reward: total was -38.540000. running mean: -32.829340\n",
      "epsilon:0.322604 episode_count: 3969. steps_count: 1782965.000000\n",
      "ep 567: ep_len:660 episode reward: total was -68.050000. running mean: -33.181547\n",
      "ep 567: ep_len:275 episode reward: total was -24.420000. running mean: -33.093931\n",
      "ep 567: ep_len:585 episode reward: total was -19.610000. running mean: -32.959092\n",
      "ep 567: ep_len:590 episode reward: total was -37.170000. running mean: -33.001201\n",
      "ep 567: ep_len:3 episode reward: total was 0.000000. running mean: -32.671189\n",
      "ep 567: ep_len:261 episode reward: total was -9.920000. running mean: -32.443677\n",
      "ep 567: ep_len:500 episode reward: total was -41.650000. running mean: -32.535740\n",
      "epsilon:0.322468 episode_count: 3976. steps_count: 1785839.000000\n",
      "ep 568: ep_len:600 episode reward: total was -40.200000. running mean: -32.612383\n",
      "ep 568: ep_len:500 episode reward: total was -33.280000. running mean: -32.619059\n",
      "ep 568: ep_len:371 episode reward: total was -23.880000. running mean: -32.531669\n",
      "ep 568: ep_len:530 episode reward: total was -45.800000. running mean: -32.664352\n",
      "ep 568: ep_len:3 episode reward: total was 0.000000. running mean: -32.337708\n",
      "ep 568: ep_len:650 episode reward: total was -42.970000. running mean: -32.444031\n",
      "ep 568: ep_len:575 episode reward: total was -49.120000. running mean: -32.610791\n",
      "epsilon:0.322331 episode_count: 3983. steps_count: 1789068.000000\n",
      "ep 569: ep_len:219 episode reward: total was -6.430000. running mean: -32.348983\n",
      "ep 569: ep_len:525 episode reward: total was -34.320000. running mean: -32.368693\n",
      "ep 569: ep_len:410 episode reward: total was -28.360000. running mean: -32.328606\n",
      "ep 569: ep_len:159 episode reward: total was -12.430000. running mean: -32.129620\n",
      "ep 569: ep_len:3 episode reward: total was 0.000000. running mean: -31.808324\n",
      "ep 569: ep_len:530 episode reward: total was -32.070000. running mean: -31.810941\n",
      "ep 569: ep_len:590 episode reward: total was -68.720000. running mean: -32.180031\n",
      "epsilon:0.322195 episode_count: 3990. steps_count: 1791504.000000\n",
      "ep 570: ep_len:620 episode reward: total was -29.650000. running mean: -32.154731\n",
      "ep 570: ep_len:375 episode reward: total was -23.320000. running mean: -32.066384\n",
      "ep 570: ep_len:560 episode reward: total was -38.510000. running mean: -32.130820\n",
      "ep 570: ep_len:520 episode reward: total was -39.630000. running mean: -32.205812\n",
      "ep 570: ep_len:3 episode reward: total was 0.000000. running mean: -31.883754\n",
      "ep 570: ep_len:505 episode reward: total was -28.360000. running mean: -31.848516\n",
      "ep 570: ep_len:530 episode reward: total was -34.150000. running mean: -31.871531\n",
      "epsilon:0.322058 episode_count: 3997. steps_count: 1794617.000000\n",
      "ep 571: ep_len:590 episode reward: total was -46.950000. running mean: -32.022316\n",
      "ep 571: ep_len:530 episode reward: total was -38.350000. running mean: -32.085592\n",
      "ep 571: ep_len:500 episode reward: total was -18.050000. running mean: -31.945237\n",
      "ep 571: ep_len:500 episode reward: total was -36.170000. running mean: -31.987484\n",
      "ep 571: ep_len:100 episode reward: total was -2.970000. running mean: -31.697309\n",
      "ep 571: ep_len:500 episode reward: total was -25.870000. running mean: -31.639036\n",
      "ep 571: ep_len:525 episode reward: total was -39.560000. running mean: -31.718246\n",
      "epsilon:0.321922 episode_count: 4004. steps_count: 1797862.000000\n",
      "ep 572: ep_len:565 episode reward: total was -24.680000. running mean: -31.647863\n",
      "ep 572: ep_len:272 episode reward: total was -23.420000. running mean: -31.565585\n",
      "ep 572: ep_len:580 episode reward: total was -61.110000. running mean: -31.861029\n",
      "ep 572: ep_len:500 episode reward: total was -23.820000. running mean: -31.780619\n",
      "ep 572: ep_len:3 episode reward: total was 0.000000. running mean: -31.462812\n",
      "ep 572: ep_len:550 episode reward: total was -28.680000. running mean: -31.434984\n",
      "ep 572: ep_len:545 episode reward: total was -43.730000. running mean: -31.557934\n",
      "epsilon:0.321785 episode_count: 4011. steps_count: 1800877.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 573: ep_len:1085 episode reward: total was -135.770000. running mean: -32.600055\n",
      "ep 573: ep_len:625 episode reward: total was -61.150000. running mean: -32.885555\n",
      "ep 573: ep_len:580 episode reward: total was -50.110000. running mean: -33.057799\n",
      "ep 573: ep_len:153 episode reward: total was -7.420000. running mean: -32.801421\n",
      "ep 573: ep_len:3 episode reward: total was 0.000000. running mean: -32.473407\n",
      "ep 573: ep_len:500 episode reward: total was -29.860000. running mean: -32.447273\n",
      "ep 573: ep_len:590 episode reward: total was -52.710000. running mean: -32.649900\n",
      "epsilon:0.321649 episode_count: 4018. steps_count: 1804413.000000\n",
      "ep 574: ep_len:550 episode reward: total was -19.670000. running mean: -32.520101\n",
      "ep 574: ep_len:500 episode reward: total was -40.180000. running mean: -32.596700\n",
      "ep 574: ep_len:570 episode reward: total was -34.420000. running mean: -32.614933\n",
      "ep 574: ep_len:505 episode reward: total was -31.180000. running mean: -32.600584\n",
      "ep 574: ep_len:3 episode reward: total was 0.000000. running mean: -32.274578\n",
      "ep 574: ep_len:645 episode reward: total was -68.130000. running mean: -32.633132\n",
      "ep 574: ep_len:505 episode reward: total was -30.480000. running mean: -32.611601\n",
      "epsilon:0.321512 episode_count: 4025. steps_count: 1807691.000000\n",
      "ep 575: ep_len:500 episode reward: total was -40.560000. running mean: -32.691085\n",
      "ep 575: ep_len:615 episode reward: total was -36.550000. running mean: -32.729674\n",
      "ep 575: ep_len:545 episode reward: total was -41.540000. running mean: -32.817777\n",
      "ep 575: ep_len:515 episode reward: total was -66.270000. running mean: -33.152299\n",
      "ep 575: ep_len:3 episode reward: total was 0.000000. running mean: -32.820776\n",
      "ep 575: ep_len:695 episode reward: total was -58.550000. running mean: -33.078069\n",
      "ep 575: ep_len:610 episode reward: total was -39.060000. running mean: -33.137888\n",
      "epsilon:0.321376 episode_count: 4032. steps_count: 1811174.000000\n",
      "ep 576: ep_len:112 episode reward: total was -3.470000. running mean: -32.841209\n",
      "ep 576: ep_len:540 episode reward: total was -8.750000. running mean: -32.600297\n",
      "ep 576: ep_len:74 episode reward: total was -2.480000. running mean: -32.299094\n",
      "ep 576: ep_len:525 episode reward: total was -49.320000. running mean: -32.469303\n",
      "ep 576: ep_len:3 episode reward: total was 0.000000. running mean: -32.144610\n",
      "ep 576: ep_len:500 episode reward: total was -42.180000. running mean: -32.244964\n",
      "ep 576: ep_len:580 episode reward: total was -26.620000. running mean: -32.188714\n",
      "epsilon:0.321239 episode_count: 4039. steps_count: 1813508.000000\n",
      "ep 577: ep_len:245 episode reward: total was -24.380000. running mean: -32.110627\n",
      "ep 577: ep_len:500 episode reward: total was -50.810000. running mean: -32.297621\n",
      "ep 577: ep_len:74 episode reward: total was -3.980000. running mean: -32.014445\n",
      "ep 577: ep_len:415 episode reward: total was -42.220000. running mean: -32.116500\n",
      "ep 577: ep_len:3 episode reward: total was 0.000000. running mean: -31.795335\n",
      "ep 577: ep_len:525 episode reward: total was -46.600000. running mean: -31.943382\n",
      "ep 577: ep_len:510 episode reward: total was -32.620000. running mean: -31.950148\n",
      "epsilon:0.321103 episode_count: 4046. steps_count: 1815780.000000\n",
      "ep 578: ep_len:500 episode reward: total was -24.800000. running mean: -31.878647\n",
      "ep 578: ep_len:500 episode reward: total was -31.280000. running mean: -31.872660\n",
      "ep 578: ep_len:595 episode reward: total was -32.650000. running mean: -31.880434\n",
      "ep 578: ep_len:595 episode reward: total was -36.550000. running mean: -31.927129\n",
      "ep 578: ep_len:3 episode reward: total was 0.000000. running mean: -31.607858\n",
      "ep 578: ep_len:565 episode reward: total was -58.600000. running mean: -31.877779\n",
      "ep 578: ep_len:590 episode reward: total was -36.080000. running mean: -31.919802\n",
      "epsilon:0.320966 episode_count: 4053. steps_count: 1819128.000000\n",
      "ep 579: ep_len:620 episode reward: total was -71.240000. running mean: -32.313003\n",
      "ep 579: ep_len:530 episode reward: total was -24.850000. running mean: -32.238373\n",
      "ep 579: ep_len:625 episode reward: total was -37.580000. running mean: -32.291790\n",
      "ep 579: ep_len:500 episode reward: total was -29.690000. running mean: -32.265772\n",
      "ep 579: ep_len:105 episode reward: total was -9.450000. running mean: -32.037614\n",
      "ep 579: ep_len:510 episode reward: total was -51.270000. running mean: -32.229938\n",
      "ep 579: ep_len:322 episode reward: total was -26.350000. running mean: -32.171139\n",
      "epsilon:0.320830 episode_count: 4060. steps_count: 1822340.000000\n",
      "ep 580: ep_len:525 episode reward: total was -32.570000. running mean: -32.175127\n",
      "ep 580: ep_len:585 episode reward: total was -29.740000. running mean: -32.150776\n",
      "ep 580: ep_len:67 episode reward: total was 1.030000. running mean: -31.818968\n",
      "ep 580: ep_len:505 episode reward: total was -55.690000. running mean: -32.057678\n",
      "ep 580: ep_len:93 episode reward: total was -3.970000. running mean: -31.776802\n",
      "ep 580: ep_len:530 episode reward: total was -39.260000. running mean: -31.851634\n",
      "ep 580: ep_len:615 episode reward: total was -29.940000. running mean: -31.832517\n",
      "epsilon:0.320693 episode_count: 4067. steps_count: 1825260.000000\n",
      "ep 581: ep_len:655 episode reward: total was -60.070000. running mean: -32.114892\n",
      "ep 581: ep_len:500 episode reward: total was -51.230000. running mean: -32.306043\n",
      "ep 581: ep_len:500 episode reward: total was -58.690000. running mean: -32.569883\n",
      "ep 581: ep_len:147 episode reward: total was -1.460000. running mean: -32.258784\n",
      "ep 581: ep_len:3 episode reward: total was 0.000000. running mean: -31.936196\n",
      "ep 581: ep_len:605 episode reward: total was -35.210000. running mean: -31.968934\n",
      "ep 581: ep_len:500 episode reward: total was -43.330000. running mean: -32.082545\n",
      "epsilon:0.320557 episode_count: 4074. steps_count: 1828170.000000\n",
      "ep 582: ep_len:675 episode reward: total was -49.980000. running mean: -32.261519\n",
      "ep 582: ep_len:550 episode reward: total was -43.660000. running mean: -32.375504\n",
      "ep 582: ep_len:645 episode reward: total was -32.700000. running mean: -32.378749\n",
      "ep 582: ep_len:500 episode reward: total was -23.250000. running mean: -32.287462\n",
      "ep 582: ep_len:78 episode reward: total was -4.460000. running mean: -32.009187\n",
      "ep 582: ep_len:790 episode reward: total was -79.950000. running mean: -32.488595\n",
      "ep 582: ep_len:500 episode reward: total was -34.150000. running mean: -32.505209\n",
      "epsilon:0.320420 episode_count: 4081. steps_count: 1831908.000000\n",
      "ep 583: ep_len:116 episode reward: total was -13.490000. running mean: -32.315057\n",
      "ep 583: ep_len:575 episode reward: total was -33.610000. running mean: -32.328007\n",
      "ep 583: ep_len:585 episode reward: total was -41.220000. running mean: -32.416927\n",
      "ep 583: ep_len:515 episode reward: total was -48.270000. running mean: -32.575457\n",
      "ep 583: ep_len:48 episode reward: total was -3.000000. running mean: -32.279703\n",
      "ep 583: ep_len:580 episode reward: total was -30.730000. running mean: -32.264206\n",
      "ep 583: ep_len:545 episode reward: total was -34.600000. running mean: -32.287564\n",
      "epsilon:0.320284 episode_count: 4088. steps_count: 1834872.000000\n",
      "ep 584: ep_len:505 episode reward: total was -48.630000. running mean: -32.450988\n",
      "ep 584: ep_len:500 episode reward: total was -19.410000. running mean: -32.320578\n",
      "ep 584: ep_len:820 episode reward: total was -85.020000. running mean: -32.847572\n",
      "ep 584: ep_len:580 episode reward: total was -36.120000. running mean: -32.880297\n",
      "ep 584: ep_len:95 episode reward: total was -0.960000. running mean: -32.561094\n",
      "ep 584: ep_len:595 episode reward: total was -33.200000. running mean: -32.567483\n",
      "ep 584: ep_len:500 episode reward: total was -47.170000. running mean: -32.713508\n",
      "epsilon:0.320147 episode_count: 4095. steps_count: 1838467.000000\n",
      "ep 585: ep_len:510 episode reward: total was -28.800000. running mean: -32.674373\n",
      "ep 585: ep_len:550 episode reward: total was -36.660000. running mean: -32.714229\n",
      "ep 585: ep_len:570 episode reward: total was -46.010000. running mean: -32.847187\n",
      "ep 585: ep_len:570 episode reward: total was -21.690000. running mean: -32.735615\n",
      "ep 585: ep_len:3 episode reward: total was 0.000000. running mean: -32.408259\n",
      "ep 585: ep_len:500 episode reward: total was -46.150000. running mean: -32.545676\n",
      "ep 585: ep_len:313 episode reward: total was -19.350000. running mean: -32.413719\n",
      "epsilon:0.320011 episode_count: 4102. steps_count: 1841483.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 586: ep_len:590 episode reward: total was -30.730000. running mean: -32.396882\n",
      "ep 586: ep_len:510 episode reward: total was -21.420000. running mean: -32.287113\n",
      "ep 586: ep_len:500 episode reward: total was -50.180000. running mean: -32.466042\n",
      "ep 586: ep_len:570 episode reward: total was -23.150000. running mean: -32.372882\n",
      "ep 586: ep_len:3 episode reward: total was 0.000000. running mean: -32.049153\n",
      "ep 586: ep_len:535 episode reward: total was -35.770000. running mean: -32.086361\n",
      "ep 586: ep_len:505 episode reward: total was -46.280000. running mean: -32.228298\n",
      "epsilon:0.319874 episode_count: 4109. steps_count: 1844696.000000\n",
      "ep 587: ep_len:580 episode reward: total was -49.020000. running mean: -32.396215\n",
      "ep 587: ep_len:372 episode reward: total was -34.860000. running mean: -32.420853\n",
      "ep 587: ep_len:444 episode reward: total was -21.770000. running mean: -32.314344\n",
      "ep 587: ep_len:505 episode reward: total was -28.200000. running mean: -32.273201\n",
      "ep 587: ep_len:3 episode reward: total was 0.000000. running mean: -31.950469\n",
      "ep 587: ep_len:590 episode reward: total was -43.960000. running mean: -32.070564\n",
      "ep 587: ep_len:540 episode reward: total was -44.760000. running mean: -32.197458\n",
      "epsilon:0.319738 episode_count: 4116. steps_count: 1847730.000000\n",
      "ep 588: ep_len:500 episode reward: total was -37.670000. running mean: -32.252184\n",
      "ep 588: ep_len:545 episode reward: total was -35.040000. running mean: -32.280062\n",
      "ep 588: ep_len:710 episode reward: total was -61.980000. running mean: -32.577061\n",
      "ep 588: ep_len:500 episode reward: total was -37.720000. running mean: -32.628491\n",
      "ep 588: ep_len:78 episode reward: total was -12.980000. running mean: -32.432006\n",
      "ep 588: ep_len:620 episode reward: total was -30.240000. running mean: -32.410086\n",
      "ep 588: ep_len:500 episode reward: total was -30.580000. running mean: -32.391785\n",
      "epsilon:0.319601 episode_count: 4123. steps_count: 1851183.000000\n",
      "ep 589: ep_len:570 episode reward: total was -38.190000. running mean: -32.449767\n",
      "ep 589: ep_len:540 episode reward: total was -29.000000. running mean: -32.415269\n",
      "ep 589: ep_len:815 episode reward: total was -83.490000. running mean: -32.926017\n",
      "ep 589: ep_len:595 episode reward: total was -39.170000. running mean: -32.988457\n",
      "ep 589: ep_len:3 episode reward: total was 0.000000. running mean: -32.658572\n",
      "ep 589: ep_len:186 episode reward: total was -8.460000. running mean: -32.416586\n",
      "ep 589: ep_len:740 episode reward: total was -80.940000. running mean: -32.901820\n",
      "epsilon:0.319465 episode_count: 4130. steps_count: 1854632.000000\n",
      "ep 590: ep_len:212 episode reward: total was -15.470000. running mean: -32.727502\n",
      "ep 590: ep_len:500 episode reward: total was -30.280000. running mean: -32.703027\n",
      "ep 590: ep_len:570 episode reward: total was -45.730000. running mean: -32.833297\n",
      "ep 590: ep_len:500 episode reward: total was -35.190000. running mean: -32.856864\n",
      "ep 590: ep_len:3 episode reward: total was 0.000000. running mean: -32.528295\n",
      "ep 590: ep_len:565 episode reward: total was -32.640000. running mean: -32.529412\n",
      "ep 590: ep_len:315 episode reward: total was -30.900000. running mean: -32.513118\n",
      "epsilon:0.319328 episode_count: 4137. steps_count: 1857297.000000\n",
      "ep 591: ep_len:510 episode reward: total was -47.890000. running mean: -32.666887\n",
      "ep 591: ep_len:510 episode reward: total was -30.180000. running mean: -32.642018\n",
      "ep 591: ep_len:575 episode reward: total was -46.690000. running mean: -32.782498\n",
      "ep 591: ep_len:665 episode reward: total was -52.140000. running mean: -32.976073\n",
      "ep 591: ep_len:50 episode reward: total was -3.500000. running mean: -32.681312\n",
      "ep 591: ep_len:160 episode reward: total was -12.950000. running mean: -32.483999\n",
      "ep 591: ep_len:615 episode reward: total was -40.990000. running mean: -32.569059\n",
      "epsilon:0.319192 episode_count: 4144. steps_count: 1860382.000000\n",
      "ep 592: ep_len:610 episode reward: total was -26.600000. running mean: -32.509369\n",
      "ep 592: ep_len:520 episode reward: total was -50.230000. running mean: -32.686575\n",
      "ep 592: ep_len:670 episode reward: total was -41.430000. running mean: -32.774009\n",
      "ep 592: ep_len:505 episode reward: total was -39.180000. running mean: -32.838069\n",
      "ep 592: ep_len:91 episode reward: total was 4.540000. running mean: -32.464288\n",
      "ep 592: ep_len:515 episode reward: total was -36.090000. running mean: -32.500545\n",
      "ep 592: ep_len:325 episode reward: total was -21.870000. running mean: -32.394240\n",
      "epsilon:0.319055 episode_count: 4151. steps_count: 1863618.000000\n",
      "ep 593: ep_len:500 episode reward: total was -26.870000. running mean: -32.338998\n",
      "ep 593: ep_len:600 episode reward: total was -42.610000. running mean: -32.441708\n",
      "ep 593: ep_len:79 episode reward: total was -6.480000. running mean: -32.182091\n",
      "ep 593: ep_len:545 episode reward: total was -64.840000. running mean: -32.508670\n",
      "ep 593: ep_len:83 episode reward: total was -7.990000. running mean: -32.263483\n",
      "ep 593: ep_len:500 episode reward: total was -31.140000. running mean: -32.252248\n",
      "ep 593: ep_len:309 episode reward: total was -17.360000. running mean: -32.103326\n",
      "epsilon:0.318919 episode_count: 4158. steps_count: 1866234.000000\n",
      "ep 594: ep_len:265 episode reward: total was -19.880000. running mean: -31.981092\n",
      "ep 594: ep_len:560 episode reward: total was -22.600000. running mean: -31.887281\n",
      "ep 594: ep_len:540 episode reward: total was -30.170000. running mean: -31.870109\n",
      "ep 594: ep_len:500 episode reward: total was -31.110000. running mean: -31.862508\n",
      "ep 594: ep_len:3 episode reward: total was 0.000000. running mean: -31.543882\n",
      "ep 594: ep_len:535 episode reward: total was -50.260000. running mean: -31.731044\n",
      "ep 594: ep_len:505 episode reward: total was -30.510000. running mean: -31.718833\n",
      "epsilon:0.318782 episode_count: 4165. steps_count: 1869142.000000\n",
      "ep 595: ep_len:99 episode reward: total was -2.970000. running mean: -31.431345\n",
      "ep 595: ep_len:535 episode reward: total was -36.190000. running mean: -31.478931\n",
      "ep 595: ep_len:575 episode reward: total was -34.120000. running mean: -31.505342\n",
      "ep 595: ep_len:166 episode reward: total was -15.460000. running mean: -31.344889\n",
      "ep 595: ep_len:107 episode reward: total was -1.980000. running mean: -31.051240\n",
      "ep 595: ep_len:283 episode reward: total was -21.370000. running mean: -30.954427\n",
      "ep 595: ep_len:580 episode reward: total was -50.680000. running mean: -31.151683\n",
      "epsilon:0.318646 episode_count: 4172. steps_count: 1871487.000000\n",
      "ep 596: ep_len:134 episode reward: total was -4.930000. running mean: -30.889466\n",
      "ep 596: ep_len:171 episode reward: total was -10.410000. running mean: -30.684672\n",
      "ep 596: ep_len:560 episode reward: total was -40.770000. running mean: -30.785525\n",
      "ep 596: ep_len:500 episode reward: total was -31.140000. running mean: -30.789070\n",
      "ep 596: ep_len:3 episode reward: total was 0.000000. running mean: -30.481179\n",
      "ep 596: ep_len:500 episode reward: total was -43.120000. running mean: -30.607567\n",
      "ep 596: ep_len:525 episode reward: total was -27.440000. running mean: -30.575892\n",
      "epsilon:0.318509 episode_count: 4179. steps_count: 1873880.000000\n",
      "ep 597: ep_len:645 episode reward: total was -39.890000. running mean: -30.669033\n",
      "ep 597: ep_len:371 episode reward: total was -28.370000. running mean: -30.646042\n",
      "ep 597: ep_len:595 episode reward: total was -33.630000. running mean: -30.675882\n",
      "ep 597: ep_len:128 episode reward: total was -5.430000. running mean: -30.423423\n",
      "ep 597: ep_len:93 episode reward: total was -13.960000. running mean: -30.258789\n",
      "ep 597: ep_len:540 episode reward: total was -52.730000. running mean: -30.483501\n",
      "ep 597: ep_len:585 episode reward: total was -45.050000. running mean: -30.629166\n",
      "epsilon:0.318373 episode_count: 4186. steps_count: 1876837.000000\n",
      "ep 598: ep_len:560 episode reward: total was -36.110000. running mean: -30.683974\n",
      "ep 598: ep_len:515 episode reward: total was -25.280000. running mean: -30.629935\n",
      "ep 598: ep_len:655 episode reward: total was -43.480000. running mean: -30.758435\n",
      "ep 598: ep_len:151 episode reward: total was -5.940000. running mean: -30.510251\n",
      "ep 598: ep_len:81 episode reward: total was -9.970000. running mean: -30.304848\n",
      "ep 598: ep_len:153 episode reward: total was -2.440000. running mean: -30.026200\n",
      "ep 598: ep_len:284 episode reward: total was -14.890000. running mean: -29.874838\n",
      "epsilon:0.318236 episode_count: 4193. steps_count: 1879236.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 599: ep_len:635 episode reward: total was -47.920000. running mean: -30.055289\n",
      "ep 599: ep_len:600 episode reward: total was -68.510000. running mean: -30.439837\n",
      "ep 599: ep_len:580 episode reward: total was -44.700000. running mean: -30.582438\n",
      "ep 599: ep_len:500 episode reward: total was -42.640000. running mean: -30.703014\n",
      "ep 599: ep_len:88 episode reward: total was -15.990000. running mean: -30.555884\n",
      "ep 599: ep_len:520 episode reward: total was -29.080000. running mean: -30.541125\n",
      "ep 599: ep_len:520 episode reward: total was -47.190000. running mean: -30.707614\n",
      "epsilon:0.318100 episode_count: 4200. steps_count: 1882679.000000\n",
      "ep 600: ep_len:247 episode reward: total was -18.900000. running mean: -30.589537\n",
      "ep 600: ep_len:670 episode reward: total was -64.080000. running mean: -30.924442\n",
      "ep 600: ep_len:500 episode reward: total was -43.640000. running mean: -31.051598\n",
      "ep 600: ep_len:655 episode reward: total was -61.570000. running mean: -31.356782\n",
      "ep 600: ep_len:3 episode reward: total was 0.000000. running mean: -31.043214\n",
      "ep 600: ep_len:510 episode reward: total was -47.860000. running mean: -31.211382\n",
      "ep 600: ep_len:535 episode reward: total was -49.800000. running mean: -31.397268\n",
      "epsilon:0.317963 episode_count: 4207. steps_count: 1885799.000000\n",
      "ep 601: ep_len:505 episode reward: total was -23.610000. running mean: -31.319395\n",
      "ep 601: ep_len:500 episode reward: total was -27.890000. running mean: -31.285101\n",
      "ep 601: ep_len:590 episode reward: total was -41.220000. running mean: -31.384450\n",
      "ep 601: ep_len:600 episode reward: total was -66.150000. running mean: -31.732106\n",
      "ep 601: ep_len:87 episode reward: total was -2.960000. running mean: -31.444385\n",
      "ep 601: ep_len:510 episode reward: total was -55.780000. running mean: -31.687741\n",
      "ep 601: ep_len:590 episode reward: total was -52.580000. running mean: -31.896663\n",
      "epsilon:0.317827 episode_count: 4214. steps_count: 1889181.000000\n",
      "ep 602: ep_len:575 episode reward: total was -33.660000. running mean: -31.914297\n",
      "ep 602: ep_len:505 episode reward: total was -26.670000. running mean: -31.861854\n",
      "ep 602: ep_len:442 episode reward: total was -31.350000. running mean: -31.856735\n",
      "ep 602: ep_len:590 episode reward: total was -51.660000. running mean: -32.054768\n",
      "ep 602: ep_len:3 episode reward: total was 0.000000. running mean: -31.734220\n",
      "ep 602: ep_len:600 episode reward: total was -34.390000. running mean: -31.760778\n",
      "ep 602: ep_len:555 episode reward: total was -42.740000. running mean: -31.870570\n",
      "epsilon:0.317690 episode_count: 4221. steps_count: 1892451.000000\n",
      "ep 603: ep_len:545 episode reward: total was -34.660000. running mean: -31.898465\n",
      "ep 603: ep_len:330 episode reward: total was -40.840000. running mean: -31.987880\n",
      "ep 603: ep_len:565 episode reward: total was -45.780000. running mean: -32.125801\n",
      "ep 603: ep_len:417 episode reward: total was -32.160000. running mean: -32.126143\n",
      "ep 603: ep_len:3 episode reward: total was 0.000000. running mean: -31.804882\n",
      "ep 603: ep_len:530 episode reward: total was -33.130000. running mean: -31.818133\n",
      "ep 603: ep_len:630 episode reward: total was -39.610000. running mean: -31.896052\n",
      "epsilon:0.317554 episode_count: 4228. steps_count: 1895471.000000\n",
      "ep 604: ep_len:500 episode reward: total was -27.330000. running mean: -31.850391\n",
      "ep 604: ep_len:283 episode reward: total was -24.450000. running mean: -31.776387\n",
      "ep 604: ep_len:885 episode reward: total was -97.070000. running mean: -32.429323\n",
      "ep 604: ep_len:166 episode reward: total was -3.400000. running mean: -32.139030\n",
      "ep 604: ep_len:3 episode reward: total was 0.000000. running mean: -31.817640\n",
      "ep 604: ep_len:665 episode reward: total was -47.900000. running mean: -31.978463\n",
      "ep 604: ep_len:500 episode reward: total was -26.020000. running mean: -31.918879\n",
      "epsilon:0.317417 episode_count: 4235. steps_count: 1898473.000000\n",
      "ep 605: ep_len:575 episode reward: total was -59.570000. running mean: -32.195390\n",
      "ep 605: ep_len:500 episode reward: total was -37.770000. running mean: -32.251136\n",
      "ep 605: ep_len:375 episode reward: total was -26.870000. running mean: -32.197325\n",
      "ep 605: ep_len:115 episode reward: total was -1.460000. running mean: -31.889951\n",
      "ep 605: ep_len:3 episode reward: total was 0.000000. running mean: -31.571052\n",
      "ep 605: ep_len:500 episode reward: total was -38.380000. running mean: -31.639141\n",
      "ep 605: ep_len:525 episode reward: total was -43.690000. running mean: -31.759650\n",
      "epsilon:0.317281 episode_count: 4242. steps_count: 1901066.000000\n",
      "ep 606: ep_len:760 episode reward: total was -70.930000. running mean: -32.151353\n",
      "ep 606: ep_len:500 episode reward: total was -54.350000. running mean: -32.373340\n",
      "ep 606: ep_len:595 episode reward: total was -22.500000. running mean: -32.274607\n",
      "ep 606: ep_len:520 episode reward: total was -29.080000. running mean: -32.242660\n",
      "ep 606: ep_len:118 episode reward: total was -0.460000. running mean: -31.924834\n",
      "ep 606: ep_len:218 episode reward: total was -13.460000. running mean: -31.740186\n",
      "ep 606: ep_len:570 episode reward: total was -35.620000. running mean: -31.778984\n",
      "epsilon:0.317144 episode_count: 4249. steps_count: 1904347.000000\n",
      "ep 607: ep_len:570 episode reward: total was -47.720000. running mean: -31.938394\n",
      "ep 607: ep_len:535 episode reward: total was -13.370000. running mean: -31.752710\n",
      "ep 607: ep_len:500 episode reward: total was -28.280000. running mean: -31.717983\n",
      "ep 607: ep_len:560 episode reward: total was -39.710000. running mean: -31.797903\n",
      "ep 607: ep_len:98 episode reward: total was 5.040000. running mean: -31.429524\n",
      "ep 607: ep_len:500 episode reward: total was -45.200000. running mean: -31.567229\n",
      "ep 607: ep_len:590 episode reward: total was -25.940000. running mean: -31.510956\n",
      "epsilon:0.317008 episode_count: 4256. steps_count: 1907700.000000\n",
      "ep 608: ep_len:505 episode reward: total was -24.770000. running mean: -31.443547\n",
      "ep 608: ep_len:500 episode reward: total was -51.360000. running mean: -31.642711\n",
      "ep 608: ep_len:615 episode reward: total was -46.440000. running mean: -31.790684\n",
      "ep 608: ep_len:500 episode reward: total was -25.650000. running mean: -31.729277\n",
      "ep 608: ep_len:3 episode reward: total was 0.000000. running mean: -31.411985\n",
      "ep 608: ep_len:510 episode reward: total was -31.250000. running mean: -31.410365\n",
      "ep 608: ep_len:560 episode reward: total was -34.700000. running mean: -31.443261\n",
      "epsilon:0.316871 episode_count: 4263. steps_count: 1910893.000000\n",
      "ep 609: ep_len:615 episode reward: total was -50.510000. running mean: -31.633929\n",
      "ep 609: ep_len:545 episode reward: total was -49.290000. running mean: -31.810489\n",
      "ep 609: ep_len:545 episode reward: total was -48.580000. running mean: -31.978184\n",
      "ep 609: ep_len:575 episode reward: total was -17.160000. running mean: -31.830003\n",
      "ep 609: ep_len:3 episode reward: total was 0.000000. running mean: -31.511702\n",
      "ep 609: ep_len:615 episode reward: total was -40.250000. running mean: -31.599085\n",
      "ep 609: ep_len:620 episode reward: total was -44.660000. running mean: -31.729695\n",
      "epsilon:0.316735 episode_count: 4270. steps_count: 1914411.000000\n",
      "ep 610: ep_len:520 episode reward: total was -53.680000. running mean: -31.949198\n",
      "ep 610: ep_len:600 episode reward: total was -18.210000. running mean: -31.811806\n",
      "ep 610: ep_len:545 episode reward: total was -40.280000. running mean: -31.896488\n",
      "ep 610: ep_len:600 episode reward: total was -12.050000. running mean: -31.698023\n",
      "ep 610: ep_len:86 episode reward: total was -12.960000. running mean: -31.510643\n",
      "ep 610: ep_len:535 episode reward: total was -44.630000. running mean: -31.641836\n",
      "ep 610: ep_len:195 episode reward: total was -18.940000. running mean: -31.514818\n",
      "epsilon:0.316598 episode_count: 4277. steps_count: 1917492.000000\n",
      "ep 611: ep_len:216 episode reward: total was -4.400000. running mean: -31.243670\n",
      "ep 611: ep_len:590 episode reward: total was -49.700000. running mean: -31.428233\n",
      "ep 611: ep_len:605 episode reward: total was -33.630000. running mean: -31.450251\n",
      "ep 611: ep_len:132 episode reward: total was -5.470000. running mean: -31.190448\n",
      "ep 611: ep_len:3 episode reward: total was 0.000000. running mean: -30.878544\n",
      "ep 611: ep_len:625 episode reward: total was -42.240000. running mean: -30.992158\n",
      "ep 611: ep_len:635 episode reward: total was -40.580000. running mean: -31.088037\n",
      "epsilon:0.316462 episode_count: 4284. steps_count: 1920298.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 612: ep_len:214 episode reward: total was -15.930000. running mean: -30.936456\n",
      "ep 612: ep_len:338 episode reward: total was -24.390000. running mean: -30.870992\n",
      "ep 612: ep_len:79 episode reward: total was -4.980000. running mean: -30.612082\n",
      "ep 612: ep_len:525 episode reward: total was -21.600000. running mean: -30.521961\n",
      "ep 612: ep_len:3 episode reward: total was 0.000000. running mean: -30.216741\n",
      "ep 612: ep_len:540 episode reward: total was -33.660000. running mean: -30.251174\n",
      "ep 612: ep_len:600 episode reward: total was -52.180000. running mean: -30.470462\n",
      "epsilon:0.316325 episode_count: 4291. steps_count: 1922597.000000\n",
      "ep 613: ep_len:505 episode reward: total was -44.610000. running mean: -30.611857\n",
      "ep 613: ep_len:500 episode reward: total was -31.620000. running mean: -30.621939\n",
      "ep 613: ep_len:575 episode reward: total was -30.400000. running mean: -30.619720\n",
      "ep 613: ep_len:585 episode reward: total was -56.800000. running mean: -30.881522\n",
      "ep 613: ep_len:49 episode reward: total was 1.500000. running mean: -30.557707\n",
      "ep 613: ep_len:154 episode reward: total was -19.920000. running mean: -30.451330\n",
      "ep 613: ep_len:301 episode reward: total was -13.830000. running mean: -30.285117\n",
      "epsilon:0.316189 episode_count: 4298. steps_count: 1925266.000000\n",
      "ep 614: ep_len:122 episode reward: total was -11.930000. running mean: -30.101566\n",
      "ep 614: ep_len:500 episode reward: total was -33.820000. running mean: -30.138750\n",
      "ep 614: ep_len:620 episode reward: total was -50.480000. running mean: -30.342162\n",
      "ep 614: ep_len:43 episode reward: total was -3.990000. running mean: -30.078641\n",
      "ep 614: ep_len:3 episode reward: total was 0.000000. running mean: -29.777854\n",
      "ep 614: ep_len:520 episode reward: total was -29.350000. running mean: -29.773576\n",
      "ep 614: ep_len:350 episode reward: total was -33.890000. running mean: -29.814740\n",
      "epsilon:0.316052 episode_count: 4305. steps_count: 1927424.000000\n",
      "ep 615: ep_len:550 episode reward: total was -36.650000. running mean: -29.883093\n",
      "ep 615: ep_len:655 episode reward: total was -31.160000. running mean: -29.895862\n",
      "ep 615: ep_len:590 episode reward: total was -40.180000. running mean: -29.998703\n",
      "ep 615: ep_len:505 episode reward: total was -14.640000. running mean: -29.845116\n",
      "ep 615: ep_len:3 episode reward: total was 0.000000. running mean: -29.546665\n",
      "ep 615: ep_len:580 episode reward: total was -46.000000. running mean: -29.711198\n",
      "ep 615: ep_len:550 episode reward: total was -76.780000. running mean: -30.181886\n",
      "epsilon:0.315916 episode_count: 4312. steps_count: 1930857.000000\n",
      "ep 616: ep_len:131 episode reward: total was -4.950000. running mean: -29.929567\n",
      "ep 616: ep_len:560 episode reward: total was -26.130000. running mean: -29.891572\n",
      "ep 616: ep_len:370 episode reward: total was -14.330000. running mean: -29.735956\n",
      "ep 616: ep_len:500 episode reward: total was -11.200000. running mean: -29.550596\n",
      "ep 616: ep_len:3 episode reward: total was 0.000000. running mean: -29.255091\n",
      "ep 616: ep_len:530 episode reward: total was -41.160000. running mean: -29.374140\n",
      "ep 616: ep_len:530 episode reward: total was -29.700000. running mean: -29.377398\n",
      "epsilon:0.315779 episode_count: 4319. steps_count: 1933481.000000\n",
      "ep 617: ep_len:510 episode reward: total was -34.710000. running mean: -29.430724\n",
      "ep 617: ep_len:560 episode reward: total was -21.230000. running mean: -29.348717\n",
      "ep 617: ep_len:355 episode reward: total was -15.810000. running mean: -29.213330\n",
      "ep 617: ep_len:600 episode reward: total was -32.620000. running mean: -29.247397\n",
      "ep 617: ep_len:36 episode reward: total was -0.500000. running mean: -28.959923\n",
      "ep 617: ep_len:590 episode reward: total was -37.210000. running mean: -29.042423\n",
      "ep 617: ep_len:615 episode reward: total was -33.460000. running mean: -29.086599\n",
      "epsilon:0.315643 episode_count: 4326. steps_count: 1936747.000000\n",
      "ep 618: ep_len:545 episode reward: total was -24.740000. running mean: -29.043133\n",
      "ep 618: ep_len:500 episode reward: total was -12.380000. running mean: -28.876502\n",
      "ep 618: ep_len:61 episode reward: total was -3.470000. running mean: -28.622437\n",
      "ep 618: ep_len:152 episode reward: total was -2.420000. running mean: -28.360412\n",
      "ep 618: ep_len:3 episode reward: total was 0.000000. running mean: -28.076808\n",
      "ep 618: ep_len:570 episode reward: total was -42.350000. running mean: -28.219540\n",
      "ep 618: ep_len:625 episode reward: total was -27.640000. running mean: -28.213745\n",
      "epsilon:0.315506 episode_count: 4333. steps_count: 1939203.000000\n",
      "ep 619: ep_len:625 episode reward: total was -57.300000. running mean: -28.504607\n",
      "ep 619: ep_len:725 episode reward: total was -67.240000. running mean: -28.891961\n",
      "ep 619: ep_len:765 episode reward: total was -86.000000. running mean: -29.463042\n",
      "ep 619: ep_len:515 episode reward: total was -11.120000. running mean: -29.279611\n",
      "ep 619: ep_len:3 episode reward: total was 0.000000. running mean: -28.986815\n",
      "ep 619: ep_len:555 episode reward: total was -57.630000. running mean: -29.273247\n",
      "ep 619: ep_len:286 episode reward: total was -9.890000. running mean: -29.079415\n",
      "epsilon:0.315370 episode_count: 4340. steps_count: 1942677.000000\n",
      "ep 620: ep_len:585 episode reward: total was -29.220000. running mean: -29.080820\n",
      "ep 620: ep_len:585 episode reward: total was -37.260000. running mean: -29.162612\n",
      "ep 620: ep_len:695 episode reward: total was -43.950000. running mean: -29.310486\n",
      "ep 620: ep_len:580 episode reward: total was -66.730000. running mean: -29.684681\n",
      "ep 620: ep_len:3 episode reward: total was 0.000000. running mean: -29.387834\n",
      "ep 620: ep_len:585 episode reward: total was -29.690000. running mean: -29.390856\n",
      "ep 620: ep_len:700 episode reward: total was -60.010000. running mean: -29.697047\n",
      "epsilon:0.315233 episode_count: 4347. steps_count: 1946410.000000\n",
      "ep 621: ep_len:605 episode reward: total was -35.700000. running mean: -29.757077\n",
      "ep 621: ep_len:163 episode reward: total was -13.460000. running mean: -29.594106\n",
      "ep 621: ep_len:510 episode reward: total was -38.580000. running mean: -29.683965\n",
      "ep 621: ep_len:515 episode reward: total was -49.660000. running mean: -29.883726\n",
      "ep 621: ep_len:3 episode reward: total was 0.000000. running mean: -29.584888\n",
      "ep 621: ep_len:530 episode reward: total was -44.220000. running mean: -29.731239\n",
      "ep 621: ep_len:615 episode reward: total was -37.470000. running mean: -29.808627\n",
      "epsilon:0.315097 episode_count: 4354. steps_count: 1949351.000000\n",
      "ep 622: ep_len:505 episode reward: total was -43.060000. running mean: -29.941141\n",
      "ep 622: ep_len:630 episode reward: total was -51.630000. running mean: -30.158029\n",
      "ep 622: ep_len:620 episode reward: total was -63.190000. running mean: -30.488349\n",
      "ep 622: ep_len:590 episode reward: total was -26.130000. running mean: -30.444766\n",
      "ep 622: ep_len:97 episode reward: total was -11.470000. running mean: -30.255018\n",
      "ep 622: ep_len:675 episode reward: total was -46.960000. running mean: -30.422068\n",
      "ep 622: ep_len:520 episode reward: total was -28.680000. running mean: -30.404647\n",
      "epsilon:0.314960 episode_count: 4361. steps_count: 1952988.000000\n",
      "ep 623: ep_len:585 episode reward: total was -30.680000. running mean: -30.407401\n",
      "ep 623: ep_len:510 episode reward: total was -25.700000. running mean: -30.360327\n",
      "ep 623: ep_len:560 episode reward: total was -47.020000. running mean: -30.526923\n",
      "ep 623: ep_len:555 episode reward: total was -45.250000. running mean: -30.674154\n",
      "ep 623: ep_len:100 episode reward: total was -0.950000. running mean: -30.376912\n",
      "ep 623: ep_len:595 episode reward: total was -75.820000. running mean: -30.831343\n",
      "ep 623: ep_len:358 episode reward: total was -29.370000. running mean: -30.816730\n",
      "epsilon:0.314824 episode_count: 4368. steps_count: 1956251.000000\n",
      "ep 624: ep_len:665 episode reward: total was -55.000000. running mean: -31.058563\n",
      "ep 624: ep_len:545 episode reward: total was -37.530000. running mean: -31.123277\n",
      "ep 624: ep_len:358 episode reward: total was -14.290000. running mean: -30.954944\n",
      "ep 624: ep_len:515 episode reward: total was -32.040000. running mean: -30.965795\n",
      "ep 624: ep_len:3 episode reward: total was 0.000000. running mean: -30.656137\n",
      "ep 624: ep_len:515 episode reward: total was -24.170000. running mean: -30.591275\n",
      "ep 624: ep_len:620 episode reward: total was -23.040000. running mean: -30.515763\n",
      "epsilon:0.314687 episode_count: 4375. steps_count: 1959472.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 625: ep_len:655 episode reward: total was -34.300000. running mean: -30.553605\n",
      "ep 625: ep_len:500 episode reward: total was -33.230000. running mean: -30.580369\n",
      "ep 625: ep_len:555 episode reward: total was -32.650000. running mean: -30.601065\n",
      "ep 625: ep_len:396 episode reward: total was -21.280000. running mean: -30.507855\n",
      "ep 625: ep_len:96 episode reward: total was -7.970000. running mean: -30.282476\n",
      "ep 625: ep_len:545 episode reward: total was -35.610000. running mean: -30.335751\n",
      "ep 625: ep_len:555 episode reward: total was -59.150000. running mean: -30.623894\n",
      "epsilon:0.314551 episode_count: 4382. steps_count: 1962774.000000\n",
      "ep 626: ep_len:241 episode reward: total was -5.910000. running mean: -30.376755\n",
      "ep 626: ep_len:500 episode reward: total was -52.220000. running mean: -30.595187\n",
      "ep 626: ep_len:500 episode reward: total was -27.840000. running mean: -30.567636\n",
      "ep 626: ep_len:500 episode reward: total was -40.700000. running mean: -30.668959\n",
      "ep 626: ep_len:3 episode reward: total was 0.000000. running mean: -30.362270\n",
      "ep 626: ep_len:505 episode reward: total was -51.210000. running mean: -30.570747\n",
      "ep 626: ep_len:595 episode reward: total was -37.520000. running mean: -30.640239\n",
      "epsilon:0.314414 episode_count: 4389. steps_count: 1965618.000000\n",
      "ep 627: ep_len:535 episode reward: total was -25.570000. running mean: -30.589537\n",
      "ep 627: ep_len:505 episode reward: total was -22.910000. running mean: -30.512742\n",
      "ep 627: ep_len:705 episode reward: total was -74.460000. running mean: -30.952214\n",
      "ep 627: ep_len:540 episode reward: total was -25.220000. running mean: -30.894892\n",
      "ep 627: ep_len:3 episode reward: total was 0.000000. running mean: -30.585943\n",
      "ep 627: ep_len:580 episode reward: total was -31.740000. running mean: -30.597484\n",
      "ep 627: ep_len:525 episode reward: total was -21.190000. running mean: -30.503409\n",
      "epsilon:0.314278 episode_count: 4396. steps_count: 1969011.000000\n",
      "ep 628: ep_len:695 episode reward: total was -66.330000. running mean: -30.861675\n",
      "ep 628: ep_len:630 episode reward: total was -63.260000. running mean: -31.185658\n",
      "ep 628: ep_len:615 episode reward: total was -51.550000. running mean: -31.389301\n",
      "ep 628: ep_len:695 episode reward: total was -48.130000. running mean: -31.556708\n",
      "ep 628: ep_len:3 episode reward: total was 0.000000. running mean: -31.241141\n",
      "ep 628: ep_len:575 episode reward: total was -51.190000. running mean: -31.440630\n",
      "ep 628: ep_len:515 episode reward: total was -30.980000. running mean: -31.436024\n",
      "epsilon:0.314141 episode_count: 4403. steps_count: 1972739.000000\n",
      "ep 629: ep_len:625 episode reward: total was -81.610000. running mean: -31.937763\n",
      "ep 629: ep_len:610 episode reward: total was -53.040000. running mean: -32.148786\n",
      "ep 629: ep_len:615 episode reward: total was -63.790000. running mean: -32.465198\n",
      "ep 629: ep_len:500 episode reward: total was -13.220000. running mean: -32.272746\n",
      "ep 629: ep_len:3 episode reward: total was 0.000000. running mean: -31.950018\n",
      "ep 629: ep_len:670 episode reward: total was -60.070000. running mean: -32.231218\n",
      "ep 629: ep_len:211 episode reward: total was -22.970000. running mean: -32.138606\n",
      "epsilon:0.314005 episode_count: 4410. steps_count: 1975973.000000\n",
      "ep 630: ep_len:510 episode reward: total was -23.250000. running mean: -32.049720\n",
      "ep 630: ep_len:500 episode reward: total was -6.390000. running mean: -31.793123\n",
      "ep 630: ep_len:600 episode reward: total was -31.900000. running mean: -31.794192\n",
      "ep 630: ep_len:545 episode reward: total was -34.620000. running mean: -31.822450\n",
      "ep 630: ep_len:3 episode reward: total was 0.000000. running mean: -31.504225\n",
      "ep 630: ep_len:510 episode reward: total was -23.680000. running mean: -31.425983\n",
      "ep 630: ep_len:610 episode reward: total was -27.380000. running mean: -31.385523\n",
      "epsilon:0.313868 episode_count: 4417. steps_count: 1979251.000000\n",
      "ep 631: ep_len:610 episode reward: total was -38.730000. running mean: -31.458968\n",
      "ep 631: ep_len:505 episode reward: total was -13.420000. running mean: -31.278578\n",
      "ep 631: ep_len:580 episode reward: total was -39.910000. running mean: -31.364892\n",
      "ep 631: ep_len:565 episode reward: total was -47.130000. running mean: -31.522544\n",
      "ep 631: ep_len:3 episode reward: total was 0.000000. running mean: -31.207318\n",
      "ep 631: ep_len:655 episode reward: total was -49.020000. running mean: -31.385445\n",
      "ep 631: ep_len:620 episode reward: total was -83.290000. running mean: -31.904490\n",
      "epsilon:0.313732 episode_count: 4424. steps_count: 1982789.000000\n",
      "ep 632: ep_len:263 episode reward: total was -8.440000. running mean: -31.669846\n",
      "ep 632: ep_len:520 episode reward: total was -24.620000. running mean: -31.599347\n",
      "ep 632: ep_len:645 episode reward: total was -61.530000. running mean: -31.898654\n",
      "ep 632: ep_len:560 episode reward: total was -52.610000. running mean: -32.105767\n",
      "ep 632: ep_len:79 episode reward: total was 3.530000. running mean: -31.749409\n",
      "ep 632: ep_len:152 episode reward: total was -22.980000. running mean: -31.661715\n",
      "ep 632: ep_len:285 episode reward: total was -16.900000. running mean: -31.514098\n",
      "epsilon:0.313595 episode_count: 4431. steps_count: 1985293.000000\n",
      "ep 633: ep_len:213 episode reward: total was -19.390000. running mean: -31.392857\n",
      "ep 633: ep_len:500 episode reward: total was -34.680000. running mean: -31.425729\n",
      "ep 633: ep_len:580 episode reward: total was -35.690000. running mean: -31.468371\n",
      "ep 633: ep_len:500 episode reward: total was -19.580000. running mean: -31.349488\n",
      "ep 633: ep_len:3 episode reward: total was 0.000000. running mean: -31.035993\n",
      "ep 633: ep_len:640 episode reward: total was -51.450000. running mean: -31.240133\n",
      "ep 633: ep_len:585 episode reward: total was -53.710000. running mean: -31.464831\n",
      "epsilon:0.313459 episode_count: 4438. steps_count: 1988314.000000\n",
      "ep 634: ep_len:590 episode reward: total was -49.200000. running mean: -31.642183\n",
      "ep 634: ep_len:525 episode reward: total was -49.110000. running mean: -31.816861\n",
      "ep 634: ep_len:74 episode reward: total was -3.980000. running mean: -31.538493\n",
      "ep 634: ep_len:56 episode reward: total was -2.480000. running mean: -31.247908\n",
      "ep 634: ep_len:3 episode reward: total was 0.000000. running mean: -30.935429\n",
      "ep 634: ep_len:560 episode reward: total was -55.820000. running mean: -31.184274\n",
      "ep 634: ep_len:208 episode reward: total was -18.460000. running mean: -31.057032\n",
      "epsilon:0.313322 episode_count: 4445. steps_count: 1990330.000000\n",
      "ep 635: ep_len:600 episode reward: total was -46.990000. running mean: -31.216361\n",
      "ep 635: ep_len:500 episode reward: total was -33.870000. running mean: -31.242898\n",
      "ep 635: ep_len:500 episode reward: total was -39.810000. running mean: -31.328569\n",
      "ep 635: ep_len:590 episode reward: total was -29.590000. running mean: -31.311183\n",
      "ep 635: ep_len:3 episode reward: total was 0.000000. running mean: -30.998071\n",
      "ep 635: ep_len:580 episode reward: total was -60.230000. running mean: -31.290391\n",
      "ep 635: ep_len:144 episode reward: total was -8.970000. running mean: -31.067187\n",
      "epsilon:0.313186 episode_count: 4452. steps_count: 1993247.000000\n",
      "ep 636: ep_len:505 episode reward: total was -24.720000. running mean: -31.003715\n",
      "ep 636: ep_len:510 episode reward: total was -40.190000. running mean: -31.095578\n",
      "ep 636: ep_len:500 episode reward: total was -18.200000. running mean: -30.966622\n",
      "ep 636: ep_len:555 episode reward: total was -31.580000. running mean: -30.972756\n",
      "ep 636: ep_len:3 episode reward: total was 0.000000. running mean: -30.663028\n",
      "ep 636: ep_len:585 episode reward: total was -44.980000. running mean: -30.806198\n",
      "ep 636: ep_len:520 episode reward: total was -41.310000. running mean: -30.911236\n",
      "epsilon:0.313049 episode_count: 4459. steps_count: 1996425.000000\n",
      "ep 637: ep_len:525 episode reward: total was -43.160000. running mean: -31.033723\n",
      "ep 637: ep_len:670 episode reward: total was -42.130000. running mean: -31.144686\n",
      "ep 637: ep_len:595 episode reward: total was -55.520000. running mean: -31.388439\n",
      "ep 637: ep_len:119 episode reward: total was -2.470000. running mean: -31.099255\n",
      "ep 637: ep_len:93 episode reward: total was -5.470000. running mean: -30.842962\n",
      "ep 637: ep_len:585 episode reward: total was -42.410000. running mean: -30.958633\n",
      "ep 637: ep_len:500 episode reward: total was -63.990000. running mean: -31.288946\n",
      "epsilon:0.312913 episode_count: 4466. steps_count: 1999512.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 638: ep_len:655 episode reward: total was -54.540000. running mean: -31.521457\n",
      "ep 638: ep_len:500 episode reward: total was -29.420000. running mean: -31.500442\n",
      "ep 638: ep_len:510 episode reward: total was -32.480000. running mean: -31.510238\n",
      "ep 638: ep_len:500 episode reward: total was -42.170000. running mean: -31.616836\n",
      "ep 638: ep_len:3 episode reward: total was 0.000000. running mean: -31.300667\n",
      "ep 638: ep_len:235 episode reward: total was -8.920000. running mean: -31.076861\n",
      "ep 638: ep_len:500 episode reward: total was -33.760000. running mean: -31.103692\n",
      "epsilon:0.312776 episode_count: 4473. steps_count: 2002415.000000\n",
      "ep 639: ep_len:170 episode reward: total was -7.960000. running mean: -30.872255\n",
      "ep 639: ep_len:865 episode reward: total was -59.440000. running mean: -31.157933\n",
      "ep 639: ep_len:715 episode reward: total was -117.820000. running mean: -32.024553\n",
      "ep 639: ep_len:505 episode reward: total was -29.090000. running mean: -31.995208\n",
      "ep 639: ep_len:3 episode reward: total was 0.000000. running mean: -31.675256\n",
      "ep 639: ep_len:510 episode reward: total was -28.250000. running mean: -31.641003\n",
      "ep 639: ep_len:500 episode reward: total was -48.610000. running mean: -31.810693\n",
      "epsilon:0.312640 episode_count: 4480. steps_count: 2005683.000000\n",
      "ep 640: ep_len:214 episode reward: total was -13.360000. running mean: -31.626186\n",
      "ep 640: ep_len:540 episode reward: total was -30.070000. running mean: -31.610624\n",
      "ep 640: ep_len:505 episode reward: total was -33.870000. running mean: -31.633218\n",
      "ep 640: ep_len:740 episode reward: total was -79.840000. running mean: -32.115286\n",
      "ep 640: ep_len:53 episode reward: total was -6.990000. running mean: -31.864033\n",
      "ep 640: ep_len:620 episode reward: total was -40.110000. running mean: -31.946493\n",
      "ep 640: ep_len:500 episode reward: total was -39.630000. running mean: -32.023328\n",
      "epsilon:0.312503 episode_count: 4487. steps_count: 2008855.000000\n",
      "ep 641: ep_len:630 episode reward: total was -48.460000. running mean: -32.187694\n",
      "ep 641: ep_len:555 episode reward: total was -40.800000. running mean: -32.273817\n",
      "ep 641: ep_len:545 episode reward: total was -27.130000. running mean: -32.222379\n",
      "ep 641: ep_len:500 episode reward: total was -30.120000. running mean: -32.201355\n",
      "ep 641: ep_len:95 episode reward: total was -5.460000. running mean: -31.933942\n",
      "ep 641: ep_len:500 episode reward: total was -34.640000. running mean: -31.961003\n",
      "ep 641: ep_len:565 episode reward: total was -33.520000. running mean: -31.976592\n",
      "epsilon:0.312367 episode_count: 4494. steps_count: 2012245.000000\n",
      "ep 642: ep_len:500 episode reward: total was -49.650000. running mean: -32.153327\n",
      "ep 642: ep_len:535 episode reward: total was -4.590000. running mean: -31.877693\n",
      "ep 642: ep_len:388 episode reward: total was -34.380000. running mean: -31.902716\n",
      "ep 642: ep_len:580 episode reward: total was -40.560000. running mean: -31.989289\n",
      "ep 642: ep_len:76 episode reward: total was -0.480000. running mean: -31.674196\n",
      "ep 642: ep_len:580 episode reward: total was -26.580000. running mean: -31.623254\n",
      "ep 642: ep_len:555 episode reward: total was -72.760000. running mean: -32.034622\n",
      "epsilon:0.312230 episode_count: 4501. steps_count: 2015459.000000\n",
      "ep 643: ep_len:500 episode reward: total was -25.750000. running mean: -31.971776\n",
      "ep 643: ep_len:500 episode reward: total was -31.590000. running mean: -31.967958\n",
      "ep 643: ep_len:595 episode reward: total was -37.730000. running mean: -32.025578\n",
      "ep 643: ep_len:551 episode reward: total was -51.550000. running mean: -32.220822\n",
      "ep 643: ep_len:3 episode reward: total was 0.000000. running mean: -31.898614\n",
      "ep 643: ep_len:500 episode reward: total was -24.340000. running mean: -31.823028\n",
      "ep 643: ep_len:545 episode reward: total was -26.200000. running mean: -31.766798\n",
      "epsilon:0.312094 episode_count: 4508. steps_count: 2018653.000000\n",
      "ep 644: ep_len:213 episode reward: total was -13.360000. running mean: -31.582730\n",
      "ep 644: ep_len:585 episode reward: total was -29.170000. running mean: -31.558603\n",
      "ep 644: ep_len:535 episode reward: total was -39.220000. running mean: -31.635217\n",
      "ep 644: ep_len:56 episode reward: total was -2.970000. running mean: -31.348564\n",
      "ep 644: ep_len:3 episode reward: total was 0.000000. running mean: -31.035079\n",
      "ep 644: ep_len:555 episode reward: total was -19.850000. running mean: -30.923228\n",
      "ep 644: ep_len:304 episode reward: total was -19.870000. running mean: -30.812696\n",
      "epsilon:0.311957 episode_count: 4515. steps_count: 2020904.000000\n",
      "ep 645: ep_len:540 episode reward: total was -48.580000. running mean: -30.990369\n",
      "ep 645: ep_len:193 episode reward: total was -25.440000. running mean: -30.934865\n",
      "ep 645: ep_len:580 episode reward: total was -59.670000. running mean: -31.222216\n",
      "ep 645: ep_len:540 episode reward: total was -57.770000. running mean: -31.487694\n",
      "ep 645: ep_len:104 episode reward: total was -12.460000. running mean: -31.297417\n",
      "ep 645: ep_len:500 episode reward: total was -37.870000. running mean: -31.363143\n",
      "ep 645: ep_len:515 episode reward: total was -57.290000. running mean: -31.622412\n",
      "epsilon:0.311821 episode_count: 4522. steps_count: 2023876.000000\n",
      "ep 646: ep_len:120 episode reward: total was -7.480000. running mean: -31.380988\n",
      "ep 646: ep_len:525 episode reward: total was -18.710000. running mean: -31.254278\n",
      "ep 646: ep_len:690 episode reward: total was -48.890000. running mean: -31.430635\n",
      "ep 646: ep_len:42 episode reward: total was -3.000000. running mean: -31.146329\n",
      "ep 646: ep_len:92 episode reward: total was -9.980000. running mean: -30.934665\n",
      "ep 646: ep_len:580 episode reward: total was -29.350000. running mean: -30.918819\n",
      "ep 646: ep_len:635 episode reward: total was -38.950000. running mean: -30.999130\n",
      "epsilon:0.311684 episode_count: 4529. steps_count: 2026560.000000\n",
      "ep 647: ep_len:560 episode reward: total was -32.690000. running mean: -31.016039\n",
      "ep 647: ep_len:540 episode reward: total was -33.530000. running mean: -31.041179\n",
      "ep 647: ep_len:535 episode reward: total was -25.230000. running mean: -30.983067\n",
      "ep 647: ep_len:515 episode reward: total was -19.250000. running mean: -30.865736\n",
      "ep 647: ep_len:3 episode reward: total was 0.000000. running mean: -30.557079\n",
      "ep 647: ep_len:308 episode reward: total was -24.840000. running mean: -30.499908\n",
      "ep 647: ep_len:610 episode reward: total was -34.170000. running mean: -30.536609\n",
      "epsilon:0.311548 episode_count: 4536. steps_count: 2029631.000000\n",
      "ep 648: ep_len:640 episode reward: total was -38.010000. running mean: -30.611343\n",
      "ep 648: ep_len:550 episode reward: total was -39.100000. running mean: -30.696229\n",
      "ep 648: ep_len:595 episode reward: total was -30.850000. running mean: -30.697767\n",
      "ep 648: ep_len:132 episode reward: total was -3.940000. running mean: -30.430190\n",
      "ep 648: ep_len:94 episode reward: total was -10.460000. running mean: -30.230488\n",
      "ep 648: ep_len:515 episode reward: total was -42.020000. running mean: -30.348383\n",
      "ep 648: ep_len:207 episode reward: total was -26.470000. running mean: -30.309599\n",
      "epsilon:0.311411 episode_count: 4543. steps_count: 2032364.000000\n",
      "ep 649: ep_len:530 episode reward: total was -41.050000. running mean: -30.417003\n",
      "ep 649: ep_len:505 episode reward: total was -6.410000. running mean: -30.176933\n",
      "ep 649: ep_len:79 episode reward: total was -7.490000. running mean: -29.950064\n",
      "ep 649: ep_len:384 episode reward: total was -31.310000. running mean: -29.963663\n",
      "ep 649: ep_len:3 episode reward: total was 0.000000. running mean: -29.664026\n",
      "ep 649: ep_len:595 episode reward: total was -41.180000. running mean: -29.779186\n",
      "ep 649: ep_len:810 episode reward: total was -67.620000. running mean: -30.157594\n",
      "epsilon:0.311275 episode_count: 4550. steps_count: 2035270.000000\n",
      "ep 650: ep_len:535 episode reward: total was -53.540000. running mean: -30.391418\n",
      "ep 650: ep_len:500 episode reward: total was -23.890000. running mean: -30.326404\n",
      "ep 650: ep_len:403 episode reward: total was -24.380000. running mean: -30.266940\n",
      "ep 650: ep_len:615 episode reward: total was -51.140000. running mean: -30.475671\n",
      "ep 650: ep_len:3 episode reward: total was 0.000000. running mean: -30.170914\n",
      "ep 650: ep_len:500 episode reward: total was -32.700000. running mean: -30.196205\n",
      "ep 650: ep_len:530 episode reward: total was -44.260000. running mean: -30.336843\n",
      "epsilon:0.311138 episode_count: 4557. steps_count: 2038356.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 651: ep_len:620 episode reward: total was -33.030000. running mean: -30.363774\n",
      "ep 651: ep_len:635 episode reward: total was -76.250000. running mean: -30.822637\n",
      "ep 651: ep_len:595 episode reward: total was -24.080000. running mean: -30.755210\n",
      "ep 651: ep_len:500 episode reward: total was -41.130000. running mean: -30.858958\n",
      "ep 651: ep_len:106 episode reward: total was 4.540000. running mean: -30.504969\n",
      "ep 651: ep_len:249 episode reward: total was -30.420000. running mean: -30.504119\n",
      "ep 651: ep_len:605 episode reward: total was -33.140000. running mean: -30.530478\n",
      "epsilon:0.311002 episode_count: 4564. steps_count: 2041666.000000\n",
      "ep 652: ep_len:500 episode reward: total was -12.230000. running mean: -30.347473\n",
      "ep 652: ep_len:270 episode reward: total was -31.380000. running mean: -30.357798\n",
      "ep 652: ep_len:69 episode reward: total was -5.980000. running mean: -30.114020\n",
      "ep 652: ep_len:590 episode reward: total was -21.590000. running mean: -30.028780\n",
      "ep 652: ep_len:93 episode reward: total was -1.980000. running mean: -29.748292\n",
      "ep 652: ep_len:505 episode reward: total was -31.580000. running mean: -29.766609\n",
      "ep 652: ep_len:279 episode reward: total was -31.980000. running mean: -29.788743\n",
      "epsilon:0.310865 episode_count: 4571. steps_count: 2043972.000000\n",
      "ep 653: ep_len:550 episode reward: total was -25.700000. running mean: -29.747856\n",
      "ep 653: ep_len:505 episode reward: total was -39.190000. running mean: -29.842277\n",
      "ep 653: ep_len:500 episode reward: total was -26.720000. running mean: -29.811054\n",
      "ep 653: ep_len:550 episode reward: total was -54.050000. running mean: -30.053444\n",
      "ep 653: ep_len:3 episode reward: total was 0.000000. running mean: -29.752909\n",
      "ep 653: ep_len:525 episode reward: total was -30.750000. running mean: -29.762880\n",
      "ep 653: ep_len:615 episode reward: total was -27.920000. running mean: -29.744452\n",
      "epsilon:0.310729 episode_count: 4578. steps_count: 2047220.000000\n",
      "ep 654: ep_len:705 episode reward: total was -61.710000. running mean: -30.064107\n",
      "ep 654: ep_len:505 episode reward: total was -19.920000. running mean: -29.962666\n",
      "ep 654: ep_len:456 episode reward: total was -33.820000. running mean: -30.001239\n",
      "ep 654: ep_len:590 episode reward: total was -20.580000. running mean: -29.907027\n",
      "ep 654: ep_len:3 episode reward: total was 0.000000. running mean: -29.607957\n",
      "ep 654: ep_len:520 episode reward: total was -42.100000. running mean: -29.732877\n",
      "ep 654: ep_len:605 episode reward: total was -29.890000. running mean: -29.734448\n",
      "epsilon:0.310592 episode_count: 4585. steps_count: 2050604.000000\n",
      "ep 655: ep_len:675 episode reward: total was -31.780000. running mean: -29.754904\n",
      "ep 655: ep_len:500 episode reward: total was -46.170000. running mean: -29.919055\n",
      "ep 655: ep_len:427 episode reward: total was -20.840000. running mean: -29.828264\n",
      "ep 655: ep_len:515 episode reward: total was -28.740000. running mean: -29.817382\n",
      "ep 655: ep_len:84 episode reward: total was -2.980000. running mean: -29.549008\n",
      "ep 655: ep_len:520 episode reward: total was -40.830000. running mean: -29.661818\n",
      "ep 655: ep_len:540 episode reward: total was -20.950000. running mean: -29.574699\n",
      "epsilon:0.310456 episode_count: 4592. steps_count: 2053865.000000\n",
      "ep 656: ep_len:665 episode reward: total was -48.870000. running mean: -29.767652\n",
      "ep 656: ep_len:590 episode reward: total was -50.160000. running mean: -29.971576\n",
      "ep 656: ep_len:565 episode reward: total was -48.260000. running mean: -30.154460\n",
      "ep 656: ep_len:540 episode reward: total was -47.320000. running mean: -30.326116\n",
      "ep 656: ep_len:3 episode reward: total was 0.000000. running mean: -30.022854\n",
      "ep 656: ep_len:510 episode reward: total was -39.720000. running mean: -30.119826\n",
      "ep 656: ep_len:595 episode reward: total was -75.760000. running mean: -30.576228\n",
      "epsilon:0.310319 episode_count: 4599. steps_count: 2057333.000000\n",
      "ep 657: ep_len:520 episode reward: total was -59.680000. running mean: -30.867265\n",
      "ep 657: ep_len:650 episode reward: total was -60.270000. running mean: -31.161293\n",
      "ep 657: ep_len:580 episode reward: total was -40.310000. running mean: -31.252780\n",
      "ep 657: ep_len:156 episode reward: total was -4.430000. running mean: -30.984552\n",
      "ep 657: ep_len:91 episode reward: total was -5.990000. running mean: -30.734606\n",
      "ep 657: ep_len:570 episode reward: total was -60.530000. running mean: -31.032560\n",
      "ep 657: ep_len:630 episode reward: total was -55.570000. running mean: -31.277935\n",
      "epsilon:0.310183 episode_count: 4606. steps_count: 2060530.000000\n",
      "ep 658: ep_len:570 episode reward: total was -41.430000. running mean: -31.379455\n",
      "ep 658: ep_len:545 episode reward: total was -11.730000. running mean: -31.182961\n",
      "ep 658: ep_len:535 episode reward: total was -47.630000. running mean: -31.347431\n",
      "ep 658: ep_len:505 episode reward: total was -20.720000. running mean: -31.241157\n",
      "ep 658: ep_len:3 episode reward: total was 0.000000. running mean: -30.928745\n",
      "ep 658: ep_len:176 episode reward: total was -21.940000. running mean: -30.838858\n",
      "ep 658: ep_len:321 episode reward: total was -16.420000. running mean: -30.694669\n",
      "epsilon:0.310046 episode_count: 4613. steps_count: 2063185.000000\n",
      "ep 659: ep_len:500 episode reward: total was -47.340000. running mean: -30.861123\n",
      "ep 659: ep_len:605 episode reward: total was -55.640000. running mean: -31.108911\n",
      "ep 659: ep_len:630 episode reward: total was -41.750000. running mean: -31.215322\n",
      "ep 659: ep_len:515 episode reward: total was -34.140000. running mean: -31.244569\n",
      "ep 659: ep_len:3 episode reward: total was 0.000000. running mean: -30.932123\n",
      "ep 659: ep_len:186 episode reward: total was -8.920000. running mean: -30.712002\n",
      "ep 659: ep_len:555 episode reward: total was -34.660000. running mean: -30.751482\n",
      "epsilon:0.309910 episode_count: 4620. steps_count: 2066179.000000\n",
      "ep 660: ep_len:565 episode reward: total was -43.730000. running mean: -30.881267\n",
      "ep 660: ep_len:500 episode reward: total was -54.910000. running mean: -31.121555\n",
      "ep 660: ep_len:457 episode reward: total was -25.320000. running mean: -31.063539\n",
      "ep 660: ep_len:510 episode reward: total was -29.080000. running mean: -31.043704\n",
      "ep 660: ep_len:3 episode reward: total was 0.000000. running mean: -30.733267\n",
      "ep 660: ep_len:520 episode reward: total was -48.620000. running mean: -30.912134\n",
      "ep 660: ep_len:540 episode reward: total was -57.560000. running mean: -31.178613\n",
      "epsilon:0.309773 episode_count: 4627. steps_count: 2069274.000000\n",
      "ep 661: ep_len:645 episode reward: total was -52.500000. running mean: -31.391827\n",
      "ep 661: ep_len:342 episode reward: total was -35.360000. running mean: -31.431508\n",
      "ep 661: ep_len:760 episode reward: total was -100.090000. running mean: -32.118093\n",
      "ep 661: ep_len:525 episode reward: total was -34.160000. running mean: -32.138512\n",
      "ep 661: ep_len:3 episode reward: total was 0.000000. running mean: -31.817127\n",
      "ep 661: ep_len:520 episode reward: total was -23.630000. running mean: -31.735256\n",
      "ep 661: ep_len:500 episode reward: total was -33.700000. running mean: -31.754903\n",
      "epsilon:0.309637 episode_count: 4634. steps_count: 2072569.000000\n",
      "ep 662: ep_len:545 episode reward: total was -30.650000. running mean: -31.743854\n",
      "ep 662: ep_len:500 episode reward: total was -58.320000. running mean: -32.009616\n",
      "ep 662: ep_len:550 episode reward: total was -36.610000. running mean: -32.055620\n",
      "ep 662: ep_len:535 episode reward: total was -39.700000. running mean: -32.132063\n",
      "ep 662: ep_len:3 episode reward: total was 0.000000. running mean: -31.810743\n",
      "ep 662: ep_len:500 episode reward: total was -22.670000. running mean: -31.719335\n",
      "ep 662: ep_len:345 episode reward: total was -22.400000. running mean: -31.626142\n",
      "epsilon:0.309500 episode_count: 4641. steps_count: 2075547.000000\n",
      "ep 663: ep_len:510 episode reward: total was -31.650000. running mean: -31.626381\n",
      "ep 663: ep_len:188 episode reward: total was -13.930000. running mean: -31.449417\n",
      "ep 663: ep_len:585 episode reward: total was -57.500000. running mean: -31.709923\n",
      "ep 663: ep_len:530 episode reward: total was -24.230000. running mean: -31.635123\n",
      "ep 663: ep_len:3 episode reward: total was 0.000000. running mean: -31.318772\n",
      "ep 663: ep_len:550 episode reward: total was -42.580000. running mean: -31.431384\n",
      "ep 663: ep_len:745 episode reward: total was -68.700000. running mean: -31.804071\n",
      "epsilon:0.309364 episode_count: 4648. steps_count: 2078658.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 664: ep_len:680 episode reward: total was -54.970000. running mean: -32.035730\n",
      "ep 664: ep_len:680 episode reward: total was -24.120000. running mean: -31.956573\n",
      "ep 664: ep_len:63 episode reward: total was -4.480000. running mean: -31.681807\n",
      "ep 664: ep_len:500 episode reward: total was -16.170000. running mean: -31.526689\n",
      "ep 664: ep_len:3 episode reward: total was 0.000000. running mean: -31.211422\n",
      "ep 664: ep_len:525 episode reward: total was -34.700000. running mean: -31.246308\n",
      "ep 664: ep_len:580 episode reward: total was -24.710000. running mean: -31.180945\n",
      "epsilon:0.309227 episode_count: 4655. steps_count: 2081689.000000\n",
      "ep 665: ep_len:205 episode reward: total was -13.400000. running mean: -31.003135\n",
      "ep 665: ep_len:585 episode reward: total was -14.210000. running mean: -30.835204\n",
      "ep 665: ep_len:540 episode reward: total was -31.650000. running mean: -30.843352\n",
      "ep 665: ep_len:555 episode reward: total was -32.690000. running mean: -30.861818\n",
      "ep 665: ep_len:3 episode reward: total was 0.000000. running mean: -30.553200\n",
      "ep 665: ep_len:590 episode reward: total was -36.120000. running mean: -30.608868\n",
      "ep 665: ep_len:520 episode reward: total was -43.250000. running mean: -30.735279\n",
      "epsilon:0.309091 episode_count: 4662. steps_count: 2084687.000000\n",
      "ep 666: ep_len:500 episode reward: total was -28.750000. running mean: -30.715427\n",
      "ep 666: ep_len:580 episode reward: total was -15.530000. running mean: -30.563572\n",
      "ep 666: ep_len:500 episode reward: total was -43.140000. running mean: -30.689337\n",
      "ep 666: ep_len:500 episode reward: total was -32.640000. running mean: -30.708843\n",
      "ep 666: ep_len:96 episode reward: total was -5.460000. running mean: -30.456355\n",
      "ep 666: ep_len:580 episode reward: total was -46.250000. running mean: -30.614291\n",
      "ep 666: ep_len:620 episode reward: total was -30.650000. running mean: -30.614648\n",
      "epsilon:0.308954 episode_count: 4669. steps_count: 2088063.000000\n",
      "ep 667: ep_len:243 episode reward: total was -13.940000. running mean: -30.447902\n",
      "ep 667: ep_len:510 episode reward: total was -57.840000. running mean: -30.721823\n",
      "ep 667: ep_len:605 episode reward: total was -34.670000. running mean: -30.761305\n",
      "ep 667: ep_len:500 episode reward: total was -21.980000. running mean: -30.673492\n",
      "ep 667: ep_len:3 episode reward: total was 0.000000. running mean: -30.366757\n",
      "ep 667: ep_len:640 episode reward: total was -34.340000. running mean: -30.406489\n",
      "ep 667: ep_len:303 episode reward: total was -19.900000. running mean: -30.301424\n",
      "epsilon:0.308818 episode_count: 4676. steps_count: 2090867.000000\n",
      "ep 668: ep_len:600 episode reward: total was -50.890000. running mean: -30.507310\n",
      "ep 668: ep_len:500 episode reward: total was -41.940000. running mean: -30.621637\n",
      "ep 668: ep_len:565 episode reward: total was -30.590000. running mean: -30.621320\n",
      "ep 668: ep_len:515 episode reward: total was -14.080000. running mean: -30.455907\n",
      "ep 668: ep_len:3 episode reward: total was 0.000000. running mean: -30.151348\n",
      "ep 668: ep_len:900 episode reward: total was -146.270000. running mean: -31.312535\n",
      "ep 668: ep_len:293 episode reward: total was -24.900000. running mean: -31.248409\n",
      "epsilon:0.308681 episode_count: 4683. steps_count: 2094243.000000\n",
      "ep 669: ep_len:765 episode reward: total was -65.590000. running mean: -31.591825\n",
      "ep 669: ep_len:500 episode reward: total was -34.660000. running mean: -31.622507\n",
      "ep 669: ep_len:665 episode reward: total was -73.750000. running mean: -32.043782\n",
      "ep 669: ep_len:510 episode reward: total was -40.820000. running mean: -32.131544\n",
      "ep 669: ep_len:3 episode reward: total was 0.000000. running mean: -31.810229\n",
      "ep 669: ep_len:635 episode reward: total was -42.450000. running mean: -31.916626\n",
      "ep 669: ep_len:525 episode reward: total was -71.250000. running mean: -32.309960\n",
      "epsilon:0.308545 episode_count: 4690. steps_count: 2097846.000000\n",
      "ep 670: ep_len:570 episode reward: total was -40.420000. running mean: -32.391061\n",
      "ep 670: ep_len:555 episode reward: total was -16.270000. running mean: -32.229850\n",
      "ep 670: ep_len:525 episode reward: total was -25.230000. running mean: -32.159851\n",
      "ep 670: ep_len:383 episode reward: total was -42.270000. running mean: -32.260953\n",
      "ep 670: ep_len:106 episode reward: total was -14.480000. running mean: -32.083143\n",
      "ep 670: ep_len:595 episode reward: total was -24.040000. running mean: -32.002712\n",
      "ep 670: ep_len:515 episode reward: total was -38.700000. running mean: -32.069685\n",
      "epsilon:0.308408 episode_count: 4697. steps_count: 2101095.000000\n",
      "ep 671: ep_len:670 episode reward: total was -49.390000. running mean: -32.242888\n",
      "ep 671: ep_len:505 episode reward: total was -22.360000. running mean: -32.144059\n",
      "ep 671: ep_len:59 episode reward: total was -5.970000. running mean: -31.882319\n",
      "ep 671: ep_len:755 episode reward: total was -108.460000. running mean: -32.648095\n",
      "ep 671: ep_len:3 episode reward: total was 0.000000. running mean: -32.321614\n",
      "ep 671: ep_len:186 episode reward: total was -8.430000. running mean: -32.082698\n",
      "ep 671: ep_len:505 episode reward: total was -32.830000. running mean: -32.090171\n",
      "epsilon:0.308272 episode_count: 4704. steps_count: 2103778.000000\n",
      "ep 672: ep_len:705 episode reward: total was -52.410000. running mean: -32.293370\n",
      "ep 672: ep_len:500 episode reward: total was -20.340000. running mean: -32.173836\n",
      "ep 672: ep_len:525 episode reward: total was -37.510000. running mean: -32.227197\n",
      "ep 672: ep_len:630 episode reward: total was -31.050000. running mean: -32.215426\n",
      "ep 672: ep_len:88 episode reward: total was 0.030000. running mean: -31.892971\n",
      "ep 672: ep_len:550 episode reward: total was -45.670000. running mean: -32.030742\n",
      "ep 672: ep_len:565 episode reward: total was -42.480000. running mean: -32.135234\n",
      "epsilon:0.308135 episode_count: 4711. steps_count: 2107341.000000\n",
      "ep 673: ep_len:585 episode reward: total was -49.670000. running mean: -32.310582\n",
      "ep 673: ep_len:505 episode reward: total was -50.300000. running mean: -32.490476\n",
      "ep 673: ep_len:555 episode reward: total was -29.630000. running mean: -32.461871\n",
      "ep 673: ep_len:619 episode reward: total was -52.160000. running mean: -32.658852\n",
      "ep 673: ep_len:80 episode reward: total was -6.500000. running mean: -32.397264\n",
      "ep 673: ep_len:1095 episode reward: total was -130.730000. running mean: -33.380591\n",
      "ep 673: ep_len:294 episode reward: total was -19.890000. running mean: -33.245685\n",
      "epsilon:0.307999 episode_count: 4718. steps_count: 2111074.000000\n",
      "ep 674: ep_len:605 episode reward: total was -38.570000. running mean: -33.298929\n",
      "ep 674: ep_len:500 episode reward: total was -43.290000. running mean: -33.398839\n",
      "ep 674: ep_len:650 episode reward: total was -63.600000. running mean: -33.700851\n",
      "ep 674: ep_len:505 episode reward: total was -53.700000. running mean: -33.900842\n",
      "ep 674: ep_len:3 episode reward: total was 0.000000. running mean: -33.561834\n",
      "ep 674: ep_len:500 episode reward: total was -52.350000. running mean: -33.749716\n",
      "ep 674: ep_len:635 episode reward: total was -56.190000. running mean: -33.974118\n",
      "epsilon:0.307862 episode_count: 4725. steps_count: 2114472.000000\n",
      "ep 675: ep_len:134 episode reward: total was -8.450000. running mean: -33.718877\n",
      "ep 675: ep_len:500 episode reward: total was -38.680000. running mean: -33.768488\n",
      "ep 675: ep_len:530 episode reward: total was -40.030000. running mean: -33.831104\n",
      "ep 675: ep_len:505 episode reward: total was -33.160000. running mean: -33.824393\n",
      "ep 675: ep_len:3 episode reward: total was 0.000000. running mean: -33.486149\n",
      "ep 675: ep_len:500 episode reward: total was -27.110000. running mean: -33.422387\n",
      "ep 675: ep_len:640 episode reward: total was -47.130000. running mean: -33.559463\n",
      "epsilon:0.307726 episode_count: 4732. steps_count: 2117284.000000\n",
      "ep 676: ep_len:570 episode reward: total was -57.340000. running mean: -33.797269\n",
      "ep 676: ep_len:625 episode reward: total was -71.620000. running mean: -34.175496\n",
      "ep 676: ep_len:620 episode reward: total was -52.760000. running mean: -34.361341\n",
      "ep 676: ep_len:170 episode reward: total was -4.930000. running mean: -34.067028\n",
      "ep 676: ep_len:3 episode reward: total was 0.000000. running mean: -33.726357\n",
      "ep 676: ep_len:715 episode reward: total was -61.020000. running mean: -33.999294\n",
      "ep 676: ep_len:575 episode reward: total was -60.180000. running mean: -34.261101\n",
      "epsilon:0.307589 episode_count: 4739. steps_count: 2120562.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 677: ep_len:525 episode reward: total was -30.980000. running mean: -34.228290\n",
      "ep 677: ep_len:610 episode reward: total was -47.630000. running mean: -34.362307\n",
      "ep 677: ep_len:585 episode reward: total was -45.190000. running mean: -34.470584\n",
      "ep 677: ep_len:500 episode reward: total was -2.670000. running mean: -34.152578\n",
      "ep 677: ep_len:3 episode reward: total was 0.000000. running mean: -33.811052\n",
      "ep 677: ep_len:535 episode reward: total was -33.360000. running mean: -33.806542\n",
      "ep 677: ep_len:615 episode reward: total was -55.170000. running mean: -34.020176\n",
      "epsilon:0.307453 episode_count: 4746. steps_count: 2123935.000000\n",
      "ep 678: ep_len:500 episode reward: total was -24.650000. running mean: -33.926475\n",
      "ep 678: ep_len:515 episode reward: total was -31.860000. running mean: -33.905810\n",
      "ep 678: ep_len:700 episode reward: total was -45.350000. running mean: -34.020252\n",
      "ep 678: ep_len:500 episode reward: total was -38.250000. running mean: -34.062549\n",
      "ep 678: ep_len:50 episode reward: total was -5.990000. running mean: -33.781824\n",
      "ep 678: ep_len:625 episode reward: total was -32.620000. running mean: -33.770205\n",
      "ep 678: ep_len:525 episode reward: total was -46.620000. running mean: -33.898703\n",
      "epsilon:0.307316 episode_count: 4753. steps_count: 2127350.000000\n",
      "ep 679: ep_len:645 episode reward: total was -54.030000. running mean: -34.100016\n",
      "ep 679: ep_len:520 episode reward: total was -29.650000. running mean: -34.055516\n",
      "ep 679: ep_len:79 episode reward: total was -4.490000. running mean: -33.759861\n",
      "ep 679: ep_len:124 episode reward: total was -3.930000. running mean: -33.461562\n",
      "ep 679: ep_len:3 episode reward: total was 0.000000. running mean: -33.126947\n",
      "ep 679: ep_len:560 episode reward: total was -26.670000. running mean: -33.062377\n",
      "ep 679: ep_len:680 episode reward: total was -117.430000. running mean: -33.906054\n",
      "epsilon:0.307180 episode_count: 4760. steps_count: 2129961.000000\n",
      "ep 680: ep_len:570 episode reward: total was -42.440000. running mean: -33.991393\n",
      "ep 680: ep_len:500 episode reward: total was -26.940000. running mean: -33.920879\n",
      "ep 680: ep_len:565 episode reward: total was -32.380000. running mean: -33.905470\n",
      "ep 680: ep_len:545 episode reward: total was -56.170000. running mean: -34.128116\n",
      "ep 680: ep_len:3 episode reward: total was 0.000000. running mean: -33.786834\n",
      "ep 680: ep_len:530 episode reward: total was -38.130000. running mean: -33.830266\n",
      "ep 680: ep_len:590 episode reward: total was -49.030000. running mean: -33.982263\n",
      "epsilon:0.307043 episode_count: 4767. steps_count: 2133264.000000\n",
      "ep 681: ep_len:500 episode reward: total was -28.440000. running mean: -33.926841\n",
      "ep 681: ep_len:620 episode reward: total was -34.550000. running mean: -33.933072\n",
      "ep 681: ep_len:620 episode reward: total was -59.240000. running mean: -34.186142\n",
      "ep 681: ep_len:515 episode reward: total was -38.560000. running mean: -34.229880\n",
      "ep 681: ep_len:52 episode reward: total was 0.500000. running mean: -33.882581\n",
      "ep 681: ep_len:500 episode reward: total was -22.830000. running mean: -33.772056\n",
      "ep 681: ep_len:605 episode reward: total was -46.250000. running mean: -33.896835\n",
      "epsilon:0.306907 episode_count: 4774. steps_count: 2136676.000000\n",
      "ep 682: ep_len:500 episode reward: total was -69.420000. running mean: -34.252067\n",
      "ep 682: ep_len:530 episode reward: total was -18.260000. running mean: -34.092146\n",
      "ep 682: ep_len:500 episode reward: total was -39.350000. running mean: -34.144725\n",
      "ep 682: ep_len:570 episode reward: total was -59.190000. running mean: -34.395177\n",
      "ep 682: ep_len:3 episode reward: total was 0.000000. running mean: -34.051226\n",
      "ep 682: ep_len:660 episode reward: total was -54.520000. running mean: -34.255913\n",
      "ep 682: ep_len:585 episode reward: total was -30.620000. running mean: -34.219554\n",
      "epsilon:0.306770 episode_count: 4781. steps_count: 2140024.000000\n",
      "ep 683: ep_len:625 episode reward: total was -57.580000. running mean: -34.453159\n",
      "ep 683: ep_len:565 episode reward: total was -24.680000. running mean: -34.355427\n",
      "ep 683: ep_len:535 episode reward: total was -30.970000. running mean: -34.321573\n",
      "ep 683: ep_len:545 episode reward: total was -29.310000. running mean: -34.271457\n",
      "ep 683: ep_len:98 episode reward: total was -2.460000. running mean: -33.953342\n",
      "ep 683: ep_len:580 episode reward: total was -28.180000. running mean: -33.895609\n",
      "ep 683: ep_len:510 episode reward: total was -30.530000. running mean: -33.861953\n",
      "epsilon:0.306634 episode_count: 4788. steps_count: 2143482.000000\n",
      "ep 684: ep_len:565 episode reward: total was -36.680000. running mean: -33.890133\n",
      "ep 684: ep_len:500 episode reward: total was -32.540000. running mean: -33.876632\n",
      "ep 684: ep_len:610 episode reward: total was -48.060000. running mean: -34.018466\n",
      "ep 684: ep_len:525 episode reward: total was -20.660000. running mean: -33.884881\n",
      "ep 684: ep_len:75 episode reward: total was -2.960000. running mean: -33.575632\n",
      "ep 684: ep_len:525 episode reward: total was -58.830000. running mean: -33.828176\n",
      "ep 684: ep_len:182 episode reward: total was -14.490000. running mean: -33.634794\n",
      "epsilon:0.306497 episode_count: 4795. steps_count: 2146464.000000\n",
      "ep 685: ep_len:255 episode reward: total was -20.950000. running mean: -33.507946\n",
      "ep 685: ep_len:510 episode reward: total was -25.600000. running mean: -33.428867\n",
      "ep 685: ep_len:520 episode reward: total was -30.730000. running mean: -33.401878\n",
      "ep 685: ep_len:520 episode reward: total was -36.680000. running mean: -33.434659\n",
      "ep 685: ep_len:3 episode reward: total was 0.000000. running mean: -33.100313\n",
      "ep 685: ep_len:219 episode reward: total was -25.360000. running mean: -33.022910\n",
      "ep 685: ep_len:500 episode reward: total was -39.110000. running mean: -33.083781\n",
      "epsilon:0.306361 episode_count: 4802. steps_count: 2148991.000000\n",
      "ep 686: ep_len:605 episode reward: total was -31.680000. running mean: -33.069743\n",
      "ep 686: ep_len:515 episode reward: total was -29.990000. running mean: -33.038945\n",
      "ep 686: ep_len:805 episode reward: total was -78.980000. running mean: -33.498356\n",
      "ep 686: ep_len:505 episode reward: total was -30.100000. running mean: -33.464372\n",
      "ep 686: ep_len:3 episode reward: total was 0.000000. running mean: -33.129729\n",
      "ep 686: ep_len:605 episode reward: total was -32.630000. running mean: -33.124731\n",
      "ep 686: ep_len:605 episode reward: total was -20.650000. running mean: -32.999984\n",
      "epsilon:0.306224 episode_count: 4809. steps_count: 2152634.000000\n",
      "ep 687: ep_len:565 episode reward: total was -45.320000. running mean: -33.123184\n",
      "ep 687: ep_len:570 episode reward: total was -23.240000. running mean: -33.024352\n",
      "ep 687: ep_len:660 episode reward: total was -37.040000. running mean: -33.064509\n",
      "ep 687: ep_len:550 episode reward: total was -35.610000. running mean: -33.089964\n",
      "ep 687: ep_len:89 episode reward: total was 0.030000. running mean: -32.758764\n",
      "ep 687: ep_len:585 episode reward: total was -36.700000. running mean: -32.798176\n",
      "ep 687: ep_len:600 episode reward: total was -45.720000. running mean: -32.927395\n",
      "epsilon:0.306088 episode_count: 4816. steps_count: 2156253.000000\n",
      "ep 688: ep_len:645 episode reward: total was -38.040000. running mean: -32.978521\n",
      "ep 688: ep_len:525 episode reward: total was -30.830000. running mean: -32.957036\n",
      "ep 688: ep_len:620 episode reward: total was -39.040000. running mean: -33.017865\n",
      "ep 688: ep_len:500 episode reward: total was -35.110000. running mean: -33.038786\n",
      "ep 688: ep_len:3 episode reward: total was 0.000000. running mean: -32.708399\n",
      "ep 688: ep_len:715 episode reward: total was -70.490000. running mean: -33.086215\n",
      "ep 688: ep_len:510 episode reward: total was -44.740000. running mean: -33.202752\n",
      "epsilon:0.305951 episode_count: 4823. steps_count: 2159771.000000\n",
      "ep 689: ep_len:665 episode reward: total was -43.180000. running mean: -33.302525\n",
      "ep 689: ep_len:625 episode reward: total was -21.140000. running mean: -33.180900\n",
      "ep 689: ep_len:590 episode reward: total was -36.220000. running mean: -33.211291\n",
      "ep 689: ep_len:500 episode reward: total was -18.090000. running mean: -33.060078\n",
      "ep 689: ep_len:3 episode reward: total was 0.000000. running mean: -32.729477\n",
      "ep 689: ep_len:575 episode reward: total was -17.680000. running mean: -32.578982\n",
      "ep 689: ep_len:545 episode reward: total was -35.970000. running mean: -32.612892\n",
      "epsilon:0.305815 episode_count: 4830. steps_count: 2163274.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 690: ep_len:510 episode reward: total was -40.370000. running mean: -32.690464\n",
      "ep 690: ep_len:500 episode reward: total was -47.840000. running mean: -32.841959\n",
      "ep 690: ep_len:605 episode reward: total was -29.060000. running mean: -32.804139\n",
      "ep 690: ep_len:140 episode reward: total was -7.420000. running mean: -32.550298\n",
      "ep 690: ep_len:3 episode reward: total was 0.000000. running mean: -32.224795\n",
      "ep 690: ep_len:505 episode reward: total was -22.350000. running mean: -32.126047\n",
      "ep 690: ep_len:515 episode reward: total was -37.520000. running mean: -32.179987\n",
      "epsilon:0.305678 episode_count: 4837. steps_count: 2166052.000000\n",
      "ep 691: ep_len:545 episode reward: total was -43.740000. running mean: -32.295587\n",
      "ep 691: ep_len:530 episode reward: total was -40.750000. running mean: -32.380131\n",
      "ep 691: ep_len:500 episode reward: total was -30.730000. running mean: -32.363629\n",
      "ep 691: ep_len:40 episode reward: total was -3.490000. running mean: -32.074893\n",
      "ep 691: ep_len:3 episode reward: total was 0.000000. running mean: -31.754144\n",
      "ep 691: ep_len:575 episode reward: total was -63.580000. running mean: -32.072403\n",
      "ep 691: ep_len:625 episode reward: total was -24.410000. running mean: -31.995779\n",
      "epsilon:0.305542 episode_count: 4844. steps_count: 2168870.000000\n",
      "ep 692: ep_len:535 episode reward: total was -29.030000. running mean: -31.966121\n",
      "ep 692: ep_len:500 episode reward: total was -36.440000. running mean: -32.010860\n",
      "ep 692: ep_len:635 episode reward: total was -64.180000. running mean: -32.332551\n",
      "ep 692: ep_len:555 episode reward: total was -34.750000. running mean: -32.356726\n",
      "ep 692: ep_len:3 episode reward: total was 0.000000. running mean: -32.033158\n",
      "ep 692: ep_len:535 episode reward: total was -48.650000. running mean: -32.199327\n",
      "ep 692: ep_len:178 episode reward: total was -13.430000. running mean: -32.011634\n",
      "epsilon:0.305405 episode_count: 4851. steps_count: 2171811.000000\n",
      "ep 693: ep_len:640 episode reward: total was -66.170000. running mean: -32.353217\n",
      "ep 693: ep_len:530 episode reward: total was -35.600000. running mean: -32.385685\n",
      "ep 693: ep_len:560 episode reward: total was -29.220000. running mean: -32.354028\n",
      "ep 693: ep_len:500 episode reward: total was -40.120000. running mean: -32.431688\n",
      "ep 693: ep_len:3 episode reward: total was 0.000000. running mean: -32.107371\n",
      "ep 693: ep_len:540 episode reward: total was -33.580000. running mean: -32.122097\n",
      "ep 693: ep_len:640 episode reward: total was -53.560000. running mean: -32.336476\n",
      "epsilon:0.305269 episode_count: 4858. steps_count: 2175224.000000\n",
      "ep 694: ep_len:500 episode reward: total was -42.070000. running mean: -32.433812\n",
      "ep 694: ep_len:520 episode reward: total was -28.550000. running mean: -32.394973\n",
      "ep 694: ep_len:560 episode reward: total was -28.450000. running mean: -32.355524\n",
      "ep 694: ep_len:106 episode reward: total was -3.940000. running mean: -32.071369\n",
      "ep 694: ep_len:3 episode reward: total was 0.000000. running mean: -31.750655\n",
      "ep 694: ep_len:525 episode reward: total was -40.600000. running mean: -31.839148\n",
      "ep 694: ep_len:500 episode reward: total was -29.700000. running mean: -31.817757\n",
      "epsilon:0.305132 episode_count: 4865. steps_count: 2177938.000000\n",
      "ep 695: ep_len:520 episode reward: total was -45.050000. running mean: -31.950079\n",
      "ep 695: ep_len:505 episode reward: total was -25.410000. running mean: -31.884678\n",
      "ep 695: ep_len:500 episode reward: total was -32.590000. running mean: -31.891732\n",
      "ep 695: ep_len:515 episode reward: total was -12.600000. running mean: -31.698814\n",
      "ep 695: ep_len:3 episode reward: total was 0.000000. running mean: -31.381826\n",
      "ep 695: ep_len:500 episode reward: total was -16.280000. running mean: -31.230808\n",
      "ep 695: ep_len:500 episode reward: total was -41.240000. running mean: -31.330900\n",
      "epsilon:0.304996 episode_count: 4872. steps_count: 2180981.000000\n",
      "ep 696: ep_len:238 episode reward: total was -25.900000. running mean: -31.276591\n",
      "ep 696: ep_len:610 episode reward: total was -72.730000. running mean: -31.691125\n",
      "ep 696: ep_len:610 episode reward: total was -56.670000. running mean: -31.940914\n",
      "ep 696: ep_len:560 episode reward: total was -17.120000. running mean: -31.792705\n",
      "ep 696: ep_len:88 episode reward: total was -12.960000. running mean: -31.604378\n",
      "ep 696: ep_len:545 episode reward: total was -40.060000. running mean: -31.688934\n",
      "ep 696: ep_len:510 episode reward: total was -32.600000. running mean: -31.698044\n",
      "epsilon:0.304859 episode_count: 4879. steps_count: 2184142.000000\n",
      "ep 697: ep_len:200 episode reward: total was -7.920000. running mean: -31.460264\n",
      "ep 697: ep_len:266 episode reward: total was -16.900000. running mean: -31.314661\n",
      "ep 697: ep_len:645 episode reward: total was -30.850000. running mean: -31.310015\n",
      "ep 697: ep_len:161 episode reward: total was -3.910000. running mean: -31.036015\n",
      "ep 697: ep_len:3 episode reward: total was 0.000000. running mean: -30.725654\n",
      "ep 697: ep_len:635 episode reward: total was -43.130000. running mean: -30.849698\n",
      "ep 697: ep_len:565 episode reward: total was -50.530000. running mean: -31.046501\n",
      "epsilon:0.304723 episode_count: 4886. steps_count: 2186617.000000\n",
      "ep 698: ep_len:520 episode reward: total was -30.300000. running mean: -31.039036\n",
      "ep 698: ep_len:292 episode reward: total was -23.410000. running mean: -30.962746\n",
      "ep 698: ep_len:79 episode reward: total was -6.970000. running mean: -30.722818\n",
      "ep 698: ep_len:361 episode reward: total was -29.250000. running mean: -30.708090\n",
      "ep 698: ep_len:98 episode reward: total was -0.440000. running mean: -30.405409\n",
      "ep 698: ep_len:560 episode reward: total was -41.770000. running mean: -30.519055\n",
      "ep 698: ep_len:505 episode reward: total was -22.420000. running mean: -30.438064\n",
      "epsilon:0.304586 episode_count: 4893. steps_count: 2189032.000000\n",
      "ep 699: ep_len:755 episode reward: total was -88.700000. running mean: -31.020684\n",
      "ep 699: ep_len:625 episode reward: total was -33.650000. running mean: -31.046977\n",
      "ep 699: ep_len:570 episode reward: total was -29.710000. running mean: -31.033607\n",
      "ep 699: ep_len:495 episode reward: total was -27.190000. running mean: -30.995171\n",
      "ep 699: ep_len:104 episode reward: total was -6.950000. running mean: -30.754719\n",
      "ep 699: ep_len:520 episode reward: total was -49.160000. running mean: -30.938772\n",
      "ep 699: ep_len:580 episode reward: total was -43.690000. running mean: -31.066284\n",
      "epsilon:0.304450 episode_count: 4900. steps_count: 2192681.000000\n",
      "ep 700: ep_len:515 episode reward: total was -41.560000. running mean: -31.171222\n",
      "ep 700: ep_len:500 episode reward: total was -22.320000. running mean: -31.082709\n",
      "ep 700: ep_len:451 episode reward: total was -31.360000. running mean: -31.085482\n",
      "ep 700: ep_len:500 episode reward: total was -17.260000. running mean: -30.947227\n",
      "ep 700: ep_len:105 episode reward: total was -3.970000. running mean: -30.677455\n",
      "ep 700: ep_len:500 episode reward: total was -30.090000. running mean: -30.671581\n",
      "ep 700: ep_len:515 episode reward: total was -32.540000. running mean: -30.690265\n",
      "epsilon:0.304313 episode_count: 4907. steps_count: 2195767.000000\n",
      "ep 701: ep_len:530 episode reward: total was -28.040000. running mean: -30.663762\n",
      "ep 701: ep_len:640 episode reward: total was -40.770000. running mean: -30.764825\n",
      "ep 701: ep_len:620 episode reward: total was -40.190000. running mean: -30.859076\n",
      "ep 701: ep_len:500 episode reward: total was -20.180000. running mean: -30.752286\n",
      "ep 701: ep_len:3 episode reward: total was 0.000000. running mean: -30.444763\n",
      "ep 701: ep_len:500 episode reward: total was -36.860000. running mean: -30.508915\n",
      "ep 701: ep_len:505 episode reward: total was -31.430000. running mean: -30.518126\n",
      "epsilon:0.304177 episode_count: 4914. steps_count: 2199065.000000\n",
      "ep 702: ep_len:575 episode reward: total was -29.750000. running mean: -30.510445\n",
      "ep 702: ep_len:535 episode reward: total was -31.300000. running mean: -30.518340\n",
      "ep 702: ep_len:500 episode reward: total was -47.600000. running mean: -30.689157\n",
      "ep 702: ep_len:625 episode reward: total was -61.710000. running mean: -30.999365\n",
      "ep 702: ep_len:3 episode reward: total was 0.000000. running mean: -30.689372\n",
      "ep 702: ep_len:214 episode reward: total was -10.440000. running mean: -30.486878\n",
      "ep 702: ep_len:500 episode reward: total was -44.590000. running mean: -30.627909\n",
      "epsilon:0.304040 episode_count: 4921. steps_count: 2202017.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 703: ep_len:650 episode reward: total was -54.550000. running mean: -30.867130\n",
      "ep 703: ep_len:775 episode reward: total was -85.570000. running mean: -31.414159\n",
      "ep 703: ep_len:575 episode reward: total was -29.240000. running mean: -31.392417\n",
      "ep 703: ep_len:500 episode reward: total was -12.520000. running mean: -31.203693\n",
      "ep 703: ep_len:120 episode reward: total was -8.930000. running mean: -30.980956\n",
      "ep 703: ep_len:540 episode reward: total was -50.750000. running mean: -31.178646\n",
      "ep 703: ep_len:285 episode reward: total was -21.460000. running mean: -31.081460\n",
      "epsilon:0.303904 episode_count: 4928. steps_count: 2205462.000000\n",
      "ep 704: ep_len:565 episode reward: total was -62.100000. running mean: -31.391645\n",
      "ep 704: ep_len:588 episode reward: total was -70.820000. running mean: -31.785929\n",
      "ep 704: ep_len:635 episode reward: total was -32.050000. running mean: -31.788570\n",
      "ep 704: ep_len:132 episode reward: total was -9.950000. running mean: -31.570184\n",
      "ep 704: ep_len:3 episode reward: total was 0.000000. running mean: -31.254482\n",
      "ep 704: ep_len:580 episode reward: total was -24.160000. running mean: -31.183537\n",
      "ep 704: ep_len:650 episode reward: total was -48.510000. running mean: -31.356802\n",
      "epsilon:0.303767 episode_count: 4935. steps_count: 2208615.000000\n",
      "ep 705: ep_len:550 episode reward: total was -35.840000. running mean: -31.401634\n",
      "ep 705: ep_len:525 episode reward: total was -34.070000. running mean: -31.428318\n",
      "ep 705: ep_len:438 episode reward: total was -18.340000. running mean: -31.297434\n",
      "ep 705: ep_len:103 episode reward: total was -3.450000. running mean: -31.018960\n",
      "ep 705: ep_len:3 episode reward: total was 0.000000. running mean: -30.708770\n",
      "ep 705: ep_len:510 episode reward: total was -30.830000. running mean: -30.709983\n",
      "ep 705: ep_len:550 episode reward: total was -34.050000. running mean: -30.743383\n",
      "epsilon:0.303631 episode_count: 4942. steps_count: 2211294.000000\n",
      "ep 706: ep_len:540 episode reward: total was -50.660000. running mean: -30.942549\n",
      "ep 706: ep_len:198 episode reward: total was -13.450000. running mean: -30.767624\n",
      "ep 706: ep_len:870 episode reward: total was -82.410000. running mean: -31.284047\n",
      "ep 706: ep_len:383 episode reward: total was -13.220000. running mean: -31.103407\n",
      "ep 706: ep_len:82 episode reward: total was -5.950000. running mean: -30.851873\n",
      "ep 706: ep_len:590 episode reward: total was -50.240000. running mean: -31.045754\n",
      "ep 706: ep_len:202 episode reward: total was -14.450000. running mean: -30.879797\n",
      "epsilon:0.303494 episode_count: 4949. steps_count: 2214159.000000\n",
      "ep 707: ep_len:650 episode reward: total was -40.920000. running mean: -30.980199\n",
      "ep 707: ep_len:625 episode reward: total was -40.060000. running mean: -31.070997\n",
      "ep 707: ep_len:545 episode reward: total was -37.580000. running mean: -31.136087\n",
      "ep 707: ep_len:505 episode reward: total was -9.680000. running mean: -30.921526\n",
      "ep 707: ep_len:3 episode reward: total was 0.000000. running mean: -30.612310\n",
      "ep 707: ep_len:530 episode reward: total was -19.900000. running mean: -30.505187\n",
      "ep 707: ep_len:620 episode reward: total was -29.380000. running mean: -30.493935\n",
      "epsilon:0.303358 episode_count: 4956. steps_count: 2217637.000000\n",
      "ep 708: ep_len:500 episode reward: total was -26.380000. running mean: -30.452796\n",
      "ep 708: ep_len:535 episode reward: total was -22.740000. running mean: -30.375668\n",
      "ep 708: ep_len:650 episode reward: total was -33.590000. running mean: -30.407811\n",
      "ep 708: ep_len:48 episode reward: total was -4.470000. running mean: -30.148433\n",
      "ep 708: ep_len:3 episode reward: total was 0.000000. running mean: -29.846949\n",
      "ep 708: ep_len:540 episode reward: total was -27.810000. running mean: -29.826580\n",
      "ep 708: ep_len:301 episode reward: total was -26.880000. running mean: -29.797114\n",
      "epsilon:0.303221 episode_count: 4963. steps_count: 2220214.000000\n",
      "ep 709: ep_len:640 episode reward: total was -57.530000. running mean: -30.074443\n",
      "ep 709: ep_len:358 episode reward: total was -34.870000. running mean: -30.122398\n",
      "ep 709: ep_len:585 episode reward: total was -47.000000. running mean: -30.291174\n",
      "ep 709: ep_len:101 episode reward: total was -4.470000. running mean: -30.032962\n",
      "ep 709: ep_len:95 episode reward: total was 4.060000. running mean: -29.692033\n",
      "ep 709: ep_len:595 episode reward: total was -37.700000. running mean: -29.772113\n",
      "ep 709: ep_len:525 episode reward: total was -51.240000. running mean: -29.986791\n",
      "epsilon:0.303085 episode_count: 4970. steps_count: 2223113.000000\n",
      "ep 710: ep_len:248 episode reward: total was -11.430000. running mean: -29.801223\n",
      "ep 710: ep_len:500 episode reward: total was -20.860000. running mean: -29.711811\n",
      "ep 710: ep_len:650 episode reward: total was -40.350000. running mean: -29.818193\n",
      "ep 710: ep_len:405 episode reward: total was -37.760000. running mean: -29.897611\n",
      "ep 710: ep_len:3 episode reward: total was 0.000000. running mean: -29.598635\n",
      "ep 710: ep_len:625 episode reward: total was -34.680000. running mean: -29.649449\n",
      "ep 710: ep_len:289 episode reward: total was -30.410000. running mean: -29.657054\n",
      "epsilon:0.302948 episode_count: 4977. steps_count: 2225833.000000\n",
      "ep 711: ep_len:520 episode reward: total was -39.100000. running mean: -29.751484\n",
      "ep 711: ep_len:605 episode reward: total was -41.070000. running mean: -29.864669\n",
      "ep 711: ep_len:550 episode reward: total was -24.850000. running mean: -29.814522\n",
      "ep 711: ep_len:500 episode reward: total was -25.070000. running mean: -29.767077\n",
      "ep 711: ep_len:90 episode reward: total was -11.450000. running mean: -29.583906\n",
      "ep 711: ep_len:665 episode reward: total was -51.970000. running mean: -29.807767\n",
      "ep 711: ep_len:625 episode reward: total was -23.360000. running mean: -29.743289\n",
      "epsilon:0.302812 episode_count: 4984. steps_count: 2229388.000000\n",
      "ep 712: ep_len:520 episode reward: total was -40.520000. running mean: -29.851057\n",
      "ep 712: ep_len:615 episode reward: total was -30.950000. running mean: -29.862046\n",
      "ep 712: ep_len:595 episode reward: total was -28.650000. running mean: -29.849926\n",
      "ep 712: ep_len:545 episode reward: total was -45.760000. running mean: -30.009026\n",
      "ep 712: ep_len:3 episode reward: total was 0.000000. running mean: -29.708936\n",
      "ep 712: ep_len:500 episode reward: total was -30.740000. running mean: -29.719247\n",
      "ep 712: ep_len:515 episode reward: total was -28.940000. running mean: -29.711454\n",
      "epsilon:0.302675 episode_count: 4991. steps_count: 2232681.000000\n",
      "ep 713: ep_len:131 episode reward: total was -1.960000. running mean: -29.433940\n",
      "ep 713: ep_len:650 episode reward: total was -26.540000. running mean: -29.405000\n",
      "ep 713: ep_len:595 episode reward: total was -33.200000. running mean: -29.442950\n",
      "ep 713: ep_len:630 episode reward: total was -41.540000. running mean: -29.563921\n",
      "ep 713: ep_len:107 episode reward: total was -0.450000. running mean: -29.272782\n",
      "ep 713: ep_len:500 episode reward: total was -44.130000. running mean: -29.421354\n",
      "ep 713: ep_len:560 episode reward: total was -33.210000. running mean: -29.459240\n",
      "epsilon:0.302539 episode_count: 4998. steps_count: 2235854.000000\n",
      "ep 714: ep_len:545 episode reward: total was -23.590000. running mean: -29.400548\n",
      "ep 714: ep_len:292 episode reward: total was -10.340000. running mean: -29.209942\n",
      "ep 714: ep_len:685 episode reward: total was -51.440000. running mean: -29.432243\n",
      "ep 714: ep_len:500 episode reward: total was -20.680000. running mean: -29.344720\n",
      "ep 714: ep_len:3 episode reward: total was 0.000000. running mean: -29.051273\n",
      "ep 714: ep_len:555 episode reward: total was -63.170000. running mean: -29.392461\n",
      "ep 714: ep_len:570 episode reward: total was -37.040000. running mean: -29.468936\n",
      "epsilon:0.302402 episode_count: 5005. steps_count: 2239004.000000\n",
      "ep 715: ep_len:605 episode reward: total was -29.730000. running mean: -29.471547\n",
      "ep 715: ep_len:500 episode reward: total was -27.810000. running mean: -29.454931\n",
      "ep 715: ep_len:670 episode reward: total was -40.390000. running mean: -29.564282\n",
      "ep 715: ep_len:414 episode reward: total was -50.250000. running mean: -29.771139\n",
      "ep 715: ep_len:131 episode reward: total was 5.540000. running mean: -29.418028\n",
      "ep 715: ep_len:525 episode reward: total was -44.570000. running mean: -29.569547\n",
      "ep 715: ep_len:575 episode reward: total was -43.090000. running mean: -29.704752\n",
      "epsilon:0.302266 episode_count: 5012. steps_count: 2242424.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 716: ep_len:645 episode reward: total was -47.940000. running mean: -29.887104\n",
      "ep 716: ep_len:600 episode reward: total was -29.250000. running mean: -29.880733\n",
      "ep 716: ep_len:645 episode reward: total was -58.730000. running mean: -30.169226\n",
      "ep 716: ep_len:500 episode reward: total was -18.210000. running mean: -30.049634\n",
      "ep 716: ep_len:93 episode reward: total was -4.980000. running mean: -29.798937\n",
      "ep 716: ep_len:580 episode reward: total was -33.630000. running mean: -29.837248\n",
      "ep 716: ep_len:530 episode reward: total was -32.510000. running mean: -29.863975\n",
      "epsilon:0.302129 episode_count: 5019. steps_count: 2246017.000000\n",
      "ep 717: ep_len:660 episode reward: total was -50.440000. running mean: -30.069736\n",
      "ep 717: ep_len:610 episode reward: total was -41.940000. running mean: -30.188438\n",
      "ep 717: ep_len:545 episode reward: total was -32.700000. running mean: -30.213554\n",
      "ep 717: ep_len:545 episode reward: total was -33.050000. running mean: -30.241918\n",
      "ep 717: ep_len:3 episode reward: total was 0.000000. running mean: -29.939499\n",
      "ep 717: ep_len:610 episode reward: total was -26.700000. running mean: -29.907104\n",
      "ep 717: ep_len:590 episode reward: total was -41.650000. running mean: -30.024533\n",
      "epsilon:0.301993 episode_count: 5026. steps_count: 2249580.000000\n",
      "ep 718: ep_len:590 episode reward: total was -31.180000. running mean: -30.036088\n",
      "ep 718: ep_len:142 episode reward: total was -16.960000. running mean: -29.905327\n",
      "ep 718: ep_len:665 episode reward: total was -57.890000. running mean: -30.185174\n",
      "ep 718: ep_len:580 episode reward: total was -33.030000. running mean: -30.213622\n",
      "ep 718: ep_len:3 episode reward: total was 0.000000. running mean: -29.911486\n",
      "ep 718: ep_len:545 episode reward: total was -54.120000. running mean: -30.153571\n",
      "ep 718: ep_len:505 episode reward: total was -40.280000. running mean: -30.254835\n",
      "epsilon:0.301856 episode_count: 5033. steps_count: 2252610.000000\n",
      "ep 719: ep_len:228 episode reward: total was -1.910000. running mean: -29.971387\n",
      "ep 719: ep_len:595 episode reward: total was -18.740000. running mean: -29.859073\n",
      "ep 719: ep_len:665 episode reward: total was -33.910000. running mean: -29.899582\n",
      "ep 719: ep_len:505 episode reward: total was -38.210000. running mean: -29.982686\n",
      "ep 719: ep_len:62 episode reward: total was -5.470000. running mean: -29.737560\n",
      "ep 719: ep_len:565 episode reward: total was -31.710000. running mean: -29.757284\n",
      "ep 719: ep_len:625 episode reward: total was -46.680000. running mean: -29.926511\n",
      "epsilon:0.301720 episode_count: 5040. steps_count: 2255855.000000\n",
      "ep 720: ep_len:655 episode reward: total was -56.710000. running mean: -30.194346\n",
      "ep 720: ep_len:500 episode reward: total was -20.540000. running mean: -30.097803\n",
      "ep 720: ep_len:845 episode reward: total was -90.510000. running mean: -30.701925\n",
      "ep 720: ep_len:580 episode reward: total was -34.170000. running mean: -30.736605\n",
      "ep 720: ep_len:3 episode reward: total was 0.000000. running mean: -30.429239\n",
      "ep 720: ep_len:640 episode reward: total was -27.560000. running mean: -30.400547\n",
      "ep 720: ep_len:565 episode reward: total was -45.090000. running mean: -30.547441\n",
      "epsilon:0.301583 episode_count: 5047. steps_count: 2259643.000000\n",
      "ep 721: ep_len:243 episode reward: total was -6.920000. running mean: -30.311167\n",
      "ep 721: ep_len:505 episode reward: total was -21.410000. running mean: -30.222155\n",
      "ep 721: ep_len:850 episode reward: total was -97.920000. running mean: -30.899134\n",
      "ep 721: ep_len:500 episode reward: total was -24.080000. running mean: -30.830942\n",
      "ep 721: ep_len:99 episode reward: total was 2.040000. running mean: -30.502233\n",
      "ep 721: ep_len:515 episode reward: total was -30.780000. running mean: -30.505011\n",
      "ep 721: ep_len:565 episode reward: total was -31.680000. running mean: -30.516761\n",
      "epsilon:0.301447 episode_count: 5054. steps_count: 2262920.000000\n",
      "ep 722: ep_len:110 episode reward: total was -6.960000. running mean: -30.281193\n",
      "ep 722: ep_len:600 episode reward: total was -35.200000. running mean: -30.330381\n",
      "ep 722: ep_len:630 episode reward: total was -56.950000. running mean: -30.596577\n",
      "ep 722: ep_len:500 episode reward: total was -15.180000. running mean: -30.442411\n",
      "ep 722: ep_len:114 episode reward: total was -0.470000. running mean: -30.142687\n",
      "ep 722: ep_len:168 episode reward: total was -12.930000. running mean: -29.970560\n",
      "ep 722: ep_len:500 episode reward: total was -33.030000. running mean: -30.001155\n",
      "epsilon:0.301310 episode_count: 5061. steps_count: 2265542.000000\n",
      "ep 723: ep_len:500 episode reward: total was -28.260000. running mean: -29.983743\n",
      "ep 723: ep_len:500 episode reward: total was -36.800000. running mean: -30.051906\n",
      "ep 723: ep_len:585 episode reward: total was -36.700000. running mean: -30.118387\n",
      "ep 723: ep_len:505 episode reward: total was -36.220000. running mean: -30.179403\n",
      "ep 723: ep_len:3 episode reward: total was 0.000000. running mean: -29.877609\n",
      "ep 723: ep_len:535 episode reward: total was -15.550000. running mean: -29.734333\n",
      "ep 723: ep_len:585 episode reward: total was -46.670000. running mean: -29.903689\n",
      "epsilon:0.301174 episode_count: 5068. steps_count: 2268755.000000\n",
      "ep 724: ep_len:625 episode reward: total was -47.950000. running mean: -30.084153\n",
      "ep 724: ep_len:590 episode reward: total was -26.150000. running mean: -30.044811\n",
      "ep 724: ep_len:505 episode reward: total was -27.990000. running mean: -30.024263\n",
      "ep 724: ep_len:540 episode reward: total was -38.120000. running mean: -30.105220\n",
      "ep 724: ep_len:95 episode reward: total was -0.960000. running mean: -29.813768\n",
      "ep 724: ep_len:520 episode reward: total was -35.610000. running mean: -29.871730\n",
      "ep 724: ep_len:188 episode reward: total was -15.950000. running mean: -29.732513\n",
      "epsilon:0.301037 episode_count: 5075. steps_count: 2271818.000000\n",
      "ep 725: ep_len:106 episode reward: total was -5.470000. running mean: -29.489888\n",
      "ep 725: ep_len:505 episode reward: total was -26.820000. running mean: -29.463189\n",
      "ep 725: ep_len:79 episode reward: total was -0.970000. running mean: -29.178257\n",
      "ep 725: ep_len:575 episode reward: total was -23.160000. running mean: -29.118075\n",
      "ep 725: ep_len:106 episode reward: total was 0.020000. running mean: -28.826694\n",
      "ep 725: ep_len:575 episode reward: total was -23.360000. running mean: -28.772027\n",
      "ep 725: ep_len:535 episode reward: total was -40.200000. running mean: -28.886307\n",
      "epsilon:0.300901 episode_count: 5082. steps_count: 2274299.000000\n",
      "ep 726: ep_len:500 episode reward: total was -22.380000. running mean: -28.821244\n",
      "ep 726: ep_len:500 episode reward: total was -31.430000. running mean: -28.847331\n",
      "ep 726: ep_len:424 episode reward: total was -9.800000. running mean: -28.656858\n",
      "ep 726: ep_len:555 episode reward: total was -51.220000. running mean: -28.882489\n",
      "ep 726: ep_len:3 episode reward: total was 0.000000. running mean: -28.593664\n",
      "ep 726: ep_len:224 episode reward: total was -12.870000. running mean: -28.436428\n",
      "ep 726: ep_len:535 episode reward: total was -39.140000. running mean: -28.543463\n",
      "epsilon:0.300764 episode_count: 5089. steps_count: 2277040.000000\n",
      "ep 727: ep_len:500 episode reward: total was -19.310000. running mean: -28.451129\n",
      "ep 727: ep_len:635 episode reward: total was -36.070000. running mean: -28.527318\n",
      "ep 727: ep_len:670 episode reward: total was -35.430000. running mean: -28.596344\n",
      "ep 727: ep_len:630 episode reward: total was -76.210000. running mean: -29.072481\n",
      "ep 727: ep_len:3 episode reward: total was 0.000000. running mean: -28.781756\n",
      "ep 727: ep_len:540 episode reward: total was -21.510000. running mean: -28.709039\n",
      "ep 727: ep_len:1080 episode reward: total was -179.840000. running mean: -30.220348\n",
      "epsilon:0.300628 episode_count: 5096. steps_count: 2281098.000000\n",
      "ep 728: ep_len:209 episode reward: total was -3.940000. running mean: -29.957545\n",
      "ep 728: ep_len:660 episode reward: total was -48.130000. running mean: -30.139269\n",
      "ep 728: ep_len:565 episode reward: total was -56.320000. running mean: -30.401077\n",
      "ep 728: ep_len:56 episode reward: total was -1.440000. running mean: -30.111466\n",
      "ep 728: ep_len:3 episode reward: total was 0.000000. running mean: -29.810351\n",
      "ep 728: ep_len:605 episode reward: total was -47.510000. running mean: -29.987348\n",
      "ep 728: ep_len:530 episode reward: total was -45.240000. running mean: -30.139874\n",
      "epsilon:0.300491 episode_count: 5103. steps_count: 2283726.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 729: ep_len:222 episode reward: total was -22.350000. running mean: -30.061975\n",
      "ep 729: ep_len:199 episode reward: total was -9.900000. running mean: -29.860356\n",
      "ep 729: ep_len:500 episode reward: total was -25.260000. running mean: -29.814352\n",
      "ep 729: ep_len:500 episode reward: total was -34.300000. running mean: -29.859209\n",
      "ep 729: ep_len:109 episode reward: total was 2.030000. running mean: -29.540317\n",
      "ep 729: ep_len:635 episode reward: total was -20.020000. running mean: -29.445113\n",
      "ep 729: ep_len:525 episode reward: total was -18.650000. running mean: -29.337162\n",
      "epsilon:0.300355 episode_count: 5110. steps_count: 2286416.000000\n",
      "ep 730: ep_len:595 episode reward: total was -38.280000. running mean: -29.426591\n",
      "ep 730: ep_len:585 episode reward: total was -22.070000. running mean: -29.353025\n",
      "ep 730: ep_len:570 episode reward: total was -19.840000. running mean: -29.257894\n",
      "ep 730: ep_len:520 episode reward: total was -25.250000. running mean: -29.217815\n",
      "ep 730: ep_len:88 episode reward: total was -9.980000. running mean: -29.025437\n",
      "ep 730: ep_len:605 episode reward: total was -37.230000. running mean: -29.107483\n",
      "ep 730: ep_len:169 episode reward: total was -16.430000. running mean: -28.980708\n",
      "epsilon:0.300218 episode_count: 5117. steps_count: 2289548.000000\n",
      "ep 731: ep_len:500 episode reward: total was -23.210000. running mean: -28.923001\n",
      "ep 731: ep_len:505 episode reward: total was -23.790000. running mean: -28.871671\n",
      "ep 731: ep_len:550 episode reward: total was -41.230000. running mean: -28.995254\n",
      "ep 731: ep_len:760 episode reward: total was -69.980000. running mean: -29.405102\n",
      "ep 731: ep_len:122 episode reward: total was -7.960000. running mean: -29.190651\n",
      "ep 731: ep_len:550 episode reward: total was -42.550000. running mean: -29.324244\n",
      "ep 731: ep_len:520 episode reward: total was -25.680000. running mean: -29.287802\n",
      "epsilon:0.300082 episode_count: 5124. steps_count: 2293055.000000\n",
      "ep 732: ep_len:640 episode reward: total was -42.590000. running mean: -29.420824\n",
      "ep 732: ep_len:515 episode reward: total was -44.110000. running mean: -29.567716\n",
      "ep 732: ep_len:394 episode reward: total was -30.340000. running mean: -29.575438\n",
      "ep 732: ep_len:605 episode reward: total was -28.050000. running mean: -29.560184\n",
      "ep 732: ep_len:3 episode reward: total was 0.000000. running mean: -29.264582\n",
      "ep 732: ep_len:640 episode reward: total was -34.350000. running mean: -29.315436\n",
      "ep 732: ep_len:590 episode reward: total was -28.100000. running mean: -29.303282\n",
      "epsilon:0.299945 episode_count: 5131. steps_count: 2296442.000000\n",
      "ep 733: ep_len:265 episode reward: total was -15.890000. running mean: -29.169149\n",
      "ep 733: ep_len:237 episode reward: total was -26.890000. running mean: -29.146358\n",
      "ep 733: ep_len:565 episode reward: total was -38.750000. running mean: -29.242394\n",
      "ep 733: ep_len:108 episode reward: total was -4.950000. running mean: -28.999470\n",
      "ep 733: ep_len:3 episode reward: total was 0.000000. running mean: -28.709475\n",
      "ep 733: ep_len:610 episode reward: total was -40.750000. running mean: -28.829881\n",
      "ep 733: ep_len:565 episode reward: total was -46.680000. running mean: -29.008382\n",
      "epsilon:0.299809 episode_count: 5138. steps_count: 2298795.000000\n",
      "ep 734: ep_len:505 episode reward: total was -36.280000. running mean: -29.081098\n",
      "ep 734: ep_len:585 episode reward: total was -63.650000. running mean: -29.426787\n",
      "ep 734: ep_len:431 episode reward: total was -20.300000. running mean: -29.335519\n",
      "ep 734: ep_len:520 episode reward: total was -23.170000. running mean: -29.273864\n",
      "ep 734: ep_len:3 episode reward: total was 0.000000. running mean: -28.981125\n",
      "ep 734: ep_len:535 episode reward: total was -37.130000. running mean: -29.062614\n",
      "ep 734: ep_len:185 episode reward: total was -15.400000. running mean: -28.925988\n",
      "epsilon:0.299672 episode_count: 5145. steps_count: 2301559.000000\n",
      "ep 735: ep_len:500 episode reward: total was -26.590000. running mean: -28.902628\n",
      "ep 735: ep_len:710 episode reward: total was -53.010000. running mean: -29.143702\n",
      "ep 735: ep_len:610 episode reward: total was -36.330000. running mean: -29.215565\n",
      "ep 735: ep_len:500 episode reward: total was -39.660000. running mean: -29.320009\n",
      "ep 735: ep_len:41 episode reward: total was -2.000000. running mean: -29.046809\n",
      "ep 735: ep_len:500 episode reward: total was -18.310000. running mean: -28.939441\n",
      "ep 735: ep_len:580 episode reward: total was -21.110000. running mean: -28.861147\n",
      "epsilon:0.299536 episode_count: 5152. steps_count: 2305000.000000\n",
      "ep 736: ep_len:229 episode reward: total was -26.870000. running mean: -28.841235\n",
      "ep 736: ep_len:555 episode reward: total was -40.650000. running mean: -28.959323\n",
      "ep 736: ep_len:391 episode reward: total was -10.340000. running mean: -28.773130\n",
      "ep 736: ep_len:500 episode reward: total was -41.680000. running mean: -28.902198\n",
      "ep 736: ep_len:89 episode reward: total was -10.970000. running mean: -28.722876\n",
      "ep 736: ep_len:150 episode reward: total was -14.920000. running mean: -28.584847\n",
      "ep 736: ep_len:530 episode reward: total was -32.170000. running mean: -28.620699\n",
      "epsilon:0.299399 episode_count: 5159. steps_count: 2307444.000000\n",
      "ep 737: ep_len:127 episode reward: total was -8.940000. running mean: -28.423892\n",
      "ep 737: ep_len:640 episode reward: total was -41.740000. running mean: -28.557053\n",
      "ep 737: ep_len:605 episode reward: total was -31.660000. running mean: -28.588083\n",
      "ep 737: ep_len:500 episode reward: total was -25.620000. running mean: -28.558402\n",
      "ep 737: ep_len:36 episode reward: total was 2.000000. running mean: -28.252818\n",
      "ep 737: ep_len:505 episode reward: total was -21.150000. running mean: -28.181790\n",
      "ep 737: ep_len:326 episode reward: total was -30.360000. running mean: -28.203572\n",
      "epsilon:0.299263 episode_count: 5166. steps_count: 2310183.000000\n",
      "ep 738: ep_len:181 episode reward: total was -7.910000. running mean: -28.000636\n",
      "ep 738: ep_len:875 episode reward: total was -100.770000. running mean: -28.728330\n",
      "ep 738: ep_len:555 episode reward: total was -48.000000. running mean: -28.921046\n",
      "ep 738: ep_len:56 episode reward: total was -0.950000. running mean: -28.641336\n",
      "ep 738: ep_len:3 episode reward: total was 0.000000. running mean: -28.354922\n",
      "ep 738: ep_len:510 episode reward: total was -24.240000. running mean: -28.313773\n",
      "ep 738: ep_len:500 episode reward: total was -41.280000. running mean: -28.443436\n",
      "epsilon:0.299126 episode_count: 5173. steps_count: 2312863.000000\n",
      "ep 739: ep_len:210 episode reward: total was -6.900000. running mean: -28.228001\n",
      "ep 739: ep_len:630 episode reward: total was -18.700000. running mean: -28.132721\n",
      "ep 739: ep_len:645 episode reward: total was -36.070000. running mean: -28.212094\n",
      "ep 739: ep_len:615 episode reward: total was -43.740000. running mean: -28.367373\n",
      "ep 739: ep_len:3 episode reward: total was 0.000000. running mean: -28.083699\n",
      "ep 739: ep_len:304 episode reward: total was -19.880000. running mean: -28.001662\n",
      "ep 739: ep_len:500 episode reward: total was -37.160000. running mean: -28.093246\n",
      "epsilon:0.298990 episode_count: 5180. steps_count: 2315770.000000\n",
      "ep 740: ep_len:575 episode reward: total was -17.500000. running mean: -27.987313\n",
      "ep 740: ep_len:510 episode reward: total was -9.240000. running mean: -27.799840\n",
      "ep 740: ep_len:505 episode reward: total was -42.110000. running mean: -27.942942\n",
      "ep 740: ep_len:615 episode reward: total was -36.670000. running mean: -28.030212\n",
      "ep 740: ep_len:3 episode reward: total was 0.000000. running mean: -27.749910\n",
      "ep 740: ep_len:162 episode reward: total was -2.450000. running mean: -27.496911\n",
      "ep 740: ep_len:515 episode reward: total was -32.970000. running mean: -27.551642\n",
      "epsilon:0.298853 episode_count: 5187. steps_count: 2318655.000000\n",
      "ep 741: ep_len:690 episode reward: total was -71.460000. running mean: -27.990725\n",
      "ep 741: ep_len:515 episode reward: total was -31.870000. running mean: -28.029518\n",
      "ep 741: ep_len:79 episode reward: total was -1.460000. running mean: -27.763823\n",
      "ep 741: ep_len:49 episode reward: total was -2.450000. running mean: -27.510685\n",
      "ep 741: ep_len:3 episode reward: total was 0.000000. running mean: -27.235578\n",
      "ep 741: ep_len:520 episode reward: total was -20.140000. running mean: -27.164622\n",
      "ep 741: ep_len:545 episode reward: total was -33.950000. running mean: -27.232476\n",
      "epsilon:0.298717 episode_count: 5194. steps_count: 2321056.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 742: ep_len:565 episode reward: total was -24.200000. running mean: -27.202151\n",
      "ep 742: ep_len:177 episode reward: total was -8.430000. running mean: -27.014430\n",
      "ep 742: ep_len:500 episode reward: total was -38.830000. running mean: -27.132585\n",
      "ep 742: ep_len:545 episode reward: total was -25.150000. running mean: -27.112760\n",
      "ep 742: ep_len:104 episode reward: total was 1.040000. running mean: -26.831232\n",
      "ep 742: ep_len:500 episode reward: total was -25.600000. running mean: -26.818920\n",
      "ep 742: ep_len:595 episode reward: total was -31.100000. running mean: -26.861730\n",
      "epsilon:0.298580 episode_count: 5201. steps_count: 2324042.000000\n",
      "ep 743: ep_len:630 episode reward: total was -27.600000. running mean: -26.869113\n",
      "ep 743: ep_len:550 episode reward: total was -23.310000. running mean: -26.833522\n",
      "ep 743: ep_len:605 episode reward: total was -37.250000. running mean: -26.937687\n",
      "ep 743: ep_len:610 episode reward: total was -25.690000. running mean: -26.925210\n",
      "ep 743: ep_len:3 episode reward: total was 0.000000. running mean: -26.655958\n",
      "ep 743: ep_len:570 episode reward: total was -29.400000. running mean: -26.683398\n",
      "ep 743: ep_len:555 episode reward: total was -42.180000. running mean: -26.838364\n",
      "epsilon:0.298444 episode_count: 5208. steps_count: 2327565.000000\n",
      "ep 744: ep_len:755 episode reward: total was -77.500000. running mean: -27.344981\n",
      "ep 744: ep_len:560 episode reward: total was -34.110000. running mean: -27.412631\n",
      "ep 744: ep_len:555 episode reward: total was -49.570000. running mean: -27.634205\n",
      "ep 744: ep_len:500 episode reward: total was -28.690000. running mean: -27.644762\n",
      "ep 744: ep_len:3 episode reward: total was 0.000000. running mean: -27.368315\n",
      "ep 744: ep_len:535 episode reward: total was -21.130000. running mean: -27.305932\n",
      "ep 744: ep_len:525 episode reward: total was -41.490000. running mean: -27.447772\n",
      "epsilon:0.298307 episode_count: 5215. steps_count: 2330998.000000\n",
      "ep 745: ep_len:620 episode reward: total was -42.910000. running mean: -27.602395\n",
      "ep 745: ep_len:500 episode reward: total was -19.420000. running mean: -27.520571\n",
      "ep 745: ep_len:79 episode reward: total was 1.050000. running mean: -27.234865\n",
      "ep 745: ep_len:56 episode reward: total was -5.480000. running mean: -27.017316\n",
      "ep 745: ep_len:74 episode reward: total was 0.520000. running mean: -26.741943\n",
      "ep 745: ep_len:550 episode reward: total was -35.260000. running mean: -26.827124\n",
      "ep 745: ep_len:336 episode reward: total was -22.840000. running mean: -26.787253\n",
      "epsilon:0.298171 episode_count: 5222. steps_count: 2333213.000000\n",
      "ep 746: ep_len:500 episode reward: total was -18.660000. running mean: -26.705980\n",
      "ep 746: ep_len:500 episode reward: total was -29.150000. running mean: -26.730420\n",
      "ep 746: ep_len:595 episode reward: total was -73.680000. running mean: -27.199916\n",
      "ep 746: ep_len:540 episode reward: total was -43.280000. running mean: -27.360717\n",
      "ep 746: ep_len:3 episode reward: total was 0.000000. running mean: -27.087110\n",
      "ep 746: ep_len:515 episode reward: total was -37.630000. running mean: -27.192539\n",
      "ep 746: ep_len:202 episode reward: total was -12.870000. running mean: -27.049313\n",
      "epsilon:0.298034 episode_count: 5229. steps_count: 2336068.000000\n",
      "ep 747: ep_len:615 episode reward: total was -23.090000. running mean: -27.009720\n",
      "ep 747: ep_len:630 episode reward: total was -57.210000. running mean: -27.311723\n",
      "ep 747: ep_len:765 episode reward: total was -66.430000. running mean: -27.702906\n",
      "ep 747: ep_len:540 episode reward: total was -29.650000. running mean: -27.722377\n",
      "ep 747: ep_len:3 episode reward: total was 0.000000. running mean: -27.445153\n",
      "ep 747: ep_len:635 episode reward: total was -51.680000. running mean: -27.687501\n",
      "ep 747: ep_len:585 episode reward: total was -20.910000. running mean: -27.619726\n",
      "epsilon:0.297898 episode_count: 5236. steps_count: 2339841.000000\n",
      "ep 748: ep_len:232 episode reward: total was -7.910000. running mean: -27.422629\n",
      "ep 748: ep_len:530 episode reward: total was -38.580000. running mean: -27.534203\n",
      "ep 748: ep_len:590 episode reward: total was -21.660000. running mean: -27.475461\n",
      "ep 748: ep_len:505 episode reward: total was -30.130000. running mean: -27.502006\n",
      "ep 748: ep_len:3 episode reward: total was 0.000000. running mean: -27.226986\n",
      "ep 748: ep_len:520 episode reward: total was -31.750000. running mean: -27.272216\n",
      "ep 748: ep_len:196 episode reward: total was -14.400000. running mean: -27.143494\n",
      "epsilon:0.297761 episode_count: 5243. steps_count: 2342417.000000\n",
      "ep 749: ep_len:265 episode reward: total was -16.930000. running mean: -27.041359\n",
      "ep 749: ep_len:575 episode reward: total was -31.770000. running mean: -27.088645\n",
      "ep 749: ep_len:590 episode reward: total was -29.100000. running mean: -27.108759\n",
      "ep 749: ep_len:545 episode reward: total was -24.200000. running mean: -27.079671\n",
      "ep 749: ep_len:3 episode reward: total was 0.000000. running mean: -26.808875\n",
      "ep 749: ep_len:234 episode reward: total was -8.940000. running mean: -26.630186\n",
      "ep 749: ep_len:525 episode reward: total was -25.450000. running mean: -26.618384\n",
      "epsilon:0.297625 episode_count: 5250. steps_count: 2345154.000000\n",
      "ep 750: ep_len:610 episode reward: total was -22.500000. running mean: -26.577200\n",
      "ep 750: ep_len:670 episode reward: total was -43.900000. running mean: -26.750428\n",
      "ep 750: ep_len:535 episode reward: total was -41.500000. running mean: -26.897924\n",
      "ep 750: ep_len:590 episode reward: total was -40.150000. running mean: -27.030445\n",
      "ep 750: ep_len:3 episode reward: total was 0.000000. running mean: -26.760140\n",
      "ep 750: ep_len:515 episode reward: total was -39.280000. running mean: -26.885339\n",
      "ep 750: ep_len:595 episode reward: total was -30.170000. running mean: -26.918185\n",
      "epsilon:0.297488 episode_count: 5257. steps_count: 2348672.000000\n",
      "ep 751: ep_len:780 episode reward: total was -88.570000. running mean: -27.534704\n",
      "ep 751: ep_len:500 episode reward: total was -23.270000. running mean: -27.492057\n",
      "ep 751: ep_len:675 episode reward: total was -21.710000. running mean: -27.434236\n",
      "ep 751: ep_len:575 episode reward: total was -13.160000. running mean: -27.291494\n",
      "ep 751: ep_len:3 episode reward: total was 0.000000. running mean: -27.018579\n",
      "ep 751: ep_len:575 episode reward: total was -67.620000. running mean: -27.424593\n",
      "ep 751: ep_len:535 episode reward: total was -27.560000. running mean: -27.425947\n",
      "epsilon:0.297352 episode_count: 5264. steps_count: 2352315.000000\n",
      "ep 752: ep_len:525 episode reward: total was -38.860000. running mean: -27.540288\n",
      "ep 752: ep_len:645 episode reward: total was -55.590000. running mean: -27.820785\n",
      "ep 752: ep_len:340 episode reward: total was -25.440000. running mean: -27.796977\n",
      "ep 752: ep_len:500 episode reward: total was -16.170000. running mean: -27.680707\n",
      "ep 752: ep_len:87 episode reward: total was -15.990000. running mean: -27.563800\n",
      "ep 752: ep_len:186 episode reward: total was -7.410000. running mean: -27.362262\n",
      "ep 752: ep_len:192 episode reward: total was -7.370000. running mean: -27.162339\n",
      "epsilon:0.297215 episode_count: 5271. steps_count: 2354790.000000\n",
      "ep 753: ep_len:216 episode reward: total was -3.400000. running mean: -26.924716\n",
      "ep 753: ep_len:505 episode reward: total was -34.620000. running mean: -27.001669\n",
      "ep 753: ep_len:500 episode reward: total was -43.620000. running mean: -27.167852\n",
      "ep 753: ep_len:48 episode reward: total was -0.950000. running mean: -26.905674\n",
      "ep 753: ep_len:3 episode reward: total was 0.000000. running mean: -26.636617\n",
      "ep 753: ep_len:610 episode reward: total was -18.630000. running mean: -26.556551\n",
      "ep 753: ep_len:296 episode reward: total was -20.330000. running mean: -26.494285\n",
      "epsilon:0.297079 episode_count: 5278. steps_count: 2356968.000000\n",
      "ep 754: ep_len:227 episode reward: total was -12.930000. running mean: -26.358642\n",
      "ep 754: ep_len:585 episode reward: total was -52.190000. running mean: -26.616956\n",
      "ep 754: ep_len:570 episode reward: total was -41.170000. running mean: -26.762486\n",
      "ep 754: ep_len:500 episode reward: total was -16.230000. running mean: -26.657161\n",
      "ep 754: ep_len:3 episode reward: total was 0.000000. running mean: -26.390590\n",
      "ep 754: ep_len:287 episode reward: total was -9.360000. running mean: -26.220284\n",
      "ep 754: ep_len:530 episode reward: total was -22.510000. running mean: -26.183181\n",
      "epsilon:0.296942 episode_count: 5285. steps_count: 2359670.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 755: ep_len:525 episode reward: total was -44.980000. running mean: -26.371149\n",
      "ep 755: ep_len:540 episode reward: total was -29.120000. running mean: -26.398638\n",
      "ep 755: ep_len:585 episode reward: total was -46.520000. running mean: -26.599851\n",
      "ep 755: ep_len:138 episode reward: total was -2.410000. running mean: -26.357953\n",
      "ep 755: ep_len:3 episode reward: total was 0.000000. running mean: -26.094373\n",
      "ep 755: ep_len:500 episode reward: total was -34.240000. running mean: -26.175830\n",
      "ep 755: ep_len:505 episode reward: total was -35.620000. running mean: -26.270271\n",
      "epsilon:0.296806 episode_count: 5292. steps_count: 2362466.000000\n",
      "ep 756: ep_len:515 episode reward: total was -33.270000. running mean: -26.340269\n",
      "ep 756: ep_len:610 episode reward: total was -18.930000. running mean: -26.266166\n",
      "ep 756: ep_len:364 episode reward: total was -16.890000. running mean: -26.172404\n",
      "ep 756: ep_len:530 episode reward: total was -16.580000. running mean: -26.076480\n",
      "ep 756: ep_len:3 episode reward: total was 0.000000. running mean: -25.815715\n",
      "ep 756: ep_len:635 episode reward: total was -46.170000. running mean: -26.019258\n",
      "ep 756: ep_len:535 episode reward: total was -38.650000. running mean: -26.145566\n",
      "epsilon:0.296669 episode_count: 5299. steps_count: 2365658.000000\n",
      "ep 757: ep_len:500 episode reward: total was -25.350000. running mean: -26.137610\n",
      "ep 757: ep_len:500 episode reward: total was -35.070000. running mean: -26.226934\n",
      "ep 757: ep_len:382 episode reward: total was -26.330000. running mean: -26.227965\n",
      "ep 757: ep_len:515 episode reward: total was -24.080000. running mean: -26.206485\n",
      "ep 757: ep_len:3 episode reward: total was 0.000000. running mean: -25.944420\n",
      "ep 757: ep_len:630 episode reward: total was -25.980000. running mean: -25.944776\n",
      "ep 757: ep_len:550 episode reward: total was -49.140000. running mean: -26.176728\n",
      "epsilon:0.296533 episode_count: 5306. steps_count: 2368738.000000\n",
      "ep 758: ep_len:630 episode reward: total was -51.590000. running mean: -26.430861\n",
      "ep 758: ep_len:379 episode reward: total was -34.340000. running mean: -26.509952\n",
      "ep 758: ep_len:540 episode reward: total was -41.170000. running mean: -26.656553\n",
      "ep 758: ep_len:510 episode reward: total was -35.690000. running mean: -26.746887\n",
      "ep 758: ep_len:98 episode reward: total was -0.470000. running mean: -26.484118\n",
      "ep 758: ep_len:500 episode reward: total was -32.660000. running mean: -26.545877\n",
      "ep 758: ep_len:540 episode reward: total was -47.580000. running mean: -26.756218\n",
      "epsilon:0.296396 episode_count: 5313. steps_count: 2371935.000000\n",
      "ep 759: ep_len:650 episode reward: total was -27.360000. running mean: -26.762256\n",
      "ep 759: ep_len:500 episode reward: total was -30.340000. running mean: -26.798034\n",
      "ep 759: ep_len:500 episode reward: total was -26.710000. running mean: -26.797153\n",
      "ep 759: ep_len:500 episode reward: total was -36.690000. running mean: -26.896082\n",
      "ep 759: ep_len:3 episode reward: total was 0.000000. running mean: -26.627121\n",
      "ep 759: ep_len:233 episode reward: total was -24.920000. running mean: -26.610050\n",
      "ep 759: ep_len:500 episode reward: total was -25.870000. running mean: -26.602649\n",
      "epsilon:0.296260 episode_count: 5320. steps_count: 2374821.000000\n",
      "ep 760: ep_len:530 episode reward: total was -34.520000. running mean: -26.681823\n",
      "ep 760: ep_len:500 episode reward: total was -13.920000. running mean: -26.554205\n",
      "ep 760: ep_len:635 episode reward: total was -79.100000. running mean: -27.079663\n",
      "ep 760: ep_len:515 episode reward: total was -49.380000. running mean: -27.302666\n",
      "ep 760: ep_len:3 episode reward: total was 0.000000. running mean: -27.029639\n",
      "ep 760: ep_len:605 episode reward: total was -33.830000. running mean: -27.097643\n",
      "ep 760: ep_len:505 episode reward: total was -28.180000. running mean: -27.108466\n",
      "epsilon:0.296123 episode_count: 5327. steps_count: 2378114.000000\n",
      "ep 761: ep_len:189 episode reward: total was -4.410000. running mean: -26.881482\n",
      "ep 761: ep_len:505 episode reward: total was -48.650000. running mean: -27.099167\n",
      "ep 761: ep_len:454 episode reward: total was -31.290000. running mean: -27.141075\n",
      "ep 761: ep_len:500 episode reward: total was -21.540000. running mean: -27.085064\n",
      "ep 761: ep_len:130 episode reward: total was -13.930000. running mean: -26.953514\n",
      "ep 761: ep_len:680 episode reward: total was -73.540000. running mean: -27.419379\n",
      "ep 761: ep_len:184 episode reward: total was -7.390000. running mean: -27.219085\n",
      "epsilon:0.295987 episode_count: 5334. steps_count: 2380756.000000\n",
      "ep 762: ep_len:600 episode reward: total was -30.240000. running mean: -27.249294\n",
      "ep 762: ep_len:500 episode reward: total was -24.680000. running mean: -27.223601\n",
      "ep 762: ep_len:625 episode reward: total was -23.080000. running mean: -27.182165\n",
      "ep 762: ep_len:500 episode reward: total was -22.250000. running mean: -27.132843\n",
      "ep 762: ep_len:113 episode reward: total was -7.470000. running mean: -26.936215\n",
      "ep 762: ep_len:585 episode reward: total was -46.480000. running mean: -27.131653\n",
      "ep 762: ep_len:279 episode reward: total was -8.830000. running mean: -26.948636\n",
      "epsilon:0.295850 episode_count: 5341. steps_count: 2383958.000000\n",
      "ep 763: ep_len:99 episode reward: total was 0.540000. running mean: -26.673750\n",
      "ep 763: ep_len:312 episode reward: total was -20.930000. running mean: -26.616312\n",
      "ep 763: ep_len:705 episode reward: total was -60.980000. running mean: -26.959949\n",
      "ep 763: ep_len:500 episode reward: total was -28.670000. running mean: -26.977050\n",
      "ep 763: ep_len:111 episode reward: total was -6.450000. running mean: -26.771779\n",
      "ep 763: ep_len:575 episode reward: total was -28.940000. running mean: -26.793462\n",
      "ep 763: ep_len:354 episode reward: total was -24.320000. running mean: -26.768727\n",
      "epsilon:0.295714 episode_count: 5348. steps_count: 2386614.000000\n",
      "ep 764: ep_len:238 episode reward: total was -12.440000. running mean: -26.625440\n",
      "ep 764: ep_len:535 episode reward: total was -27.750000. running mean: -26.636685\n",
      "ep 764: ep_len:810 episode reward: total was -69.820000. running mean: -27.068518\n",
      "ep 764: ep_len:570 episode reward: total was -19.680000. running mean: -26.994633\n",
      "ep 764: ep_len:89 episode reward: total was -8.450000. running mean: -26.809187\n",
      "ep 764: ep_len:560 episode reward: total was -37.790000. running mean: -26.918995\n",
      "ep 764: ep_len:358 episode reward: total was -26.850000. running mean: -26.918305\n",
      "epsilon:0.295577 episode_count: 5355. steps_count: 2389774.000000\n",
      "ep 765: ep_len:540 episode reward: total was -44.950000. running mean: -27.098622\n",
      "ep 765: ep_len:575 episode reward: total was -23.240000. running mean: -27.060036\n",
      "ep 765: ep_len:555 episode reward: total was -28.420000. running mean: -27.073635\n",
      "ep 765: ep_len:525 episode reward: total was -29.600000. running mean: -27.098899\n",
      "ep 765: ep_len:111 episode reward: total was -7.480000. running mean: -26.902710\n",
      "ep 765: ep_len:505 episode reward: total was -42.160000. running mean: -27.055283\n",
      "ep 765: ep_len:550 episode reward: total was -23.660000. running mean: -27.021330\n",
      "epsilon:0.295441 episode_count: 5362. steps_count: 2393135.000000\n",
      "ep 766: ep_len:545 episode reward: total was -29.650000. running mean: -27.047617\n",
      "ep 766: ep_len:505 episode reward: total was -15.230000. running mean: -26.929441\n",
      "ep 766: ep_len:655 episode reward: total was -41.070000. running mean: -27.070846\n",
      "ep 766: ep_len:505 episode reward: total was -24.230000. running mean: -27.042438\n",
      "ep 766: ep_len:3 episode reward: total was 0.000000. running mean: -26.772013\n",
      "ep 766: ep_len:515 episode reward: total was -35.660000. running mean: -26.860893\n",
      "ep 766: ep_len:590 episode reward: total was -39.530000. running mean: -26.987584\n",
      "epsilon:0.295304 episode_count: 5369. steps_count: 2396453.000000\n",
      "ep 767: ep_len:640 episode reward: total was -24.320000. running mean: -26.960909\n",
      "ep 767: ep_len:500 episode reward: total was -35.140000. running mean: -27.042699\n",
      "ep 767: ep_len:660 episode reward: total was -57.220000. running mean: -27.344472\n",
      "ep 767: ep_len:550 episode reward: total was -54.710000. running mean: -27.618128\n",
      "ep 767: ep_len:116 episode reward: total was -5.960000. running mean: -27.401546\n",
      "ep 767: ep_len:585 episode reward: total was -55.570000. running mean: -27.683231\n",
      "ep 767: ep_len:585 episode reward: total was -46.100000. running mean: -27.867399\n",
      "epsilon:0.295168 episode_count: 5376. steps_count: 2400089.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 768: ep_len:615 episode reward: total was -40.640000. running mean: -27.995125\n",
      "ep 768: ep_len:281 episode reward: total was -23.940000. running mean: -27.954573\n",
      "ep 768: ep_len:585 episode reward: total was -41.130000. running mean: -28.086328\n",
      "ep 768: ep_len:600 episode reward: total was -24.670000. running mean: -28.052164\n",
      "ep 768: ep_len:96 episode reward: total was -3.960000. running mean: -27.811243\n",
      "ep 768: ep_len:585 episode reward: total was -35.690000. running mean: -27.890030\n",
      "ep 768: ep_len:610 episode reward: total was -32.060000. running mean: -27.931730\n",
      "epsilon:0.295031 episode_count: 5383. steps_count: 2403461.000000\n",
      "ep 769: ep_len:259 episode reward: total was -12.880000. running mean: -27.781213\n",
      "ep 769: ep_len:525 episode reward: total was -14.760000. running mean: -27.651001\n",
      "ep 769: ep_len:595 episode reward: total was -28.230000. running mean: -27.656791\n",
      "ep 769: ep_len:565 episode reward: total was -28.580000. running mean: -27.666023\n",
      "ep 769: ep_len:3 episode reward: total was 0.000000. running mean: -27.389363\n",
      "ep 769: ep_len:515 episode reward: total was -24.630000. running mean: -27.361769\n",
      "ep 769: ep_len:500 episode reward: total was -36.550000. running mean: -27.453651\n",
      "epsilon:0.294895 episode_count: 5390. steps_count: 2406423.000000\n",
      "ep 770: ep_len:565 episode reward: total was -33.750000. running mean: -27.516615\n",
      "ep 770: ep_len:260 episode reward: total was -21.910000. running mean: -27.460549\n",
      "ep 770: ep_len:75 episode reward: total was -6.480000. running mean: -27.250743\n",
      "ep 770: ep_len:500 episode reward: total was -34.130000. running mean: -27.319536\n",
      "ep 770: ep_len:85 episode reward: total was -11.950000. running mean: -27.165840\n",
      "ep 770: ep_len:530 episode reward: total was -18.570000. running mean: -27.079882\n",
      "ep 770: ep_len:500 episode reward: total was -26.970000. running mean: -27.078783\n",
      "epsilon:0.294758 episode_count: 5397. steps_count: 2408938.000000\n",
      "ep 771: ep_len:505 episode reward: total was -43.560000. running mean: -27.243595\n",
      "ep 771: ep_len:605 episode reward: total was -19.170000. running mean: -27.162859\n",
      "ep 771: ep_len:79 episode reward: total was -3.970000. running mean: -26.930931\n",
      "ep 771: ep_len:385 episode reward: total was -13.730000. running mean: -26.798921\n",
      "ep 771: ep_len:3 episode reward: total was 0.000000. running mean: -26.530932\n",
      "ep 771: ep_len:660 episode reward: total was -65.760000. running mean: -26.923223\n",
      "ep 771: ep_len:530 episode reward: total was -56.700000. running mean: -27.220991\n",
      "epsilon:0.294622 episode_count: 5404. steps_count: 2411705.000000\n",
      "ep 772: ep_len:675 episode reward: total was -93.230000. running mean: -27.881081\n",
      "ep 772: ep_len:585 episode reward: total was -71.760000. running mean: -28.319870\n",
      "ep 772: ep_len:79 episode reward: total was -5.470000. running mean: -28.091371\n",
      "ep 772: ep_len:515 episode reward: total was -19.690000. running mean: -28.007357\n",
      "ep 772: ep_len:3 episode reward: total was 0.000000. running mean: -27.727284\n",
      "ep 772: ep_len:630 episode reward: total was -20.530000. running mean: -27.655311\n",
      "ep 772: ep_len:500 episode reward: total was -31.750000. running mean: -27.696258\n",
      "epsilon:0.294485 episode_count: 5411. steps_count: 2414692.000000\n",
      "ep 773: ep_len:815 episode reward: total was -89.540000. running mean: -28.314695\n",
      "ep 773: ep_len:620 episode reward: total was -38.010000. running mean: -28.411648\n",
      "ep 773: ep_len:655 episode reward: total was -31.470000. running mean: -28.442232\n",
      "ep 773: ep_len:540 episode reward: total was -41.260000. running mean: -28.570410\n",
      "ep 773: ep_len:3 episode reward: total was 0.000000. running mean: -28.284706\n",
      "ep 773: ep_len:635 episode reward: total was -29.890000. running mean: -28.300758\n",
      "ep 773: ep_len:298 episode reward: total was -21.350000. running mean: -28.231251\n",
      "epsilon:0.294349 episode_count: 5418. steps_count: 2418258.000000\n",
      "ep 774: ep_len:575 episode reward: total was -17.700000. running mean: -28.125938\n",
      "ep 774: ep_len:308 episode reward: total was -16.850000. running mean: -28.013179\n",
      "ep 774: ep_len:510 episode reward: total was -45.030000. running mean: -28.183347\n",
      "ep 774: ep_len:525 episode reward: total was -43.260000. running mean: -28.334114\n",
      "ep 774: ep_len:3 episode reward: total was 0.000000. running mean: -28.050773\n",
      "ep 774: ep_len:720 episode reward: total was -46.830000. running mean: -28.238565\n",
      "ep 774: ep_len:339 episode reward: total was -17.860000. running mean: -28.134779\n",
      "epsilon:0.294212 episode_count: 5425. steps_count: 2421238.000000\n",
      "ep 775: ep_len:600 episode reward: total was -27.480000. running mean: -28.128231\n",
      "ep 775: ep_len:675 episode reward: total was -34.100000. running mean: -28.187949\n",
      "ep 775: ep_len:570 episode reward: total was -42.440000. running mean: -28.330470\n",
      "ep 775: ep_len:500 episode reward: total was -39.630000. running mean: -28.443465\n",
      "ep 775: ep_len:3 episode reward: total was 0.000000. running mean: -28.159030\n",
      "ep 775: ep_len:620 episode reward: total was -74.250000. running mean: -28.619940\n",
      "ep 775: ep_len:610 episode reward: total was -50.580000. running mean: -28.839541\n",
      "epsilon:0.294076 episode_count: 5432. steps_count: 2424816.000000\n",
      "ep 776: ep_len:760 episode reward: total was -65.400000. running mean: -29.205145\n",
      "ep 776: ep_len:500 episode reward: total was -43.900000. running mean: -29.352094\n",
      "ep 776: ep_len:710 episode reward: total was -70.530000. running mean: -29.763873\n",
      "ep 776: ep_len:500 episode reward: total was -19.720000. running mean: -29.663434\n",
      "ep 776: ep_len:3 episode reward: total was 0.000000. running mean: -29.366800\n",
      "ep 776: ep_len:540 episode reward: total was -30.050000. running mean: -29.373632\n",
      "ep 776: ep_len:590 episode reward: total was -27.420000. running mean: -29.354095\n",
      "epsilon:0.293939 episode_count: 5439. steps_count: 2428419.000000\n",
      "ep 777: ep_len:500 episode reward: total was -32.330000. running mean: -29.383854\n",
      "ep 777: ep_len:605 episode reward: total was -48.790000. running mean: -29.577916\n",
      "ep 777: ep_len:530 episode reward: total was -39.000000. running mean: -29.672137\n",
      "ep 777: ep_len:500 episode reward: total was -32.260000. running mean: -29.698015\n",
      "ep 777: ep_len:3 episode reward: total was 0.000000. running mean: -29.401035\n",
      "ep 777: ep_len:620 episode reward: total was -26.100000. running mean: -29.368025\n",
      "ep 777: ep_len:595 episode reward: total was -36.190000. running mean: -29.436245\n",
      "epsilon:0.293803 episode_count: 5446. steps_count: 2431772.000000\n",
      "ep 778: ep_len:675 episode reward: total was -37.900000. running mean: -29.520882\n",
      "ep 778: ep_len:500 episode reward: total was -22.410000. running mean: -29.449773\n",
      "ep 778: ep_len:61 episode reward: total was -2.500000. running mean: -29.180276\n",
      "ep 778: ep_len:510 episode reward: total was -39.670000. running mean: -29.285173\n",
      "ep 778: ep_len:3 episode reward: total was 0.000000. running mean: -28.992321\n",
      "ep 778: ep_len:710 episode reward: total was -123.900000. running mean: -29.941398\n",
      "ep 778: ep_len:342 episode reward: total was -16.350000. running mean: -29.805484\n",
      "epsilon:0.293666 episode_count: 5453. steps_count: 2434573.000000\n",
      "ep 779: ep_len:915 episode reward: total was -67.830000. running mean: -30.185729\n",
      "ep 779: ep_len:505 episode reward: total was -59.900000. running mean: -30.482872\n",
      "ep 779: ep_len:500 episode reward: total was -33.700000. running mean: -30.515043\n",
      "ep 779: ep_len:605 episode reward: total was -26.610000. running mean: -30.475993\n",
      "ep 779: ep_len:3 episode reward: total was 0.000000. running mean: -30.171233\n",
      "ep 779: ep_len:550 episode reward: total was -41.750000. running mean: -30.287020\n",
      "ep 779: ep_len:500 episode reward: total was -45.260000. running mean: -30.436750\n",
      "epsilon:0.293530 episode_count: 5460. steps_count: 2438151.000000\n",
      "ep 780: ep_len:206 episode reward: total was -13.460000. running mean: -30.266983\n",
      "ep 780: ep_len:555 episode reward: total was -45.170000. running mean: -30.416013\n",
      "ep 780: ep_len:615 episode reward: total was -45.230000. running mean: -30.564153\n",
      "ep 780: ep_len:500 episode reward: total was -30.780000. running mean: -30.566311\n",
      "ep 780: ep_len:3 episode reward: total was 0.000000. running mean: -30.260648\n",
      "ep 780: ep_len:615 episode reward: total was -37.120000. running mean: -30.329242\n",
      "ep 780: ep_len:205 episode reward: total was -19.980000. running mean: -30.225749\n",
      "epsilon:0.293393 episode_count: 5467. steps_count: 2440850.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 781: ep_len:640 episode reward: total was -61.540000. running mean: -30.538892\n",
      "ep 781: ep_len:281 episode reward: total was -17.920000. running mean: -30.412703\n",
      "ep 781: ep_len:70 episode reward: total was -6.980000. running mean: -30.178376\n",
      "ep 781: ep_len:540 episode reward: total was -32.800000. running mean: -30.204592\n",
      "ep 781: ep_len:3 episode reward: total was 0.000000. running mean: -29.902546\n",
      "ep 781: ep_len:540 episode reward: total was -43.760000. running mean: -30.041121\n",
      "ep 781: ep_len:211 episode reward: total was -18.440000. running mean: -29.925109\n",
      "epsilon:0.293257 episode_count: 5474. steps_count: 2443135.000000\n",
      "ep 782: ep_len:219 episode reward: total was -9.910000. running mean: -29.724958\n",
      "ep 782: ep_len:333 episode reward: total was -30.930000. running mean: -29.737009\n",
      "ep 782: ep_len:620 episode reward: total was -31.630000. running mean: -29.755939\n",
      "ep 782: ep_len:560 episode reward: total was -23.200000. running mean: -29.690379\n",
      "ep 782: ep_len:54 episode reward: total was 3.500000. running mean: -29.358475\n",
      "ep 782: ep_len:600 episode reward: total was -15.110000. running mean: -29.215991\n",
      "ep 782: ep_len:505 episode reward: total was -49.740000. running mean: -29.421231\n",
      "epsilon:0.293120 episode_count: 5481. steps_count: 2446026.000000\n",
      "ep 783: ep_len:108 episode reward: total was -4.940000. running mean: -29.176419\n",
      "ep 783: ep_len:535 episode reward: total was -23.180000. running mean: -29.116454\n",
      "ep 783: ep_len:626 episode reward: total was -92.730000. running mean: -29.752590\n",
      "ep 783: ep_len:96 episode reward: total was -3.460000. running mean: -29.489664\n",
      "ep 783: ep_len:97 episode reward: total was 2.040000. running mean: -29.174367\n",
      "ep 783: ep_len:510 episode reward: total was -47.090000. running mean: -29.353524\n",
      "ep 783: ep_len:560 episode reward: total was -22.110000. running mean: -29.281088\n",
      "epsilon:0.292984 episode_count: 5488. steps_count: 2448558.000000\n",
      "ep 784: ep_len:620 episode reward: total was -50.680000. running mean: -29.495077\n",
      "ep 784: ep_len:590 episode reward: total was -67.290000. running mean: -29.873027\n",
      "ep 784: ep_len:520 episode reward: total was -51.570000. running mean: -30.089996\n",
      "ep 784: ep_len:530 episode reward: total was -46.640000. running mean: -30.255496\n",
      "ep 784: ep_len:3 episode reward: total was 0.000000. running mean: -29.952941\n",
      "ep 784: ep_len:550 episode reward: total was -48.130000. running mean: -30.134712\n",
      "ep 784: ep_len:311 episode reward: total was -16.380000. running mean: -29.997165\n",
      "epsilon:0.292847 episode_count: 5495. steps_count: 2451682.000000\n",
      "ep 785: ep_len:615 episode reward: total was -27.180000. running mean: -29.968993\n",
      "ep 785: ep_len:500 episode reward: total was -45.740000. running mean: -30.126703\n",
      "ep 785: ep_len:560 episode reward: total was -24.160000. running mean: -30.067036\n",
      "ep 785: ep_len:575 episode reward: total was -23.080000. running mean: -29.997166\n",
      "ep 785: ep_len:3 episode reward: total was 0.000000. running mean: -29.697194\n",
      "ep 785: ep_len:545 episode reward: total was -45.110000. running mean: -29.851322\n",
      "ep 785: ep_len:545 episode reward: total was -47.020000. running mean: -30.023009\n",
      "epsilon:0.292711 episode_count: 5502. steps_count: 2455025.000000\n",
      "ep 786: ep_len:186 episode reward: total was -14.960000. running mean: -29.872379\n",
      "ep 786: ep_len:560 episode reward: total was -29.210000. running mean: -29.865755\n",
      "ep 786: ep_len:78 episode reward: total was -5.470000. running mean: -29.621798\n",
      "ep 786: ep_len:550 episode reward: total was -44.250000. running mean: -29.768080\n",
      "ep 786: ep_len:3 episode reward: total was 0.000000. running mean: -29.470399\n",
      "ep 786: ep_len:520 episode reward: total was -16.560000. running mean: -29.341295\n",
      "ep 786: ep_len:324 episode reward: total was -14.880000. running mean: -29.196682\n",
      "epsilon:0.292574 episode_count: 5509. steps_count: 2457246.000000\n",
      "ep 787: ep_len:251 episode reward: total was -6.410000. running mean: -28.968815\n",
      "ep 787: ep_len:570 episode reward: total was -50.580000. running mean: -29.184927\n",
      "ep 787: ep_len:500 episode reward: total was -33.610000. running mean: -29.229178\n",
      "ep 787: ep_len:500 episode reward: total was -30.120000. running mean: -29.238086\n",
      "ep 787: ep_len:114 episode reward: total was -4.990000. running mean: -28.995605\n",
      "ep 787: ep_len:650 episode reward: total was -23.050000. running mean: -28.936149\n",
      "ep 787: ep_len:565 episode reward: total was -26.460000. running mean: -28.911388\n",
      "epsilon:0.292438 episode_count: 5516. steps_count: 2460396.000000\n",
      "ep 788: ep_len:128 episode reward: total was -1.420000. running mean: -28.636474\n",
      "ep 788: ep_len:580 episode reward: total was -25.640000. running mean: -28.606509\n",
      "ep 788: ep_len:500 episode reward: total was -23.700000. running mean: -28.557444\n",
      "ep 788: ep_len:565 episode reward: total was -21.570000. running mean: -28.487569\n",
      "ep 788: ep_len:3 episode reward: total was 0.000000. running mean: -28.202694\n",
      "ep 788: ep_len:500 episode reward: total was -41.130000. running mean: -28.331967\n",
      "ep 788: ep_len:500 episode reward: total was -26.810000. running mean: -28.316747\n",
      "epsilon:0.292301 episode_count: 5523. steps_count: 2463172.000000\n",
      "ep 789: ep_len:635 episode reward: total was -75.160000. running mean: -28.785180\n",
      "ep 789: ep_len:715 episode reward: total was -58.510000. running mean: -29.082428\n",
      "ep 789: ep_len:625 episode reward: total was -40.380000. running mean: -29.195404\n",
      "ep 789: ep_len:505 episode reward: total was -27.760000. running mean: -29.181050\n",
      "ep 789: ep_len:3 episode reward: total was 0.000000. running mean: -28.889239\n",
      "ep 789: ep_len:525 episode reward: total was -35.650000. running mean: -28.956847\n",
      "ep 789: ep_len:580 episode reward: total was -17.670000. running mean: -28.843978\n",
      "epsilon:0.292165 episode_count: 5530. steps_count: 2466760.000000\n",
      "ep 790: ep_len:660 episode reward: total was -41.880000. running mean: -28.974338\n",
      "ep 790: ep_len:565 episode reward: total was -18.060000. running mean: -28.865195\n",
      "ep 790: ep_len:500 episode reward: total was -19.550000. running mean: -28.772043\n",
      "ep 790: ep_len:407 episode reward: total was -15.230000. running mean: -28.636623\n",
      "ep 790: ep_len:3 episode reward: total was 0.000000. running mean: -28.350256\n",
      "ep 790: ep_len:670 episode reward: total was -32.860000. running mean: -28.395354\n",
      "ep 790: ep_len:585 episode reward: total was -40.530000. running mean: -28.516700\n",
      "epsilon:0.292028 episode_count: 5537. steps_count: 2470150.000000\n",
      "ep 791: ep_len:630 episode reward: total was -24.050000. running mean: -28.472033\n",
      "ep 791: ep_len:565 episode reward: total was -45.610000. running mean: -28.643413\n",
      "ep 791: ep_len:560 episode reward: total was -45.550000. running mean: -28.812479\n",
      "ep 791: ep_len:505 episode reward: total was -20.680000. running mean: -28.731154\n",
      "ep 791: ep_len:55 episode reward: total was -6.500000. running mean: -28.508843\n",
      "ep 791: ep_len:545 episode reward: total was -27.480000. running mean: -28.498554\n",
      "ep 791: ep_len:550 episode reward: total was -38.230000. running mean: -28.595869\n",
      "epsilon:0.291892 episode_count: 5544. steps_count: 2473560.000000\n",
      "ep 792: ep_len:224 episode reward: total was -9.390000. running mean: -28.403810\n",
      "ep 792: ep_len:570 episode reward: total was -39.690000. running mean: -28.516672\n",
      "ep 792: ep_len:365 episode reward: total was -25.350000. running mean: -28.485005\n",
      "ep 792: ep_len:54 episode reward: total was -2.460000. running mean: -28.224755\n",
      "ep 792: ep_len:3 episode reward: total was 0.000000. running mean: -27.942507\n",
      "ep 792: ep_len:510 episode reward: total was -23.710000. running mean: -27.900182\n",
      "ep 792: ep_len:500 episode reward: total was -49.820000. running mean: -28.119381\n",
      "epsilon:0.291755 episode_count: 5551. steps_count: 2475786.000000\n",
      "ep 793: ep_len:500 episode reward: total was -74.900000. running mean: -28.587187\n",
      "ep 793: ep_len:510 episode reward: total was -47.040000. running mean: -28.771715\n",
      "ep 793: ep_len:560 episode reward: total was -24.160000. running mean: -28.725598\n",
      "ep 793: ep_len:540 episode reward: total was -54.640000. running mean: -28.984742\n",
      "ep 793: ep_len:3 episode reward: total was 0.000000. running mean: -28.694894\n",
      "ep 793: ep_len:500 episode reward: total was -41.840000. running mean: -28.826345\n",
      "ep 793: ep_len:615 episode reward: total was -27.490000. running mean: -28.812982\n",
      "epsilon:0.291619 episode_count: 5558. steps_count: 2479014.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 794: ep_len:805 episode reward: total was -69.910000. running mean: -29.223952\n",
      "ep 794: ep_len:510 episode reward: total was -39.920000. running mean: -29.330913\n",
      "ep 794: ep_len:500 episode reward: total was -20.700000. running mean: -29.244603\n",
      "ep 794: ep_len:56 episode reward: total was -0.930000. running mean: -28.961457\n",
      "ep 794: ep_len:3 episode reward: total was 0.000000. running mean: -28.671843\n",
      "ep 794: ep_len:610 episode reward: total was -51.260000. running mean: -28.897724\n",
      "ep 794: ep_len:530 episode reward: total was -31.440000. running mean: -28.923147\n",
      "epsilon:0.291482 episode_count: 5565. steps_count: 2482028.000000\n",
      "ep 795: ep_len:530 episode reward: total was -44.070000. running mean: -29.074616\n",
      "ep 795: ep_len:200 episode reward: total was -10.930000. running mean: -28.893170\n",
      "ep 795: ep_len:660 episode reward: total was -66.520000. running mean: -29.269438\n",
      "ep 795: ep_len:143 episode reward: total was -6.950000. running mean: -29.046243\n",
      "ep 795: ep_len:3 episode reward: total was 0.000000. running mean: -28.755781\n",
      "ep 795: ep_len:130 episode reward: total was -8.430000. running mean: -28.552523\n",
      "ep 795: ep_len:590 episode reward: total was -31.460000. running mean: -28.581598\n",
      "epsilon:0.291346 episode_count: 5572. steps_count: 2484284.000000\n",
      "ep 796: ep_len:990 episode reward: total was -44.070000. running mean: -28.736482\n",
      "ep 796: ep_len:515 episode reward: total was -48.750000. running mean: -28.936617\n",
      "ep 796: ep_len:590 episode reward: total was -32.170000. running mean: -28.968951\n",
      "ep 796: ep_len:515 episode reward: total was -12.650000. running mean: -28.805762\n",
      "ep 796: ep_len:3 episode reward: total was 0.000000. running mean: -28.517704\n",
      "ep 796: ep_len:600 episode reward: total was -36.960000. running mean: -28.602127\n",
      "ep 796: ep_len:580 episode reward: total was -29.530000. running mean: -28.611406\n",
      "epsilon:0.291209 episode_count: 5579. steps_count: 2488077.000000\n",
      "ep 797: ep_len:735 episode reward: total was -56.820000. running mean: -28.893492\n",
      "ep 797: ep_len:535 episode reward: total was -8.740000. running mean: -28.691957\n",
      "ep 797: ep_len:795 episode reward: total was -53.100000. running mean: -28.936037\n",
      "ep 797: ep_len:137 episode reward: total was -10.890000. running mean: -28.755577\n",
      "ep 797: ep_len:3 episode reward: total was 0.000000. running mean: -28.468021\n",
      "ep 797: ep_len:178 episode reward: total was -10.430000. running mean: -28.287641\n",
      "ep 797: ep_len:575 episode reward: total was -30.990000. running mean: -28.314664\n",
      "epsilon:0.291073 episode_count: 5586. steps_count: 2491035.000000\n",
      "ep 798: ep_len:590 episode reward: total was -36.270000. running mean: -28.394218\n",
      "ep 798: ep_len:500 episode reward: total was -34.730000. running mean: -28.457575\n",
      "ep 798: ep_len:505 episode reward: total was -45.660000. running mean: -28.629600\n",
      "ep 798: ep_len:500 episode reward: total was -31.670000. running mean: -28.660004\n",
      "ep 798: ep_len:3 episode reward: total was 0.000000. running mean: -28.373404\n",
      "ep 798: ep_len:685 episode reward: total was -62.160000. running mean: -28.711270\n",
      "ep 798: ep_len:311 episode reward: total was -16.330000. running mean: -28.587457\n",
      "epsilon:0.290936 episode_count: 5593. steps_count: 2494129.000000\n",
      "ep 799: ep_len:505 episode reward: total was -24.670000. running mean: -28.548282\n",
      "ep 799: ep_len:590 episode reward: total was -50.560000. running mean: -28.768400\n",
      "ep 799: ep_len:750 episode reward: total was -60.320000. running mean: -29.083916\n",
      "ep 799: ep_len:56 episode reward: total was -3.440000. running mean: -28.827476\n",
      "ep 799: ep_len:93 episode reward: total was -3.480000. running mean: -28.574002\n",
      "ep 799: ep_len:510 episode reward: total was -46.770000. running mean: -28.755962\n",
      "ep 799: ep_len:615 episode reward: total was -65.110000. running mean: -29.119502\n",
      "epsilon:0.290800 episode_count: 5600. steps_count: 2497248.000000\n",
      "ep 800: ep_len:585 episode reward: total was -54.230000. running mean: -29.370607\n",
      "ep 800: ep_len:770 episode reward: total was -96.080000. running mean: -30.037701\n",
      "ep 800: ep_len:570 episode reward: total was -44.750000. running mean: -30.184824\n",
      "ep 800: ep_len:525 episode reward: total was -18.540000. running mean: -30.068376\n",
      "ep 800: ep_len:49 episode reward: total was 1.500000. running mean: -29.752692\n",
      "ep 800: ep_len:980 episode reward: total was -111.870000. running mean: -30.573865\n",
      "ep 800: ep_len:520 episode reward: total was -22.950000. running mean: -30.497626\n",
      "epsilon:0.290663 episode_count: 5607. steps_count: 2501247.000000\n",
      "ep 801: ep_len:500 episode reward: total was -42.260000. running mean: -30.615250\n",
      "ep 801: ep_len:580 episode reward: total was -21.710000. running mean: -30.526198\n",
      "ep 801: ep_len:371 episode reward: total was -16.340000. running mean: -30.384336\n",
      "ep 801: ep_len:116 episode reward: total was -1.950000. running mean: -30.099992\n",
      "ep 801: ep_len:92 episode reward: total was 0.530000. running mean: -29.793692\n",
      "ep 801: ep_len:500 episode reward: total was -25.720000. running mean: -29.752955\n",
      "ep 801: ep_len:610 episode reward: total was -39.610000. running mean: -29.851526\n",
      "epsilon:0.290527 episode_count: 5614. steps_count: 2504016.000000\n",
      "ep 802: ep_len:615 episode reward: total was -42.080000. running mean: -29.973811\n",
      "ep 802: ep_len:193 episode reward: total was -11.900000. running mean: -29.793073\n",
      "ep 802: ep_len:535 episode reward: total was -42.790000. running mean: -29.923042\n",
      "ep 802: ep_len:570 episode reward: total was -44.130000. running mean: -30.065111\n",
      "ep 802: ep_len:3 episode reward: total was 0.000000. running mean: -29.764460\n",
      "ep 802: ep_len:540 episode reward: total was -26.720000. running mean: -29.734016\n",
      "ep 802: ep_len:585 episode reward: total was -27.950000. running mean: -29.716175\n",
      "epsilon:0.290390 episode_count: 5621. steps_count: 2507057.000000\n",
      "ep 803: ep_len:620 episode reward: total was -23.210000. running mean: -29.651114\n",
      "ep 803: ep_len:720 episode reward: total was -37.460000. running mean: -29.729203\n",
      "ep 803: ep_len:825 episode reward: total was -84.900000. running mean: -30.280911\n",
      "ep 803: ep_len:154 episode reward: total was -3.410000. running mean: -30.012201\n",
      "ep 803: ep_len:3 episode reward: total was 0.000000. running mean: -29.712079\n",
      "ep 803: ep_len:645 episode reward: total was -39.000000. running mean: -29.804959\n",
      "ep 803: ep_len:595 episode reward: total was -44.710000. running mean: -29.954009\n",
      "epsilon:0.290254 episode_count: 5628. steps_count: 2510619.000000\n",
      "ep 804: ep_len:505 episode reward: total was -46.160000. running mean: -30.116069\n",
      "ep 804: ep_len:585 episode reward: total was -42.140000. running mean: -30.236308\n",
      "ep 804: ep_len:434 episode reward: total was -29.860000. running mean: -30.232545\n",
      "ep 804: ep_len:545 episode reward: total was -29.690000. running mean: -30.227120\n",
      "ep 804: ep_len:94 episode reward: total was -14.970000. running mean: -30.074549\n",
      "ep 804: ep_len:560 episode reward: total was -47.050000. running mean: -30.244303\n",
      "ep 804: ep_len:500 episode reward: total was -26.290000. running mean: -30.204760\n",
      "epsilon:0.290117 episode_count: 5635. steps_count: 2513842.000000\n",
      "ep 805: ep_len:615 episode reward: total was -59.280000. running mean: -30.495512\n",
      "ep 805: ep_len:505 episode reward: total was -65.900000. running mean: -30.849557\n",
      "ep 805: ep_len:760 episode reward: total was -64.360000. running mean: -31.184662\n",
      "ep 805: ep_len:505 episode reward: total was -11.550000. running mean: -30.988315\n",
      "ep 805: ep_len:93 episode reward: total was -0.480000. running mean: -30.683232\n",
      "ep 805: ep_len:500 episode reward: total was -23.780000. running mean: -30.614200\n",
      "ep 805: ep_len:570 episode reward: total was -34.010000. running mean: -30.648158\n",
      "epsilon:0.289981 episode_count: 5642. steps_count: 2517390.000000\n",
      "ep 806: ep_len:595 episode reward: total was -53.300000. running mean: -30.874676\n",
      "ep 806: ep_len:535 episode reward: total was -26.090000. running mean: -30.826829\n",
      "ep 806: ep_len:590 episode reward: total was -51.500000. running mean: -31.033561\n",
      "ep 806: ep_len:500 episode reward: total was -44.210000. running mean: -31.165325\n",
      "ep 806: ep_len:47 episode reward: total was 0.000000. running mean: -30.853672\n",
      "ep 806: ep_len:630 episode reward: total was -31.280000. running mean: -30.857935\n",
      "ep 806: ep_len:565 episode reward: total was -50.740000. running mean: -31.056756\n",
      "epsilon:0.289844 episode_count: 5649. steps_count: 2520852.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 807: ep_len:520 episode reward: total was -27.770000. running mean: -31.023889\n",
      "ep 807: ep_len:515 episode reward: total was -45.610000. running mean: -31.169750\n",
      "ep 807: ep_len:660 episode reward: total was -47.590000. running mean: -31.333952\n",
      "ep 807: ep_len:620 episode reward: total was -43.180000. running mean: -31.452413\n",
      "ep 807: ep_len:3 episode reward: total was 0.000000. running mean: -31.137888\n",
      "ep 807: ep_len:565 episode reward: total was -27.840000. running mean: -31.104910\n",
      "ep 807: ep_len:545 episode reward: total was -23.060000. running mean: -31.024461\n",
      "epsilon:0.289708 episode_count: 5656. steps_count: 2524280.000000\n",
      "ep 808: ep_len:575 episode reward: total was -75.840000. running mean: -31.472616\n",
      "ep 808: ep_len:535 episode reward: total was -29.200000. running mean: -31.449890\n",
      "ep 808: ep_len:585 episode reward: total was -29.450000. running mean: -31.429891\n",
      "ep 808: ep_len:590 episode reward: total was -46.290000. running mean: -31.578492\n",
      "ep 808: ep_len:3 episode reward: total was 0.000000. running mean: -31.262707\n",
      "ep 808: ep_len:640 episode reward: total was -37.860000. running mean: -31.328680\n",
      "ep 808: ep_len:550 episode reward: total was -30.210000. running mean: -31.317493\n",
      "epsilon:0.289571 episode_count: 5663. steps_count: 2527758.000000\n",
      "ep 809: ep_len:535 episode reward: total was -35.630000. running mean: -31.360618\n",
      "ep 809: ep_len:500 episode reward: total was -31.640000. running mean: -31.363412\n",
      "ep 809: ep_len:560 episode reward: total was -25.650000. running mean: -31.306278\n",
      "ep 809: ep_len:500 episode reward: total was -33.140000. running mean: -31.324615\n",
      "ep 809: ep_len:3 episode reward: total was 0.000000. running mean: -31.011369\n",
      "ep 809: ep_len:585 episode reward: total was -41.710000. running mean: -31.118355\n",
      "ep 809: ep_len:610 episode reward: total was -35.490000. running mean: -31.162072\n",
      "epsilon:0.289435 episode_count: 5670. steps_count: 2531051.000000\n",
      "ep 810: ep_len:615 episode reward: total was -39.890000. running mean: -31.249351\n",
      "ep 810: ep_len:277 episode reward: total was -15.420000. running mean: -31.091058\n",
      "ep 810: ep_len:500 episode reward: total was -41.810000. running mean: -31.198247\n",
      "ep 810: ep_len:500 episode reward: total was -37.210000. running mean: -31.258364\n",
      "ep 810: ep_len:3 episode reward: total was 0.000000. running mean: -30.945781\n",
      "ep 810: ep_len:500 episode reward: total was -23.570000. running mean: -30.872023\n",
      "ep 810: ep_len:615 episode reward: total was -43.500000. running mean: -30.998303\n",
      "epsilon:0.289298 episode_count: 5677. steps_count: 2534061.000000\n",
      "ep 811: ep_len:114 episode reward: total was -8.480000. running mean: -30.773120\n",
      "ep 811: ep_len:510 episode reward: total was -24.960000. running mean: -30.714989\n",
      "ep 811: ep_len:565 episode reward: total was -25.390000. running mean: -30.661739\n",
      "ep 811: ep_len:545 episode reward: total was -14.650000. running mean: -30.501621\n",
      "ep 811: ep_len:39 episode reward: total was 2.000000. running mean: -30.176605\n",
      "ep 811: ep_len:585 episode reward: total was -28.320000. running mean: -30.158039\n",
      "ep 811: ep_len:334 episode reward: total was -21.360000. running mean: -30.070059\n",
      "epsilon:0.289162 episode_count: 5684. steps_count: 2536753.000000\n",
      "ep 812: ep_len:625 episode reward: total was -28.620000. running mean: -30.055558\n",
      "ep 812: ep_len:500 episode reward: total was -10.370000. running mean: -29.858702\n",
      "ep 812: ep_len:565 episode reward: total was -33.090000. running mean: -29.891015\n",
      "ep 812: ep_len:500 episode reward: total was -40.240000. running mean: -29.994505\n",
      "ep 812: ep_len:84 episode reward: total was -0.470000. running mean: -29.699260\n",
      "ep 812: ep_len:575 episode reward: total was -45.160000. running mean: -29.853868\n",
      "ep 812: ep_len:595 episode reward: total was -44.130000. running mean: -29.996629\n",
      "epsilon:0.289025 episode_count: 5691. steps_count: 2540197.000000\n",
      "ep 813: ep_len:515 episode reward: total was -25.550000. running mean: -29.952163\n",
      "ep 813: ep_len:545 episode reward: total was -44.090000. running mean: -30.093541\n",
      "ep 813: ep_len:1130 episode reward: total was -166.840000. running mean: -31.461006\n",
      "ep 813: ep_len:500 episode reward: total was -24.600000. running mean: -31.392396\n",
      "ep 813: ep_len:3 episode reward: total was 0.000000. running mean: -31.078472\n",
      "ep 813: ep_len:665 episode reward: total was -28.800000. running mean: -31.055687\n",
      "ep 813: ep_len:535 episode reward: total was -34.630000. running mean: -31.091430\n",
      "epsilon:0.288889 episode_count: 5698. steps_count: 2544090.000000\n",
      "ep 814: ep_len:202 episode reward: total was -7.440000. running mean: -30.854916\n",
      "ep 814: ep_len:545 episode reward: total was -15.610000. running mean: -30.702467\n",
      "ep 814: ep_len:605 episode reward: total was -31.740000. running mean: -30.712842\n",
      "ep 814: ep_len:570 episode reward: total was -28.150000. running mean: -30.687213\n",
      "ep 814: ep_len:3 episode reward: total was 0.000000. running mean: -30.380341\n",
      "ep 814: ep_len:550 episode reward: total was -26.730000. running mean: -30.343838\n",
      "ep 814: ep_len:540 episode reward: total was -32.200000. running mean: -30.362400\n",
      "epsilon:0.288752 episode_count: 5705. steps_count: 2547105.000000\n",
      "ep 815: ep_len:550 episode reward: total was -50.090000. running mean: -30.559676\n",
      "ep 815: ep_len:620 episode reward: total was -21.070000. running mean: -30.464779\n",
      "ep 815: ep_len:625 episode reward: total was -30.520000. running mean: -30.465331\n",
      "ep 815: ep_len:500 episode reward: total was -25.620000. running mean: -30.416878\n",
      "ep 815: ep_len:3 episode reward: total was 0.000000. running mean: -30.112709\n",
      "ep 815: ep_len:500 episode reward: total was -24.230000. running mean: -30.053882\n",
      "ep 815: ep_len:164 episode reward: total was -12.370000. running mean: -29.877043\n",
      "epsilon:0.288616 episode_count: 5712. steps_count: 2550067.000000\n",
      "ep 816: ep_len:505 episode reward: total was -42.620000. running mean: -30.004473\n",
      "ep 816: ep_len:500 episode reward: total was -26.110000. running mean: -29.965528\n",
      "ep 816: ep_len:505 episode reward: total was -16.160000. running mean: -29.827473\n",
      "ep 816: ep_len:625 episode reward: total was -34.060000. running mean: -29.869798\n",
      "ep 816: ep_len:3 episode reward: total was 0.000000. running mean: -29.571100\n",
      "ep 816: ep_len:605 episode reward: total was -37.100000. running mean: -29.646389\n",
      "ep 816: ep_len:500 episode reward: total was -34.010000. running mean: -29.690025\n",
      "epsilon:0.288479 episode_count: 5719. steps_count: 2553310.000000\n",
      "ep 817: ep_len:585 episode reward: total was -36.750000. running mean: -29.760625\n",
      "ep 817: ep_len:630 episode reward: total was -22.140000. running mean: -29.684419\n",
      "ep 817: ep_len:371 episode reward: total was -3.830000. running mean: -29.425874\n",
      "ep 817: ep_len:55 episode reward: total was -6.960000. running mean: -29.201216\n",
      "ep 817: ep_len:3 episode reward: total was 0.000000. running mean: -28.909203\n",
      "ep 817: ep_len:230 episode reward: total was -5.940000. running mean: -28.679511\n",
      "ep 817: ep_len:505 episode reward: total was -30.540000. running mean: -28.698116\n",
      "epsilon:0.288343 episode_count: 5726. steps_count: 2555689.000000\n",
      "ep 818: ep_len:540 episode reward: total was -54.670000. running mean: -28.957835\n",
      "ep 818: ep_len:520 episode reward: total was -30.840000. running mean: -28.976657\n",
      "ep 818: ep_len:505 episode reward: total was -47.220000. running mean: -29.159090\n",
      "ep 818: ep_len:530 episode reward: total was -86.940000. running mean: -29.736899\n",
      "ep 818: ep_len:3 episode reward: total was 0.000000. running mean: -29.439530\n",
      "ep 818: ep_len:640 episode reward: total was -29.510000. running mean: -29.440235\n",
      "ep 818: ep_len:305 episode reward: total was -20.890000. running mean: -29.354733\n",
      "epsilon:0.288206 episode_count: 5733. steps_count: 2558732.000000\n",
      "ep 819: ep_len:206 episode reward: total was -14.920000. running mean: -29.210385\n",
      "ep 819: ep_len:540 episode reward: total was -10.780000. running mean: -29.026081\n",
      "ep 819: ep_len:645 episode reward: total was -37.080000. running mean: -29.106621\n",
      "ep 819: ep_len:505 episode reward: total was -16.280000. running mean: -28.978354\n",
      "ep 819: ep_len:82 episode reward: total was -2.460000. running mean: -28.713171\n",
      "ep 819: ep_len:500 episode reward: total was -28.240000. running mean: -28.708439\n",
      "ep 819: ep_len:530 episode reward: total was -26.090000. running mean: -28.682255\n",
      "epsilon:0.288070 episode_count: 5740. steps_count: 2561740.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 820: ep_len:129 episode reward: total was 0.570000. running mean: -28.389732\n",
      "ep 820: ep_len:174 episode reward: total was -2.910000. running mean: -28.134935\n",
      "ep 820: ep_len:500 episode reward: total was -27.160000. running mean: -28.125186\n",
      "ep 820: ep_len:565 episode reward: total was -18.570000. running mean: -28.029634\n",
      "ep 820: ep_len:3 episode reward: total was 0.000000. running mean: -27.749337\n",
      "ep 820: ep_len:715 episode reward: total was -39.810000. running mean: -27.869944\n",
      "ep 820: ep_len:595 episode reward: total was -30.160000. running mean: -27.892845\n",
      "epsilon:0.287933 episode_count: 5747. steps_count: 2564421.000000\n",
      "ep 821: ep_len:500 episode reward: total was -13.690000. running mean: -27.750816\n",
      "ep 821: ep_len:515 episode reward: total was -51.210000. running mean: -27.985408\n",
      "ep 821: ep_len:600 episode reward: total was -28.190000. running mean: -27.987454\n",
      "ep 821: ep_len:500 episode reward: total was -16.080000. running mean: -27.868379\n",
      "ep 821: ep_len:3 episode reward: total was 0.000000. running mean: -27.589696\n",
      "ep 821: ep_len:580 episode reward: total was -26.170000. running mean: -27.575499\n",
      "ep 821: ep_len:530 episode reward: total was -28.720000. running mean: -27.586944\n",
      "epsilon:0.287797 episode_count: 5754. steps_count: 2567649.000000\n",
      "ep 822: ep_len:575 episode reward: total was -38.880000. running mean: -27.699874\n",
      "ep 822: ep_len:590 episode reward: total was -53.780000. running mean: -27.960675\n",
      "ep 822: ep_len:620 episode reward: total was -39.590000. running mean: -28.076969\n",
      "ep 822: ep_len:535 episode reward: total was -59.770000. running mean: -28.393899\n",
      "ep 822: ep_len:3 episode reward: total was 0.000000. running mean: -28.109960\n",
      "ep 822: ep_len:615 episode reward: total was -63.860000. running mean: -28.467460\n",
      "ep 822: ep_len:287 episode reward: total was -11.860000. running mean: -28.301386\n",
      "epsilon:0.287660 episode_count: 5761. steps_count: 2570874.000000\n",
      "ep 823: ep_len:500 episode reward: total was -35.020000. running mean: -28.368572\n",
      "ep 823: ep_len:585 episode reward: total was -35.660000. running mean: -28.441486\n",
      "ep 823: ep_len:500 episode reward: total was -31.160000. running mean: -28.468671\n",
      "ep 823: ep_len:500 episode reward: total was -9.210000. running mean: -28.276085\n",
      "ep 823: ep_len:54 episode reward: total was 3.500000. running mean: -27.958324\n",
      "ep 823: ep_len:500 episode reward: total was -34.210000. running mean: -28.020841\n",
      "ep 823: ep_len:570 episode reward: total was -22.630000. running mean: -27.966932\n",
      "epsilon:0.287524 episode_count: 5768. steps_count: 2574083.000000\n",
      "ep 824: ep_len:119 episode reward: total was -11.970000. running mean: -27.806963\n",
      "ep 824: ep_len:347 episode reward: total was -43.400000. running mean: -27.962893\n",
      "ep 824: ep_len:640 episode reward: total was -45.950000. running mean: -28.142764\n",
      "ep 824: ep_len:515 episode reward: total was -26.640000. running mean: -28.127737\n",
      "ep 824: ep_len:71 episode reward: total was -11.490000. running mean: -27.961359\n",
      "ep 824: ep_len:184 episode reward: total was -22.920000. running mean: -27.910946\n",
      "ep 824: ep_len:625 episode reward: total was -44.100000. running mean: -28.072836\n",
      "epsilon:0.287387 episode_count: 5775. steps_count: 2576584.000000\n",
      "ep 825: ep_len:640 episode reward: total was -31.360000. running mean: -28.105708\n",
      "ep 825: ep_len:560 episode reward: total was -62.190000. running mean: -28.446551\n",
      "ep 825: ep_len:580 episode reward: total was -39.440000. running mean: -28.556485\n",
      "ep 825: ep_len:110 episode reward: total was -5.450000. running mean: -28.325420\n",
      "ep 825: ep_len:49 episode reward: total was 3.000000. running mean: -28.012166\n",
      "ep 825: ep_len:500 episode reward: total was -15.830000. running mean: -27.890345\n",
      "ep 825: ep_len:575 episode reward: total was -35.500000. running mean: -27.966441\n",
      "epsilon:0.287251 episode_count: 5782. steps_count: 2579598.000000\n",
      "ep 826: ep_len:565 episode reward: total was -20.110000. running mean: -27.887877\n",
      "ep 826: ep_len:605 episode reward: total was -40.510000. running mean: -28.014098\n",
      "ep 826: ep_len:505 episode reward: total was -33.480000. running mean: -28.068757\n",
      "ep 826: ep_len:635 episode reward: total was -56.640000. running mean: -28.354469\n",
      "ep 826: ep_len:3 episode reward: total was 0.000000. running mean: -28.070925\n",
      "ep 826: ep_len:655 episode reward: total was -68.530000. running mean: -28.475515\n",
      "ep 826: ep_len:525 episode reward: total was -27.040000. running mean: -28.461160\n",
      "epsilon:0.287114 episode_count: 5789. steps_count: 2583091.000000\n",
      "ep 827: ep_len:735 episode reward: total was -37.580000. running mean: -28.552349\n",
      "ep 827: ep_len:500 episode reward: total was -28.260000. running mean: -28.549425\n",
      "ep 827: ep_len:555 episode reward: total was -33.440000. running mean: -28.598331\n",
      "ep 827: ep_len:500 episode reward: total was -11.560000. running mean: -28.427948\n",
      "ep 827: ep_len:3 episode reward: total was 0.000000. running mean: -28.143668\n",
      "ep 827: ep_len:680 episode reward: total was -26.310000. running mean: -28.125331\n",
      "ep 827: ep_len:545 episode reward: total was -40.590000. running mean: -28.249978\n",
      "epsilon:0.286978 episode_count: 5796. steps_count: 2586609.000000\n",
      "ep 828: ep_len:530 episode reward: total was -53.600000. running mean: -28.503478\n",
      "ep 828: ep_len:505 episode reward: total was -49.160000. running mean: -28.710044\n",
      "ep 828: ep_len:535 episode reward: total was -31.220000. running mean: -28.735143\n",
      "ep 828: ep_len:610 episode reward: total was -17.700000. running mean: -28.624792\n",
      "ep 828: ep_len:3 episode reward: total was 0.000000. running mean: -28.338544\n",
      "ep 828: ep_len:306 episode reward: total was -10.920000. running mean: -28.164358\n",
      "ep 828: ep_len:545 episode reward: total was -33.690000. running mean: -28.219615\n",
      "epsilon:0.286841 episode_count: 5803. steps_count: 2589643.000000\n",
      "ep 829: ep_len:500 episode reward: total was -26.110000. running mean: -28.198519\n",
      "ep 829: ep_len:595 episode reward: total was -7.200000. running mean: -27.988533\n",
      "ep 829: ep_len:635 episode reward: total was -20.110000. running mean: -27.909748\n",
      "ep 829: ep_len:132 episode reward: total was 0.590000. running mean: -27.624751\n",
      "ep 829: ep_len:84 episode reward: total was -11.960000. running mean: -27.468103\n",
      "ep 829: ep_len:575 episode reward: total was -71.280000. running mean: -27.906222\n",
      "ep 829: ep_len:535 episode reward: total was -38.990000. running mean: -28.017060\n",
      "epsilon:0.286705 episode_count: 5810. steps_count: 2592699.000000\n",
      "ep 830: ep_len:705 episode reward: total was -45.890000. running mean: -28.195789\n",
      "ep 830: ep_len:500 episode reward: total was -35.680000. running mean: -28.270631\n",
      "ep 830: ep_len:540 episode reward: total was -25.130000. running mean: -28.239225\n",
      "ep 830: ep_len:380 episode reward: total was -12.200000. running mean: -28.078833\n",
      "ep 830: ep_len:3 episode reward: total was 0.000000. running mean: -27.798044\n",
      "ep 830: ep_len:635 episode reward: total was -28.620000. running mean: -27.806264\n",
      "ep 830: ep_len:515 episode reward: total was -44.070000. running mean: -27.968901\n",
      "epsilon:0.286568 episode_count: 5817. steps_count: 2595977.000000\n",
      "ep 831: ep_len:565 episode reward: total was -26.080000. running mean: -27.950012\n",
      "ep 831: ep_len:535 episode reward: total was -25.140000. running mean: -27.921912\n",
      "ep 831: ep_len:66 episode reward: total was -6.470000. running mean: -27.707393\n",
      "ep 831: ep_len:500 episode reward: total was -33.130000. running mean: -27.761619\n",
      "ep 831: ep_len:3 episode reward: total was 0.000000. running mean: -27.484003\n",
      "ep 831: ep_len:610 episode reward: total was -34.190000. running mean: -27.551063\n",
      "ep 831: ep_len:540 episode reward: total was -35.670000. running mean: -27.632252\n",
      "epsilon:0.286432 episode_count: 5824. steps_count: 2598796.000000\n",
      "ep 832: ep_len:500 episode reward: total was -33.960000. running mean: -27.695530\n",
      "ep 832: ep_len:500 episode reward: total was -24.630000. running mean: -27.664875\n",
      "ep 832: ep_len:64 episode reward: total was -3.480000. running mean: -27.423026\n",
      "ep 832: ep_len:505 episode reward: total was -49.780000. running mean: -27.646596\n",
      "ep 832: ep_len:3 episode reward: total was 0.000000. running mean: -27.370130\n",
      "ep 832: ep_len:515 episode reward: total was -32.380000. running mean: -27.420228\n",
      "ep 832: ep_len:555 episode reward: total was -59.060000. running mean: -27.736626\n",
      "epsilon:0.286295 episode_count: 5831. steps_count: 2601438.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 833: ep_len:540 episode reward: total was -42.010000. running mean: -27.879360\n",
      "ep 833: ep_len:505 episode reward: total was -43.310000. running mean: -28.033666\n",
      "ep 833: ep_len:605 episode reward: total was -35.640000. running mean: -28.109729\n",
      "ep 833: ep_len:700 episode reward: total was -73.630000. running mean: -28.564932\n",
      "ep 833: ep_len:3 episode reward: total was 0.000000. running mean: -28.279283\n",
      "ep 833: ep_len:296 episode reward: total was -25.350000. running mean: -28.249990\n",
      "ep 833: ep_len:560 episode reward: total was -29.820000. running mean: -28.265690\n",
      "epsilon:0.286159 episode_count: 5838. steps_count: 2604647.000000\n",
      "ep 834: ep_len:625 episode reward: total was -41.100000. running mean: -28.394033\n",
      "ep 834: ep_len:625 episode reward: total was -53.660000. running mean: -28.646693\n",
      "ep 834: ep_len:540 episode reward: total was -34.130000. running mean: -28.701526\n",
      "ep 834: ep_len:565 episode reward: total was -22.130000. running mean: -28.635811\n",
      "ep 834: ep_len:3 episode reward: total was 0.000000. running mean: -28.349453\n",
      "ep 834: ep_len:655 episode reward: total was -34.150000. running mean: -28.407458\n",
      "ep 834: ep_len:505 episode reward: total was -22.400000. running mean: -28.347384\n",
      "epsilon:0.286022 episode_count: 5845. steps_count: 2608165.000000\n",
      "ep 835: ep_len:605 episode reward: total was -35.270000. running mean: -28.416610\n",
      "ep 835: ep_len:187 episode reward: total was -10.470000. running mean: -28.237144\n",
      "ep 835: ep_len:530 episode reward: total was -39.490000. running mean: -28.349672\n",
      "ep 835: ep_len:500 episode reward: total was -35.140000. running mean: -28.417575\n",
      "ep 835: ep_len:3 episode reward: total was 0.000000. running mean: -28.133400\n",
      "ep 835: ep_len:186 episode reward: total was -8.460000. running mean: -27.936666\n",
      "ep 835: ep_len:332 episode reward: total was -29.840000. running mean: -27.955699\n",
      "epsilon:0.285886 episode_count: 5852. steps_count: 2610508.000000\n",
      "ep 836: ep_len:505 episode reward: total was -22.180000. running mean: -27.897942\n",
      "ep 836: ep_len:505 episode reward: total was -33.850000. running mean: -27.957463\n",
      "ep 836: ep_len:535 episode reward: total was -22.700000. running mean: -27.904888\n",
      "ep 836: ep_len:655 episode reward: total was -35.610000. running mean: -27.981939\n",
      "ep 836: ep_len:3 episode reward: total was 0.000000. running mean: -27.702120\n",
      "ep 836: ep_len:505 episode reward: total was -35.330000. running mean: -27.778399\n",
      "ep 836: ep_len:206 episode reward: total was -12.910000. running mean: -27.629715\n",
      "epsilon:0.285749 episode_count: 5859. steps_count: 2613422.000000\n",
      "ep 837: ep_len:227 episode reward: total was -1.900000. running mean: -27.372417\n",
      "ep 837: ep_len:575 episode reward: total was -3.460000. running mean: -27.133293\n",
      "ep 837: ep_len:565 episode reward: total was -42.480000. running mean: -27.286760\n",
      "ep 837: ep_len:170 episode reward: total was -9.440000. running mean: -27.108293\n",
      "ep 837: ep_len:110 episode reward: total was -1.970000. running mean: -26.856910\n",
      "ep 837: ep_len:520 episode reward: total was -23.030000. running mean: -26.818641\n",
      "ep 837: ep_len:585 episode reward: total was -28.560000. running mean: -26.836054\n",
      "epsilon:0.285613 episode_count: 5866. steps_count: 2616174.000000\n",
      "ep 838: ep_len:635 episode reward: total was -42.590000. running mean: -26.993594\n",
      "ep 838: ep_len:520 episode reward: total was -23.590000. running mean: -26.959558\n",
      "ep 838: ep_len:610 episode reward: total was -51.160000. running mean: -27.201562\n",
      "ep 838: ep_len:505 episode reward: total was -29.180000. running mean: -27.221347\n",
      "ep 838: ep_len:88 episode reward: total was -11.950000. running mean: -27.068633\n",
      "ep 838: ep_len:500 episode reward: total was -26.620000. running mean: -27.064147\n",
      "ep 838: ep_len:550 episode reward: total was -52.250000. running mean: -27.316005\n",
      "epsilon:0.285476 episode_count: 5873. steps_count: 2619582.000000\n",
      "ep 839: ep_len:225 episode reward: total was -3.900000. running mean: -27.081845\n",
      "ep 839: ep_len:515 episode reward: total was -46.560000. running mean: -27.276627\n",
      "ep 839: ep_len:565 episode reward: total was -30.570000. running mean: -27.309561\n",
      "ep 839: ep_len:545 episode reward: total was -19.510000. running mean: -27.231565\n",
      "ep 839: ep_len:3 episode reward: total was 0.000000. running mean: -26.959249\n",
      "ep 839: ep_len:640 episode reward: total was -33.160000. running mean: -27.021257\n",
      "ep 839: ep_len:530 episode reward: total was -38.590000. running mean: -27.136944\n",
      "epsilon:0.285340 episode_count: 5880. steps_count: 2622605.000000\n",
      "ep 840: ep_len:615 episode reward: total was -59.620000. running mean: -27.461775\n",
      "ep 840: ep_len:510 episode reward: total was -26.660000. running mean: -27.453757\n",
      "ep 840: ep_len:535 episode reward: total was -22.560000. running mean: -27.404819\n",
      "ep 840: ep_len:525 episode reward: total was -44.110000. running mean: -27.571871\n",
      "ep 840: ep_len:51 episode reward: total was -2.500000. running mean: -27.321153\n",
      "ep 840: ep_len:500 episode reward: total was -35.220000. running mean: -27.400141\n",
      "ep 840: ep_len:500 episode reward: total was -29.350000. running mean: -27.419640\n",
      "epsilon:0.285203 episode_count: 5887. steps_count: 2625841.000000\n",
      "ep 841: ep_len:585 episode reward: total was -30.160000. running mean: -27.447043\n",
      "ep 841: ep_len:500 episode reward: total was -19.590000. running mean: -27.368473\n",
      "ep 841: ep_len:640 episode reward: total was -37.580000. running mean: -27.470588\n",
      "ep 841: ep_len:510 episode reward: total was -33.080000. running mean: -27.526682\n",
      "ep 841: ep_len:103 episode reward: total was -11.460000. running mean: -27.366015\n",
      "ep 841: ep_len:590 episode reward: total was -41.700000. running mean: -27.509355\n",
      "ep 841: ep_len:540 episode reward: total was -28.940000. running mean: -27.523662\n",
      "epsilon:0.285067 episode_count: 5894. steps_count: 2629309.000000\n",
      "ep 842: ep_len:660 episode reward: total was -47.450000. running mean: -27.722925\n",
      "ep 842: ep_len:515 episode reward: total was -34.650000. running mean: -27.792196\n",
      "ep 842: ep_len:625 episode reward: total was -21.490000. running mean: -27.729174\n",
      "ep 842: ep_len:530 episode reward: total was -21.090000. running mean: -27.662782\n",
      "ep 842: ep_len:3 episode reward: total was 0.000000. running mean: -27.386154\n",
      "ep 842: ep_len:535 episode reward: total was -23.720000. running mean: -27.349493\n",
      "ep 842: ep_len:565 episode reward: total was -37.020000. running mean: -27.446198\n",
      "epsilon:0.284930 episode_count: 5901. steps_count: 2632742.000000\n",
      "ep 843: ep_len:545 episode reward: total was -15.010000. running mean: -27.321836\n",
      "ep 843: ep_len:330 episode reward: total was -23.430000. running mean: -27.282917\n",
      "ep 843: ep_len:431 episode reward: total was -23.840000. running mean: -27.248488\n",
      "ep 843: ep_len:155 episode reward: total was -4.890000. running mean: -27.024903\n",
      "ep 843: ep_len:129 episode reward: total was -13.940000. running mean: -26.894054\n",
      "ep 843: ep_len:515 episode reward: total was -59.690000. running mean: -27.222014\n",
      "ep 843: ep_len:555 episode reward: total was -40.220000. running mean: -27.351994\n",
      "epsilon:0.284794 episode_count: 5908. steps_count: 2635402.000000\n",
      "ep 844: ep_len:500 episode reward: total was -55.810000. running mean: -27.636574\n",
      "ep 844: ep_len:500 episode reward: total was -39.380000. running mean: -27.754008\n",
      "ep 844: ep_len:600 episode reward: total was -39.890000. running mean: -27.875368\n",
      "ep 844: ep_len:545 episode reward: total was -14.030000. running mean: -27.736914\n",
      "ep 844: ep_len:69 episode reward: total was -4.480000. running mean: -27.504345\n",
      "ep 844: ep_len:605 episode reward: total was -46.040000. running mean: -27.689702\n",
      "ep 844: ep_len:316 episode reward: total was -26.900000. running mean: -27.681805\n",
      "epsilon:0.284657 episode_count: 5915. steps_count: 2638537.000000\n",
      "ep 845: ep_len:236 episode reward: total was -9.900000. running mean: -27.503987\n",
      "ep 845: ep_len:500 episode reward: total was -39.410000. running mean: -27.623047\n",
      "ep 845: ep_len:455 episode reward: total was -28.280000. running mean: -27.629616\n",
      "ep 845: ep_len:510 episode reward: total was -40.730000. running mean: -27.760620\n",
      "ep 845: ep_len:3 episode reward: total was 0.000000. running mean: -27.483014\n",
      "ep 845: ep_len:575 episode reward: total was -34.860000. running mean: -27.556784\n",
      "ep 845: ep_len:510 episode reward: total was -39.070000. running mean: -27.671916\n",
      "epsilon:0.284521 episode_count: 5922. steps_count: 2641326.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 846: ep_len:525 episode reward: total was -41.000000. running mean: -27.805197\n",
      "ep 846: ep_len:510 episode reward: total was -28.230000. running mean: -27.809445\n",
      "ep 846: ep_len:655 episode reward: total was -61.910000. running mean: -28.150450\n",
      "ep 846: ep_len:167 episode reward: total was -4.930000. running mean: -27.918246\n",
      "ep 846: ep_len:102 episode reward: total was -0.460000. running mean: -27.643663\n",
      "ep 846: ep_len:610 episode reward: total was -62.530000. running mean: -27.992527\n",
      "ep 846: ep_len:505 episode reward: total was -30.940000. running mean: -28.022001\n",
      "epsilon:0.284384 episode_count: 5929. steps_count: 2644400.000000\n",
      "ep 847: ep_len:545 episode reward: total was -25.030000. running mean: -27.992081\n",
      "ep 847: ep_len:201 episode reward: total was -14.940000. running mean: -27.861561\n",
      "ep 847: ep_len:590 episode reward: total was -45.440000. running mean: -28.037345\n",
      "ep 847: ep_len:550 episode reward: total was -38.140000. running mean: -28.138372\n",
      "ep 847: ep_len:109 episode reward: total was -3.970000. running mean: -27.896688\n",
      "ep 847: ep_len:550 episode reward: total was -22.530000. running mean: -27.843021\n",
      "ep 847: ep_len:196 episode reward: total was -16.940000. running mean: -27.733991\n",
      "epsilon:0.284248 episode_count: 5936. steps_count: 2647141.000000\n",
      "ep 848: ep_len:215 episode reward: total was -23.390000. running mean: -27.690551\n",
      "ep 848: ep_len:705 episode reward: total was -64.020000. running mean: -28.053845\n",
      "ep 848: ep_len:565 episode reward: total was -30.090000. running mean: -28.074207\n",
      "ep 848: ep_len:381 episode reward: total was -29.750000. running mean: -28.090965\n",
      "ep 848: ep_len:3 episode reward: total was 0.000000. running mean: -27.810055\n",
      "ep 848: ep_len:570 episode reward: total was -9.480000. running mean: -27.626755\n",
      "ep 848: ep_len:500 episode reward: total was -24.080000. running mean: -27.591287\n",
      "epsilon:0.284111 episode_count: 5943. steps_count: 2650080.000000\n",
      "ep 849: ep_len:585 episode reward: total was -50.430000. running mean: -27.819674\n",
      "ep 849: ep_len:885 episode reward: total was -76.870000. running mean: -28.310177\n",
      "ep 849: ep_len:500 episode reward: total was -29.330000. running mean: -28.320376\n",
      "ep 849: ep_len:500 episode reward: total was -24.700000. running mean: -28.284172\n",
      "ep 849: ep_len:3 episode reward: total was 0.000000. running mean: -28.001330\n",
      "ep 849: ep_len:695 episode reward: total was -15.790000. running mean: -27.879217\n",
      "ep 849: ep_len:605 episode reward: total was -37.080000. running mean: -27.971225\n",
      "epsilon:0.283975 episode_count: 5950. steps_count: 2653853.000000\n",
      "ep 850: ep_len:500 episode reward: total was -34.050000. running mean: -28.032013\n",
      "ep 850: ep_len:645 episode reward: total was -3.030000. running mean: -27.781992\n",
      "ep 850: ep_len:615 episode reward: total was -39.380000. running mean: -27.897972\n",
      "ep 850: ep_len:540 episode reward: total was -35.570000. running mean: -27.974693\n",
      "ep 850: ep_len:3 episode reward: total was 0.000000. running mean: -27.694946\n",
      "ep 850: ep_len:500 episode reward: total was -24.570000. running mean: -27.663696\n",
      "ep 850: ep_len:187 episode reward: total was -11.940000. running mean: -27.506459\n",
      "epsilon:0.283838 episode_count: 5957. steps_count: 2656843.000000\n",
      "ep 851: ep_len:575 episode reward: total was -42.400000. running mean: -27.655395\n",
      "ep 851: ep_len:560 episode reward: total was -33.680000. running mean: -27.715641\n",
      "ep 851: ep_len:570 episode reward: total was -55.060000. running mean: -27.989084\n",
      "ep 851: ep_len:137 episode reward: total was -15.450000. running mean: -27.863694\n",
      "ep 851: ep_len:95 episode reward: total was -6.960000. running mean: -27.654657\n",
      "ep 851: ep_len:515 episode reward: total was -40.120000. running mean: -27.779310\n",
      "ep 851: ep_len:510 episode reward: total was -40.020000. running mean: -27.901717\n",
      "epsilon:0.283702 episode_count: 5964. steps_count: 2659805.000000\n",
      "ep 852: ep_len:630 episode reward: total was -31.610000. running mean: -27.938800\n",
      "ep 852: ep_len:500 episode reward: total was -5.370000. running mean: -27.713112\n",
      "ep 852: ep_len:560 episode reward: total was -30.270000. running mean: -27.738681\n",
      "ep 852: ep_len:505 episode reward: total was -24.760000. running mean: -27.708894\n",
      "ep 852: ep_len:3 episode reward: total was 0.000000. running mean: -27.431805\n",
      "ep 852: ep_len:505 episode reward: total was -46.260000. running mean: -27.620087\n",
      "ep 852: ep_len:505 episode reward: total was -25.080000. running mean: -27.594686\n",
      "epsilon:0.283565 episode_count: 5971. steps_count: 2663013.000000\n",
      "ep 853: ep_len:205 episode reward: total was -11.470000. running mean: -27.433439\n",
      "ep 853: ep_len:560 episode reward: total was -54.170000. running mean: -27.700805\n",
      "ep 853: ep_len:399 episode reward: total was -24.840000. running mean: -27.672197\n",
      "ep 853: ep_len:500 episode reward: total was -14.570000. running mean: -27.541175\n",
      "ep 853: ep_len:90 episode reward: total was -11.450000. running mean: -27.380263\n",
      "ep 853: ep_len:570 episode reward: total was -26.700000. running mean: -27.373460\n",
      "ep 853: ep_len:500 episode reward: total was -30.360000. running mean: -27.403326\n",
      "epsilon:0.283429 episode_count: 5978. steps_count: 2665837.000000\n",
      "ep 854: ep_len:500 episode reward: total was -23.700000. running mean: -27.366293\n",
      "ep 854: ep_len:510 episode reward: total was -45.630000. running mean: -27.548930\n",
      "ep 854: ep_len:535 episode reward: total was -23.720000. running mean: -27.510640\n",
      "ep 854: ep_len:520 episode reward: total was -21.680000. running mean: -27.452334\n",
      "ep 854: ep_len:3 episode reward: total was 0.000000. running mean: -27.177811\n",
      "ep 854: ep_len:291 episode reward: total was -16.920000. running mean: -27.075232\n",
      "ep 854: ep_len:199 episode reward: total was -15.420000. running mean: -26.958680\n",
      "epsilon:0.283292 episode_count: 5985. steps_count: 2668395.000000\n",
      "ep 855: ep_len:735 episode reward: total was -53.360000. running mean: -27.222693\n",
      "ep 855: ep_len:500 episode reward: total was -21.120000. running mean: -27.161666\n",
      "ep 855: ep_len:640 episode reward: total was -33.870000. running mean: -27.228750\n",
      "ep 855: ep_len:590 episode reward: total was -7.650000. running mean: -27.032962\n",
      "ep 855: ep_len:126 episode reward: total was -4.450000. running mean: -26.807133\n",
      "ep 855: ep_len:515 episode reward: total was -27.190000. running mean: -26.810961\n",
      "ep 855: ep_len:190 episode reward: total was -20.970000. running mean: -26.752552\n",
      "epsilon:0.283156 episode_count: 5992. steps_count: 2671691.000000\n",
      "ep 856: ep_len:225 episode reward: total was -9.950000. running mean: -26.584526\n",
      "ep 856: ep_len:500 episode reward: total was -30.310000. running mean: -26.621781\n",
      "ep 856: ep_len:362 episode reward: total was -21.450000. running mean: -26.570063\n",
      "ep 856: ep_len:505 episode reward: total was -39.770000. running mean: -26.702062\n",
      "ep 856: ep_len:96 episode reward: total was -1.970000. running mean: -26.454742\n",
      "ep 856: ep_len:615 episode reward: total was -26.190000. running mean: -26.452094\n",
      "ep 856: ep_len:179 episode reward: total was -16.920000. running mean: -26.356773\n",
      "epsilon:0.283019 episode_count: 5999. steps_count: 2674173.000000\n",
      "ep 857: ep_len:610 episode reward: total was -57.560000. running mean: -26.668806\n",
      "ep 857: ep_len:540 episode reward: total was -25.130000. running mean: -26.653418\n",
      "ep 857: ep_len:515 episode reward: total was -31.500000. running mean: -26.701884\n",
      "ep 857: ep_len:107 episode reward: total was -3.450000. running mean: -26.469365\n",
      "ep 857: ep_len:133 episode reward: total was 1.560000. running mean: -26.189071\n",
      "ep 857: ep_len:585 episode reward: total was -37.210000. running mean: -26.299280\n",
      "ep 857: ep_len:199 episode reward: total was -11.890000. running mean: -26.155188\n",
      "epsilon:0.282883 episode_count: 6006. steps_count: 2676862.000000\n",
      "ep 858: ep_len:134 episode reward: total was -2.940000. running mean: -25.923036\n",
      "ep 858: ep_len:530 episode reward: total was -40.990000. running mean: -26.073705\n",
      "ep 858: ep_len:500 episode reward: total was -15.020000. running mean: -25.963168\n",
      "ep 858: ep_len:130 episode reward: total was -0.940000. running mean: -25.712937\n",
      "ep 858: ep_len:3 episode reward: total was 0.000000. running mean: -25.455807\n",
      "ep 858: ep_len:645 episode reward: total was -28.930000. running mean: -25.490549\n",
      "ep 858: ep_len:500 episode reward: total was -33.850000. running mean: -25.574144\n",
      "epsilon:0.282746 episode_count: 6013. steps_count: 2679304.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 859: ep_len:605 episode reward: total was -73.340000. running mean: -26.051802\n",
      "ep 859: ep_len:600 episode reward: total was -57.500000. running mean: -26.366284\n",
      "ep 859: ep_len:540 episode reward: total was -34.480000. running mean: -26.447421\n",
      "ep 859: ep_len:505 episode reward: total was -19.640000. running mean: -26.379347\n",
      "ep 859: ep_len:51 episode reward: total was 2.000000. running mean: -26.095554\n",
      "ep 859: ep_len:525 episode reward: total was -25.610000. running mean: -26.090698\n",
      "ep 859: ep_len:600 episode reward: total was -27.910000. running mean: -26.108891\n",
      "epsilon:0.282610 episode_count: 6020. steps_count: 2682730.000000\n",
      "ep 860: ep_len:515 episode reward: total was -31.550000. running mean: -26.163302\n",
      "ep 860: ep_len:540 episode reward: total was -26.720000. running mean: -26.168869\n",
      "ep 860: ep_len:595 episode reward: total was -29.340000. running mean: -26.200580\n",
      "ep 860: ep_len:56 episode reward: total was -0.460000. running mean: -25.943175\n",
      "ep 860: ep_len:3 episode reward: total was 0.000000. running mean: -25.683743\n",
      "ep 860: ep_len:304 episode reward: total was -37.850000. running mean: -25.805406\n",
      "ep 860: ep_len:590 episode reward: total was -24.420000. running mean: -25.791551\n",
      "epsilon:0.282473 episode_count: 6027. steps_count: 2685333.000000\n",
      "ep 861: ep_len:500 episode reward: total was -46.270000. running mean: -25.996336\n",
      "ep 861: ep_len:505 episode reward: total was -28.880000. running mean: -26.025173\n",
      "ep 861: ep_len:615 episode reward: total was -16.580000. running mean: -25.930721\n",
      "ep 861: ep_len:520 episode reward: total was -11.570000. running mean: -25.787114\n",
      "ep 861: ep_len:54 episode reward: total was -1.000000. running mean: -25.539243\n",
      "ep 861: ep_len:620 episode reward: total was -32.140000. running mean: -25.605250\n",
      "ep 861: ep_len:203 episode reward: total was -15.400000. running mean: -25.503198\n",
      "epsilon:0.282337 episode_count: 6034. steps_count: 2688350.000000\n",
      "ep 862: ep_len:610 episode reward: total was -34.360000. running mean: -25.591766\n",
      "ep 862: ep_len:595 episode reward: total was -27.470000. running mean: -25.610548\n",
      "ep 862: ep_len:550 episode reward: total was -23.230000. running mean: -25.586742\n",
      "ep 862: ep_len:530 episode reward: total was -34.640000. running mean: -25.677275\n",
      "ep 862: ep_len:3 episode reward: total was 0.000000. running mean: -25.420502\n",
      "ep 862: ep_len:540 episode reward: total was -34.350000. running mean: -25.509797\n",
      "ep 862: ep_len:590 episode reward: total was -43.090000. running mean: -25.685599\n",
      "epsilon:0.282200 episode_count: 6041. steps_count: 2691768.000000\n",
      "ep 863: ep_len:234 episode reward: total was 0.100000. running mean: -25.427743\n",
      "ep 863: ep_len:535 episode reward: total was -52.110000. running mean: -25.694566\n",
      "ep 863: ep_len:565 episode reward: total was -18.670000. running mean: -25.624320\n",
      "ep 863: ep_len:500 episode reward: total was -33.660000. running mean: -25.704677\n",
      "ep 863: ep_len:3 episode reward: total was 0.000000. running mean: -25.447630\n",
      "ep 863: ep_len:242 episode reward: total was -10.910000. running mean: -25.302254\n",
      "ep 863: ep_len:575 episode reward: total was -34.010000. running mean: -25.389331\n",
      "epsilon:0.282064 episode_count: 6048. steps_count: 2694422.000000\n",
      "ep 864: ep_len:120 episode reward: total was -4.940000. running mean: -25.184838\n",
      "ep 864: ep_len:525 episode reward: total was -26.670000. running mean: -25.199690\n",
      "ep 864: ep_len:585 episode reward: total was -30.890000. running mean: -25.256593\n",
      "ep 864: ep_len:515 episode reward: total was -30.080000. running mean: -25.304827\n",
      "ep 864: ep_len:3 episode reward: total was 0.000000. running mean: -25.051779\n",
      "ep 864: ep_len:555 episode reward: total was -31.220000. running mean: -25.113461\n",
      "ep 864: ep_len:500 episode reward: total was -24.820000. running mean: -25.110526\n",
      "epsilon:0.281927 episode_count: 6055. steps_count: 2697225.000000\n",
      "ep 865: ep_len:580 episode reward: total was -34.050000. running mean: -25.199921\n",
      "ep 865: ep_len:174 episode reward: total was -13.470000. running mean: -25.082622\n",
      "ep 865: ep_len:570 episode reward: total was -27.630000. running mean: -25.108096\n",
      "ep 865: ep_len:510 episode reward: total was -32.280000. running mean: -25.179815\n",
      "ep 865: ep_len:3 episode reward: total was 0.000000. running mean: -24.928016\n",
      "ep 865: ep_len:575 episode reward: total was -26.730000. running mean: -24.946036\n",
      "ep 865: ep_len:510 episode reward: total was -36.270000. running mean: -25.059276\n",
      "epsilon:0.281791 episode_count: 6062. steps_count: 2700147.000000\n",
      "ep 866: ep_len:640 episode reward: total was -35.800000. running mean: -25.166683\n",
      "ep 866: ep_len:525 episode reward: total was -29.480000. running mean: -25.209816\n",
      "ep 866: ep_len:570 episode reward: total was -47.170000. running mean: -25.429418\n",
      "ep 866: ep_len:555 episode reward: total was -21.210000. running mean: -25.387224\n",
      "ep 866: ep_len:3 episode reward: total was 0.000000. running mean: -25.133352\n",
      "ep 866: ep_len:545 episode reward: total was -48.090000. running mean: -25.362918\n",
      "ep 866: ep_len:550 episode reward: total was -35.580000. running mean: -25.465089\n",
      "epsilon:0.281654 episode_count: 6069. steps_count: 2703535.000000\n",
      "ep 867: ep_len:500 episode reward: total was -29.310000. running mean: -25.503538\n",
      "ep 867: ep_len:620 episode reward: total was -22.160000. running mean: -25.470103\n",
      "ep 867: ep_len:670 episode reward: total was -44.490000. running mean: -25.660302\n",
      "ep 867: ep_len:535 episode reward: total was -52.690000. running mean: -25.930599\n",
      "ep 867: ep_len:3 episode reward: total was 0.000000. running mean: -25.671293\n",
      "ep 867: ep_len:610 episode reward: total was -27.410000. running mean: -25.688680\n",
      "ep 867: ep_len:630 episode reward: total was -63.670000. running mean: -26.068493\n",
      "epsilon:0.281518 episode_count: 6076. steps_count: 2707103.000000\n",
      "ep 868: ep_len:535 episode reward: total was -26.290000. running mean: -26.070708\n",
      "ep 868: ep_len:500 episode reward: total was -27.950000. running mean: -26.089501\n",
      "ep 868: ep_len:373 episode reward: total was -5.820000. running mean: -25.886806\n",
      "ep 868: ep_len:515 episode reward: total was -28.610000. running mean: -25.914038\n",
      "ep 868: ep_len:3 episode reward: total was 0.000000. running mean: -25.654898\n",
      "ep 868: ep_len:600 episode reward: total was -24.200000. running mean: -25.640349\n",
      "ep 868: ep_len:510 episode reward: total was -73.370000. running mean: -26.117645\n",
      "epsilon:0.281381 episode_count: 6083. steps_count: 2710139.000000\n",
      "ep 869: ep_len:203 episode reward: total was -29.900000. running mean: -26.155469\n",
      "ep 869: ep_len:555 episode reward: total was -0.760000. running mean: -25.901514\n",
      "ep 869: ep_len:650 episode reward: total was -37.540000. running mean: -26.017899\n",
      "ep 869: ep_len:530 episode reward: total was -42.630000. running mean: -26.184020\n",
      "ep 869: ep_len:3 episode reward: total was 0.000000. running mean: -25.922180\n",
      "ep 869: ep_len:171 episode reward: total was -19.390000. running mean: -25.856858\n",
      "ep 869: ep_len:500 episode reward: total was -29.620000. running mean: -25.894489\n",
      "epsilon:0.281245 episode_count: 6090. steps_count: 2712751.000000\n",
      "ep 870: ep_len:585 episode reward: total was -22.970000. running mean: -25.865244\n",
      "ep 870: ep_len:510 episode reward: total was -17.370000. running mean: -25.780292\n",
      "ep 870: ep_len:745 episode reward: total was -55.340000. running mean: -26.075889\n",
      "ep 870: ep_len:505 episode reward: total was -9.670000. running mean: -25.911830\n",
      "ep 870: ep_len:107 episode reward: total was -17.970000. running mean: -25.832412\n",
      "ep 870: ep_len:680 episode reward: total was -17.780000. running mean: -25.751888\n",
      "ep 870: ep_len:595 episode reward: total was -52.220000. running mean: -26.016569\n",
      "epsilon:0.281108 episode_count: 6097. steps_count: 2716478.000000\n",
      "ep 871: ep_len:134 episode reward: total was -2.450000. running mean: -25.780903\n",
      "ep 871: ep_len:635 episode reward: total was -12.570000. running mean: -25.648794\n",
      "ep 871: ep_len:78 episode reward: total was -2.470000. running mean: -25.417006\n",
      "ep 871: ep_len:520 episode reward: total was -15.120000. running mean: -25.314036\n",
      "ep 871: ep_len:3 episode reward: total was 0.000000. running mean: -25.060896\n",
      "ep 871: ep_len:590 episode reward: total was -41.450000. running mean: -25.224787\n",
      "ep 871: ep_len:510 episode reward: total was -25.850000. running mean: -25.231039\n",
      "epsilon:0.280972 episode_count: 6104. steps_count: 2718948.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 872: ep_len:600 episode reward: total was -23.540000. running mean: -25.214129\n",
      "ep 872: ep_len:540 episode reward: total was -32.160000. running mean: -25.283587\n",
      "ep 872: ep_len:391 episode reward: total was -10.310000. running mean: -25.133851\n",
      "ep 872: ep_len:505 episode reward: total was -10.110000. running mean: -24.983613\n",
      "ep 872: ep_len:91 episode reward: total was 0.530000. running mean: -24.728477\n",
      "ep 872: ep_len:540 episode reward: total was -33.840000. running mean: -24.819592\n",
      "ep 872: ep_len:595 episode reward: total was -23.430000. running mean: -24.805696\n",
      "epsilon:0.280835 episode_count: 6111. steps_count: 2722210.000000\n",
      "ep 873: ep_len:605 episode reward: total was -54.210000. running mean: -25.099739\n",
      "ep 873: ep_len:565 episode reward: total was -33.610000. running mean: -25.184842\n",
      "ep 873: ep_len:68 episode reward: total was -7.480000. running mean: -25.007793\n",
      "ep 873: ep_len:427 episode reward: total was -11.700000. running mean: -24.874715\n",
      "ep 873: ep_len:3 episode reward: total was 0.000000. running mean: -24.625968\n",
      "ep 873: ep_len:550 episode reward: total was -41.900000. running mean: -24.798708\n",
      "ep 873: ep_len:610 episode reward: total was -28.940000. running mean: -24.840121\n",
      "epsilon:0.280699 episode_count: 6118. steps_count: 2725038.000000\n",
      "ep 874: ep_len:254 episode reward: total was -17.420000. running mean: -24.765920\n",
      "ep 874: ep_len:540 episode reward: total was -21.010000. running mean: -24.728361\n",
      "ep 874: ep_len:625 episode reward: total was -31.090000. running mean: -24.791977\n",
      "ep 874: ep_len:132 episode reward: total was 0.100000. running mean: -24.543058\n",
      "ep 874: ep_len:3 episode reward: total was 0.000000. running mean: -24.297627\n",
      "ep 874: ep_len:224 episode reward: total was -11.430000. running mean: -24.168951\n",
      "ep 874: ep_len:500 episode reward: total was -30.980000. running mean: -24.237061\n",
      "epsilon:0.280562 episode_count: 6125. steps_count: 2727316.000000\n",
      "ep 875: ep_len:595 episode reward: total was -35.260000. running mean: -24.347291\n",
      "ep 875: ep_len:500 episode reward: total was -25.070000. running mean: -24.354518\n",
      "ep 875: ep_len:585 episode reward: total was -51.480000. running mean: -24.625773\n",
      "ep 875: ep_len:590 episode reward: total was -38.680000. running mean: -24.766315\n",
      "ep 875: ep_len:113 episode reward: total was -14.460000. running mean: -24.663252\n",
      "ep 875: ep_len:620 episode reward: total was -31.660000. running mean: -24.733219\n",
      "ep 875: ep_len:550 episode reward: total was -41.190000. running mean: -24.897787\n",
      "epsilon:0.280426 episode_count: 6132. steps_count: 2730869.000000\n",
      "ep 876: ep_len:500 episode reward: total was -31.670000. running mean: -24.965509\n",
      "ep 876: ep_len:575 episode reward: total was -40.740000. running mean: -25.123254\n",
      "ep 876: ep_len:500 episode reward: total was -35.510000. running mean: -25.227121\n",
      "ep 876: ep_len:500 episode reward: total was -23.620000. running mean: -25.211050\n",
      "ep 876: ep_len:86 episode reward: total was -10.970000. running mean: -25.068640\n",
      "ep 876: ep_len:545 episode reward: total was -49.730000. running mean: -25.315253\n",
      "ep 876: ep_len:500 episode reward: total was -28.470000. running mean: -25.346801\n",
      "epsilon:0.280289 episode_count: 6139. steps_count: 2734075.000000\n",
      "ep 877: ep_len:530 episode reward: total was -40.440000. running mean: -25.497733\n",
      "ep 877: ep_len:630 episode reward: total was -56.030000. running mean: -25.803055\n",
      "ep 877: ep_len:500 episode reward: total was -17.330000. running mean: -25.718325\n",
      "ep 877: ep_len:540 episode reward: total was -30.550000. running mean: -25.766642\n",
      "ep 877: ep_len:55 episode reward: total was -0.500000. running mean: -25.513975\n",
      "ep 877: ep_len:505 episode reward: total was -56.220000. running mean: -25.821036\n",
      "ep 877: ep_len:565 episode reward: total was -38.060000. running mean: -25.943425\n",
      "epsilon:0.280153 episode_count: 6146. steps_count: 2737400.000000\n",
      "ep 878: ep_len:520 episode reward: total was -23.980000. running mean: -25.923791\n",
      "ep 878: ep_len:505 episode reward: total was -14.880000. running mean: -25.813353\n",
      "ep 878: ep_len:635 episode reward: total was -26.030000. running mean: -25.815519\n",
      "ep 878: ep_len:505 episode reward: total was -10.010000. running mean: -25.657464\n",
      "ep 878: ep_len:3 episode reward: total was 0.000000. running mean: -25.400890\n",
      "ep 878: ep_len:540 episode reward: total was -50.230000. running mean: -25.649181\n",
      "ep 878: ep_len:336 episode reward: total was -20.340000. running mean: -25.596089\n",
      "epsilon:0.280016 episode_count: 6153. steps_count: 2740444.000000\n",
      "ep 879: ep_len:510 episode reward: total was -21.210000. running mean: -25.552228\n",
      "ep 879: ep_len:640 episode reward: total was -55.670000. running mean: -25.853406\n",
      "ep 879: ep_len:635 episode reward: total was -44.440000. running mean: -26.039272\n",
      "ep 879: ep_len:500 episode reward: total was -38.670000. running mean: -26.165579\n",
      "ep 879: ep_len:3 episode reward: total was 0.000000. running mean: -25.903923\n",
      "ep 879: ep_len:500 episode reward: total was -43.700000. running mean: -26.081884\n",
      "ep 879: ep_len:510 episode reward: total was -46.740000. running mean: -26.288465\n",
      "epsilon:0.279880 episode_count: 6160. steps_count: 2743742.000000\n",
      "ep 880: ep_len:540 episode reward: total was -33.000000. running mean: -26.355580\n",
      "ep 880: ep_len:555 episode reward: total was -37.080000. running mean: -26.462825\n",
      "ep 880: ep_len:770 episode reward: total was -66.920000. running mean: -26.867396\n",
      "ep 880: ep_len:505 episode reward: total was -28.170000. running mean: -26.880422\n",
      "ep 880: ep_len:121 episode reward: total was -4.980000. running mean: -26.661418\n",
      "ep 880: ep_len:520 episode reward: total was -38.210000. running mean: -26.776904\n",
      "ep 880: ep_len:193 episode reward: total was -9.890000. running mean: -26.608035\n",
      "epsilon:0.279743 episode_count: 6167. steps_count: 2746946.000000\n",
      "ep 881: ep_len:216 episode reward: total was -25.360000. running mean: -26.595555\n",
      "ep 881: ep_len:500 episode reward: total was -29.350000. running mean: -26.623099\n",
      "ep 881: ep_len:540 episode reward: total was -29.920000. running mean: -26.656068\n",
      "ep 881: ep_len:535 episode reward: total was -13.030000. running mean: -26.519807\n",
      "ep 881: ep_len:3 episode reward: total was 0.000000. running mean: -26.254609\n",
      "ep 881: ep_len:615 episode reward: total was -46.670000. running mean: -26.458763\n",
      "ep 881: ep_len:535 episode reward: total was -38.650000. running mean: -26.580676\n",
      "epsilon:0.279607 episode_count: 6174. steps_count: 2749890.000000\n",
      "ep 882: ep_len:675 episode reward: total was -44.360000. running mean: -26.758469\n",
      "ep 882: ep_len:352 episode reward: total was -17.310000. running mean: -26.663984\n",
      "ep 882: ep_len:650 episode reward: total was -59.960000. running mean: -26.996944\n",
      "ep 882: ep_len:540 episode reward: total was -9.680000. running mean: -26.823775\n",
      "ep 882: ep_len:132 episode reward: total was -14.980000. running mean: -26.705337\n",
      "ep 882: ep_len:520 episode reward: total was -34.570000. running mean: -26.783984\n",
      "ep 882: ep_len:565 episode reward: total was -42.000000. running mean: -26.936144\n",
      "epsilon:0.279470 episode_count: 6181. steps_count: 2753324.000000\n",
      "ep 883: ep_len:540 episode reward: total was -54.130000. running mean: -27.208083\n",
      "ep 883: ep_len:560 episode reward: total was -20.060000. running mean: -27.136602\n",
      "ep 883: ep_len:575 episode reward: total was -44.420000. running mean: -27.309436\n",
      "ep 883: ep_len:540 episode reward: total was -30.190000. running mean: -27.338241\n",
      "ep 883: ep_len:3 episode reward: total was 0.000000. running mean: -27.064859\n",
      "ep 883: ep_len:320 episode reward: total was -3.360000. running mean: -26.827810\n",
      "ep 883: ep_len:530 episode reward: total was -75.410000. running mean: -27.313632\n",
      "epsilon:0.279334 episode_count: 6188. steps_count: 2756392.000000\n",
      "ep 884: ep_len:545 episode reward: total was -30.370000. running mean: -27.344196\n",
      "ep 884: ep_len:570 episode reward: total was -7.730000. running mean: -27.148054\n",
      "ep 884: ep_len:525 episode reward: total was -31.910000. running mean: -27.195673\n",
      "ep 884: ep_len:505 episode reward: total was -46.310000. running mean: -27.386817\n",
      "ep 884: ep_len:3 episode reward: total was 0.000000. running mean: -27.112948\n",
      "ep 884: ep_len:615 episode reward: total was -66.290000. running mean: -27.504719\n",
      "ep 884: ep_len:700 episode reward: total was -72.040000. running mean: -27.950072\n",
      "epsilon:0.279197 episode_count: 6195. steps_count: 2759855.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 885: ep_len:780 episode reward: total was -66.600000. running mean: -28.336571\n",
      "ep 885: ep_len:540 episode reward: total was -29.980000. running mean: -28.353005\n",
      "ep 885: ep_len:79 episode reward: total was -3.970000. running mean: -28.109175\n",
      "ep 885: ep_len:56 episode reward: total was -1.940000. running mean: -27.847484\n",
      "ep 885: ep_len:3 episode reward: total was 0.000000. running mean: -27.569009\n",
      "ep 885: ep_len:625 episode reward: total was -33.710000. running mean: -27.630419\n",
      "ep 885: ep_len:500 episode reward: total was -33.660000. running mean: -27.690714\n",
      "epsilon:0.279061 episode_count: 6202. steps_count: 2762438.000000\n",
      "ep 886: ep_len:505 episode reward: total was -42.320000. running mean: -27.837007\n",
      "ep 886: ep_len:620 episode reward: total was -31.650000. running mean: -27.875137\n",
      "ep 886: ep_len:500 episode reward: total was -33.280000. running mean: -27.929186\n",
      "ep 886: ep_len:500 episode reward: total was -35.290000. running mean: -28.002794\n",
      "ep 886: ep_len:3 episode reward: total was 0.000000. running mean: -27.722766\n",
      "ep 886: ep_len:555 episode reward: total was -38.540000. running mean: -27.830938\n",
      "ep 886: ep_len:610 episode reward: total was -36.010000. running mean: -27.912729\n",
      "epsilon:0.278924 episode_count: 6209. steps_count: 2765731.000000\n",
      "ep 887: ep_len:650 episode reward: total was -38.440000. running mean: -28.018002\n",
      "ep 887: ep_len:500 episode reward: total was -24.900000. running mean: -27.986822\n",
      "ep 887: ep_len:715 episode reward: total was -59.980000. running mean: -28.306754\n",
      "ep 887: ep_len:520 episode reward: total was -45.710000. running mean: -28.480786\n",
      "ep 887: ep_len:52 episode reward: total was -7.000000. running mean: -28.265978\n",
      "ep 887: ep_len:580 episode reward: total was -33.680000. running mean: -28.320118\n",
      "ep 887: ep_len:555 episode reward: total was -38.670000. running mean: -28.423617\n",
      "epsilon:0.278788 episode_count: 6216. steps_count: 2769303.000000\n",
      "ep 888: ep_len:815 episode reward: total was -80.590000. running mean: -28.945281\n",
      "ep 888: ep_len:500 episode reward: total was -6.380000. running mean: -28.719628\n",
      "ep 888: ep_len:575 episode reward: total was -24.670000. running mean: -28.679132\n",
      "ep 888: ep_len:500 episode reward: total was -9.110000. running mean: -28.483441\n",
      "ep 888: ep_len:3 episode reward: total was 0.000000. running mean: -28.198606\n",
      "ep 888: ep_len:500 episode reward: total was -16.820000. running mean: -28.084820\n",
      "ep 888: ep_len:540 episode reward: total was -46.190000. running mean: -28.265872\n",
      "epsilon:0.278651 episode_count: 6223. steps_count: 2772736.000000\n",
      "ep 889: ep_len:565 episode reward: total was -20.720000. running mean: -28.190413\n",
      "ep 889: ep_len:510 episode reward: total was -11.850000. running mean: -28.027009\n",
      "ep 889: ep_len:545 episode reward: total was -48.820000. running mean: -28.234939\n",
      "ep 889: ep_len:595 episode reward: total was -21.550000. running mean: -28.168090\n",
      "ep 889: ep_len:73 episode reward: total was -5.970000. running mean: -27.946109\n",
      "ep 889: ep_len:625 episode reward: total was -60.350000. running mean: -28.270148\n",
      "ep 889: ep_len:510 episode reward: total was -30.990000. running mean: -28.297346\n",
      "epsilon:0.278515 episode_count: 6230. steps_count: 2776159.000000\n",
      "ep 890: ep_len:650 episode reward: total was -40.370000. running mean: -28.418073\n",
      "ep 890: ep_len:550 episode reward: total was -17.540000. running mean: -28.309292\n",
      "ep 890: ep_len:580 episode reward: total was -21.930000. running mean: -28.245499\n",
      "ep 890: ep_len:530 episode reward: total was -28.190000. running mean: -28.244944\n",
      "ep 890: ep_len:3 episode reward: total was 0.000000. running mean: -27.962495\n",
      "ep 890: ep_len:261 episode reward: total was -3.370000. running mean: -27.716570\n",
      "ep 890: ep_len:605 episode reward: total was -31.640000. running mean: -27.755804\n",
      "epsilon:0.278378 episode_count: 6237. steps_count: 2779338.000000\n",
      "ep 891: ep_len:540 episode reward: total was -39.040000. running mean: -27.868646\n",
      "ep 891: ep_len:500 episode reward: total was -50.330000. running mean: -28.093259\n",
      "ep 891: ep_len:570 episode reward: total was -25.140000. running mean: -28.063727\n",
      "ep 891: ep_len:585 episode reward: total was -39.020000. running mean: -28.173290\n",
      "ep 891: ep_len:3 episode reward: total was 0.000000. running mean: -27.891557\n",
      "ep 891: ep_len:550 episode reward: total was -29.490000. running mean: -27.907541\n",
      "ep 891: ep_len:500 episode reward: total was -49.870000. running mean: -28.127166\n",
      "epsilon:0.278242 episode_count: 6244. steps_count: 2782586.000000\n",
      "ep 892: ep_len:134 episode reward: total was -4.440000. running mean: -27.890294\n",
      "ep 892: ep_len:715 episode reward: total was -49.910000. running mean: -28.110491\n",
      "ep 892: ep_len:690 episode reward: total was -61.970000. running mean: -28.449086\n",
      "ep 892: ep_len:600 episode reward: total was -27.100000. running mean: -28.435595\n",
      "ep 892: ep_len:3 episode reward: total was 0.000000. running mean: -28.151239\n",
      "ep 892: ep_len:500 episode reward: total was -17.820000. running mean: -28.047927\n",
      "ep 892: ep_len:500 episode reward: total was -20.480000. running mean: -27.972248\n",
      "epsilon:0.278105 episode_count: 6251. steps_count: 2785728.000000\n",
      "ep 893: ep_len:575 episode reward: total was -40.400000. running mean: -28.096525\n",
      "ep 893: ep_len:505 episode reward: total was -29.420000. running mean: -28.109760\n",
      "ep 893: ep_len:585 episode reward: total was -36.440000. running mean: -28.193062\n",
      "ep 893: ep_len:535 episode reward: total was -26.560000. running mean: -28.176732\n",
      "ep 893: ep_len:3 episode reward: total was 0.000000. running mean: -27.894964\n",
      "ep 893: ep_len:525 episode reward: total was -38.630000. running mean: -28.002315\n",
      "ep 893: ep_len:660 episode reward: total was -36.950000. running mean: -28.091792\n",
      "epsilon:0.277969 episode_count: 6258. steps_count: 2789116.000000\n",
      "ep 894: ep_len:555 episode reward: total was -32.690000. running mean: -28.137774\n",
      "ep 894: ep_len:500 episode reward: total was -38.360000. running mean: -28.239996\n",
      "ep 894: ep_len:855 episode reward: total was -48.220000. running mean: -28.439796\n",
      "ep 894: ep_len:110 episode reward: total was -6.960000. running mean: -28.224998\n",
      "ep 894: ep_len:3 episode reward: total was 0.000000. running mean: -27.942748\n",
      "ep 894: ep_len:585 episode reward: total was -34.680000. running mean: -28.010121\n",
      "ep 894: ep_len:535 episode reward: total was -42.170000. running mean: -28.151719\n",
      "epsilon:0.277832 episode_count: 6265. steps_count: 2792259.000000\n",
      "ep 895: ep_len:580 episode reward: total was -46.210000. running mean: -28.332302\n",
      "ep 895: ep_len:625 episode reward: total was -38.550000. running mean: -28.434479\n",
      "ep 895: ep_len:535 episode reward: total was -64.670000. running mean: -28.796834\n",
      "ep 895: ep_len:500 episode reward: total was -33.250000. running mean: -28.841366\n",
      "ep 895: ep_len:3 episode reward: total was 0.000000. running mean: -28.552952\n",
      "ep 895: ep_len:620 episode reward: total was -59.530000. running mean: -28.862723\n",
      "ep 895: ep_len:535 episode reward: total was -33.190000. running mean: -28.905996\n",
      "epsilon:0.277696 episode_count: 6272. steps_count: 2795657.000000\n",
      "ep 896: ep_len:500 episode reward: total was -21.860000. running mean: -28.835536\n",
      "ep 896: ep_len:500 episode reward: total was -43.590000. running mean: -28.983080\n",
      "ep 896: ep_len:500 episode reward: total was -17.730000. running mean: -28.870550\n",
      "ep 896: ep_len:500 episode reward: total was -30.640000. running mean: -28.888244\n",
      "ep 896: ep_len:43 episode reward: total was -8.000000. running mean: -28.679362\n",
      "ep 896: ep_len:575 episode reward: total was -24.500000. running mean: -28.637568\n",
      "ep 896: ep_len:500 episode reward: total was -24.070000. running mean: -28.591892\n",
      "epsilon:0.277559 episode_count: 6279. steps_count: 2798775.000000\n",
      "ep 897: ep_len:590 episode reward: total was -37.360000. running mean: -28.679573\n",
      "ep 897: ep_len:500 episode reward: total was -22.920000. running mean: -28.621978\n",
      "ep 897: ep_len:575 episode reward: total was -22.660000. running mean: -28.562358\n",
      "ep 897: ep_len:560 episode reward: total was -16.620000. running mean: -28.442934\n",
      "ep 897: ep_len:3 episode reward: total was 0.000000. running mean: -28.158505\n",
      "ep 897: ep_len:620 episode reward: total was -46.610000. running mean: -28.343020\n",
      "ep 897: ep_len:585 episode reward: total was -42.170000. running mean: -28.481290\n",
      "epsilon:0.277423 episode_count: 6286. steps_count: 2802208.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 898: ep_len:217 episode reward: total was -10.430000. running mean: -28.300777\n",
      "ep 898: ep_len:597 episode reward: total was -69.620000. running mean: -28.713969\n",
      "ep 898: ep_len:650 episode reward: total was -36.850000. running mean: -28.795329\n",
      "ep 898: ep_len:530 episode reward: total was -25.550000. running mean: -28.762876\n",
      "ep 898: ep_len:3 episode reward: total was 0.000000. running mean: -28.475247\n",
      "ep 898: ep_len:500 episode reward: total was -27.790000. running mean: -28.468395\n",
      "ep 898: ep_len:256 episode reward: total was -18.900000. running mean: -28.372711\n",
      "epsilon:0.277286 episode_count: 6293. steps_count: 2804961.000000\n",
      "ep 899: ep_len:590 episode reward: total was -20.730000. running mean: -28.296284\n",
      "ep 899: ep_len:535 episode reward: total was -18.880000. running mean: -28.202121\n",
      "ep 899: ep_len:565 episode reward: total was -20.190000. running mean: -28.122000\n",
      "ep 899: ep_len:520 episode reward: total was -37.660000. running mean: -28.217380\n",
      "ep 899: ep_len:3 episode reward: total was 0.000000. running mean: -27.935206\n",
      "ep 899: ep_len:565 episode reward: total was -25.400000. running mean: -27.909854\n",
      "ep 899: ep_len:180 episode reward: total was -22.910000. running mean: -27.859855\n",
      "epsilon:0.277150 episode_count: 6300. steps_count: 2807919.000000\n",
      "ep 900: ep_len:500 episode reward: total was -16.710000. running mean: -27.748357\n",
      "ep 900: ep_len:500 episode reward: total was -13.330000. running mean: -27.604173\n",
      "ep 900: ep_len:645 episode reward: total was -47.670000. running mean: -27.804831\n",
      "ep 900: ep_len:510 episode reward: total was -4.700000. running mean: -27.573783\n",
      "ep 900: ep_len:3 episode reward: total was 0.000000. running mean: -27.298045\n",
      "ep 900: ep_len:333 episode reward: total was -19.870000. running mean: -27.223765\n",
      "ep 900: ep_len:195 episode reward: total was -13.890000. running mean: -27.090427\n",
      "epsilon:0.277013 episode_count: 6307. steps_count: 2810605.000000\n",
      "ep 901: ep_len:585 episode reward: total was -35.830000. running mean: -27.177823\n",
      "ep 901: ep_len:500 episode reward: total was -11.340000. running mean: -27.019445\n",
      "ep 901: ep_len:585 episode reward: total was -62.570000. running mean: -27.374950\n",
      "ep 901: ep_len:525 episode reward: total was -33.180000. running mean: -27.433001\n",
      "ep 901: ep_len:3 episode reward: total was 0.000000. running mean: -27.158671\n",
      "ep 901: ep_len:550 episode reward: total was -35.750000. running mean: -27.244584\n",
      "ep 901: ep_len:291 episode reward: total was -15.390000. running mean: -27.126038\n",
      "epsilon:0.276877 episode_count: 6314. steps_count: 2813644.000000\n",
      "ep 902: ep_len:545 episode reward: total was -22.650000. running mean: -27.081278\n",
      "ep 902: ep_len:620 episode reward: total was -27.050000. running mean: -27.080965\n",
      "ep 902: ep_len:64 episode reward: total was -2.500000. running mean: -26.835155\n",
      "ep 902: ep_len:595 episode reward: total was -31.450000. running mean: -26.881304\n",
      "ep 902: ep_len:86 episode reward: total was -6.460000. running mean: -26.677091\n",
      "ep 902: ep_len:630 episode reward: total was -27.620000. running mean: -26.686520\n",
      "ep 902: ep_len:763 episode reward: total was -66.940000. running mean: -27.089055\n",
      "epsilon:0.276740 episode_count: 6321. steps_count: 2816947.000000\n",
      "ep 903: ep_len:650 episode reward: total was -40.370000. running mean: -27.221864\n",
      "ep 903: ep_len:500 episode reward: total was -42.790000. running mean: -27.377546\n",
      "ep 903: ep_len:555 episode reward: total was -28.390000. running mean: -27.387670\n",
      "ep 903: ep_len:535 episode reward: total was -49.200000. running mean: -27.605793\n",
      "ep 903: ep_len:38 episode reward: total was 2.000000. running mean: -27.309735\n",
      "ep 903: ep_len:620 episode reward: total was -42.770000. running mean: -27.464338\n",
      "ep 903: ep_len:515 episode reward: total was -25.690000. running mean: -27.446595\n",
      "epsilon:0.276604 episode_count: 6328. steps_count: 2820360.000000\n",
      "ep 904: ep_len:125 episode reward: total was -6.960000. running mean: -27.241729\n",
      "ep 904: ep_len:630 episode reward: total was -51.220000. running mean: -27.481511\n",
      "ep 904: ep_len:690 episode reward: total was -55.930000. running mean: -27.765996\n",
      "ep 904: ep_len:500 episode reward: total was -30.110000. running mean: -27.789436\n",
      "ep 904: ep_len:3 episode reward: total was 0.000000. running mean: -27.511542\n",
      "ep 904: ep_len:500 episode reward: total was -25.320000. running mean: -27.489627\n",
      "ep 904: ep_len:535 episode reward: total was -13.400000. running mean: -27.348730\n",
      "epsilon:0.276467 episode_count: 6335. steps_count: 2823343.000000\n",
      "ep 905: ep_len:1010 episode reward: total was -155.330000. running mean: -28.628543\n",
      "ep 905: ep_len:545 episode reward: total was -30.520000. running mean: -28.647458\n",
      "ep 905: ep_len:387 episode reward: total was -24.870000. running mean: -28.609683\n",
      "ep 905: ep_len:550 episode reward: total was -26.670000. running mean: -28.590286\n",
      "ep 905: ep_len:74 episode reward: total was -8.480000. running mean: -28.389183\n",
      "ep 905: ep_len:500 episode reward: total was -22.580000. running mean: -28.331091\n",
      "ep 905: ep_len:550 episode reward: total was -37.660000. running mean: -28.424381\n",
      "epsilon:0.276331 episode_count: 6342. steps_count: 2826959.000000\n",
      "ep 906: ep_len:990 episode reward: total was -101.660000. running mean: -29.156737\n",
      "ep 906: ep_len:515 episode reward: total was -46.110000. running mean: -29.326269\n",
      "ep 906: ep_len:565 episode reward: total was -50.720000. running mean: -29.540207\n",
      "ep 906: ep_len:545 episode reward: total was -37.620000. running mean: -29.621005\n",
      "ep 906: ep_len:119 episode reward: total was -7.950000. running mean: -29.404295\n",
      "ep 906: ep_len:800 episode reward: total was -74.490000. running mean: -29.855152\n",
      "ep 906: ep_len:500 episode reward: total was -25.030000. running mean: -29.806900\n",
      "epsilon:0.276194 episode_count: 6349. steps_count: 2830993.000000\n",
      "ep 907: ep_len:625 episode reward: total was -19.510000. running mean: -29.703931\n",
      "ep 907: ep_len:184 episode reward: total was -14.950000. running mean: -29.556392\n",
      "ep 907: ep_len:416 episode reward: total was -17.900000. running mean: -29.439828\n",
      "ep 907: ep_len:500 episode reward: total was -36.160000. running mean: -29.507030\n",
      "ep 907: ep_len:21 episode reward: total was -2.500000. running mean: -29.236959\n",
      "ep 907: ep_len:525 episode reward: total was -50.080000. running mean: -29.445390\n",
      "ep 907: ep_len:585 episode reward: total was -19.870000. running mean: -29.349636\n",
      "epsilon:0.276058 episode_count: 6356. steps_count: 2833849.000000\n",
      "ep 908: ep_len:645 episode reward: total was -33.030000. running mean: -29.386439\n",
      "ep 908: ep_len:540 episode reward: total was -46.750000. running mean: -29.560075\n",
      "ep 908: ep_len:690 episode reward: total was -38.450000. running mean: -29.648974\n",
      "ep 908: ep_len:510 episode reward: total was -32.160000. running mean: -29.674085\n",
      "ep 908: ep_len:3 episode reward: total was 0.000000. running mean: -29.377344\n",
      "ep 908: ep_len:600 episode reward: total was -35.640000. running mean: -29.439970\n",
      "ep 908: ep_len:555 episode reward: total was -33.520000. running mean: -29.480771\n",
      "epsilon:0.275921 episode_count: 6363. steps_count: 2837392.000000\n",
      "ep 909: ep_len:515 episode reward: total was -24.320000. running mean: -29.429163\n",
      "ep 909: ep_len:500 episode reward: total was -15.410000. running mean: -29.288971\n",
      "ep 909: ep_len:500 episode reward: total was -24.040000. running mean: -29.236482\n",
      "ep 909: ep_len:500 episode reward: total was -35.640000. running mean: -29.300517\n",
      "ep 909: ep_len:1 episode reward: total was 0.000000. running mean: -29.007512\n",
      "ep 909: ep_len:620 episode reward: total was -22.800000. running mean: -28.945436\n",
      "ep 909: ep_len:181 episode reward: total was -19.920000. running mean: -28.855182\n",
      "epsilon:0.275785 episode_count: 6370. steps_count: 2840209.000000\n",
      "ep 910: ep_len:580 episode reward: total was -20.190000. running mean: -28.768530\n",
      "ep 910: ep_len:510 episode reward: total was -9.400000. running mean: -28.574845\n",
      "ep 910: ep_len:427 episode reward: total was -26.850000. running mean: -28.557597\n",
      "ep 910: ep_len:505 episode reward: total was -33.220000. running mean: -28.604221\n",
      "ep 910: ep_len:91 episode reward: total was -3.480000. running mean: -28.352978\n",
      "ep 910: ep_len:595 episode reward: total was -36.880000. running mean: -28.438249\n",
      "ep 910: ep_len:505 episode reward: total was -55.750000. running mean: -28.711366\n",
      "epsilon:0.275648 episode_count: 6377. steps_count: 2843422.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 911: ep_len:590 episode reward: total was -32.680000. running mean: -28.751052\n",
      "ep 911: ep_len:600 episode reward: total was -20.250000. running mean: -28.666042\n",
      "ep 911: ep_len:610 episode reward: total was -41.430000. running mean: -28.793681\n",
      "ep 911: ep_len:565 episode reward: total was -23.660000. running mean: -28.742345\n",
      "ep 911: ep_len:3 episode reward: total was 0.000000. running mean: -28.454921\n",
      "ep 911: ep_len:690 episode reward: total was -17.730000. running mean: -28.347672\n",
      "ep 911: ep_len:615 episode reward: total was -37.470000. running mean: -28.438895\n",
      "epsilon:0.275512 episode_count: 6384. steps_count: 2847095.000000\n",
      "ep 912: ep_len:505 episode reward: total was -19.720000. running mean: -28.351706\n",
      "ep 912: ep_len:555 episode reward: total was -40.220000. running mean: -28.470389\n",
      "ep 912: ep_len:580 episode reward: total was -42.040000. running mean: -28.606085\n",
      "ep 912: ep_len:545 episode reward: total was -16.650000. running mean: -28.486525\n",
      "ep 912: ep_len:76 episode reward: total was -2.470000. running mean: -28.226359\n",
      "ep 912: ep_len:620 episode reward: total was -51.150000. running mean: -28.455596\n",
      "ep 912: ep_len:199 episode reward: total was -19.420000. running mean: -28.365240\n",
      "epsilon:0.275375 episode_count: 6391. steps_count: 2850175.000000\n",
      "ep 913: ep_len:500 episode reward: total was -33.820000. running mean: -28.419787\n",
      "ep 913: ep_len:535 episode reward: total was -26.620000. running mean: -28.401789\n",
      "ep 913: ep_len:515 episode reward: total was -24.020000. running mean: -28.357972\n",
      "ep 913: ep_len:530 episode reward: total was -18.130000. running mean: -28.255692\n",
      "ep 913: ep_len:106 episode reward: total was -1.460000. running mean: -27.987735\n",
      "ep 913: ep_len:520 episode reward: total was -50.130000. running mean: -28.209158\n",
      "ep 913: ep_len:338 episode reward: total was -24.310000. running mean: -28.170166\n",
      "epsilon:0.275239 episode_count: 6398. steps_count: 2853219.000000\n",
      "ep 914: ep_len:575 episode reward: total was -27.770000. running mean: -28.166164\n",
      "ep 914: ep_len:535 episode reward: total was -27.600000. running mean: -28.160503\n",
      "ep 914: ep_len:580 episode reward: total was -24.820000. running mean: -28.127098\n",
      "ep 914: ep_len:560 episode reward: total was -8.630000. running mean: -27.932127\n",
      "ep 914: ep_len:97 episode reward: total was 0.050000. running mean: -27.652305\n",
      "ep 914: ep_len:700 episode reward: total was -49.420000. running mean: -27.869982\n",
      "ep 914: ep_len:500 episode reward: total was -34.250000. running mean: -27.933783\n",
      "epsilon:0.275102 episode_count: 6405. steps_count: 2856766.000000\n",
      "ep 915: ep_len:247 episode reward: total was -17.410000. running mean: -27.828545\n",
      "ep 915: ep_len:555 episode reward: total was -19.220000. running mean: -27.742459\n",
      "ep 915: ep_len:625 episode reward: total was -43.920000. running mean: -27.904235\n",
      "ep 915: ep_len:500 episode reward: total was -33.660000. running mean: -27.961792\n",
      "ep 915: ep_len:107 episode reward: total was 1.540000. running mean: -27.666774\n",
      "ep 915: ep_len:174 episode reward: total was -7.440000. running mean: -27.464507\n",
      "ep 915: ep_len:710 episode reward: total was -54.420000. running mean: -27.734062\n",
      "epsilon:0.274966 episode_count: 6412. steps_count: 2859684.000000\n",
      "ep 916: ep_len:775 episode reward: total was -49.630000. running mean: -27.953021\n",
      "ep 916: ep_len:560 episode reward: total was -17.580000. running mean: -27.849291\n",
      "ep 916: ep_len:580 episode reward: total was -33.070000. running mean: -27.901498\n",
      "ep 916: ep_len:164 episode reward: total was -6.930000. running mean: -27.691783\n",
      "ep 916: ep_len:3 episode reward: total was 0.000000. running mean: -27.414865\n",
      "ep 916: ep_len:510 episode reward: total was -30.630000. running mean: -27.447016\n",
      "ep 916: ep_len:605 episode reward: total was -46.690000. running mean: -27.639446\n",
      "epsilon:0.274829 episode_count: 6419. steps_count: 2862881.000000\n",
      "ep 917: ep_len:590 episode reward: total was -59.020000. running mean: -27.953252\n",
      "ep 917: ep_len:500 episode reward: total was -38.240000. running mean: -28.056119\n",
      "ep 917: ep_len:437 episode reward: total was -36.330000. running mean: -28.138858\n",
      "ep 917: ep_len:530 episode reward: total was -47.180000. running mean: -28.329269\n",
      "ep 917: ep_len:3 episode reward: total was 0.000000. running mean: -28.045977\n",
      "ep 917: ep_len:575 episode reward: total was -31.700000. running mean: -28.082517\n",
      "ep 917: ep_len:610 episode reward: total was -29.690000. running mean: -28.098592\n",
      "epsilon:0.274693 episode_count: 6426. steps_count: 2866126.000000\n",
      "ep 918: ep_len:685 episode reward: total was -41.360000. running mean: -28.231206\n",
      "ep 918: ep_len:640 episode reward: total was -30.740000. running mean: -28.256294\n",
      "ep 918: ep_len:515 episode reward: total was -46.540000. running mean: -28.439131\n",
      "ep 918: ep_len:570 episode reward: total was -62.400000. running mean: -28.778740\n",
      "ep 918: ep_len:106 episode reward: total was -0.450000. running mean: -28.495452\n",
      "ep 918: ep_len:585 episode reward: total was -18.550000. running mean: -28.395998\n",
      "ep 918: ep_len:309 episode reward: total was -22.400000. running mean: -28.336038\n",
      "epsilon:0.274556 episode_count: 6433. steps_count: 2869536.000000\n",
      "ep 919: ep_len:500 episode reward: total was -20.260000. running mean: -28.255277\n",
      "ep 919: ep_len:779 episode reward: total was -88.080000. running mean: -28.853525\n",
      "ep 919: ep_len:545 episode reward: total was -45.510000. running mean: -29.020089\n",
      "ep 919: ep_len:515 episode reward: total was -28.050000. running mean: -29.010388\n",
      "ep 919: ep_len:3 episode reward: total was 0.000000. running mean: -28.720285\n",
      "ep 919: ep_len:700 episode reward: total was -70.940000. running mean: -29.142482\n",
      "ep 919: ep_len:645 episode reward: total was -38.580000. running mean: -29.236857\n",
      "epsilon:0.274420 episode_count: 6440. steps_count: 2873223.000000\n",
      "ep 920: ep_len:134 episode reward: total was -12.450000. running mean: -29.068988\n",
      "ep 920: ep_len:510 episode reward: total was -58.700000. running mean: -29.365298\n",
      "ep 920: ep_len:670 episode reward: total was -31.460000. running mean: -29.386245\n",
      "ep 920: ep_len:500 episode reward: total was -21.130000. running mean: -29.303683\n",
      "ep 920: ep_len:3 episode reward: total was 0.000000. running mean: -29.010646\n",
      "ep 920: ep_len:615 episode reward: total was -38.980000. running mean: -29.110340\n",
      "ep 920: ep_len:340 episode reward: total was -24.900000. running mean: -29.068236\n",
      "epsilon:0.274283 episode_count: 6447. steps_count: 2875995.000000\n",
      "ep 921: ep_len:620 episode reward: total was -20.180000. running mean: -28.979354\n",
      "ep 921: ep_len:655 episode reward: total was -42.650000. running mean: -29.116060\n",
      "ep 921: ep_len:500 episode reward: total was -13.660000. running mean: -28.961500\n",
      "ep 921: ep_len:372 episode reward: total was -31.220000. running mean: -28.984085\n",
      "ep 921: ep_len:3 episode reward: total was 0.000000. running mean: -28.694244\n",
      "ep 921: ep_len:500 episode reward: total was -12.060000. running mean: -28.527902\n",
      "ep 921: ep_len:580 episode reward: total was -27.470000. running mean: -28.517323\n",
      "epsilon:0.274147 episode_count: 6454. steps_count: 2879225.000000\n",
      "ep 922: ep_len:570 episode reward: total was -29.730000. running mean: -28.529449\n",
      "ep 922: ep_len:161 episode reward: total was -15.940000. running mean: -28.403555\n",
      "ep 922: ep_len:500 episode reward: total was -46.690000. running mean: -28.586419\n",
      "ep 922: ep_len:610 episode reward: total was -24.600000. running mean: -28.546555\n",
      "ep 922: ep_len:3 episode reward: total was 0.000000. running mean: -28.261090\n",
      "ep 922: ep_len:575 episode reward: total was -32.200000. running mean: -28.300479\n",
      "ep 922: ep_len:570 episode reward: total was -28.110000. running mean: -28.298574\n",
      "epsilon:0.274010 episode_count: 6461. steps_count: 2882214.000000\n",
      "ep 923: ep_len:500 episode reward: total was -28.860000. running mean: -28.304188\n",
      "ep 923: ep_len:500 episode reward: total was -24.890000. running mean: -28.270046\n",
      "ep 923: ep_len:665 episode reward: total was -55.520000. running mean: -28.542546\n",
      "ep 923: ep_len:555 episode reward: total was -26.270000. running mean: -28.519820\n",
      "ep 923: ep_len:3 episode reward: total was 0.000000. running mean: -28.234622\n",
      "ep 923: ep_len:595 episode reward: total was -30.390000. running mean: -28.256176\n",
      "ep 923: ep_len:525 episode reward: total was -59.620000. running mean: -28.569814\n",
      "epsilon:0.273874 episode_count: 6468. steps_count: 2885557.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 924: ep_len:545 episode reward: total was -33.140000. running mean: -28.615516\n",
      "ep 924: ep_len:500 episode reward: total was -3.410000. running mean: -28.363461\n",
      "ep 924: ep_len:79 episode reward: total was -6.480000. running mean: -28.144626\n",
      "ep 924: ep_len:535 episode reward: total was -27.120000. running mean: -28.134380\n",
      "ep 924: ep_len:3 episode reward: total was 0.000000. running mean: -27.853036\n",
      "ep 924: ep_len:234 episode reward: total was -9.920000. running mean: -27.673706\n",
      "ep 924: ep_len:520 episode reward: total was -29.140000. running mean: -27.688369\n",
      "epsilon:0.273737 episode_count: 6475. steps_count: 2887973.000000\n",
      "ep 925: ep_len:615 episode reward: total was -36.120000. running mean: -27.772685\n",
      "ep 925: ep_len:545 episode reward: total was -41.020000. running mean: -27.905158\n",
      "ep 925: ep_len:635 episode reward: total was -53.630000. running mean: -28.162407\n",
      "ep 925: ep_len:620 episode reward: total was -42.190000. running mean: -28.302683\n",
      "ep 925: ep_len:3 episode reward: total was 0.000000. running mean: -28.019656\n",
      "ep 925: ep_len:615 episode reward: total was -32.230000. running mean: -28.061759\n",
      "ep 925: ep_len:500 episode reward: total was -29.480000. running mean: -28.075942\n",
      "epsilon:0.273601 episode_count: 6482. steps_count: 2891506.000000\n",
      "ep 926: ep_len:530 episode reward: total was -24.110000. running mean: -28.036282\n",
      "ep 926: ep_len:550 episode reward: total was -7.700000. running mean: -27.832919\n",
      "ep 926: ep_len:610 episode reward: total was -54.150000. running mean: -28.096090\n",
      "ep 926: ep_len:50 episode reward: total was -2.960000. running mean: -27.844729\n",
      "ep 926: ep_len:3 episode reward: total was 0.000000. running mean: -27.566282\n",
      "ep 926: ep_len:505 episode reward: total was -19.820000. running mean: -27.488819\n",
      "ep 926: ep_len:500 episode reward: total was -29.730000. running mean: -27.511231\n",
      "epsilon:0.273464 episode_count: 6489. steps_count: 2894254.000000\n",
      "ep 927: ep_len:760 episode reward: total was -56.430000. running mean: -27.800419\n",
      "ep 927: ep_len:520 episode reward: total was -11.710000. running mean: -27.639514\n",
      "ep 927: ep_len:585 episode reward: total was -23.650000. running mean: -27.599619\n",
      "ep 927: ep_len:555 episode reward: total was -23.290000. running mean: -27.556523\n",
      "ep 927: ep_len:3 episode reward: total was 0.000000. running mean: -27.280958\n",
      "ep 927: ep_len:565 episode reward: total was -14.540000. running mean: -27.153548\n",
      "ep 927: ep_len:595 episode reward: total was -44.680000. running mean: -27.328813\n",
      "epsilon:0.273328 episode_count: 6496. steps_count: 2897837.000000\n",
      "ep 928: ep_len:500 episode reward: total was -27.270000. running mean: -27.328225\n",
      "ep 928: ep_len:510 episode reward: total was -29.320000. running mean: -27.348142\n",
      "ep 928: ep_len:555 episode reward: total was -20.150000. running mean: -27.276161\n",
      "ep 928: ep_len:590 episode reward: total was -20.230000. running mean: -27.205699\n",
      "ep 928: ep_len:3 episode reward: total was 0.000000. running mean: -26.933642\n",
      "ep 928: ep_len:500 episode reward: total was -19.660000. running mean: -26.860906\n",
      "ep 928: ep_len:570 episode reward: total was -31.230000. running mean: -26.904597\n",
      "epsilon:0.273191 episode_count: 6503. steps_count: 2901065.000000\n",
      "ep 929: ep_len:505 episode reward: total was -23.520000. running mean: -26.870751\n",
      "ep 929: ep_len:575 episode reward: total was -24.600000. running mean: -26.848043\n",
      "ep 929: ep_len:700 episode reward: total was -48.340000. running mean: -27.062963\n",
      "ep 929: ep_len:510 episode reward: total was -36.120000. running mean: -27.153533\n",
      "ep 929: ep_len:72 episode reward: total was -8.980000. running mean: -26.971798\n",
      "ep 929: ep_len:515 episode reward: total was -25.700000. running mean: -26.959080\n",
      "ep 929: ep_len:590 episode reward: total was -23.490000. running mean: -26.924389\n",
      "epsilon:0.273055 episode_count: 6510. steps_count: 2904532.000000\n",
      "ep 930: ep_len:243 episode reward: total was -7.940000. running mean: -26.734545\n",
      "ep 930: ep_len:520 episode reward: total was -5.230000. running mean: -26.519500\n",
      "ep 930: ep_len:600 episode reward: total was -45.600000. running mean: -26.710305\n",
      "ep 930: ep_len:500 episode reward: total was -36.740000. running mean: -26.810602\n",
      "ep 930: ep_len:3 episode reward: total was 0.000000. running mean: -26.542496\n",
      "ep 930: ep_len:580 episode reward: total was -27.130000. running mean: -26.548371\n",
      "ep 930: ep_len:540 episode reward: total was -47.290000. running mean: -26.755787\n",
      "epsilon:0.272918 episode_count: 6517. steps_count: 2907518.000000\n",
      "ep 931: ep_len:174 episode reward: total was -18.410000. running mean: -26.672329\n",
      "ep 931: ep_len:645 episode reward: total was -46.120000. running mean: -26.866806\n",
      "ep 931: ep_len:580 episode reward: total was -40.430000. running mean: -27.002438\n",
      "ep 931: ep_len:500 episode reward: total was -35.160000. running mean: -27.084014\n",
      "ep 931: ep_len:123 episode reward: total was 2.550000. running mean: -26.787673\n",
      "ep 931: ep_len:565 episode reward: total was -27.650000. running mean: -26.796297\n",
      "ep 931: ep_len:500 episode reward: total was -25.310000. running mean: -26.781434\n",
      "epsilon:0.272782 episode_count: 6524. steps_count: 2910605.000000\n",
      "ep 932: ep_len:710 episode reward: total was -73.490000. running mean: -27.248519\n",
      "ep 932: ep_len:500 episode reward: total was -11.840000. running mean: -27.094434\n",
      "ep 932: ep_len:404 episode reward: total was -20.850000. running mean: -27.031990\n",
      "ep 932: ep_len:500 episode reward: total was -28.160000. running mean: -27.043270\n",
      "ep 932: ep_len:92 episode reward: total was -0.480000. running mean: -26.777637\n",
      "ep 932: ep_len:550 episode reward: total was -28.230000. running mean: -26.792161\n",
      "ep 932: ep_len:358 episode reward: total was -21.320000. running mean: -26.737439\n",
      "epsilon:0.272645 episode_count: 6531. steps_count: 2913719.000000\n",
      "ep 933: ep_len:610 episode reward: total was -26.710000. running mean: -26.737165\n",
      "ep 933: ep_len:500 episode reward: total was -8.900000. running mean: -26.558793\n",
      "ep 933: ep_len:550 episode reward: total was -24.680000. running mean: -26.540005\n",
      "ep 933: ep_len:395 episode reward: total was -14.210000. running mean: -26.416705\n",
      "ep 933: ep_len:41 episode reward: total was 1.000000. running mean: -26.142538\n",
      "ep 933: ep_len:182 episode reward: total was -4.460000. running mean: -25.925713\n",
      "ep 933: ep_len:525 episode reward: total was -35.290000. running mean: -26.019356\n",
      "epsilon:0.272509 episode_count: 6538. steps_count: 2916522.000000\n",
      "ep 934: ep_len:510 episode reward: total was -29.680000. running mean: -26.055962\n",
      "ep 934: ep_len:525 episode reward: total was -39.070000. running mean: -26.186103\n",
      "ep 934: ep_len:555 episode reward: total was -28.320000. running mean: -26.207441\n",
      "ep 934: ep_len:565 episode reward: total was -19.210000. running mean: -26.137467\n",
      "ep 934: ep_len:88 episode reward: total was 6.030000. running mean: -25.815792\n",
      "ep 934: ep_len:289 episode reward: total was -12.370000. running mean: -25.681334\n",
      "ep 934: ep_len:328 episode reward: total was -24.330000. running mean: -25.667821\n",
      "epsilon:0.272372 episode_count: 6545. steps_count: 2919382.000000\n",
      "ep 935: ep_len:660 episode reward: total was -49.900000. running mean: -25.910143\n",
      "ep 935: ep_len:510 episode reward: total was -41.180000. running mean: -26.062841\n",
      "ep 935: ep_len:520 episode reward: total was -28.980000. running mean: -26.092013\n",
      "ep 935: ep_len:412 episode reward: total was -27.180000. running mean: -26.102893\n",
      "ep 935: ep_len:3 episode reward: total was 0.000000. running mean: -25.841864\n",
      "ep 935: ep_len:545 episode reward: total was -27.190000. running mean: -25.855345\n",
      "ep 935: ep_len:630 episode reward: total was -41.420000. running mean: -26.010992\n",
      "epsilon:0.272236 episode_count: 6552. steps_count: 2922662.000000\n",
      "ep 936: ep_len:645 episode reward: total was -20.460000. running mean: -25.955482\n",
      "ep 936: ep_len:665 episode reward: total was -58.470000. running mean: -26.280627\n",
      "ep 936: ep_len:66 episode reward: total was -3.470000. running mean: -26.052521\n",
      "ep 936: ep_len:535 episode reward: total was -43.810000. running mean: -26.230096\n",
      "ep 936: ep_len:45 episode reward: total was -0.490000. running mean: -25.972695\n",
      "ep 936: ep_len:520 episode reward: total was -43.600000. running mean: -26.148968\n",
      "ep 936: ep_len:670 episode reward: total was -42.380000. running mean: -26.311278\n",
      "epsilon:0.272099 episode_count: 6559. steps_count: 2925808.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 937: ep_len:705 episode reward: total was -36.710000. running mean: -26.415265\n",
      "ep 937: ep_len:580 episode reward: total was -30.160000. running mean: -26.452713\n",
      "ep 937: ep_len:580 episode reward: total was -31.250000. running mean: -26.500686\n",
      "ep 937: ep_len:590 episode reward: total was -13.570000. running mean: -26.371379\n",
      "ep 937: ep_len:3 episode reward: total was 0.000000. running mean: -26.107665\n",
      "ep 937: ep_len:555 episode reward: total was -33.210000. running mean: -26.178688\n",
      "ep 937: ep_len:670 episode reward: total was -39.380000. running mean: -26.310701\n",
      "epsilon:0.271963 episode_count: 6566. steps_count: 2929491.000000\n",
      "ep 938: ep_len:560 episode reward: total was -21.650000. running mean: -26.264094\n",
      "ep 938: ep_len:525 episode reward: total was -34.540000. running mean: -26.346853\n",
      "ep 938: ep_len:595 episode reward: total was -34.310000. running mean: -26.426485\n",
      "ep 938: ep_len:515 episode reward: total was -20.700000. running mean: -26.369220\n",
      "ep 938: ep_len:3 episode reward: total was 0.000000. running mean: -26.105528\n",
      "ep 938: ep_len:520 episode reward: total was -33.230000. running mean: -26.176773\n",
      "ep 938: ep_len:505 episode reward: total was -42.090000. running mean: -26.335905\n",
      "epsilon:0.271826 episode_count: 6573. steps_count: 2932714.000000\n",
      "ep 939: ep_len:555 episode reward: total was -31.120000. running mean: -26.383746\n",
      "ep 939: ep_len:500 episode reward: total was -16.070000. running mean: -26.280608\n",
      "ep 939: ep_len:690 episode reward: total was -37.810000. running mean: -26.395902\n",
      "ep 939: ep_len:500 episode reward: total was -26.560000. running mean: -26.397543\n",
      "ep 939: ep_len:3 episode reward: total was 0.000000. running mean: -26.133568\n",
      "ep 939: ep_len:590 episode reward: total was -22.140000. running mean: -26.093632\n",
      "ep 939: ep_len:585 episode reward: total was -26.450000. running mean: -26.097196\n",
      "epsilon:0.271690 episode_count: 6580. steps_count: 2936137.000000\n",
      "ep 940: ep_len:545 episode reward: total was -21.020000. running mean: -26.046424\n",
      "ep 940: ep_len:500 episode reward: total was -27.860000. running mean: -26.064560\n",
      "ep 940: ep_len:515 episode reward: total was -27.540000. running mean: -26.079314\n",
      "ep 940: ep_len:401 episode reward: total was -16.230000. running mean: -25.980821\n",
      "ep 940: ep_len:3 episode reward: total was 0.000000. running mean: -25.721013\n",
      "ep 940: ep_len:515 episode reward: total was -50.870000. running mean: -25.972503\n",
      "ep 940: ep_len:299 episode reward: total was -11.830000. running mean: -25.831078\n",
      "epsilon:0.271553 episode_count: 6587. steps_count: 2938915.000000\n",
      "ep 941: ep_len:118 episode reward: total was -7.470000. running mean: -25.647467\n",
      "ep 941: ep_len:580 episode reward: total was -12.220000. running mean: -25.513192\n",
      "ep 941: ep_len:620 episode reward: total was -33.010000. running mean: -25.588160\n",
      "ep 941: ep_len:525 episode reward: total was -11.540000. running mean: -25.447679\n",
      "ep 941: ep_len:3 episode reward: total was 0.000000. running mean: -25.193202\n",
      "ep 941: ep_len:540 episode reward: total was -36.580000. running mean: -25.307070\n",
      "ep 941: ep_len:500 episode reward: total was -21.720000. running mean: -25.271199\n",
      "epsilon:0.271417 episode_count: 6594. steps_count: 2941801.000000\n",
      "ep 942: ep_len:218 episode reward: total was -26.890000. running mean: -25.287387\n",
      "ep 942: ep_len:530 episode reward: total was -47.130000. running mean: -25.505813\n",
      "ep 942: ep_len:500 episode reward: total was -35.730000. running mean: -25.608055\n",
      "ep 942: ep_len:500 episode reward: total was -27.110000. running mean: -25.623074\n",
      "ep 942: ep_len:87 episode reward: total was -12.960000. running mean: -25.496444\n",
      "ep 942: ep_len:186 episode reward: total was -15.430000. running mean: -25.395779\n",
      "ep 942: ep_len:725 episode reward: total was -56.350000. running mean: -25.705322\n",
      "epsilon:0.271280 episode_count: 6601. steps_count: 2944547.000000\n",
      "ep 943: ep_len:500 episode reward: total was -34.860000. running mean: -25.796868\n",
      "ep 943: ep_len:510 episode reward: total was -15.790000. running mean: -25.696800\n",
      "ep 943: ep_len:620 episode reward: total was -16.620000. running mean: -25.606032\n",
      "ep 943: ep_len:590 episode reward: total was -22.640000. running mean: -25.576371\n",
      "ep 943: ep_len:100 episode reward: total was -9.460000. running mean: -25.415208\n",
      "ep 943: ep_len:525 episode reward: total was -28.620000. running mean: -25.447256\n",
      "ep 943: ep_len:575 episode reward: total was -47.040000. running mean: -25.663183\n",
      "epsilon:0.271144 episode_count: 6608. steps_count: 2947967.000000\n",
      "ep 944: ep_len:500 episode reward: total was -40.270000. running mean: -25.809251\n",
      "ep 944: ep_len:535 episode reward: total was -13.780000. running mean: -25.688959\n",
      "ep 944: ep_len:610 episode reward: total was -50.010000. running mean: -25.932169\n",
      "ep 944: ep_len:615 episode reward: total was -43.130000. running mean: -26.104147\n",
      "ep 944: ep_len:98 episode reward: total was -6.960000. running mean: -25.912706\n",
      "ep 944: ep_len:545 episode reward: total was -38.540000. running mean: -26.038979\n",
      "ep 944: ep_len:560 episode reward: total was -56.240000. running mean: -26.340989\n",
      "epsilon:0.271007 episode_count: 6615. steps_count: 2951430.000000\n",
      "ep 945: ep_len:570 episode reward: total was -29.170000. running mean: -26.369279\n",
      "ep 945: ep_len:500 episode reward: total was -13.840000. running mean: -26.243986\n",
      "ep 945: ep_len:560 episode reward: total was -45.500000. running mean: -26.436546\n",
      "ep 945: ep_len:42 episode reward: total was 0.550000. running mean: -26.166681\n",
      "ep 945: ep_len:3 episode reward: total was 0.000000. running mean: -25.905014\n",
      "ep 945: ep_len:500 episode reward: total was -27.770000. running mean: -25.923664\n",
      "ep 945: ep_len:515 episode reward: total was -43.230000. running mean: -26.096727\n",
      "epsilon:0.270871 episode_count: 6622. steps_count: 2954120.000000\n",
      "ep 946: ep_len:660 episode reward: total was -59.060000. running mean: -26.426360\n",
      "ep 946: ep_len:575 episode reward: total was -15.730000. running mean: -26.319397\n",
      "ep 946: ep_len:645 episode reward: total was -22.490000. running mean: -26.281103\n",
      "ep 946: ep_len:605 episode reward: total was -39.190000. running mean: -26.410192\n",
      "ep 946: ep_len:3 episode reward: total was 0.000000. running mean: -26.146090\n",
      "ep 946: ep_len:570 episode reward: total was -19.100000. running mean: -26.075629\n",
      "ep 946: ep_len:705 episode reward: total was -48.920000. running mean: -26.304072\n",
      "epsilon:0.270734 episode_count: 6629. steps_count: 2957883.000000\n",
      "ep 947: ep_len:198 episode reward: total was -3.900000. running mean: -26.080032\n",
      "ep 947: ep_len:530 episode reward: total was -28.850000. running mean: -26.107731\n",
      "ep 947: ep_len:660 episode reward: total was -45.560000. running mean: -26.302254\n",
      "ep 947: ep_len:535 episode reward: total was -10.140000. running mean: -26.140632\n",
      "ep 947: ep_len:3 episode reward: total was 0.000000. running mean: -25.879225\n",
      "ep 947: ep_len:825 episode reward: total was -74.420000. running mean: -26.364633\n",
      "ep 947: ep_len:680 episode reward: total was -59.510000. running mean: -26.696087\n",
      "epsilon:0.270598 episode_count: 6636. steps_count: 2961314.000000\n",
      "ep 948: ep_len:695 episode reward: total was -71.940000. running mean: -27.148526\n",
      "ep 948: ep_len:600 episode reward: total was -36.170000. running mean: -27.238741\n",
      "ep 948: ep_len:605 episode reward: total was -34.620000. running mean: -27.312553\n",
      "ep 948: ep_len:535 episode reward: total was -27.180000. running mean: -27.311228\n",
      "ep 948: ep_len:3 episode reward: total was 0.000000. running mean: -27.038115\n",
      "ep 948: ep_len:640 episode reward: total was -88.810000. running mean: -27.655834\n",
      "ep 948: ep_len:570 episode reward: total was -30.130000. running mean: -27.680576\n",
      "epsilon:0.270461 episode_count: 6643. steps_count: 2964962.000000\n",
      "ep 949: ep_len:605 episode reward: total was -14.630000. running mean: -27.550070\n",
      "ep 949: ep_len:500 episode reward: total was -27.890000. running mean: -27.553469\n",
      "ep 949: ep_len:625 episode reward: total was -21.800000. running mean: -27.495935\n",
      "ep 949: ep_len:115 episode reward: total was -6.450000. running mean: -27.285475\n",
      "ep 949: ep_len:2 episode reward: total was 0.000000. running mean: -27.012621\n",
      "ep 949: ep_len:530 episode reward: total was -36.250000. running mean: -27.104994\n",
      "ep 949: ep_len:505 episode reward: total was -38.200000. running mean: -27.215944\n",
      "epsilon:0.270325 episode_count: 6650. steps_count: 2967844.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 950: ep_len:209 episode reward: total was -2.410000. running mean: -26.967885\n",
      "ep 950: ep_len:655 episode reward: total was -11.550000. running mean: -26.813706\n",
      "ep 950: ep_len:580 episode reward: total was -46.690000. running mean: -27.012469\n",
      "ep 950: ep_len:530 episode reward: total was -12.650000. running mean: -26.868844\n",
      "ep 950: ep_len:118 episode reward: total was -5.970000. running mean: -26.659856\n",
      "ep 950: ep_len:600 episode reward: total was -17.580000. running mean: -26.569057\n",
      "ep 950: ep_len:555 episode reward: total was -27.580000. running mean: -26.579167\n",
      "epsilon:0.270188 episode_count: 6657. steps_count: 2971091.000000\n",
      "ep 951: ep_len:655 episode reward: total was -31.440000. running mean: -26.627775\n",
      "ep 951: ep_len:257 episode reward: total was -22.470000. running mean: -26.586197\n",
      "ep 951: ep_len:500 episode reward: total was -22.960000. running mean: -26.549935\n",
      "ep 951: ep_len:382 episode reward: total was -39.750000. running mean: -26.681936\n",
      "ep 951: ep_len:91 episode reward: total was 3.040000. running mean: -26.384717\n",
      "ep 951: ep_len:308 episode reward: total was -10.420000. running mean: -26.225070\n",
      "ep 951: ep_len:319 episode reward: total was -22.850000. running mean: -26.191319\n",
      "epsilon:0.270052 episode_count: 6664. steps_count: 2973603.000000\n",
      "ep 952: ep_len:555 episode reward: total was -33.050000. running mean: -26.259906\n",
      "ep 952: ep_len:545 episode reward: total was 5.870000. running mean: -25.938607\n",
      "ep 952: ep_len:565 episode reward: total was -28.480000. running mean: -25.964021\n",
      "ep 952: ep_len:500 episode reward: total was -18.970000. running mean: -25.894080\n",
      "ep 952: ep_len:3 episode reward: total was 0.000000. running mean: -25.635140\n",
      "ep 952: ep_len:500 episode reward: total was -49.650000. running mean: -25.875288\n",
      "ep 952: ep_len:515 episode reward: total was -22.140000. running mean: -25.837935\n",
      "epsilon:0.269915 episode_count: 6671. steps_count: 2976786.000000\n",
      "ep 953: ep_len:650 episode reward: total was -9.890000. running mean: -25.678456\n",
      "ep 953: ep_len:515 episode reward: total was -33.540000. running mean: -25.757071\n",
      "ep 953: ep_len:530 episode reward: total was -26.180000. running mean: -25.761301\n",
      "ep 953: ep_len:750 episode reward: total was -48.280000. running mean: -25.986488\n",
      "ep 953: ep_len:54 episode reward: total was -2.000000. running mean: -25.746623\n",
      "ep 953: ep_len:580 episode reward: total was -29.630000. running mean: -25.785457\n",
      "ep 953: ep_len:640 episode reward: total was -48.450000. running mean: -26.012102\n",
      "epsilon:0.269779 episode_count: 6678. steps_count: 2980505.000000\n",
      "ep 954: ep_len:236 episode reward: total was -5.930000. running mean: -25.811281\n",
      "ep 954: ep_len:635 episode reward: total was -34.460000. running mean: -25.897768\n",
      "ep 954: ep_len:740 episode reward: total was -52.790000. running mean: -26.166690\n",
      "ep 954: ep_len:404 episode reward: total was -32.150000. running mean: -26.226524\n",
      "ep 954: ep_len:88 episode reward: total was -11.950000. running mean: -26.083758\n",
      "ep 954: ep_len:625 episode reward: total was -26.560000. running mean: -26.088521\n",
      "ep 954: ep_len:500 episode reward: total was -31.660000. running mean: -26.144235\n",
      "epsilon:0.269642 episode_count: 6685. steps_count: 2983733.000000\n",
      "ep 955: ep_len:258 episode reward: total was -1.350000. running mean: -25.896293\n",
      "ep 955: ep_len:505 episode reward: total was -20.560000. running mean: -25.842930\n",
      "ep 955: ep_len:580 episode reward: total was -31.620000. running mean: -25.900701\n",
      "ep 955: ep_len:56 episode reward: total was -4.450000. running mean: -25.686194\n",
      "ep 955: ep_len:92 episode reward: total was -6.950000. running mean: -25.498832\n",
      "ep 955: ep_len:530 episode reward: total was -31.600000. running mean: -25.559844\n",
      "ep 955: ep_len:500 episode reward: total was -26.070000. running mean: -25.564945\n",
      "epsilon:0.269506 episode_count: 6692. steps_count: 2986254.000000\n",
      "ep 956: ep_len:665 episode reward: total was -37.370000. running mean: -25.682996\n",
      "ep 956: ep_len:595 episode reward: total was -10.420000. running mean: -25.530366\n",
      "ep 956: ep_len:79 episode reward: total was -5.990000. running mean: -25.334962\n",
      "ep 956: ep_len:640 episode reward: total was -38.900000. running mean: -25.470613\n",
      "ep 956: ep_len:3 episode reward: total was 0.000000. running mean: -25.215906\n",
      "ep 956: ep_len:505 episode reward: total was -58.790000. running mean: -25.551647\n",
      "ep 956: ep_len:500 episode reward: total was -24.510000. running mean: -25.541231\n",
      "epsilon:0.269369 episode_count: 6699. steps_count: 2989241.000000\n",
      "ep 957: ep_len:203 episode reward: total was -5.410000. running mean: -25.339919\n",
      "ep 957: ep_len:500 episode reward: total was -36.120000. running mean: -25.447719\n",
      "ep 957: ep_len:442 episode reward: total was -26.300000. running mean: -25.456242\n",
      "ep 957: ep_len:109 episode reward: total was -0.940000. running mean: -25.211080\n",
      "ep 957: ep_len:3 episode reward: total was 0.000000. running mean: -24.958969\n",
      "ep 957: ep_len:505 episode reward: total was -31.470000. running mean: -25.024079\n",
      "ep 957: ep_len:670 episode reward: total was -49.050000. running mean: -25.264338\n",
      "epsilon:0.269233 episode_count: 6706. steps_count: 2991673.000000\n",
      "ep 958: ep_len:575 episode reward: total was -28.780000. running mean: -25.299495\n",
      "ep 958: ep_len:510 episode reward: total was -5.340000. running mean: -25.099900\n",
      "ep 958: ep_len:555 episode reward: total was -34.940000. running mean: -25.198301\n",
      "ep 958: ep_len:590 episode reward: total was -20.170000. running mean: -25.148018\n",
      "ep 958: ep_len:3 episode reward: total was 0.000000. running mean: -24.896538\n",
      "ep 958: ep_len:505 episode reward: total was -24.100000. running mean: -24.888573\n",
      "ep 958: ep_len:500 episode reward: total was -30.560000. running mean: -24.945287\n",
      "epsilon:0.269096 episode_count: 6713. steps_count: 2994911.000000\n",
      "ep 959: ep_len:610 episode reward: total was -61.880000. running mean: -25.314634\n",
      "ep 959: ep_len:505 episode reward: total was -34.080000. running mean: -25.402288\n",
      "ep 959: ep_len:460 episode reward: total was -22.320000. running mean: -25.371465\n",
      "ep 959: ep_len:500 episode reward: total was -37.180000. running mean: -25.489550\n",
      "ep 959: ep_len:3 episode reward: total was 0.000000. running mean: -25.234655\n",
      "ep 959: ep_len:570 episode reward: total was -30.720000. running mean: -25.289508\n",
      "ep 959: ep_len:275 episode reward: total was -21.360000. running mean: -25.250213\n",
      "epsilon:0.268960 episode_count: 6720. steps_count: 2997834.000000\n",
      "ep 960: ep_len:580 episode reward: total was -42.910000. running mean: -25.426811\n",
      "ep 960: ep_len:540 episode reward: total was -27.040000. running mean: -25.442943\n",
      "ep 960: ep_len:615 episode reward: total was -64.260000. running mean: -25.831113\n",
      "ep 960: ep_len:500 episode reward: total was -20.260000. running mean: -25.775402\n",
      "ep 960: ep_len:3 episode reward: total was 0.000000. running mean: -25.517648\n",
      "ep 960: ep_len:500 episode reward: total was -17.700000. running mean: -25.439472\n",
      "ep 960: ep_len:525 episode reward: total was -29.950000. running mean: -25.484577\n",
      "epsilon:0.268823 episode_count: 6727. steps_count: 3001097.000000\n",
      "ep 961: ep_len:500 episode reward: total was -28.320000. running mean: -25.512931\n",
      "ep 961: ep_len:270 episode reward: total was -17.420000. running mean: -25.432002\n",
      "ep 961: ep_len:570 episode reward: total was -37.810000. running mean: -25.555782\n",
      "ep 961: ep_len:515 episode reward: total was -28.210000. running mean: -25.582324\n",
      "ep 961: ep_len:131 episode reward: total was 2.060000. running mean: -25.305901\n",
      "ep 961: ep_len:635 episode reward: total was -34.480000. running mean: -25.397642\n",
      "ep 961: ep_len:565 episode reward: total was -51.780000. running mean: -25.661465\n",
      "epsilon:0.268687 episode_count: 6734. steps_count: 3004283.000000\n",
      "ep 962: ep_len:565 episode reward: total was -43.020000. running mean: -25.835051\n",
      "ep 962: ep_len:341 episode reward: total was -29.910000. running mean: -25.875800\n",
      "ep 962: ep_len:500 episode reward: total was -12.200000. running mean: -25.739042\n",
      "ep 962: ep_len:505 episode reward: total was -25.630000. running mean: -25.737952\n",
      "ep 962: ep_len:3 episode reward: total was 0.000000. running mean: -25.480572\n",
      "ep 962: ep_len:535 episode reward: total was -24.530000. running mean: -25.471067\n",
      "ep 962: ep_len:590 episode reward: total was -32.670000. running mean: -25.543056\n",
      "epsilon:0.268550 episode_count: 6741. steps_count: 3007322.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 963: ep_len:93 episode reward: total was -0.480000. running mean: -25.292425\n",
      "ep 963: ep_len:530 episode reward: total was -56.660000. running mean: -25.606101\n",
      "ep 963: ep_len:400 episode reward: total was -22.400000. running mean: -25.574040\n",
      "ep 963: ep_len:102 episode reward: total was -0.450000. running mean: -25.322800\n",
      "ep 963: ep_len:82 episode reward: total was -13.980000. running mean: -25.209372\n",
      "ep 963: ep_len:261 episode reward: total was -7.400000. running mean: -25.031278\n",
      "ep 963: ep_len:315 episode reward: total was -21.370000. running mean: -24.994665\n",
      "epsilon:0.268414 episode_count: 6748. steps_count: 3009105.000000\n",
      "ep 964: ep_len:223 episode reward: total was -10.930000. running mean: -24.854019\n",
      "ep 964: ep_len:285 episode reward: total was -20.410000. running mean: -24.809578\n",
      "ep 964: ep_len:500 episode reward: total was -40.600000. running mean: -24.967483\n",
      "ep 964: ep_len:500 episode reward: total was -26.100000. running mean: -24.978808\n",
      "ep 964: ep_len:3 episode reward: total was 0.000000. running mean: -24.729020\n",
      "ep 964: ep_len:186 episode reward: total was -2.430000. running mean: -24.506029\n",
      "ep 964: ep_len:540 episode reward: total was -26.940000. running mean: -24.530369\n",
      "epsilon:0.268277 episode_count: 6755. steps_count: 3011342.000000\n",
      "ep 965: ep_len:242 episode reward: total was -9.910000. running mean: -24.384165\n",
      "ep 965: ep_len:185 episode reward: total was -7.440000. running mean: -24.214724\n",
      "ep 965: ep_len:79 episode reward: total was -5.960000. running mean: -24.032177\n",
      "ep 965: ep_len:170 episode reward: total was -5.370000. running mean: -23.845555\n",
      "ep 965: ep_len:102 episode reward: total was -3.970000. running mean: -23.646799\n",
      "ep 965: ep_len:545 episode reward: total was -24.650000. running mean: -23.656831\n",
      "ep 965: ep_len:505 episode reward: total was -33.570000. running mean: -23.755963\n",
      "epsilon:0.268141 episode_count: 6762. steps_count: 3013170.000000\n",
      "ep 966: ep_len:109 episode reward: total was 0.040000. running mean: -23.518003\n",
      "ep 966: ep_len:615 episode reward: total was -54.560000. running mean: -23.828423\n",
      "ep 966: ep_len:580 episode reward: total was -25.100000. running mean: -23.841139\n",
      "ep 966: ep_len:525 episode reward: total was -19.760000. running mean: -23.800328\n",
      "ep 966: ep_len:3 episode reward: total was 0.000000. running mean: -23.562324\n",
      "ep 966: ep_len:500 episode reward: total was -10.160000. running mean: -23.428301\n",
      "ep 966: ep_len:560 episode reward: total was -43.640000. running mean: -23.630418\n",
      "epsilon:0.268004 episode_count: 6769. steps_count: 3016062.000000\n",
      "ep 967: ep_len:570 episode reward: total was -57.560000. running mean: -23.969714\n",
      "ep 967: ep_len:575 episode reward: total was -37.010000. running mean: -24.100117\n",
      "ep 967: ep_len:615 episode reward: total was -38.380000. running mean: -24.242916\n",
      "ep 967: ep_len:500 episode reward: total was -26.180000. running mean: -24.262286\n",
      "ep 967: ep_len:3 episode reward: total was 0.000000. running mean: -24.019664\n",
      "ep 967: ep_len:505 episode reward: total was -54.210000. running mean: -24.321567\n",
      "ep 967: ep_len:635 episode reward: total was -39.120000. running mean: -24.469551\n",
      "epsilon:0.267868 episode_count: 6776. steps_count: 3019465.000000\n",
      "ep 968: ep_len:575 episode reward: total was -53.540000. running mean: -24.760256\n",
      "ep 968: ep_len:515 episode reward: total was -59.220000. running mean: -25.104853\n",
      "ep 968: ep_len:500 episode reward: total was -40.890000. running mean: -25.262705\n",
      "ep 968: ep_len:118 episode reward: total was -2.930000. running mean: -25.039378\n",
      "ep 968: ep_len:3 episode reward: total was 0.000000. running mean: -24.788984\n",
      "ep 968: ep_len:530 episode reward: total was -36.140000. running mean: -24.902494\n",
      "ep 968: ep_len:545 episode reward: total was -43.560000. running mean: -25.089069\n",
      "epsilon:0.267731 episode_count: 6783. steps_count: 3022251.000000\n",
      "ep 969: ep_len:540 episode reward: total was -44.080000. running mean: -25.278978\n",
      "ep 969: ep_len:515 episode reward: total was -33.090000. running mean: -25.357089\n",
      "ep 969: ep_len:515 episode reward: total was -38.140000. running mean: -25.484918\n",
      "ep 969: ep_len:605 episode reward: total was -32.180000. running mean: -25.551869\n",
      "ep 969: ep_len:3 episode reward: total was 0.000000. running mean: -25.296350\n",
      "ep 969: ep_len:227 episode reward: total was -7.960000. running mean: -25.122986\n",
      "ep 969: ep_len:525 episode reward: total was -42.270000. running mean: -25.294457\n",
      "epsilon:0.267595 episode_count: 6790. steps_count: 3025181.000000\n",
      "ep 970: ep_len:540 episode reward: total was -23.990000. running mean: -25.281412\n",
      "ep 970: ep_len:525 episode reward: total was -17.370000. running mean: -25.202298\n",
      "ep 970: ep_len:635 episode reward: total was -44.120000. running mean: -25.391475\n",
      "ep 970: ep_len:500 episode reward: total was -39.750000. running mean: -25.535060\n",
      "ep 970: ep_len:3 episode reward: total was 0.000000. running mean: -25.279710\n",
      "ep 970: ep_len:675 episode reward: total was -57.580000. running mean: -25.602712\n",
      "ep 970: ep_len:500 episode reward: total was -36.570000. running mean: -25.712385\n",
      "epsilon:0.267458 episode_count: 6797. steps_count: 3028559.000000\n",
      "ep 971: ep_len:530 episode reward: total was -23.070000. running mean: -25.685961\n",
      "ep 971: ep_len:256 episode reward: total was -18.370000. running mean: -25.612802\n",
      "ep 971: ep_len:580 episode reward: total was -33.880000. running mean: -25.695474\n",
      "ep 971: ep_len:530 episode reward: total was -33.200000. running mean: -25.770519\n",
      "ep 971: ep_len:133 episode reward: total was -0.950000. running mean: -25.522314\n",
      "ep 971: ep_len:500 episode reward: total was -42.390000. running mean: -25.690991\n",
      "ep 971: ep_len:550 episode reward: total was -48.050000. running mean: -25.914581\n",
      "epsilon:0.267322 episode_count: 6804. steps_count: 3031638.000000\n",
      "ep 972: ep_len:625 episode reward: total was -50.920000. running mean: -26.164635\n",
      "ep 972: ep_len:680 episode reward: total was -20.050000. running mean: -26.103489\n",
      "ep 972: ep_len:570 episode reward: total was -34.960000. running mean: -26.192054\n",
      "ep 972: ep_len:500 episode reward: total was -28.190000. running mean: -26.212033\n",
      "ep 972: ep_len:3 episode reward: total was 0.000000. running mean: -25.949913\n",
      "ep 972: ep_len:625 episode reward: total was -26.190000. running mean: -25.952314\n",
      "ep 972: ep_len:640 episode reward: total was -39.580000. running mean: -26.088591\n",
      "epsilon:0.267185 episode_count: 6811. steps_count: 3035281.000000\n",
      "ep 973: ep_len:540 episode reward: total was -18.030000. running mean: -26.008005\n",
      "ep 973: ep_len:600 episode reward: total was -28.100000. running mean: -26.028925\n",
      "ep 973: ep_len:585 episode reward: total was -21.320000. running mean: -25.981835\n",
      "ep 973: ep_len:500 episode reward: total was -17.100000. running mean: -25.893017\n",
      "ep 973: ep_len:3 episode reward: total was 0.000000. running mean: -25.634087\n",
      "ep 973: ep_len:505 episode reward: total was -38.850000. running mean: -25.766246\n",
      "ep 973: ep_len:185 episode reward: total was -13.900000. running mean: -25.647584\n",
      "epsilon:0.267049 episode_count: 6818. steps_count: 3038199.000000\n",
      "ep 974: ep_len:500 episode reward: total was -17.320000. running mean: -25.564308\n",
      "ep 974: ep_len:505 episode reward: total was -28.130000. running mean: -25.589965\n",
      "ep 974: ep_len:79 episode reward: total was -4.980000. running mean: -25.383865\n",
      "ep 974: ep_len:535 episode reward: total was -20.120000. running mean: -25.331226\n",
      "ep 974: ep_len:3 episode reward: total was 0.000000. running mean: -25.077914\n",
      "ep 974: ep_len:620 episode reward: total was -18.850000. running mean: -25.015635\n",
      "ep 974: ep_len:525 episode reward: total was -33.530000. running mean: -25.100779\n",
      "epsilon:0.266912 episode_count: 6825. steps_count: 3040966.000000\n",
      "ep 975: ep_len:229 episode reward: total was -9.920000. running mean: -24.948971\n",
      "ep 975: ep_len:500 episode reward: total was -7.920000. running mean: -24.778681\n",
      "ep 975: ep_len:520 episode reward: total was -27.000000. running mean: -24.800894\n",
      "ep 975: ep_len:575 episode reward: total was -36.620000. running mean: -24.919085\n",
      "ep 975: ep_len:84 episode reward: total was -11.960000. running mean: -24.789495\n",
      "ep 975: ep_len:505 episode reward: total was -32.890000. running mean: -24.870500\n",
      "ep 975: ep_len:211 episode reward: total was -17.900000. running mean: -24.800795\n",
      "epsilon:0.266776 episode_count: 6832. steps_count: 3043590.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 976: ep_len:580 episode reward: total was -58.030000. running mean: -25.133087\n",
      "ep 976: ep_len:500 episode reward: total was -33.920000. running mean: -25.220956\n",
      "ep 976: ep_len:535 episode reward: total was -12.180000. running mean: -25.090546\n",
      "ep 976: ep_len:525 episode reward: total was -37.730000. running mean: -25.216941\n",
      "ep 976: ep_len:131 episode reward: total was 6.060000. running mean: -24.904171\n",
      "ep 976: ep_len:500 episode reward: total was -17.990000. running mean: -24.835030\n",
      "ep 976: ep_len:585 episode reward: total was -36.650000. running mean: -24.953179\n",
      "epsilon:0.266639 episode_count: 6839. steps_count: 3046946.000000\n",
      "ep 977: ep_len:500 episode reward: total was -23.200000. running mean: -24.935648\n",
      "ep 977: ep_len:500 episode reward: total was -38.200000. running mean: -25.068291\n",
      "ep 977: ep_len:595 episode reward: total was -28.560000. running mean: -25.103208\n",
      "ep 977: ep_len:372 episode reward: total was -12.240000. running mean: -24.974576\n",
      "ep 977: ep_len:3 episode reward: total was 0.000000. running mean: -24.724830\n",
      "ep 977: ep_len:525 episode reward: total was -31.560000. running mean: -24.793182\n",
      "ep 977: ep_len:515 episode reward: total was -33.000000. running mean: -24.875250\n",
      "epsilon:0.266503 episode_count: 6846. steps_count: 3049956.000000\n",
      "ep 978: ep_len:107 episode reward: total was -4.480000. running mean: -24.671298\n",
      "ep 978: ep_len:515 episode reward: total was -20.220000. running mean: -24.626785\n",
      "ep 978: ep_len:600 episode reward: total was -66.560000. running mean: -25.046117\n",
      "ep 978: ep_len:805 episode reward: total was -72.670000. running mean: -25.522356\n",
      "ep 978: ep_len:3 episode reward: total was 0.000000. running mean: -25.267132\n",
      "ep 978: ep_len:650 episode reward: total was -39.910000. running mean: -25.413561\n",
      "ep 978: ep_len:580 episode reward: total was -35.510000. running mean: -25.514525\n",
      "epsilon:0.266366 episode_count: 6853. steps_count: 3053216.000000\n",
      "ep 979: ep_len:500 episode reward: total was -17.840000. running mean: -25.437780\n",
      "ep 979: ep_len:500 episode reward: total was -17.340000. running mean: -25.356802\n",
      "ep 979: ep_len:575 episode reward: total was -54.680000. running mean: -25.650034\n",
      "ep 979: ep_len:500 episode reward: total was -45.690000. running mean: -25.850434\n",
      "ep 979: ep_len:3 episode reward: total was 0.000000. running mean: -25.591929\n",
      "ep 979: ep_len:505 episode reward: total was -34.860000. running mean: -25.684610\n",
      "ep 979: ep_len:515 episode reward: total was -64.810000. running mean: -26.075864\n",
      "epsilon:0.266230 episode_count: 6860. steps_count: 3056314.000000\n",
      "ep 980: ep_len:545 episode reward: total was -36.240000. running mean: -26.177505\n",
      "ep 980: ep_len:510 episode reward: total was -27.250000. running mean: -26.188230\n",
      "ep 980: ep_len:372 episode reward: total was -14.880000. running mean: -26.075148\n",
      "ep 980: ep_len:125 episode reward: total was -2.940000. running mean: -25.843797\n",
      "ep 980: ep_len:3 episode reward: total was 0.000000. running mean: -25.585359\n",
      "ep 980: ep_len:675 episode reward: total was -77.780000. running mean: -26.107305\n",
      "ep 980: ep_len:181 episode reward: total was -20.460000. running mean: -26.050832\n",
      "epsilon:0.266093 episode_count: 6867. steps_count: 3058725.000000\n",
      "ep 981: ep_len:500 episode reward: total was -28.710000. running mean: -26.077424\n",
      "ep 981: ep_len:770 episode reward: total was -103.140000. running mean: -26.848049\n",
      "ep 981: ep_len:386 episode reward: total was -6.830000. running mean: -26.647869\n",
      "ep 981: ep_len:590 episode reward: total was -24.080000. running mean: -26.622190\n",
      "ep 981: ep_len:3 episode reward: total was 0.000000. running mean: -26.355968\n",
      "ep 981: ep_len:725 episode reward: total was -64.920000. running mean: -26.741609\n",
      "ep 981: ep_len:625 episode reward: total was -49.460000. running mean: -26.968793\n",
      "epsilon:0.265957 episode_count: 6874. steps_count: 3062324.000000\n",
      "ep 982: ep_len:580 episode reward: total was -16.630000. running mean: -26.865405\n",
      "ep 982: ep_len:500 episode reward: total was -38.220000. running mean: -26.978951\n",
      "ep 982: ep_len:560 episode reward: total was -39.620000. running mean: -27.105361\n",
      "ep 982: ep_len:155 episode reward: total was -2.890000. running mean: -26.863207\n",
      "ep 982: ep_len:89 episode reward: total was -13.480000. running mean: -26.729375\n",
      "ep 982: ep_len:515 episode reward: total was -26.130000. running mean: -26.723382\n",
      "ep 982: ep_len:600 episode reward: total was -30.450000. running mean: -26.760648\n",
      "epsilon:0.265820 episode_count: 6881. steps_count: 3065323.000000\n",
      "ep 983: ep_len:116 episode reward: total was -1.430000. running mean: -26.507341\n",
      "ep 983: ep_len:187 episode reward: total was -13.440000. running mean: -26.376668\n",
      "ep 983: ep_len:525 episode reward: total was -31.480000. running mean: -26.427701\n",
      "ep 983: ep_len:545 episode reward: total was -16.190000. running mean: -26.325324\n",
      "ep 983: ep_len:94 episode reward: total was 0.530000. running mean: -26.056771\n",
      "ep 983: ep_len:540 episode reward: total was -50.580000. running mean: -26.302003\n",
      "ep 983: ep_len:545 episode reward: total was -36.460000. running mean: -26.403583\n",
      "epsilon:0.265684 episode_count: 6888. steps_count: 3067875.000000\n",
      "ep 984: ep_len:610 episode reward: total was -45.690000. running mean: -26.596447\n",
      "ep 984: ep_len:530 episode reward: total was -29.620000. running mean: -26.626683\n",
      "ep 984: ep_len:365 episode reward: total was -12.870000. running mean: -26.489116\n",
      "ep 984: ep_len:605 episode reward: total was -61.500000. running mean: -26.839225\n",
      "ep 984: ep_len:3 episode reward: total was 0.000000. running mean: -26.570833\n",
      "ep 984: ep_len:500 episode reward: total was -11.040000. running mean: -26.415524\n",
      "ep 984: ep_len:308 episode reward: total was -16.880000. running mean: -26.320169\n",
      "epsilon:0.265547 episode_count: 6895. steps_count: 3070796.000000\n",
      "ep 985: ep_len:510 episode reward: total was -24.270000. running mean: -26.299667\n",
      "ep 985: ep_len:515 episode reward: total was -34.090000. running mean: -26.377571\n",
      "ep 985: ep_len:605 episode reward: total was -89.260000. running mean: -27.006395\n",
      "ep 985: ep_len:364 episode reward: total was -15.270000. running mean: -26.889031\n",
      "ep 985: ep_len:3 episode reward: total was 0.000000. running mean: -26.620141\n",
      "ep 985: ep_len:172 episode reward: total was 2.590000. running mean: -26.328039\n",
      "ep 985: ep_len:570 episode reward: total was -20.910000. running mean: -26.273859\n",
      "epsilon:0.265411 episode_count: 6902. steps_count: 3073535.000000\n",
      "ep 986: ep_len:134 episode reward: total was -8.970000. running mean: -26.100820\n",
      "ep 986: ep_len:510 episode reward: total was -13.720000. running mean: -25.977012\n",
      "ep 986: ep_len:750 episode reward: total was -39.280000. running mean: -26.110042\n",
      "ep 986: ep_len:550 episode reward: total was -18.150000. running mean: -26.030442\n",
      "ep 986: ep_len:34 episode reward: total was -4.000000. running mean: -25.810137\n",
      "ep 986: ep_len:630 episode reward: total was -39.140000. running mean: -25.943436\n",
      "ep 986: ep_len:510 episode reward: total was -35.780000. running mean: -26.041802\n",
      "epsilon:0.265274 episode_count: 6909. steps_count: 3076653.000000\n",
      "ep 987: ep_len:585 episode reward: total was -25.140000. running mean: -26.032784\n",
      "ep 987: ep_len:520 episode reward: total was -30.180000. running mean: -26.074256\n",
      "ep 987: ep_len:705 episode reward: total was -38.830000. running mean: -26.201813\n",
      "ep 987: ep_len:105 episode reward: total was -6.450000. running mean: -26.004295\n",
      "ep 987: ep_len:3 episode reward: total was 0.000000. running mean: -25.744252\n",
      "ep 987: ep_len:500 episode reward: total was -22.180000. running mean: -25.708610\n",
      "ep 987: ep_len:575 episode reward: total was -35.170000. running mean: -25.803223\n",
      "epsilon:0.265138 episode_count: 6916. steps_count: 3079646.000000\n",
      "ep 988: ep_len:610 episode reward: total was -23.160000. running mean: -25.776791\n",
      "ep 988: ep_len:505 episode reward: total was -12.480000. running mean: -25.643823\n",
      "ep 988: ep_len:555 episode reward: total was -26.890000. running mean: -25.656285\n",
      "ep 988: ep_len:379 episode reward: total was -25.240000. running mean: -25.652122\n",
      "ep 988: ep_len:93 episode reward: total was -5.460000. running mean: -25.450201\n",
      "ep 988: ep_len:685 episode reward: total was -31.850000. running mean: -25.514199\n",
      "ep 988: ep_len:625 episode reward: total was -29.640000. running mean: -25.555457\n",
      "epsilon:0.265001 episode_count: 6923. steps_count: 3083098.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 989: ep_len:211 episode reward: total was -8.450000. running mean: -25.384402\n",
      "ep 989: ep_len:510 episode reward: total was -19.110000. running mean: -25.321658\n",
      "ep 989: ep_len:76 episode reward: total was -2.470000. running mean: -25.093142\n",
      "ep 989: ep_len:570 episode reward: total was -18.260000. running mean: -25.024810\n",
      "ep 989: ep_len:3 episode reward: total was 0.000000. running mean: -24.774562\n",
      "ep 989: ep_len:500 episode reward: total was -31.880000. running mean: -24.845617\n",
      "ep 989: ep_len:298 episode reward: total was -14.370000. running mean: -24.740860\n",
      "epsilon:0.264865 episode_count: 6930. steps_count: 3085266.000000\n",
      "ep 990: ep_len:500 episode reward: total was -21.790000. running mean: -24.711352\n",
      "ep 990: ep_len:540 episode reward: total was -17.200000. running mean: -24.636238\n",
      "ep 990: ep_len:560 episode reward: total was -31.560000. running mean: -24.705476\n",
      "ep 990: ep_len:520 episode reward: total was -26.620000. running mean: -24.724621\n",
      "ep 990: ep_len:3 episode reward: total was 0.000000. running mean: -24.477375\n",
      "ep 990: ep_len:635 episode reward: total was -24.360000. running mean: -24.476201\n",
      "ep 990: ep_len:575 episode reward: total was -48.650000. running mean: -24.717939\n",
      "epsilon:0.264728 episode_count: 6937. steps_count: 3088599.000000\n",
      "ep 991: ep_len:695 episode reward: total was -48.910000. running mean: -24.959860\n",
      "ep 991: ep_len:500 episode reward: total was -30.580000. running mean: -25.016061\n",
      "ep 991: ep_len:565 episode reward: total was -30.470000. running mean: -25.070601\n",
      "ep 991: ep_len:151 episode reward: total was -6.930000. running mean: -24.889195\n",
      "ep 991: ep_len:61 episode reward: total was -1.460000. running mean: -24.654903\n",
      "ep 991: ep_len:535 episode reward: total was -21.940000. running mean: -24.627754\n",
      "ep 991: ep_len:620 episode reward: total was -34.130000. running mean: -24.722776\n",
      "epsilon:0.264592 episode_count: 6944. steps_count: 3091726.000000\n",
      "ep 992: ep_len:500 episode reward: total was -29.310000. running mean: -24.768648\n",
      "ep 992: ep_len:555 episode reward: total was -41.020000. running mean: -24.931162\n",
      "ep 992: ep_len:500 episode reward: total was -49.750000. running mean: -25.179350\n",
      "ep 992: ep_len:500 episode reward: total was -30.630000. running mean: -25.233857\n",
      "ep 992: ep_len:3 episode reward: total was 0.000000. running mean: -24.981518\n",
      "ep 992: ep_len:570 episode reward: total was -34.740000. running mean: -25.079103\n",
      "ep 992: ep_len:595 episode reward: total was -39.040000. running mean: -25.218712\n",
      "epsilon:0.264455 episode_count: 6951. steps_count: 3094949.000000\n",
      "ep 993: ep_len:565 episode reward: total was -31.080000. running mean: -25.277325\n",
      "ep 993: ep_len:610 episode reward: total was -83.770000. running mean: -25.862252\n",
      "ep 993: ep_len:625 episode reward: total was -40.630000. running mean: -26.009929\n",
      "ep 993: ep_len:610 episode reward: total was -18.030000. running mean: -25.930130\n",
      "ep 993: ep_len:124 episode reward: total was -4.970000. running mean: -25.720528\n",
      "ep 993: ep_len:292 episode reward: total was -13.400000. running mean: -25.597323\n",
      "ep 993: ep_len:575 episode reward: total was -33.680000. running mean: -25.678150\n",
      "epsilon:0.264319 episode_count: 6958. steps_count: 3098350.000000\n",
      "ep 994: ep_len:565 episode reward: total was -26.760000. running mean: -25.688968\n",
      "ep 994: ep_len:500 episode reward: total was -32.830000. running mean: -25.760379\n",
      "ep 994: ep_len:515 episode reward: total was -21.770000. running mean: -25.720475\n",
      "ep 994: ep_len:535 episode reward: total was -31.670000. running mean: -25.779970\n",
      "ep 994: ep_len:3 episode reward: total was 0.000000. running mean: -25.522171\n",
      "ep 994: ep_len:325 episode reward: total was -15.960000. running mean: -25.426549\n",
      "ep 994: ep_len:211 episode reward: total was -14.390000. running mean: -25.316183\n",
      "epsilon:0.264182 episode_count: 6965. steps_count: 3101004.000000\n",
      "ep 995: ep_len:545 episode reward: total was -40.470000. running mean: -25.467722\n",
      "ep 995: ep_len:640 episode reward: total was -47.140000. running mean: -25.684444\n",
      "ep 995: ep_len:500 episode reward: total was -21.220000. running mean: -25.639800\n",
      "ep 995: ep_len:500 episode reward: total was -33.200000. running mean: -25.715402\n",
      "ep 995: ep_len:3 episode reward: total was 0.000000. running mean: -25.458248\n",
      "ep 995: ep_len:510 episode reward: total was -17.300000. running mean: -25.376665\n",
      "ep 995: ep_len:500 episode reward: total was -40.070000. running mean: -25.523599\n",
      "epsilon:0.264046 episode_count: 6972. steps_count: 3104202.000000\n",
      "ep 996: ep_len:227 episode reward: total was -15.360000. running mean: -25.421963\n",
      "ep 996: ep_len:560 episode reward: total was -14.960000. running mean: -25.317343\n",
      "ep 996: ep_len:500 episode reward: total was -23.560000. running mean: -25.299770\n",
      "ep 996: ep_len:500 episode reward: total was -28.610000. running mean: -25.332872\n",
      "ep 996: ep_len:3 episode reward: total was 0.000000. running mean: -25.079543\n",
      "ep 996: ep_len:505 episode reward: total was -32.320000. running mean: -25.151948\n",
      "ep 996: ep_len:300 episode reward: total was -8.820000. running mean: -24.988628\n",
      "epsilon:0.263909 episode_count: 6979. steps_count: 3106797.000000\n",
      "ep 997: ep_len:500 episode reward: total was -25.730000. running mean: -24.996042\n",
      "ep 997: ep_len:280 episode reward: total was -23.410000. running mean: -24.980182\n",
      "ep 997: ep_len:555 episode reward: total was -31.040000. running mean: -25.040780\n",
      "ep 997: ep_len:500 episode reward: total was -37.710000. running mean: -25.167472\n",
      "ep 997: ep_len:3 episode reward: total was 0.000000. running mean: -24.915797\n",
      "ep 997: ep_len:515 episode reward: total was -38.090000. running mean: -25.047539\n",
      "ep 997: ep_len:605 episode reward: total was -41.230000. running mean: -25.209364\n",
      "epsilon:0.263773 episode_count: 6986. steps_count: 3109755.000000\n",
      "ep 998: ep_len:620 episode reward: total was -24.540000. running mean: -25.202670\n",
      "ep 998: ep_len:590 episode reward: total was -25.230000. running mean: -25.202944\n",
      "ep 998: ep_len:417 episode reward: total was -19.340000. running mean: -25.144314\n",
      "ep 998: ep_len:56 episode reward: total was -1.960000. running mean: -24.912471\n",
      "ep 998: ep_len:3 episode reward: total was 0.000000. running mean: -24.663346\n",
      "ep 998: ep_len:500 episode reward: total was -21.780000. running mean: -24.634513\n",
      "ep 998: ep_len:189 episode reward: total was -8.850000. running mean: -24.476668\n",
      "epsilon:0.263636 episode_count: 6993. steps_count: 3112130.000000\n",
      "ep 999: ep_len:500 episode reward: total was -18.330000. running mean: -24.415201\n",
      "ep 999: ep_len:535 episode reward: total was -31.230000. running mean: -24.483349\n",
      "ep 999: ep_len:620 episode reward: total was -65.260000. running mean: -24.891116\n",
      "ep 999: ep_len:500 episode reward: total was -9.650000. running mean: -24.738704\n",
      "ep 999: ep_len:84 episode reward: total was -7.950000. running mean: -24.570817\n",
      "ep 999: ep_len:705 episode reward: total was -50.410000. running mean: -24.829209\n",
      "ep 999: ep_len:210 episode reward: total was -8.820000. running mean: -24.669117\n",
      "epsilon:0.263500 episode_count: 7000. steps_count: 3115284.000000\n",
      "ep 1000: ep_len:660 episode reward: total was -54.950000. running mean: -24.971926\n",
      "ep 1000: ep_len:500 episode reward: total was -10.350000. running mean: -24.825707\n",
      "ep 1000: ep_len:565 episode reward: total was -20.690000. running mean: -24.784350\n",
      "ep 1000: ep_len:500 episode reward: total was -65.870000. running mean: -25.195206\n",
      "ep 1000: ep_len:3 episode reward: total was 0.000000. running mean: -24.943254\n",
      "ep 1000: ep_len:515 episode reward: total was -13.630000. running mean: -24.830121\n",
      "ep 1000: ep_len:545 episode reward: total was -21.090000. running mean: -24.792720\n",
      "epsilon:0.263363 episode_count: 7007. steps_count: 3118572.000000\n",
      "ep 1001: ep_len:655 episode reward: total was -49.610000. running mean: -25.040893\n",
      "ep 1001: ep_len:550 episode reward: total was -29.570000. running mean: -25.086184\n",
      "ep 1001: ep_len:590 episode reward: total was -28.340000. running mean: -25.118722\n",
      "ep 1001: ep_len:500 episode reward: total was -19.580000. running mean: -25.063335\n",
      "ep 1001: ep_len:3 episode reward: total was 0.000000. running mean: -24.812702\n",
      "ep 1001: ep_len:620 episode reward: total was -30.670000. running mean: -24.871275\n",
      "ep 1001: ep_len:695 episode reward: total was -80.520000. running mean: -25.427762\n",
      "epsilon:0.263227 episode_count: 7014. steps_count: 3122185.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1002: ep_len:565 episode reward: total was -33.210000. running mean: -25.505584\n",
      "ep 1002: ep_len:680 episode reward: total was -57.020000. running mean: -25.820728\n",
      "ep 1002: ep_len:560 episode reward: total was -23.220000. running mean: -25.794721\n",
      "ep 1002: ep_len:505 episode reward: total was -24.530000. running mean: -25.782074\n",
      "ep 1002: ep_len:3 episode reward: total was 0.000000. running mean: -25.524253\n",
      "ep 1002: ep_len:630 episode reward: total was -35.900000. running mean: -25.628011\n",
      "ep 1002: ep_len:515 episode reward: total was -28.170000. running mean: -25.653431\n",
      "epsilon:0.263090 episode_count: 7021. steps_count: 3125643.000000\n",
      "ep 1003: ep_len:187 episode reward: total was -6.920000. running mean: -25.466096\n",
      "ep 1003: ep_len:545 episode reward: total was -6.230000. running mean: -25.273735\n",
      "ep 1003: ep_len:575 episode reward: total was -54.230000. running mean: -25.563298\n",
      "ep 1003: ep_len:500 episode reward: total was -29.630000. running mean: -25.603965\n",
      "ep 1003: ep_len:3 episode reward: total was 0.000000. running mean: -25.347925\n",
      "ep 1003: ep_len:233 episode reward: total was -9.890000. running mean: -25.193346\n",
      "ep 1003: ep_len:560 episode reward: total was -83.300000. running mean: -25.774413\n",
      "epsilon:0.262954 episode_count: 7028. steps_count: 3128246.000000\n",
      "ep 1004: ep_len:510 episode reward: total was -26.560000. running mean: -25.782269\n",
      "ep 1004: ep_len:635 episode reward: total was -44.750000. running mean: -25.971946\n",
      "ep 1004: ep_len:580 episode reward: total was -35.430000. running mean: -26.066526\n",
      "ep 1004: ep_len:500 episode reward: total was -32.660000. running mean: -26.132461\n",
      "ep 1004: ep_len:3 episode reward: total was 0.000000. running mean: -25.871137\n",
      "ep 1004: ep_len:655 episode reward: total was -40.420000. running mean: -26.016625\n",
      "ep 1004: ep_len:324 episode reward: total was -12.860000. running mean: -25.885059\n",
      "epsilon:0.262817 episode_count: 7035. steps_count: 3131453.000000\n",
      "ep 1005: ep_len:195 episode reward: total was -10.420000. running mean: -25.730408\n",
      "ep 1005: ep_len:510 episode reward: total was -21.440000. running mean: -25.687504\n",
      "ep 1005: ep_len:395 episode reward: total was -30.420000. running mean: -25.734829\n",
      "ep 1005: ep_len:374 episode reward: total was -25.260000. running mean: -25.730081\n",
      "ep 1005: ep_len:49 episode reward: total was 0.000000. running mean: -25.472780\n",
      "ep 1005: ep_len:580 episode reward: total was -41.130000. running mean: -25.629352\n",
      "ep 1005: ep_len:575 episode reward: total was -32.560000. running mean: -25.698659\n",
      "epsilon:0.262681 episode_count: 7042. steps_count: 3134131.000000\n",
      "ep 1006: ep_len:265 episode reward: total was -4.890000. running mean: -25.490572\n",
      "ep 1006: ep_len:510 episode reward: total was -20.300000. running mean: -25.438666\n",
      "ep 1006: ep_len:359 episode reward: total was -7.280000. running mean: -25.257080\n",
      "ep 1006: ep_len:635 episode reward: total was -33.120000. running mean: -25.335709\n",
      "ep 1006: ep_len:3 episode reward: total was 0.000000. running mean: -25.082352\n",
      "ep 1006: ep_len:605 episode reward: total was -32.150000. running mean: -25.153028\n",
      "ep 1006: ep_len:200 episode reward: total was -23.910000. running mean: -25.140598\n",
      "epsilon:0.262544 episode_count: 7049. steps_count: 3136708.000000\n",
      "ep 1007: ep_len:500 episode reward: total was -21.790000. running mean: -25.107092\n",
      "ep 1007: ep_len:525 episode reward: total was -15.260000. running mean: -25.008621\n",
      "ep 1007: ep_len:615 episode reward: total was -20.150000. running mean: -24.960035\n",
      "ep 1007: ep_len:570 episode reward: total was -24.190000. running mean: -24.952335\n",
      "ep 1007: ep_len:3 episode reward: total was 0.000000. running mean: -24.702811\n",
      "ep 1007: ep_len:670 episode reward: total was -45.900000. running mean: -24.914783\n",
      "ep 1007: ep_len:590 episode reward: total was -42.150000. running mean: -25.087135\n",
      "epsilon:0.262408 episode_count: 7056. steps_count: 3140181.000000\n",
      "ep 1008: ep_len:575 episode reward: total was -37.640000. running mean: -25.212664\n",
      "ep 1008: ep_len:640 episode reward: total was -27.200000. running mean: -25.232537\n",
      "ep 1008: ep_len:462 episode reward: total was -30.820000. running mean: -25.288412\n",
      "ep 1008: ep_len:500 episode reward: total was -22.070000. running mean: -25.256228\n",
      "ep 1008: ep_len:3 episode reward: total was 0.000000. running mean: -25.003666\n",
      "ep 1008: ep_len:580 episode reward: total was -22.630000. running mean: -24.979929\n",
      "ep 1008: ep_len:505 episode reward: total was -29.640000. running mean: -25.026530\n",
      "epsilon:0.262271 episode_count: 7063. steps_count: 3143446.000000\n",
      "ep 1009: ep_len:555 episode reward: total was -26.570000. running mean: -25.041964\n",
      "ep 1009: ep_len:520 episode reward: total was -19.210000. running mean: -24.983645\n",
      "ep 1009: ep_len:595 episode reward: total was -23.190000. running mean: -24.965708\n",
      "ep 1009: ep_len:500 episode reward: total was -30.630000. running mean: -25.022351\n",
      "ep 1009: ep_len:3 episode reward: total was 0.000000. running mean: -24.772128\n",
      "ep 1009: ep_len:307 episode reward: total was -36.860000. running mean: -24.893006\n",
      "ep 1009: ep_len:330 episode reward: total was -32.920000. running mean: -24.973276\n",
      "epsilon:0.262135 episode_count: 7070. steps_count: 3146256.000000\n",
      "ep 1010: ep_len:525 episode reward: total was -45.180000. running mean: -25.175344\n",
      "ep 1010: ep_len:505 episode reward: total was -30.210000. running mean: -25.225690\n",
      "ep 1010: ep_len:369 episode reward: total was -17.860000. running mean: -25.152033\n",
      "ep 1010: ep_len:550 episode reward: total was -11.690000. running mean: -25.017413\n",
      "ep 1010: ep_len:3 episode reward: total was 0.000000. running mean: -24.767239\n",
      "ep 1010: ep_len:565 episode reward: total was -29.900000. running mean: -24.818566\n",
      "ep 1010: ep_len:500 episode reward: total was -37.100000. running mean: -24.941381\n",
      "epsilon:0.261998 episode_count: 7077. steps_count: 3149273.000000\n",
      "ep 1011: ep_len:620 episode reward: total was -21.470000. running mean: -24.906667\n",
      "ep 1011: ep_len:500 episode reward: total was -32.140000. running mean: -24.979000\n",
      "ep 1011: ep_len:595 episode reward: total was -34.160000. running mean: -25.070810\n",
      "ep 1011: ep_len:530 episode reward: total was -36.710000. running mean: -25.187202\n",
      "ep 1011: ep_len:3 episode reward: total was 0.000000. running mean: -24.935330\n",
      "ep 1011: ep_len:530 episode reward: total was -37.070000. running mean: -25.056677\n",
      "ep 1011: ep_len:211 episode reward: total was -16.940000. running mean: -24.975510\n",
      "epsilon:0.261862 episode_count: 7084. steps_count: 3152262.000000\n",
      "ep 1012: ep_len:590 episode reward: total was -50.970000. running mean: -25.235455\n",
      "ep 1012: ep_len:620 episode reward: total was -36.160000. running mean: -25.344700\n",
      "ep 1012: ep_len:585 episode reward: total was -34.390000. running mean: -25.435153\n",
      "ep 1012: ep_len:56 episode reward: total was -0.460000. running mean: -25.185402\n",
      "ep 1012: ep_len:111 episode reward: total was -13.460000. running mean: -25.068148\n",
      "ep 1012: ep_len:500 episode reward: total was -21.800000. running mean: -25.035466\n",
      "ep 1012: ep_len:286 episode reward: total was -10.830000. running mean: -24.893412\n",
      "epsilon:0.261725 episode_count: 7091. steps_count: 3155010.000000\n",
      "ep 1013: ep_len:500 episode reward: total was -14.200000. running mean: -24.786478\n",
      "ep 1013: ep_len:510 episode reward: total was -23.410000. running mean: -24.772713\n",
      "ep 1013: ep_len:590 episode reward: total was -41.470000. running mean: -24.939686\n",
      "ep 1013: ep_len:500 episode reward: total was -15.710000. running mean: -24.847389\n",
      "ep 1013: ep_len:97 episode reward: total was -1.450000. running mean: -24.613415\n",
      "ep 1013: ep_len:520 episode reward: total was -30.030000. running mean: -24.667581\n",
      "ep 1013: ep_len:535 episode reward: total was -47.560000. running mean: -24.896505\n",
      "epsilon:0.261589 episode_count: 7098. steps_count: 3158262.000000\n",
      "ep 1014: ep_len:130 episode reward: total was -1.930000. running mean: -24.666840\n",
      "ep 1014: ep_len:555 episode reward: total was -24.580000. running mean: -24.665972\n",
      "ep 1014: ep_len:510 episode reward: total was -27.180000. running mean: -24.691112\n",
      "ep 1014: ep_len:595 episode reward: total was -21.740000. running mean: -24.661601\n",
      "ep 1014: ep_len:3 episode reward: total was 0.000000. running mean: -24.414985\n",
      "ep 1014: ep_len:595 episode reward: total was -61.330000. running mean: -24.784135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1014: ep_len:610 episode reward: total was -48.090000. running mean: -25.017193\n",
      "epsilon:0.261452 episode_count: 7105. steps_count: 3161260.000000\n",
      "ep 1015: ep_len:640 episode reward: total was -22.520000. running mean: -24.992222\n",
      "ep 1015: ep_len:515 episode reward: total was -23.770000. running mean: -24.979999\n",
      "ep 1015: ep_len:500 episode reward: total was -52.260000. running mean: -25.252799\n",
      "ep 1015: ep_len:510 episode reward: total was -23.050000. running mean: -25.230771\n",
      "ep 1015: ep_len:102 episode reward: total was -9.960000. running mean: -25.078064\n",
      "ep 1015: ep_len:500 episode reward: total was -40.080000. running mean: -25.228083\n",
      "ep 1015: ep_len:252 episode reward: total was -21.350000. running mean: -25.189302\n",
      "epsilon:0.261316 episode_count: 7112. steps_count: 3164279.000000\n",
      "ep 1016: ep_len:775 episode reward: total was -81.560000. running mean: -25.753009\n",
      "ep 1016: ep_len:370 episode reward: total was -30.340000. running mean: -25.798879\n",
      "ep 1016: ep_len:545 episode reward: total was -20.160000. running mean: -25.742490\n",
      "ep 1016: ep_len:500 episode reward: total was -21.710000. running mean: -25.702165\n",
      "ep 1016: ep_len:3 episode reward: total was 0.000000. running mean: -25.445144\n",
      "ep 1016: ep_len:650 episode reward: total was -41.110000. running mean: -25.601792\n",
      "ep 1016: ep_len:590 episode reward: total was -29.400000. running mean: -25.639774\n",
      "epsilon:0.261179 episode_count: 7119. steps_count: 3167712.000000\n",
      "ep 1017: ep_len:134 episode reward: total was -0.950000. running mean: -25.392877\n",
      "ep 1017: ep_len:200 episode reward: total was -1.350000. running mean: -25.152448\n",
      "ep 1017: ep_len:347 episode reward: total was -13.340000. running mean: -25.034323\n",
      "ep 1017: ep_len:520 episode reward: total was -46.720000. running mean: -25.251180\n",
      "ep 1017: ep_len:49 episode reward: total was 1.500000. running mean: -24.983668\n",
      "ep 1017: ep_len:505 episode reward: total was -14.600000. running mean: -24.879832\n",
      "ep 1017: ep_len:340 episode reward: total was -21.330000. running mean: -24.844333\n",
      "epsilon:0.261043 episode_count: 7126. steps_count: 3169807.000000\n",
      "ep 1018: ep_len:665 episode reward: total was -56.470000. running mean: -25.160590\n",
      "ep 1018: ep_len:346 episode reward: total was -30.800000. running mean: -25.216984\n",
      "ep 1018: ep_len:625 episode reward: total was -67.640000. running mean: -25.641214\n",
      "ep 1018: ep_len:815 episode reward: total was -82.980000. running mean: -26.214602\n",
      "ep 1018: ep_len:3 episode reward: total was 0.000000. running mean: -25.952456\n",
      "ep 1018: ep_len:535 episode reward: total was -39.000000. running mean: -26.082932\n",
      "ep 1018: ep_len:610 episode reward: total was -25.600000. running mean: -26.078102\n",
      "epsilon:0.260906 episode_count: 7133. steps_count: 3173406.000000\n",
      "ep 1019: ep_len:600 episode reward: total was -33.050000. running mean: -26.147821\n",
      "ep 1019: ep_len:645 episode reward: total was -74.190000. running mean: -26.628243\n",
      "ep 1019: ep_len:550 episode reward: total was -51.530000. running mean: -26.877261\n",
      "ep 1019: ep_len:500 episode reward: total was -27.600000. running mean: -26.884488\n",
      "ep 1019: ep_len:51 episode reward: total was -1.000000. running mean: -26.625643\n",
      "ep 1019: ep_len:655 episode reward: total was -39.380000. running mean: -26.753187\n",
      "ep 1019: ep_len:520 episode reward: total was -33.270000. running mean: -26.818355\n",
      "epsilon:0.260770 episode_count: 7140. steps_count: 3176927.000000\n",
      "ep 1020: ep_len:640 episode reward: total was -59.400000. running mean: -27.144171\n",
      "ep 1020: ep_len:350 episode reward: total was -51.380000. running mean: -27.386529\n",
      "ep 1020: ep_len:340 episode reward: total was -13.830000. running mean: -27.250964\n",
      "ep 1020: ep_len:570 episode reward: total was -31.160000. running mean: -27.290055\n",
      "ep 1020: ep_len:3 episode reward: total was 0.000000. running mean: -27.017154\n",
      "ep 1020: ep_len:530 episode reward: total was -31.270000. running mean: -27.059682\n",
      "ep 1020: ep_len:545 episode reward: total was -24.830000. running mean: -27.037386\n",
      "epsilon:0.260633 episode_count: 7147. steps_count: 3179905.000000\n",
      "ep 1021: ep_len:540 episode reward: total was -22.070000. running mean: -26.987712\n",
      "ep 1021: ep_len:282 episode reward: total was -23.380000. running mean: -26.951635\n",
      "ep 1021: ep_len:505 episode reward: total was -28.080000. running mean: -26.962918\n",
      "ep 1021: ep_len:111 episode reward: total was -8.950000. running mean: -26.782789\n",
      "ep 1021: ep_len:3 episode reward: total was 0.000000. running mean: -26.514961\n",
      "ep 1021: ep_len:500 episode reward: total was -26.020000. running mean: -26.510012\n",
      "ep 1021: ep_len:605 episode reward: total was -22.360000. running mean: -26.468512\n",
      "epsilon:0.260497 episode_count: 7154. steps_count: 3182451.000000\n",
      "ep 1022: ep_len:795 episode reward: total was -63.610000. running mean: -26.839926\n",
      "ep 1022: ep_len:540 episode reward: total was -67.240000. running mean: -27.243927\n",
      "ep 1022: ep_len:453 episode reward: total was -12.770000. running mean: -27.099188\n",
      "ep 1022: ep_len:510 episode reward: total was -34.590000. running mean: -27.174096\n",
      "ep 1022: ep_len:3 episode reward: total was 0.000000. running mean: -26.902355\n",
      "ep 1022: ep_len:500 episode reward: total was -21.290000. running mean: -26.846231\n",
      "ep 1022: ep_len:500 episode reward: total was -39.720000. running mean: -26.974969\n",
      "epsilon:0.260360 episode_count: 7161. steps_count: 3185752.000000\n",
      "ep 1023: ep_len:238 episode reward: total was -5.890000. running mean: -26.764119\n",
      "ep 1023: ep_len:500 episode reward: total was -37.680000. running mean: -26.873278\n",
      "ep 1023: ep_len:550 episode reward: total was -30.390000. running mean: -26.908446\n",
      "ep 1023: ep_len:535 episode reward: total was -18.510000. running mean: -26.824461\n",
      "ep 1023: ep_len:99 episode reward: total was -12.470000. running mean: -26.680916\n",
      "ep 1023: ep_len:545 episode reward: total was -41.160000. running mean: -26.825707\n",
      "ep 1023: ep_len:505 episode reward: total was -28.670000. running mean: -26.844150\n",
      "epsilon:0.260224 episode_count: 7168. steps_count: 3188724.000000\n",
      "ep 1024: ep_len:535 episode reward: total was -19.730000. running mean: -26.773009\n",
      "ep 1024: ep_len:505 episode reward: total was -12.400000. running mean: -26.629279\n",
      "ep 1024: ep_len:640 episode reward: total was -49.920000. running mean: -26.862186\n",
      "ep 1024: ep_len:627 episode reward: total was -52.070000. running mean: -27.114264\n",
      "ep 1024: ep_len:3 episode reward: total was 0.000000. running mean: -26.843121\n",
      "ep 1024: ep_len:730 episode reward: total was -34.780000. running mean: -26.922490\n",
      "ep 1024: ep_len:525 episode reward: total was -19.330000. running mean: -26.846565\n",
      "epsilon:0.260087 episode_count: 7175. steps_count: 3192289.000000\n",
      "ep 1025: ep_len:580 episode reward: total was -29.720000. running mean: -26.875300\n",
      "ep 1025: ep_len:500 episode reward: total was -11.440000. running mean: -26.720947\n",
      "ep 1025: ep_len:62 episode reward: total was 1.020000. running mean: -26.443537\n",
      "ep 1025: ep_len:525 episode reward: total was -30.060000. running mean: -26.479702\n",
      "ep 1025: ep_len:3 episode reward: total was 0.000000. running mean: -26.214905\n",
      "ep 1025: ep_len:550 episode reward: total was -49.130000. running mean: -26.444056\n",
      "ep 1025: ep_len:655 episode reward: total was -48.500000. running mean: -26.664615\n",
      "epsilon:0.259951 episode_count: 7182. steps_count: 3195164.000000\n",
      "ep 1026: ep_len:675 episode reward: total was -59.020000. running mean: -26.988169\n",
      "ep 1026: ep_len:500 episode reward: total was -36.300000. running mean: -27.081287\n",
      "ep 1026: ep_len:445 episode reward: total was -21.240000. running mean: -27.022874\n",
      "ep 1026: ep_len:531 episode reward: total was -23.460000. running mean: -26.987246\n",
      "ep 1026: ep_len:97 episode reward: total was -5.950000. running mean: -26.776873\n",
      "ep 1026: ep_len:625 episode reward: total was -31.680000. running mean: -26.825904\n",
      "ep 1026: ep_len:625 episode reward: total was -35.460000. running mean: -26.912245\n",
      "epsilon:0.259814 episode_count: 7189. steps_count: 3198662.000000\n",
      "ep 1027: ep_len:112 episode reward: total was -5.980000. running mean: -26.702923\n",
      "ep 1027: ep_len:540 episode reward: total was -28.590000. running mean: -26.721794\n",
      "ep 1027: ep_len:675 episode reward: total was -31.870000. running mean: -26.773276\n",
      "ep 1027: ep_len:515 episode reward: total was -30.110000. running mean: -26.806643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1027: ep_len:3 episode reward: total was 0.000000. running mean: -26.538577\n",
      "ep 1027: ep_len:660 episode reward: total was -45.380000. running mean: -26.726991\n",
      "ep 1027: ep_len:535 episode reward: total was -23.980000. running mean: -26.699521\n",
      "epsilon:0.259678 episode_count: 7196. steps_count: 3201702.000000\n",
      "ep 1028: ep_len:665 episode reward: total was -40.570000. running mean: -26.838226\n",
      "ep 1028: ep_len:720 episode reward: total was -54.930000. running mean: -27.119143\n",
      "ep 1028: ep_len:372 episode reward: total was -20.400000. running mean: -27.051952\n",
      "ep 1028: ep_len:500 episode reward: total was -20.210000. running mean: -26.983533\n",
      "ep 1028: ep_len:3 episode reward: total was 0.000000. running mean: -26.713697\n",
      "ep 1028: ep_len:590 episode reward: total was -17.670000. running mean: -26.623260\n",
      "ep 1028: ep_len:198 episode reward: total was -13.420000. running mean: -26.491228\n",
      "epsilon:0.259541 episode_count: 7203. steps_count: 3204750.000000\n",
      "ep 1029: ep_len:545 episode reward: total was -45.980000. running mean: -26.686115\n",
      "ep 1029: ep_len:510 episode reward: total was -3.710000. running mean: -26.456354\n",
      "ep 1029: ep_len:432 episode reward: total was -17.230000. running mean: -26.364091\n",
      "ep 1029: ep_len:515 episode reward: total was -19.230000. running mean: -26.292750\n",
      "ep 1029: ep_len:70 episode reward: total was -2.970000. running mean: -26.059522\n",
      "ep 1029: ep_len:645 episode reward: total was -54.060000. running mean: -26.339527\n",
      "ep 1029: ep_len:500 episode reward: total was -26.650000. running mean: -26.342632\n",
      "epsilon:0.259405 episode_count: 7210. steps_count: 3207967.000000\n",
      "ep 1030: ep_len:600 episode reward: total was -27.250000. running mean: -26.351705\n",
      "ep 1030: ep_len:505 episode reward: total was -38.080000. running mean: -26.468988\n",
      "ep 1030: ep_len:640 episode reward: total was -40.080000. running mean: -26.605098\n",
      "ep 1030: ep_len:500 episode reward: total was -21.800000. running mean: -26.557048\n",
      "ep 1030: ep_len:96 episode reward: total was -7.970000. running mean: -26.371177\n",
      "ep 1030: ep_len:625 episode reward: total was -36.650000. running mean: -26.473965\n",
      "ep 1030: ep_len:283 episode reward: total was -21.900000. running mean: -26.428226\n",
      "epsilon:0.259268 episode_count: 7217. steps_count: 3211216.000000\n",
      "ep 1031: ep_len:500 episode reward: total was -42.850000. running mean: -26.592443\n",
      "ep 1031: ep_len:590 episode reward: total was -37.640000. running mean: -26.702919\n",
      "ep 1031: ep_len:590 episode reward: total was -30.200000. running mean: -26.737890\n",
      "ep 1031: ep_len:520 episode reward: total was -45.750000. running mean: -26.928011\n",
      "ep 1031: ep_len:3 episode reward: total was 0.000000. running mean: -26.658731\n",
      "ep 1031: ep_len:186 episode reward: total was -5.950000. running mean: -26.451643\n",
      "ep 1031: ep_len:610 episode reward: total was -31.040000. running mean: -26.497527\n",
      "epsilon:0.259132 episode_count: 7224. steps_count: 3214215.000000\n",
      "ep 1032: ep_len:645 episode reward: total was -37.290000. running mean: -26.605452\n",
      "ep 1032: ep_len:505 episode reward: total was -23.270000. running mean: -26.572097\n",
      "ep 1032: ep_len:620 episode reward: total was -85.340000. running mean: -27.159776\n",
      "ep 1032: ep_len:515 episode reward: total was -47.250000. running mean: -27.360678\n",
      "ep 1032: ep_len:3 episode reward: total was 0.000000. running mean: -27.087072\n",
      "ep 1032: ep_len:500 episode reward: total was -33.060000. running mean: -27.146801\n",
      "ep 1032: ep_len:208 episode reward: total was -11.390000. running mean: -26.989233\n",
      "epsilon:0.258995 episode_count: 7231. steps_count: 3217211.000000\n",
      "ep 1033: ep_len:500 episode reward: total was -32.670000. running mean: -27.046041\n",
      "ep 1033: ep_len:174 episode reward: total was -19.950000. running mean: -26.975080\n",
      "ep 1033: ep_len:600 episode reward: total was -33.640000. running mean: -27.041729\n",
      "ep 1033: ep_len:618 episode reward: total was -36.960000. running mean: -27.140912\n",
      "ep 1033: ep_len:3 episode reward: total was 0.000000. running mean: -26.869503\n",
      "ep 1033: ep_len:540 episode reward: total was -43.700000. running mean: -27.037808\n",
      "ep 1033: ep_len:550 episode reward: total was -46.580000. running mean: -27.233230\n",
      "epsilon:0.258859 episode_count: 7238. steps_count: 3220196.000000\n",
      "ep 1034: ep_len:565 episode reward: total was -23.580000. running mean: -27.196698\n",
      "ep 1034: ep_len:500 episode reward: total was -48.820000. running mean: -27.412931\n",
      "ep 1034: ep_len:590 episode reward: total was -33.530000. running mean: -27.474101\n",
      "ep 1034: ep_len:615 episode reward: total was -55.140000. running mean: -27.750760\n",
      "ep 1034: ep_len:3 episode reward: total was 0.000000. running mean: -27.473253\n",
      "ep 1034: ep_len:600 episode reward: total was -58.020000. running mean: -27.778720\n",
      "ep 1034: ep_len:535 episode reward: total was -38.040000. running mean: -27.881333\n",
      "epsilon:0.258722 episode_count: 7245. steps_count: 3223604.000000\n",
      "ep 1035: ep_len:645 episode reward: total was -37.320000. running mean: -27.975720\n",
      "ep 1035: ep_len:510 episode reward: total was -35.690000. running mean: -28.052862\n",
      "ep 1035: ep_len:555 episode reward: total was -33.710000. running mean: -28.109434\n",
      "ep 1035: ep_len:510 episode reward: total was -33.550000. running mean: -28.163839\n",
      "ep 1035: ep_len:3 episode reward: total was 0.000000. running mean: -27.882201\n",
      "ep 1035: ep_len:327 episode reward: total was -10.850000. running mean: -27.711879\n",
      "ep 1035: ep_len:580 episode reward: total was -70.240000. running mean: -28.137160\n",
      "epsilon:0.258586 episode_count: 7252. steps_count: 3226734.000000\n",
      "ep 1036: ep_len:134 episode reward: total was -11.470000. running mean: -27.970489\n",
      "ep 1036: ep_len:645 episode reward: total was -44.760000. running mean: -28.138384\n",
      "ep 1036: ep_len:473 episode reward: total was -28.820000. running mean: -28.145200\n",
      "ep 1036: ep_len:510 episode reward: total was -55.770000. running mean: -28.421448\n",
      "ep 1036: ep_len:3 episode reward: total was 0.000000. running mean: -28.137233\n",
      "ep 1036: ep_len:510 episode reward: total was -24.730000. running mean: -28.103161\n",
      "ep 1036: ep_len:341 episode reward: total was -22.380000. running mean: -28.045930\n",
      "epsilon:0.258449 episode_count: 7259. steps_count: 3229350.000000\n",
      "ep 1037: ep_len:540 episode reward: total was -34.500000. running mean: -28.110470\n",
      "ep 1037: ep_len:530 episode reward: total was -25.030000. running mean: -28.079666\n",
      "ep 1037: ep_len:540 episode reward: total was -38.440000. running mean: -28.183269\n",
      "ep 1037: ep_len:379 episode reward: total was -11.220000. running mean: -28.013636\n",
      "ep 1037: ep_len:3 episode reward: total was 0.000000. running mean: -27.733500\n",
      "ep 1037: ep_len:520 episode reward: total was -57.840000. running mean: -28.034565\n",
      "ep 1037: ep_len:565 episode reward: total was -35.520000. running mean: -28.109419\n",
      "epsilon:0.258313 episode_count: 7266. steps_count: 3232427.000000\n",
      "ep 1038: ep_len:505 episode reward: total was -47.290000. running mean: -28.301225\n",
      "ep 1038: ep_len:500 episode reward: total was -29.360000. running mean: -28.311813\n",
      "ep 1038: ep_len:655 episode reward: total was -47.590000. running mean: -28.504595\n",
      "ep 1038: ep_len:370 episode reward: total was -17.790000. running mean: -28.397449\n",
      "ep 1038: ep_len:110 episode reward: total was -6.960000. running mean: -28.183074\n",
      "ep 1038: ep_len:535 episode reward: total was -20.520000. running mean: -28.106443\n",
      "ep 1038: ep_len:510 episode reward: total was -19.390000. running mean: -28.019279\n",
      "epsilon:0.258176 episode_count: 7273. steps_count: 3235612.000000\n",
      "ep 1039: ep_len:640 episode reward: total was -45.470000. running mean: -28.193786\n",
      "ep 1039: ep_len:500 episode reward: total was -31.330000. running mean: -28.225148\n",
      "ep 1039: ep_len:565 episode reward: total was -34.950000. running mean: -28.292397\n",
      "ep 1039: ep_len:570 episode reward: total was -20.660000. running mean: -28.216073\n",
      "ep 1039: ep_len:3 episode reward: total was 0.000000. running mean: -27.933912\n",
      "ep 1039: ep_len:510 episode reward: total was -40.540000. running mean: -28.059973\n",
      "ep 1039: ep_len:510 episode reward: total was -21.380000. running mean: -27.993173\n",
      "epsilon:0.258040 episode_count: 7280. steps_count: 3238910.000000\n",
      "ep 1040: ep_len:650 episode reward: total was -49.090000. running mean: -28.204142\n",
      "ep 1040: ep_len:550 episode reward: total was -34.000000. running mean: -28.262100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1040: ep_len:500 episode reward: total was -43.910000. running mean: -28.418579\n",
      "ep 1040: ep_len:570 episode reward: total was -28.720000. running mean: -28.421593\n",
      "ep 1040: ep_len:3 episode reward: total was 0.000000. running mean: -28.137377\n",
      "ep 1040: ep_len:530 episode reward: total was -26.100000. running mean: -28.117004\n",
      "ep 1040: ep_len:560 episode reward: total was -38.230000. running mean: -28.218134\n",
      "epsilon:0.257903 episode_count: 7287. steps_count: 3242273.000000\n",
      "ep 1041: ep_len:605 episode reward: total was -66.550000. running mean: -28.601452\n",
      "ep 1041: ep_len:500 episode reward: total was -22.590000. running mean: -28.541338\n",
      "ep 1041: ep_len:595 episode reward: total was -36.560000. running mean: -28.621524\n",
      "ep 1041: ep_len:505 episode reward: total was -19.180000. running mean: -28.527109\n",
      "ep 1041: ep_len:3 episode reward: total was 0.000000. running mean: -28.241838\n",
      "ep 1041: ep_len:500 episode reward: total was -33.810000. running mean: -28.297520\n",
      "ep 1041: ep_len:500 episode reward: total was -30.000000. running mean: -28.314544\n",
      "epsilon:0.257767 episode_count: 7294. steps_count: 3245481.000000\n",
      "ep 1042: ep_len:525 episode reward: total was -41.810000. running mean: -28.449499\n",
      "ep 1042: ep_len:640 episode reward: total was -27.070000. running mean: -28.435704\n",
      "ep 1042: ep_len:705 episode reward: total was -59.860000. running mean: -28.749947\n",
      "ep 1042: ep_len:580 episode reward: total was -21.680000. running mean: -28.679248\n",
      "ep 1042: ep_len:3 episode reward: total was 0.000000. running mean: -28.392455\n",
      "ep 1042: ep_len:580 episode reward: total was -14.810000. running mean: -28.256631\n",
      "ep 1042: ep_len:620 episode reward: total was -19.300000. running mean: -28.167064\n",
      "epsilon:0.257630 episode_count: 7301. steps_count: 3249134.000000\n",
      "ep 1043: ep_len:505 episode reward: total was -18.840000. running mean: -28.073794\n",
      "ep 1043: ep_len:550 episode reward: total was -37.130000. running mean: -28.164356\n",
      "ep 1043: ep_len:500 episode reward: total was -21.290000. running mean: -28.095612\n",
      "ep 1043: ep_len:580 episode reward: total was -16.000000. running mean: -27.974656\n",
      "ep 1043: ep_len:3 episode reward: total was 0.000000. running mean: -27.694909\n",
      "ep 1043: ep_len:655 episode reward: total was -31.320000. running mean: -27.731160\n",
      "ep 1043: ep_len:195 episode reward: total was -21.960000. running mean: -27.673449\n",
      "epsilon:0.257494 episode_count: 7308. steps_count: 3252122.000000\n",
      "ep 1044: ep_len:600 episode reward: total was -37.100000. running mean: -27.767714\n",
      "ep 1044: ep_len:500 episode reward: total was -18.550000. running mean: -27.675537\n",
      "ep 1044: ep_len:500 episode reward: total was -31.120000. running mean: -27.709982\n",
      "ep 1044: ep_len:106 episode reward: total was -4.950000. running mean: -27.482382\n",
      "ep 1044: ep_len:74 episode reward: total was -10.970000. running mean: -27.317258\n",
      "ep 1044: ep_len:595 episode reward: total was -25.670000. running mean: -27.300785\n",
      "ep 1044: ep_len:630 episode reward: total was -54.030000. running mean: -27.568078\n",
      "epsilon:0.257357 episode_count: 7315. steps_count: 3255127.000000\n",
      "ep 1045: ep_len:575 episode reward: total was -38.180000. running mean: -27.674197\n",
      "ep 1045: ep_len:500 episode reward: total was -13.380000. running mean: -27.531255\n",
      "ep 1045: ep_len:635 episode reward: total was -28.340000. running mean: -27.539342\n",
      "ep 1045: ep_len:600 episode reward: total was -31.020000. running mean: -27.574149\n",
      "ep 1045: ep_len:3 episode reward: total was 0.000000. running mean: -27.298407\n",
      "ep 1045: ep_len:595 episode reward: total was -22.640000. running mean: -27.251823\n",
      "ep 1045: ep_len:595 episode reward: total was -40.480000. running mean: -27.384105\n",
      "epsilon:0.257221 episode_count: 7322. steps_count: 3258630.000000\n",
      "ep 1046: ep_len:520 episode reward: total was -10.160000. running mean: -27.211864\n",
      "ep 1046: ep_len:595 episode reward: total was -46.040000. running mean: -27.400145\n",
      "ep 1046: ep_len:530 episode reward: total was -74.850000. running mean: -27.874644\n",
      "ep 1046: ep_len:510 episode reward: total was -24.730000. running mean: -27.843198\n",
      "ep 1046: ep_len:3 episode reward: total was 0.000000. running mean: -27.564766\n",
      "ep 1046: ep_len:500 episode reward: total was -28.090000. running mean: -27.570018\n",
      "ep 1046: ep_len:595 episode reward: total was -31.700000. running mean: -27.611318\n",
      "epsilon:0.257084 episode_count: 7329. steps_count: 3261883.000000\n",
      "ep 1047: ep_len:505 episode reward: total was -41.130000. running mean: -27.746505\n",
      "ep 1047: ep_len:680 episode reward: total was -35.410000. running mean: -27.823139\n",
      "ep 1047: ep_len:550 episode reward: total was -24.740000. running mean: -27.792308\n",
      "ep 1047: ep_len:500 episode reward: total was -25.060000. running mean: -27.764985\n",
      "ep 1047: ep_len:3 episode reward: total was 0.000000. running mean: -27.487335\n",
      "ep 1047: ep_len:318 episode reward: total was -11.360000. running mean: -27.326062\n",
      "ep 1047: ep_len:535 episode reward: total was -52.740000. running mean: -27.580201\n",
      "epsilon:0.256948 episode_count: 7336. steps_count: 3264974.000000\n",
      "ep 1048: ep_len:590 episode reward: total was -62.330000. running mean: -27.927699\n",
      "ep 1048: ep_len:292 episode reward: total was -19.330000. running mean: -27.841722\n",
      "ep 1048: ep_len:630 episode reward: total was -28.030000. running mean: -27.843605\n",
      "ep 1048: ep_len:520 episode reward: total was -42.610000. running mean: -27.991269\n",
      "ep 1048: ep_len:97 episode reward: total was -2.460000. running mean: -27.735956\n",
      "ep 1048: ep_len:585 episode reward: total was -13.470000. running mean: -27.593297\n",
      "ep 1048: ep_len:645 episode reward: total was -28.440000. running mean: -27.601764\n",
      "epsilon:0.256811 episode_count: 7343. steps_count: 3268333.000000\n",
      "ep 1049: ep_len:560 episode reward: total was -39.690000. running mean: -27.722646\n",
      "ep 1049: ep_len:295 episode reward: total was -20.380000. running mean: -27.649220\n",
      "ep 1049: ep_len:500 episode reward: total was -37.510000. running mean: -27.747827\n",
      "ep 1049: ep_len:510 episode reward: total was -23.090000. running mean: -27.701249\n",
      "ep 1049: ep_len:2 episode reward: total was 0.000000. running mean: -27.424237\n",
      "ep 1049: ep_len:831 episode reward: total was -65.350000. running mean: -27.803494\n",
      "ep 1049: ep_len:580 episode reward: total was -64.710000. running mean: -28.172559\n",
      "epsilon:0.256675 episode_count: 7350. steps_count: 3271611.000000\n",
      "ep 1050: ep_len:595 episode reward: total was -51.940000. running mean: -28.410234\n",
      "ep 1050: ep_len:570 episode reward: total was -50.630000. running mean: -28.632431\n",
      "ep 1050: ep_len:565 episode reward: total was -32.220000. running mean: -28.668307\n",
      "ep 1050: ep_len:393 episode reward: total was -16.680000. running mean: -28.548424\n",
      "ep 1050: ep_len:3 episode reward: total was 0.000000. running mean: -28.262940\n",
      "ep 1050: ep_len:585 episode reward: total was -57.070000. running mean: -28.551010\n",
      "ep 1050: ep_len:540 episode reward: total was -22.090000. running mean: -28.486400\n",
      "epsilon:0.256538 episode_count: 7357. steps_count: 3274862.000000\n",
      "ep 1051: ep_len:570 episode reward: total was -45.290000. running mean: -28.654436\n",
      "ep 1051: ep_len:550 episode reward: total was -31.620000. running mean: -28.684092\n",
      "ep 1051: ep_len:710 episode reward: total was -33.750000. running mean: -28.734751\n",
      "ep 1051: ep_len:605 episode reward: total was -29.250000. running mean: -28.739903\n",
      "ep 1051: ep_len:78 episode reward: total was -3.970000. running mean: -28.492204\n",
      "ep 1051: ep_len:172 episode reward: total was -8.460000. running mean: -28.291882\n",
      "ep 1051: ep_len:570 episode reward: total was -34.690000. running mean: -28.355864\n",
      "epsilon:0.256402 episode_count: 7364. steps_count: 3278117.000000\n",
      "ep 1052: ep_len:555 episode reward: total was -17.700000. running mean: -28.249305\n",
      "ep 1052: ep_len:500 episode reward: total was -25.580000. running mean: -28.222612\n",
      "ep 1052: ep_len:605 episode reward: total was -25.190000. running mean: -28.192286\n",
      "ep 1052: ep_len:535 episode reward: total was -27.590000. running mean: -28.186263\n",
      "ep 1052: ep_len:104 episode reward: total was -12.440000. running mean: -28.028800\n",
      "ep 1052: ep_len:500 episode reward: total was -36.850000. running mean: -28.117012\n",
      "ep 1052: ep_len:600 episode reward: total was -29.690000. running mean: -28.132742\n",
      "epsilon:0.256265 episode_count: 7371. steps_count: 3281516.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1053: ep_len:560 episode reward: total was -26.030000. running mean: -28.111715\n",
      "ep 1053: ep_len:580 episode reward: total was -18.070000. running mean: -28.011298\n",
      "ep 1053: ep_len:555 episode reward: total was -19.670000. running mean: -27.927885\n",
      "ep 1053: ep_len:530 episode reward: total was -8.090000. running mean: -27.729506\n",
      "ep 1053: ep_len:3 episode reward: total was 0.000000. running mean: -27.452211\n",
      "ep 1053: ep_len:170 episode reward: total was -17.940000. running mean: -27.357089\n",
      "ep 1053: ep_len:575 episode reward: total was -28.110000. running mean: -27.364618\n",
      "epsilon:0.256129 episode_count: 7378. steps_count: 3284489.000000\n",
      "ep 1054: ep_len:264 episode reward: total was -17.930000. running mean: -27.270272\n",
      "ep 1054: ep_len:505 episode reward: total was -11.870000. running mean: -27.116269\n",
      "ep 1054: ep_len:550 episode reward: total was -13.150000. running mean: -26.976606\n",
      "ep 1054: ep_len:530 episode reward: total was -25.660000. running mean: -26.963440\n",
      "ep 1054: ep_len:93 episode reward: total was -8.960000. running mean: -26.783406\n",
      "ep 1054: ep_len:555 episode reward: total was -25.450000. running mean: -26.770072\n",
      "ep 1054: ep_len:520 episode reward: total was -61.760000. running mean: -27.119971\n",
      "epsilon:0.255992 episode_count: 7385. steps_count: 3287506.000000\n",
      "ep 1055: ep_len:620 episode reward: total was -72.140000. running mean: -27.570171\n",
      "ep 1055: ep_len:500 episode reward: total was -28.930000. running mean: -27.583769\n",
      "ep 1055: ep_len:359 episode reward: total was -22.370000. running mean: -27.531632\n",
      "ep 1055: ep_len:615 episode reward: total was -20.470000. running mean: -27.461015\n",
      "ep 1055: ep_len:3 episode reward: total was 0.000000. running mean: -27.186405\n",
      "ep 1055: ep_len:540 episode reward: total was -27.170000. running mean: -27.186241\n",
      "ep 1055: ep_len:500 episode reward: total was -22.310000. running mean: -27.137479\n",
      "epsilon:0.255856 episode_count: 7392. steps_count: 3290643.000000\n",
      "ep 1056: ep_len:580 episode reward: total was -38.680000. running mean: -27.252904\n",
      "ep 1056: ep_len:500 episode reward: total was -7.860000. running mean: -27.058975\n",
      "ep 1056: ep_len:515 episode reward: total was -35.970000. running mean: -27.148085\n",
      "ep 1056: ep_len:515 episode reward: total was -42.170000. running mean: -27.298304\n",
      "ep 1056: ep_len:3 episode reward: total was 0.000000. running mean: -27.025321\n",
      "ep 1056: ep_len:180 episode reward: total was -2.930000. running mean: -26.784368\n",
      "ep 1056: ep_len:185 episode reward: total was -14.920000. running mean: -26.665724\n",
      "epsilon:0.255719 episode_count: 7399. steps_count: 3293121.000000\n",
      "ep 1057: ep_len:535 episode reward: total was -43.050000. running mean: -26.829567\n",
      "ep 1057: ep_len:168 episode reward: total was -17.460000. running mean: -26.735872\n",
      "ep 1057: ep_len:425 episode reward: total was -19.800000. running mean: -26.666513\n",
      "ep 1057: ep_len:630 episode reward: total was -24.020000. running mean: -26.640048\n",
      "ep 1057: ep_len:108 episode reward: total was -8.450000. running mean: -26.458147\n",
      "ep 1057: ep_len:625 episode reward: total was -12.960000. running mean: -26.323166\n",
      "ep 1057: ep_len:595 episode reward: total was -43.080000. running mean: -26.490734\n",
      "epsilon:0.255583 episode_count: 7406. steps_count: 3296207.000000\n",
      "ep 1058: ep_len:265 episode reward: total was -9.870000. running mean: -26.324527\n",
      "ep 1058: ep_len:570 episode reward: total was -18.720000. running mean: -26.248481\n",
      "ep 1058: ep_len:550 episode reward: total was -33.360000. running mean: -26.319597\n",
      "ep 1058: ep_len:555 episode reward: total was -43.230000. running mean: -26.488701\n",
      "ep 1058: ep_len:3 episode reward: total was 0.000000. running mean: -26.223814\n",
      "ep 1058: ep_len:500 episode reward: total was -29.850000. running mean: -26.260076\n",
      "ep 1058: ep_len:187 episode reward: total was -29.480000. running mean: -26.292275\n",
      "epsilon:0.255446 episode_count: 7413. steps_count: 3298837.000000\n",
      "ep 1059: ep_len:540 episode reward: total was -32.590000. running mean: -26.355252\n",
      "ep 1059: ep_len:185 episode reward: total was -17.470000. running mean: -26.266400\n",
      "ep 1059: ep_len:665 episode reward: total was -46.130000. running mean: -26.465036\n",
      "ep 1059: ep_len:510 episode reward: total was -27.610000. running mean: -26.476485\n",
      "ep 1059: ep_len:3 episode reward: total was 0.000000. running mean: -26.211720\n",
      "ep 1059: ep_len:515 episode reward: total was -29.850000. running mean: -26.248103\n",
      "ep 1059: ep_len:535 episode reward: total was -33.010000. running mean: -26.315722\n",
      "epsilon:0.255310 episode_count: 7420. steps_count: 3301790.000000\n",
      "ep 1060: ep_len:530 episode reward: total was -28.840000. running mean: -26.340965\n",
      "ep 1060: ep_len:640 episode reward: total was -59.610000. running mean: -26.673655\n",
      "ep 1060: ep_len:393 episode reward: total was -27.350000. running mean: -26.680419\n",
      "ep 1060: ep_len:500 episode reward: total was -38.730000. running mean: -26.800915\n",
      "ep 1060: ep_len:90 episode reward: total was 0.530000. running mean: -26.527605\n",
      "ep 1060: ep_len:685 episode reward: total was -62.030000. running mean: -26.882629\n",
      "ep 1060: ep_len:201 episode reward: total was -27.430000. running mean: -26.888103\n",
      "epsilon:0.255173 episode_count: 7427. steps_count: 3304829.000000\n",
      "ep 1061: ep_len:134 episode reward: total was -2.450000. running mean: -26.643722\n",
      "ep 1061: ep_len:615 episode reward: total was -31.180000. running mean: -26.689085\n",
      "ep 1061: ep_len:500 episode reward: total was -29.790000. running mean: -26.720094\n",
      "ep 1061: ep_len:505 episode reward: total was -37.190000. running mean: -26.824793\n",
      "ep 1061: ep_len:3 episode reward: total was 0.000000. running mean: -26.556545\n",
      "ep 1061: ep_len:301 episode reward: total was -16.890000. running mean: -26.459880\n",
      "ep 1061: ep_len:515 episode reward: total was -47.120000. running mean: -26.666481\n",
      "epsilon:0.255037 episode_count: 7434. steps_count: 3307402.000000\n",
      "ep 1062: ep_len:675 episode reward: total was -44.880000. running mean: -26.848616\n",
      "ep 1062: ep_len:635 episode reward: total was -16.060000. running mean: -26.740730\n",
      "ep 1062: ep_len:580 episode reward: total was -59.750000. running mean: -27.070823\n",
      "ep 1062: ep_len:500 episode reward: total was -27.090000. running mean: -27.071014\n",
      "ep 1062: ep_len:94 episode reward: total was -13.960000. running mean: -26.939904\n",
      "ep 1062: ep_len:590 episode reward: total was -23.140000. running mean: -26.901905\n",
      "ep 1062: ep_len:500 episode reward: total was -61.860000. running mean: -27.251486\n",
      "epsilon:0.254900 episode_count: 7441. steps_count: 3310976.000000\n",
      "ep 1063: ep_len:500 episode reward: total was -39.350000. running mean: -27.372471\n",
      "ep 1063: ep_len:510 episode reward: total was -33.130000. running mean: -27.430046\n",
      "ep 1063: ep_len:665 episode reward: total was -38.040000. running mean: -27.536146\n",
      "ep 1063: ep_len:500 episode reward: total was -13.160000. running mean: -27.392385\n",
      "ep 1063: ep_len:3 episode reward: total was 0.000000. running mean: -27.118461\n",
      "ep 1063: ep_len:500 episode reward: total was -23.340000. running mean: -27.080676\n",
      "ep 1063: ep_len:555 episode reward: total was -42.770000. running mean: -27.237569\n",
      "epsilon:0.254764 episode_count: 7448. steps_count: 3314209.000000\n",
      "ep 1064: ep_len:210 episode reward: total was -18.820000. running mean: -27.153394\n",
      "ep 1064: ep_len:505 episode reward: total was -14.070000. running mean: -27.022560\n",
      "ep 1064: ep_len:79 episode reward: total was -1.460000. running mean: -26.766934\n",
      "ep 1064: ep_len:525 episode reward: total was -14.160000. running mean: -26.640865\n",
      "ep 1064: ep_len:3 episode reward: total was 0.000000. running mean: -26.374456\n",
      "ep 1064: ep_len:565 episode reward: total was -45.560000. running mean: -26.566312\n",
      "ep 1064: ep_len:580 episode reward: total was -30.550000. running mean: -26.606148\n",
      "epsilon:0.254627 episode_count: 7455. steps_count: 3316676.000000\n",
      "ep 1065: ep_len:555 episode reward: total was -23.070000. running mean: -26.570787\n",
      "ep 1065: ep_len:640 episode reward: total was -42.080000. running mean: -26.725879\n",
      "ep 1065: ep_len:515 episode reward: total was -22.930000. running mean: -26.687920\n",
      "ep 1065: ep_len:550 episode reward: total was -30.220000. running mean: -26.723241\n",
      "ep 1065: ep_len:3 episode reward: total was 0.000000. running mean: -26.456009\n",
      "ep 1065: ep_len:655 episode reward: total was -30.110000. running mean: -26.492549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1065: ep_len:510 episode reward: total was -21.920000. running mean: -26.446823\n",
      "epsilon:0.254491 episode_count: 7462. steps_count: 3320104.000000\n",
      "ep 1066: ep_len:640 episode reward: total was -27.570000. running mean: -26.458055\n",
      "ep 1066: ep_len:201 episode reward: total was -7.930000. running mean: -26.272774\n",
      "ep 1066: ep_len:505 episode reward: total was -35.050000. running mean: -26.360547\n",
      "ep 1066: ep_len:655 episode reward: total was -68.120000. running mean: -26.778141\n",
      "ep 1066: ep_len:3 episode reward: total was 0.000000. running mean: -26.510360\n",
      "ep 1066: ep_len:650 episode reward: total was -43.140000. running mean: -26.676656\n",
      "ep 1066: ep_len:505 episode reward: total was -18.120000. running mean: -26.591090\n",
      "epsilon:0.254354 episode_count: 7469. steps_count: 3323263.000000\n",
      "ep 1067: ep_len:247 episode reward: total was -3.400000. running mean: -26.359179\n",
      "ep 1067: ep_len:500 episode reward: total was -9.530000. running mean: -26.190887\n",
      "ep 1067: ep_len:525 episode reward: total was -43.020000. running mean: -26.359178\n",
      "ep 1067: ep_len:525 episode reward: total was -53.210000. running mean: -26.627686\n",
      "ep 1067: ep_len:3 episode reward: total was 0.000000. running mean: -26.361409\n",
      "ep 1067: ep_len:620 episode reward: total was -40.710000. running mean: -26.504895\n",
      "ep 1067: ep_len:500 episode reward: total was -23.020000. running mean: -26.470046\n",
      "epsilon:0.254218 episode_count: 7476. steps_count: 3326183.000000\n",
      "ep 1068: ep_len:560 episode reward: total was -34.170000. running mean: -26.547046\n",
      "ep 1068: ep_len:615 episode reward: total was -26.450000. running mean: -26.546075\n",
      "ep 1068: ep_len:630 episode reward: total was -47.370000. running mean: -26.754315\n",
      "ep 1068: ep_len:500 episode reward: total was -24.150000. running mean: -26.728272\n",
      "ep 1068: ep_len:3 episode reward: total was 0.000000. running mean: -26.460989\n",
      "ep 1068: ep_len:525 episode reward: total was -22.990000. running mean: -26.426279\n",
      "ep 1068: ep_len:585 episode reward: total was -50.480000. running mean: -26.666816\n",
      "epsilon:0.254081 episode_count: 7483. steps_count: 3329601.000000\n",
      "ep 1069: ep_len:525 episode reward: total was -29.560000. running mean: -26.695748\n",
      "ep 1069: ep_len:505 episode reward: total was -12.210000. running mean: -26.550890\n",
      "ep 1069: ep_len:570 episode reward: total was -42.940000. running mean: -26.714782\n",
      "ep 1069: ep_len:500 episode reward: total was -6.100000. running mean: -26.508634\n",
      "ep 1069: ep_len:109 episode reward: total was -9.460000. running mean: -26.338147\n",
      "ep 1069: ep_len:610 episode reward: total was -97.830000. running mean: -27.053066\n",
      "ep 1069: ep_len:615 episode reward: total was -48.660000. running mean: -27.269135\n",
      "epsilon:0.253945 episode_count: 7490. steps_count: 3333035.000000\n",
      "ep 1070: ep_len:640 episode reward: total was -43.800000. running mean: -27.434444\n",
      "ep 1070: ep_len:500 episode reward: total was -25.100000. running mean: -27.411099\n",
      "ep 1070: ep_len:446 episode reward: total was -4.250000. running mean: -27.179488\n",
      "ep 1070: ep_len:515 episode reward: total was -29.140000. running mean: -27.199094\n",
      "ep 1070: ep_len:3 episode reward: total was 0.000000. running mean: -26.927103\n",
      "ep 1070: ep_len:580 episode reward: total was -28.400000. running mean: -26.941832\n",
      "ep 1070: ep_len:510 episode reward: total was -39.340000. running mean: -27.065813\n",
      "epsilon:0.253808 episode_count: 7497. steps_count: 3336229.000000\n",
      "ep 1071: ep_len:212 episode reward: total was -1.390000. running mean: -26.809055\n",
      "ep 1071: ep_len:630 episode reward: total was -46.360000. running mean: -27.004565\n",
      "ep 1071: ep_len:720 episode reward: total was -40.410000. running mean: -27.138619\n",
      "ep 1071: ep_len:590 episode reward: total was -22.080000. running mean: -27.088033\n",
      "ep 1071: ep_len:3 episode reward: total was 0.000000. running mean: -26.817152\n",
      "ep 1071: ep_len:620 episode reward: total was -21.640000. running mean: -26.765381\n",
      "ep 1071: ep_len:560 episode reward: total was -29.630000. running mean: -26.794027\n",
      "epsilon:0.253672 episode_count: 7504. steps_count: 3339564.000000\n",
      "ep 1072: ep_len:183 episode reward: total was 0.080000. running mean: -26.525287\n",
      "ep 1072: ep_len:500 episode reward: total was -48.310000. running mean: -26.743134\n",
      "ep 1072: ep_len:525 episode reward: total was -34.710000. running mean: -26.822803\n",
      "ep 1072: ep_len:585 episode reward: total was -59.150000. running mean: -27.146075\n",
      "ep 1072: ep_len:3 episode reward: total was 0.000000. running mean: -26.874614\n",
      "ep 1072: ep_len:525 episode reward: total was -30.210000. running mean: -26.907968\n",
      "ep 1072: ep_len:590 episode reward: total was -42.490000. running mean: -27.063788\n",
      "epsilon:0.253535 episode_count: 7511. steps_count: 3342475.000000\n",
      "ep 1073: ep_len:595 episode reward: total was -48.630000. running mean: -27.279450\n",
      "ep 1073: ep_len:500 episode reward: total was -27.050000. running mean: -27.277156\n",
      "ep 1073: ep_len:575 episode reward: total was -51.040000. running mean: -27.514784\n",
      "ep 1073: ep_len:515 episode reward: total was -28.610000. running mean: -27.525736\n",
      "ep 1073: ep_len:3 episode reward: total was 0.000000. running mean: -27.250479\n",
      "ep 1073: ep_len:525 episode reward: total was -42.640000. running mean: -27.404374\n",
      "ep 1073: ep_len:750 episode reward: total was -91.200000. running mean: -28.042330\n",
      "epsilon:0.253399 episode_count: 7518. steps_count: 3345938.000000\n",
      "ep 1074: ep_len:665 episode reward: total was -46.030000. running mean: -28.222207\n",
      "ep 1074: ep_len:500 episode reward: total was -27.350000. running mean: -28.213485\n",
      "ep 1074: ep_len:535 episode reward: total was -40.460000. running mean: -28.335950\n",
      "ep 1074: ep_len:595 episode reward: total was -19.620000. running mean: -28.248791\n",
      "ep 1074: ep_len:33 episode reward: total was 1.500000. running mean: -27.951303\n",
      "ep 1074: ep_len:525 episode reward: total was -22.680000. running mean: -27.898590\n",
      "ep 1074: ep_len:605 episode reward: total was -34.000000. running mean: -27.959604\n",
      "epsilon:0.253262 episode_count: 7525. steps_count: 3349396.000000\n",
      "ep 1075: ep_len:555 episode reward: total was -24.060000. running mean: -27.920608\n",
      "ep 1075: ep_len:560 episode reward: total was -17.580000. running mean: -27.817202\n",
      "ep 1075: ep_len:605 episode reward: total was -42.590000. running mean: -27.964930\n",
      "ep 1075: ep_len:500 episode reward: total was -7.130000. running mean: -27.756580\n",
      "ep 1075: ep_len:3 episode reward: total was 0.000000. running mean: -27.479015\n",
      "ep 1075: ep_len:665 episode reward: total was -25.890000. running mean: -27.463124\n",
      "ep 1075: ep_len:500 episode reward: total was -35.210000. running mean: -27.540593\n",
      "epsilon:0.253126 episode_count: 7532. steps_count: 3352784.000000\n",
      "ep 1076: ep_len:580 episode reward: total was -31.010000. running mean: -27.575287\n",
      "ep 1076: ep_len:505 episode reward: total was -34.060000. running mean: -27.640134\n",
      "ep 1076: ep_len:388 episode reward: total was -17.290000. running mean: -27.536633\n",
      "ep 1076: ep_len:510 episode reward: total was -26.150000. running mean: -27.522767\n",
      "ep 1076: ep_len:115 episode reward: total was -1.950000. running mean: -27.267039\n",
      "ep 1076: ep_len:505 episode reward: total was -33.570000. running mean: -27.330069\n",
      "ep 1076: ep_len:292 episode reward: total was -19.400000. running mean: -27.250768\n",
      "epsilon:0.252989 episode_count: 7539. steps_count: 3355679.000000\n",
      "ep 1077: ep_len:505 episode reward: total was -18.220000. running mean: -27.160460\n",
      "ep 1077: ep_len:187 episode reward: total was -22.930000. running mean: -27.118156\n",
      "ep 1077: ep_len:67 episode reward: total was 0.050000. running mean: -26.846474\n",
      "ep 1077: ep_len:570 episode reward: total was -31.650000. running mean: -26.894509\n",
      "ep 1077: ep_len:3 episode reward: total was 0.000000. running mean: -26.625564\n",
      "ep 1077: ep_len:595 episode reward: total was -27.850000. running mean: -26.637809\n",
      "ep 1077: ep_len:500 episode reward: total was -21.370000. running mean: -26.585131\n",
      "epsilon:0.252853 episode_count: 7546. steps_count: 3358106.000000\n",
      "ep 1078: ep_len:620 episode reward: total was -48.960000. running mean: -26.808879\n",
      "ep 1078: ep_len:550 episode reward: total was -21.250000. running mean: -26.753290\n",
      "ep 1078: ep_len:545 episode reward: total was -33.480000. running mean: -26.820558\n",
      "ep 1078: ep_len:545 episode reward: total was -57.640000. running mean: -27.128752\n",
      "ep 1078: ep_len:3 episode reward: total was 0.000000. running mean: -26.857464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1078: ep_len:283 episode reward: total was -20.390000. running mean: -26.792790\n",
      "ep 1078: ep_len:520 episode reward: total was -35.520000. running mean: -26.880062\n",
      "epsilon:0.252716 episode_count: 7553. steps_count: 3361172.000000\n",
      "ep 1079: ep_len:500 episode reward: total was -21.680000. running mean: -26.828061\n",
      "ep 1079: ep_len:550 episode reward: total was -17.770000. running mean: -26.737481\n",
      "ep 1079: ep_len:710 episode reward: total was -35.290000. running mean: -26.823006\n",
      "ep 1079: ep_len:530 episode reward: total was -34.080000. running mean: -26.895576\n",
      "ep 1079: ep_len:3 episode reward: total was 0.000000. running mean: -26.626620\n",
      "ep 1079: ep_len:545 episode reward: total was -40.470000. running mean: -26.765054\n",
      "ep 1079: ep_len:605 episode reward: total was -31.570000. running mean: -26.813103\n",
      "epsilon:0.252580 episode_count: 7560. steps_count: 3364615.000000\n",
      "ep 1080: ep_len:500 episode reward: total was -36.480000. running mean: -26.909772\n",
      "ep 1080: ep_len:169 episode reward: total was -19.940000. running mean: -26.840075\n",
      "ep 1080: ep_len:570 episode reward: total was -26.830000. running mean: -26.839974\n",
      "ep 1080: ep_len:56 episode reward: total was -5.980000. running mean: -26.631374\n",
      "ep 1080: ep_len:3 episode reward: total was 0.000000. running mean: -26.365060\n",
      "ep 1080: ep_len:232 episode reward: total was -1.380000. running mean: -26.115210\n",
      "ep 1080: ep_len:565 episode reward: total was -22.610000. running mean: -26.080158\n",
      "epsilon:0.252443 episode_count: 7567. steps_count: 3366710.000000\n",
      "ep 1081: ep_len:565 episode reward: total was -20.200000. running mean: -26.021356\n",
      "ep 1081: ep_len:555 episode reward: total was -40.540000. running mean: -26.166543\n",
      "ep 1081: ep_len:680 episode reward: total was -40.330000. running mean: -26.308177\n",
      "ep 1081: ep_len:570 episode reward: total was -22.120000. running mean: -26.266295\n",
      "ep 1081: ep_len:3 episode reward: total was 0.000000. running mean: -26.003632\n",
      "ep 1081: ep_len:500 episode reward: total was -52.220000. running mean: -26.265796\n",
      "ep 1081: ep_len:615 episode reward: total was -57.260000. running mean: -26.575738\n",
      "epsilon:0.252307 episode_count: 7574. steps_count: 3370198.000000\n",
      "ep 1082: ep_len:510 episode reward: total was -33.150000. running mean: -26.641481\n",
      "ep 1082: ep_len:550 episode reward: total was -26.440000. running mean: -26.639466\n",
      "ep 1082: ep_len:555 episode reward: total was -33.040000. running mean: -26.703471\n",
      "ep 1082: ep_len:505 episode reward: total was -16.780000. running mean: -26.604237\n",
      "ep 1082: ep_len:3 episode reward: total was 0.000000. running mean: -26.338194\n",
      "ep 1082: ep_len:535 episode reward: total was -40.700000. running mean: -26.481812\n",
      "ep 1082: ep_len:520 episode reward: total was -17.630000. running mean: -26.393294\n",
      "epsilon:0.252170 episode_count: 7581. steps_count: 3373376.000000\n",
      "ep 1083: ep_len:720 episode reward: total was -48.830000. running mean: -26.617661\n",
      "ep 1083: ep_len:515 episode reward: total was -14.790000. running mean: -26.499385\n",
      "ep 1083: ep_len:600 episode reward: total was -23.020000. running mean: -26.464591\n",
      "ep 1083: ep_len:500 episode reward: total was -24.280000. running mean: -26.442745\n",
      "ep 1083: ep_len:3 episode reward: total was 0.000000. running mean: -26.178317\n",
      "ep 1083: ep_len:595 episode reward: total was -21.200000. running mean: -26.128534\n",
      "ep 1083: ep_len:565 episode reward: total was -32.950000. running mean: -26.196749\n",
      "epsilon:0.252034 episode_count: 7588. steps_count: 3376874.000000\n",
      "ep 1084: ep_len:127 episode reward: total was -3.470000. running mean: -25.969481\n",
      "ep 1084: ep_len:580 episode reward: total was -35.140000. running mean: -26.061187\n",
      "ep 1084: ep_len:540 episode reward: total was -52.020000. running mean: -26.320775\n",
      "ep 1084: ep_len:600 episode reward: total was -36.660000. running mean: -26.424167\n",
      "ep 1084: ep_len:106 episode reward: total was -14.970000. running mean: -26.309625\n",
      "ep 1084: ep_len:235 episode reward: total was -5.800000. running mean: -26.104529\n",
      "ep 1084: ep_len:605 episode reward: total was -30.750000. running mean: -26.150984\n",
      "epsilon:0.251897 episode_count: 7595. steps_count: 3379667.000000\n",
      "ep 1085: ep_len:595 episode reward: total was -19.660000. running mean: -26.086074\n",
      "ep 1085: ep_len:595 episode reward: total was -15.750000. running mean: -25.982713\n",
      "ep 1085: ep_len:630 episode reward: total was -31.310000. running mean: -26.035986\n",
      "ep 1085: ep_len:124 episode reward: total was -5.980000. running mean: -25.835426\n",
      "ep 1085: ep_len:3 episode reward: total was 0.000000. running mean: -25.577072\n",
      "ep 1085: ep_len:520 episode reward: total was -9.130000. running mean: -25.412601\n",
      "ep 1085: ep_len:530 episode reward: total was -27.190000. running mean: -25.430375\n",
      "epsilon:0.251761 episode_count: 7602. steps_count: 3382664.000000\n",
      "ep 1086: ep_len:500 episode reward: total was -11.130000. running mean: -25.287371\n",
      "ep 1086: ep_len:545 episode reward: total was -60.210000. running mean: -25.636598\n",
      "ep 1086: ep_len:555 episode reward: total was -36.990000. running mean: -25.750132\n",
      "ep 1086: ep_len:525 episode reward: total was -12.130000. running mean: -25.613930\n",
      "ep 1086: ep_len:3 episode reward: total was 0.000000. running mean: -25.357791\n",
      "ep 1086: ep_len:500 episode reward: total was -14.000000. running mean: -25.244213\n",
      "ep 1086: ep_len:605 episode reward: total was -31.180000. running mean: -25.303571\n",
      "epsilon:0.251624 episode_count: 7609. steps_count: 3385897.000000\n",
      "ep 1087: ep_len:580 episode reward: total was -25.050000. running mean: -25.301035\n",
      "ep 1087: ep_len:530 episode reward: total was -16.060000. running mean: -25.208625\n",
      "ep 1087: ep_len:645 episode reward: total was -35.880000. running mean: -25.315339\n",
      "ep 1087: ep_len:500 episode reward: total was -19.060000. running mean: -25.252785\n",
      "ep 1087: ep_len:3 episode reward: total was 0.000000. running mean: -25.000257\n",
      "ep 1087: ep_len:500 episode reward: total was -33.330000. running mean: -25.083555\n",
      "ep 1087: ep_len:635 episode reward: total was -39.480000. running mean: -25.227519\n",
      "epsilon:0.251488 episode_count: 7616. steps_count: 3389290.000000\n",
      "ep 1088: ep_len:253 episode reward: total was -13.920000. running mean: -25.114444\n",
      "ep 1088: ep_len:500 episode reward: total was -24.630000. running mean: -25.109600\n",
      "ep 1088: ep_len:387 episode reward: total was -20.830000. running mean: -25.066804\n",
      "ep 1088: ep_len:170 episode reward: total was -1.410000. running mean: -24.830236\n",
      "ep 1088: ep_len:3 episode reward: total was 0.000000. running mean: -24.581933\n",
      "ep 1088: ep_len:500 episode reward: total was -44.190000. running mean: -24.778014\n",
      "ep 1088: ep_len:570 episode reward: total was -29.240000. running mean: -24.822634\n",
      "epsilon:0.251351 episode_count: 7623. steps_count: 3391673.000000\n",
      "ep 1089: ep_len:630 episode reward: total was -58.830000. running mean: -25.162708\n",
      "ep 1089: ep_len:525 episode reward: total was -29.000000. running mean: -25.201080\n",
      "ep 1089: ep_len:510 episode reward: total was -13.220000. running mean: -25.081270\n",
      "ep 1089: ep_len:385 episode reward: total was -27.200000. running mean: -25.102457\n",
      "ep 1089: ep_len:3 episode reward: total was 0.000000. running mean: -24.851432\n",
      "ep 1089: ep_len:510 episode reward: total was -29.810000. running mean: -24.901018\n",
      "ep 1089: ep_len:540 episode reward: total was -22.420000. running mean: -24.876208\n",
      "epsilon:0.251215 episode_count: 7630. steps_count: 3394776.000000\n",
      "ep 1090: ep_len:580 episode reward: total was -15.710000. running mean: -24.784546\n",
      "ep 1090: ep_len:175 episode reward: total was -9.420000. running mean: -24.630900\n",
      "ep 1090: ep_len:600 episode reward: total was -24.150000. running mean: -24.626091\n",
      "ep 1090: ep_len:170 episode reward: total was -8.930000. running mean: -24.469130\n",
      "ep 1090: ep_len:3 episode reward: total was 0.000000. running mean: -24.224439\n",
      "ep 1090: ep_len:640 episode reward: total was -30.920000. running mean: -24.291395\n",
      "ep 1090: ep_len:203 episode reward: total was -15.370000. running mean: -24.202181\n",
      "epsilon:0.251078 episode_count: 7637. steps_count: 3397147.000000\n",
      "ep 1091: ep_len:660 episode reward: total was -44.990000. running mean: -24.410059\n",
      "ep 1091: ep_len:167 episode reward: total was -13.920000. running mean: -24.305158\n",
      "ep 1091: ep_len:56 episode reward: total was 0.030000. running mean: -24.061807\n",
      "ep 1091: ep_len:169 episode reward: total was -4.430000. running mean: -23.865489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1091: ep_len:3 episode reward: total was 0.000000. running mean: -23.626834\n",
      "ep 1091: ep_len:590 episode reward: total was -31.200000. running mean: -23.702566\n",
      "ep 1091: ep_len:585 episode reward: total was -26.190000. running mean: -23.727440\n",
      "epsilon:0.250942 episode_count: 7644. steps_count: 3399377.000000\n",
      "ep 1092: ep_len:595 episode reward: total was -30.110000. running mean: -23.791265\n",
      "ep 1092: ep_len:565 episode reward: total was -31.790000. running mean: -23.871253\n",
      "ep 1092: ep_len:590 episode reward: total was -57.230000. running mean: -24.204840\n",
      "ep 1092: ep_len:615 episode reward: total was -38.100000. running mean: -24.343792\n",
      "ep 1092: ep_len:3 episode reward: total was 0.000000. running mean: -24.100354\n",
      "ep 1092: ep_len:570 episode reward: total was -30.690000. running mean: -24.166250\n",
      "ep 1092: ep_len:560 episode reward: total was -18.610000. running mean: -24.110688\n",
      "epsilon:0.250805 episode_count: 7651. steps_count: 3402875.000000\n",
      "ep 1093: ep_len:545 episode reward: total was -41.910000. running mean: -24.288681\n",
      "ep 1093: ep_len:178 episode reward: total was -8.930000. running mean: -24.135094\n",
      "ep 1093: ep_len:386 episode reward: total was -9.800000. running mean: -23.991743\n",
      "ep 1093: ep_len:114 episode reward: total was -2.950000. running mean: -23.781326\n",
      "ep 1093: ep_len:3 episode reward: total was 0.000000. running mean: -23.543513\n",
      "ep 1093: ep_len:520 episode reward: total was -36.770000. running mean: -23.675777\n",
      "ep 1093: ep_len:201 episode reward: total was -12.370000. running mean: -23.562720\n",
      "epsilon:0.250669 episode_count: 7658. steps_count: 3404822.000000\n",
      "ep 1094: ep_len:625 episode reward: total was -24.550000. running mean: -23.572592\n",
      "ep 1094: ep_len:500 episode reward: total was -37.670000. running mean: -23.713567\n",
      "ep 1094: ep_len:665 episode reward: total was -45.390000. running mean: -23.930331\n",
      "ep 1094: ep_len:115 episode reward: total was -4.440000. running mean: -23.735428\n",
      "ep 1094: ep_len:102 episode reward: total was -1.440000. running mean: -23.512473\n",
      "ep 1094: ep_len:715 episode reward: total was -47.860000. running mean: -23.755949\n",
      "ep 1094: ep_len:560 episode reward: total was -43.790000. running mean: -23.956289\n",
      "epsilon:0.250532 episode_count: 7665. steps_count: 3408104.000000\n",
      "ep 1095: ep_len:775 episode reward: total was -59.370000. running mean: -24.310426\n",
      "ep 1095: ep_len:500 episode reward: total was -25.250000. running mean: -24.319822\n",
      "ep 1095: ep_len:735 episode reward: total was -70.420000. running mean: -24.780824\n",
      "ep 1095: ep_len:520 episode reward: total was -26.610000. running mean: -24.799115\n",
      "ep 1095: ep_len:114 episode reward: total was -10.940000. running mean: -24.660524\n",
      "ep 1095: ep_len:500 episode reward: total was -55.860000. running mean: -24.972519\n",
      "ep 1095: ep_len:530 episode reward: total was -26.630000. running mean: -24.989094\n",
      "epsilon:0.250396 episode_count: 7672. steps_count: 3411778.000000\n",
      "ep 1096: ep_len:560 episode reward: total was -49.530000. running mean: -25.234503\n",
      "ep 1096: ep_len:595 episode reward: total was -13.550000. running mean: -25.117658\n",
      "ep 1096: ep_len:500 episode reward: total was -10.150000. running mean: -24.967981\n",
      "ep 1096: ep_len:555 episode reward: total was -38.170000. running mean: -25.100002\n",
      "ep 1096: ep_len:82 episode reward: total was -10.460000. running mean: -24.953602\n",
      "ep 1096: ep_len:505 episode reward: total was -37.810000. running mean: -25.082166\n",
      "ep 1096: ep_len:331 episode reward: total was -23.250000. running mean: -25.063844\n",
      "epsilon:0.250259 episode_count: 7679. steps_count: 3414906.000000\n",
      "ep 1097: ep_len:500 episode reward: total was -40.070000. running mean: -25.213905\n",
      "ep 1097: ep_len:590 episode reward: total was -3.170000. running mean: -24.993466\n",
      "ep 1097: ep_len:368 episode reward: total was -10.830000. running mean: -24.851832\n",
      "ep 1097: ep_len:118 episode reward: total was -3.920000. running mean: -24.642513\n",
      "ep 1097: ep_len:3 episode reward: total was 0.000000. running mean: -24.396088\n",
      "ep 1097: ep_len:510 episode reward: total was -19.250000. running mean: -24.344627\n",
      "ep 1097: ep_len:500 episode reward: total was -43.150000. running mean: -24.532681\n",
      "epsilon:0.250123 episode_count: 7686. steps_count: 3417495.000000\n",
      "ep 1098: ep_len:705 episode reward: total was -87.820000. running mean: -25.165554\n",
      "ep 1098: ep_len:605 episode reward: total was -15.620000. running mean: -25.070099\n",
      "ep 1098: ep_len:500 episode reward: total was -20.160000. running mean: -25.020998\n",
      "ep 1098: ep_len:103 episode reward: total was -2.470000. running mean: -24.795488\n",
      "ep 1098: ep_len:86 episode reward: total was -0.980000. running mean: -24.557333\n",
      "ep 1098: ep_len:575 episode reward: total was -27.720000. running mean: -24.588960\n",
      "ep 1098: ep_len:620 episode reward: total was -26.440000. running mean: -24.607470\n",
      "epsilon:0.249986 episode_count: 7693. steps_count: 3420689.000000\n",
      "ep 1099: ep_len:500 episode reward: total was -28.300000. running mean: -24.644395\n",
      "ep 1099: ep_len:585 episode reward: total was -26.510000. running mean: -24.663051\n",
      "ep 1099: ep_len:680 episode reward: total was -20.260000. running mean: -24.619021\n",
      "ep 1099: ep_len:505 episode reward: total was -19.550000. running mean: -24.568331\n",
      "ep 1099: ep_len:3 episode reward: total was 0.000000. running mean: -24.322647\n",
      "ep 1099: ep_len:171 episode reward: total was -13.960000. running mean: -24.219021\n",
      "ep 1099: ep_len:625 episode reward: total was -35.530000. running mean: -24.332131\n",
      "epsilon:0.249850 episode_count: 7700. steps_count: 3423758.000000\n",
      "ep 1100: ep_len:580 episode reward: total was -36.090000. running mean: -24.449709\n",
      "ep 1100: ep_len:645 episode reward: total was -10.510000. running mean: -24.310312\n",
      "ep 1100: ep_len:515 episode reward: total was -37.660000. running mean: -24.443809\n",
      "ep 1100: ep_len:404 episode reward: total was -35.700000. running mean: -24.556371\n",
      "ep 1100: ep_len:78 episode reward: total was -10.960000. running mean: -24.420407\n",
      "ep 1100: ep_len:319 episode reward: total was -12.890000. running mean: -24.305103\n",
      "ep 1100: ep_len:193 episode reward: total was -9.360000. running mean: -24.155652\n",
      "epsilon:0.249713 episode_count: 7707. steps_count: 3426492.000000\n",
      "ep 1101: ep_len:131 episode reward: total was 0.580000. running mean: -23.908296\n",
      "ep 1101: ep_len:200 episode reward: total was -16.390000. running mean: -23.833113\n",
      "ep 1101: ep_len:545 episode reward: total was -21.130000. running mean: -23.806082\n",
      "ep 1101: ep_len:505 episode reward: total was -38.280000. running mean: -23.950821\n",
      "ep 1101: ep_len:3 episode reward: total was 0.000000. running mean: -23.711313\n",
      "ep 1101: ep_len:301 episode reward: total was -15.310000. running mean: -23.627299\n",
      "ep 1101: ep_len:515 episode reward: total was -22.610000. running mean: -23.617126\n",
      "epsilon:0.249577 episode_count: 7714. steps_count: 3428692.000000\n",
      "ep 1102: ep_len:217 episode reward: total was -31.400000. running mean: -23.694955\n",
      "ep 1102: ep_len:550 episode reward: total was -35.240000. running mean: -23.810406\n",
      "ep 1102: ep_len:79 episode reward: total was -0.970000. running mean: -23.582002\n",
      "ep 1102: ep_len:500 episode reward: total was -14.020000. running mean: -23.486382\n",
      "ep 1102: ep_len:49 episode reward: total was 3.000000. running mean: -23.221518\n",
      "ep 1102: ep_len:520 episode reward: total was -40.620000. running mean: -23.395503\n",
      "ep 1102: ep_len:515 episode reward: total was -45.110000. running mean: -23.612648\n",
      "epsilon:0.249440 episode_count: 7721. steps_count: 3431122.000000\n",
      "ep 1103: ep_len:565 episode reward: total was -24.130000. running mean: -23.617821\n",
      "ep 1103: ep_len:525 episode reward: total was -59.860000. running mean: -23.980243\n",
      "ep 1103: ep_len:510 episode reward: total was -36.120000. running mean: -24.101640\n",
      "ep 1103: ep_len:155 episode reward: total was -3.920000. running mean: -23.899824\n",
      "ep 1103: ep_len:109 episode reward: total was -9.950000. running mean: -23.760326\n",
      "ep 1103: ep_len:605 episode reward: total was -58.090000. running mean: -24.103623\n",
      "ep 1103: ep_len:545 episode reward: total was -40.690000. running mean: -24.269486\n",
      "epsilon:0.249304 episode_count: 7728. steps_count: 3434136.000000\n",
      "ep 1104: ep_len:580 episode reward: total was -58.200000. running mean: -24.608791\n",
      "ep 1104: ep_len:575 episode reward: total was -75.210000. running mean: -25.114804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1104: ep_len:590 episode reward: total was -39.970000. running mean: -25.263355\n",
      "ep 1104: ep_len:530 episode reward: total was -16.190000. running mean: -25.172622\n",
      "ep 1104: ep_len:84 episode reward: total was -7.950000. running mean: -25.000396\n",
      "ep 1104: ep_len:585 episode reward: total was -29.250000. running mean: -25.042892\n",
      "ep 1104: ep_len:272 episode reward: total was -10.320000. running mean: -24.895663\n",
      "epsilon:0.249167 episode_count: 7735. steps_count: 3437352.000000\n",
      "ep 1105: ep_len:535 episode reward: total was -55.590000. running mean: -25.202606\n",
      "ep 1105: ep_len:505 episode reward: total was -45.810000. running mean: -25.408680\n",
      "ep 1105: ep_len:500 episode reward: total was -20.190000. running mean: -25.356493\n",
      "ep 1105: ep_len:170 episode reward: total was -2.910000. running mean: -25.132028\n",
      "ep 1105: ep_len:121 episode reward: total was -18.970000. running mean: -25.070408\n",
      "ep 1105: ep_len:241 episode reward: total was -9.350000. running mean: -24.913204\n",
      "ep 1105: ep_len:353 episode reward: total was -22.790000. running mean: -24.891972\n",
      "epsilon:0.249031 episode_count: 7742. steps_count: 3439777.000000\n",
      "ep 1106: ep_len:665 episode reward: total was -69.030000. running mean: -25.333352\n",
      "ep 1106: ep_len:560 episode reward: total was -13.240000. running mean: -25.212419\n",
      "ep 1106: ep_len:565 episode reward: total was -48.510000. running mean: -25.445395\n",
      "ep 1106: ep_len:500 episode reward: total was -42.180000. running mean: -25.612741\n",
      "ep 1106: ep_len:106 episode reward: total was -4.460000. running mean: -25.401213\n",
      "ep 1106: ep_len:600 episode reward: total was -48.230000. running mean: -25.629501\n",
      "ep 1106: ep_len:535 episode reward: total was -47.620000. running mean: -25.849406\n",
      "epsilon:0.248894 episode_count: 7749. steps_count: 3443308.000000\n",
      "ep 1107: ep_len:585 episode reward: total was -23.670000. running mean: -25.827612\n",
      "ep 1107: ep_len:550 episode reward: total was -74.220000. running mean: -26.311536\n",
      "ep 1107: ep_len:575 episode reward: total was -37.930000. running mean: -26.427721\n",
      "ep 1107: ep_len:56 episode reward: total was -7.970000. running mean: -26.243143\n",
      "ep 1107: ep_len:78 episode reward: total was -10.960000. running mean: -26.090312\n",
      "ep 1107: ep_len:610 episode reward: total was -22.680000. running mean: -26.056209\n",
      "ep 1107: ep_len:565 episode reward: total was -44.600000. running mean: -26.241647\n",
      "epsilon:0.248758 episode_count: 7756. steps_count: 3446327.000000\n",
      "ep 1108: ep_len:625 episode reward: total was -49.970000. running mean: -26.478930\n",
      "ep 1108: ep_len:371 episode reward: total was -45.310000. running mean: -26.667241\n",
      "ep 1108: ep_len:520 episode reward: total was -25.150000. running mean: -26.652069\n",
      "ep 1108: ep_len:377 episode reward: total was -31.230000. running mean: -26.697848\n",
      "ep 1108: ep_len:3 episode reward: total was 0.000000. running mean: -26.430869\n",
      "ep 1108: ep_len:760 episode reward: total was -55.390000. running mean: -26.720461\n",
      "ep 1108: ep_len:655 episode reward: total was -58.080000. running mean: -27.034056\n",
      "epsilon:0.248621 episode_count: 7763. steps_count: 3449638.000000\n",
      "ep 1109: ep_len:585 episode reward: total was -38.880000. running mean: -27.152515\n",
      "ep 1109: ep_len:525 episode reward: total was -44.870000. running mean: -27.329690\n",
      "ep 1109: ep_len:447 episode reward: total was -54.330000. running mean: -27.599693\n",
      "ep 1109: ep_len:56 episode reward: total was -4.990000. running mean: -27.373596\n",
      "ep 1109: ep_len:3 episode reward: total was 0.000000. running mean: -27.099861\n",
      "ep 1109: ep_len:625 episode reward: total was -27.090000. running mean: -27.099762\n",
      "ep 1109: ep_len:550 episode reward: total was -39.680000. running mean: -27.225564\n",
      "epsilon:0.248485 episode_count: 7770. steps_count: 3452429.000000\n",
      "ep 1110: ep_len:264 episode reward: total was -10.890000. running mean: -27.062209\n",
      "ep 1110: ep_len:600 episode reward: total was -43.730000. running mean: -27.228887\n",
      "ep 1110: ep_len:79 episode reward: total was -6.970000. running mean: -27.026298\n",
      "ep 1110: ep_len:51 episode reward: total was -1.950000. running mean: -26.775535\n",
      "ep 1110: ep_len:3 episode reward: total was 0.000000. running mean: -26.507779\n",
      "ep 1110: ep_len:520 episode reward: total was -35.610000. running mean: -26.598802\n",
      "ep 1110: ep_len:635 episode reward: total was -43.600000. running mean: -26.768814\n",
      "epsilon:0.248348 episode_count: 7777. steps_count: 3454581.000000\n",
      "ep 1111: ep_len:615 episode reward: total was -17.480000. running mean: -26.675925\n",
      "ep 1111: ep_len:560 episode reward: total was -28.250000. running mean: -26.691666\n",
      "ep 1111: ep_len:72 episode reward: total was -2.970000. running mean: -26.454450\n",
      "ep 1111: ep_len:505 episode reward: total was -36.620000. running mean: -26.556105\n",
      "ep 1111: ep_len:78 episode reward: total was -3.480000. running mean: -26.325344\n",
      "ep 1111: ep_len:635 episode reward: total was -49.980000. running mean: -26.561891\n",
      "ep 1111: ep_len:570 episode reward: total was -33.000000. running mean: -26.626272\n",
      "epsilon:0.248212 episode_count: 7784. steps_count: 3457616.000000\n",
      "ep 1112: ep_len:500 episode reward: total was -18.170000. running mean: -26.541709\n",
      "ep 1112: ep_len:500 episode reward: total was -33.290000. running mean: -26.609192\n",
      "ep 1112: ep_len:640 episode reward: total was -40.560000. running mean: -26.748700\n",
      "ep 1112: ep_len:525 episode reward: total was -38.200000. running mean: -26.863213\n",
      "ep 1112: ep_len:3 episode reward: total was 0.000000. running mean: -26.594581\n",
      "ep 1112: ep_len:500 episode reward: total was -30.370000. running mean: -26.632335\n",
      "ep 1112: ep_len:681 episode reward: total was -58.940000. running mean: -26.955412\n",
      "epsilon:0.248075 episode_count: 7791. steps_count: 3460965.000000\n",
      "ep 1113: ep_len:194 episode reward: total was -1.910000. running mean: -26.704958\n",
      "ep 1113: ep_len:585 episode reward: total was -41.200000. running mean: -26.849908\n",
      "ep 1113: ep_len:545 episode reward: total was -26.030000. running mean: -26.841709\n",
      "ep 1113: ep_len:510 episode reward: total was -40.620000. running mean: -26.979492\n",
      "ep 1113: ep_len:48 episode reward: total was 0.000000. running mean: -26.709697\n",
      "ep 1113: ep_len:309 episode reward: total was -26.390000. running mean: -26.706500\n",
      "ep 1113: ep_len:515 episode reward: total was -39.580000. running mean: -26.835235\n",
      "epsilon:0.247939 episode_count: 7798. steps_count: 3463671.000000\n",
      "ep 1114: ep_len:545 episode reward: total was -17.010000. running mean: -26.736983\n",
      "ep 1114: ep_len:525 episode reward: total was -33.640000. running mean: -26.806013\n",
      "ep 1114: ep_len:520 episode reward: total was -22.890000. running mean: -26.766853\n",
      "ep 1114: ep_len:515 episode reward: total was -43.700000. running mean: -26.936184\n",
      "ep 1114: ep_len:90 episode reward: total was -2.470000. running mean: -26.691522\n",
      "ep 1114: ep_len:515 episode reward: total was -49.810000. running mean: -26.922707\n",
      "ep 1114: ep_len:500 episode reward: total was -39.000000. running mean: -27.043480\n",
      "epsilon:0.247802 episode_count: 7805. steps_count: 3466881.000000\n",
      "ep 1115: ep_len:560 episode reward: total was -46.160000. running mean: -27.234645\n",
      "ep 1115: ep_len:680 episode reward: total was -32.150000. running mean: -27.283799\n",
      "ep 1115: ep_len:500 episode reward: total was -71.840000. running mean: -27.729361\n",
      "ep 1115: ep_len:550 episode reward: total was 2.920000. running mean: -27.422867\n",
      "ep 1115: ep_len:3 episode reward: total was 0.000000. running mean: -27.148638\n",
      "ep 1115: ep_len:580 episode reward: total was -31.580000. running mean: -27.192952\n",
      "ep 1115: ep_len:575 episode reward: total was -28.160000. running mean: -27.202622\n",
      "epsilon:0.247666 episode_count: 7812. steps_count: 3470329.000000\n",
      "ep 1116: ep_len:550 episode reward: total was -37.130000. running mean: -27.301896\n",
      "ep 1116: ep_len:500 episode reward: total was -32.430000. running mean: -27.353177\n",
      "ep 1116: ep_len:411 episode reward: total was -28.860000. running mean: -27.368246\n",
      "ep 1116: ep_len:500 episode reward: total was -33.620000. running mean: -27.430763\n",
      "ep 1116: ep_len:3 episode reward: total was 0.000000. running mean: -27.156455\n",
      "ep 1116: ep_len:595 episode reward: total was -41.020000. running mean: -27.295091\n",
      "ep 1116: ep_len:585 episode reward: total was -43.130000. running mean: -27.453440\n",
      "epsilon:0.247529 episode_count: 7819. steps_count: 3473473.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1117: ep_len:600 episode reward: total was -35.160000. running mean: -27.530506\n",
      "ep 1117: ep_len:500 episode reward: total was -27.040000. running mean: -27.525601\n",
      "ep 1117: ep_len:585 episode reward: total was -54.640000. running mean: -27.796745\n",
      "ep 1117: ep_len:422 episode reward: total was -25.790000. running mean: -27.776677\n",
      "ep 1117: ep_len:3 episode reward: total was 0.000000. running mean: -27.498910\n",
      "ep 1117: ep_len:540 episode reward: total was -46.130000. running mean: -27.685221\n",
      "ep 1117: ep_len:565 episode reward: total was -57.110000. running mean: -27.979469\n",
      "epsilon:0.247393 episode_count: 7826. steps_count: 3476688.000000\n",
      "ep 1118: ep_len:248 episode reward: total was -27.830000. running mean: -27.977974\n",
      "ep 1118: ep_len:500 episode reward: total was -36.850000. running mean: -28.066695\n",
      "ep 1118: ep_len:515 episode reward: total was -37.180000. running mean: -28.157828\n",
      "ep 1118: ep_len:500 episode reward: total was -42.720000. running mean: -28.303449\n",
      "ep 1118: ep_len:94 episode reward: total was -1.970000. running mean: -28.040115\n",
      "ep 1118: ep_len:500 episode reward: total was -36.070000. running mean: -28.120414\n",
      "ep 1118: ep_len:189 episode reward: total was -9.430000. running mean: -27.933510\n",
      "epsilon:0.247256 episode_count: 7833. steps_count: 3479234.000000\n",
      "ep 1119: ep_len:116 episode reward: total was -4.990000. running mean: -27.704074\n",
      "ep 1119: ep_len:525 episode reward: total was -34.480000. running mean: -27.771834\n",
      "ep 1119: ep_len:660 episode reward: total was -40.400000. running mean: -27.898115\n",
      "ep 1119: ep_len:529 episode reward: total was -31.630000. running mean: -27.935434\n",
      "ep 1119: ep_len:126 episode reward: total was -1.970000. running mean: -27.675780\n",
      "ep 1119: ep_len:900 episode reward: total was -116.080000. running mean: -28.559822\n",
      "ep 1119: ep_len:595 episode reward: total was -35.460000. running mean: -28.628824\n",
      "epsilon:0.247120 episode_count: 7840. steps_count: 3482685.000000\n",
      "ep 1120: ep_len:535 episode reward: total was -23.060000. running mean: -28.573136\n",
      "ep 1120: ep_len:500 episode reward: total was -39.840000. running mean: -28.685804\n",
      "ep 1120: ep_len:755 episode reward: total was -67.410000. running mean: -29.073046\n",
      "ep 1120: ep_len:56 episode reward: total was -1.960000. running mean: -28.801916\n",
      "ep 1120: ep_len:90 episode reward: total was -0.970000. running mean: -28.523597\n",
      "ep 1120: ep_len:500 episode reward: total was -16.580000. running mean: -28.404161\n",
      "ep 1120: ep_len:500 episode reward: total was -44.600000. running mean: -28.566119\n",
      "epsilon:0.246983 episode_count: 7847. steps_count: 3485621.000000\n",
      "ep 1121: ep_len:255 episode reward: total was -2.400000. running mean: -28.304458\n",
      "ep 1121: ep_len:500 episode reward: total was -58.710000. running mean: -28.608513\n",
      "ep 1121: ep_len:560 episode reward: total was -41.440000. running mean: -28.736828\n",
      "ep 1121: ep_len:500 episode reward: total was -20.760000. running mean: -28.657060\n",
      "ep 1121: ep_len:3 episode reward: total was 0.000000. running mean: -28.370489\n",
      "ep 1121: ep_len:505 episode reward: total was -42.320000. running mean: -28.509984\n",
      "ep 1121: ep_len:505 episode reward: total was -49.300000. running mean: -28.717885\n",
      "epsilon:0.246847 episode_count: 7854. steps_count: 3488449.000000\n",
      "ep 1122: ep_len:117 episode reward: total was -9.480000. running mean: -28.525506\n",
      "ep 1122: ep_len:515 episode reward: total was -5.290000. running mean: -28.293151\n",
      "ep 1122: ep_len:500 episode reward: total was -30.110000. running mean: -28.311319\n",
      "ep 1122: ep_len:500 episode reward: total was -23.190000. running mean: -28.260106\n",
      "ep 1122: ep_len:84 episode reward: total was -2.480000. running mean: -28.002305\n",
      "ep 1122: ep_len:545 episode reward: total was -40.310000. running mean: -28.125382\n",
      "ep 1122: ep_len:565 episode reward: total was -22.450000. running mean: -28.068628\n",
      "epsilon:0.246710 episode_count: 7861. steps_count: 3491275.000000\n",
      "ep 1123: ep_len:252 episode reward: total was -15.930000. running mean: -27.947242\n",
      "ep 1123: ep_len:690 episode reward: total was -35.100000. running mean: -28.018769\n",
      "ep 1123: ep_len:605 episode reward: total was -39.490000. running mean: -28.133482\n",
      "ep 1123: ep_len:570 episode reward: total was -22.690000. running mean: -28.079047\n",
      "ep 1123: ep_len:3 episode reward: total was 0.000000. running mean: -27.798256\n",
      "ep 1123: ep_len:540 episode reward: total was -27.060000. running mean: -27.790874\n",
      "ep 1123: ep_len:185 episode reward: total was -7.900000. running mean: -27.591965\n",
      "epsilon:0.246574 episode_count: 7868. steps_count: 3494120.000000\n",
      "ep 1124: ep_len:670 episode reward: total was -52.940000. running mean: -27.845445\n",
      "ep 1124: ep_len:555 episode reward: total was -2.140000. running mean: -27.588391\n",
      "ep 1124: ep_len:500 episode reward: total was -34.500000. running mean: -27.657507\n",
      "ep 1124: ep_len:500 episode reward: total was -34.120000. running mean: -27.722132\n",
      "ep 1124: ep_len:3 episode reward: total was 0.000000. running mean: -27.444911\n",
      "ep 1124: ep_len:525 episode reward: total was -19.730000. running mean: -27.367762\n",
      "ep 1124: ep_len:505 episode reward: total was -25.490000. running mean: -27.348984\n",
      "epsilon:0.246437 episode_count: 7875. steps_count: 3497378.000000\n",
      "ep 1125: ep_len:570 episode reward: total was -16.240000. running mean: -27.237894\n",
      "ep 1125: ep_len:500 episode reward: total was -22.740000. running mean: -27.192915\n",
      "ep 1125: ep_len:640 episode reward: total was -31.150000. running mean: -27.232486\n",
      "ep 1125: ep_len:599 episode reward: total was -52.090000. running mean: -27.481061\n",
      "ep 1125: ep_len:87 episode reward: total was 3.030000. running mean: -27.175950\n",
      "ep 1125: ep_len:237 episode reward: total was -2.410000. running mean: -26.928291\n",
      "ep 1125: ep_len:191 episode reward: total was -9.910000. running mean: -26.758108\n",
      "epsilon:0.246301 episode_count: 7882. steps_count: 3500202.000000\n",
      "ep 1126: ep_len:510 episode reward: total was -27.610000. running mean: -26.766627\n",
      "ep 1126: ep_len:630 episode reward: total was -22.590000. running mean: -26.724861\n",
      "ep 1126: ep_len:505 episode reward: total was -35.180000. running mean: -26.809412\n",
      "ep 1126: ep_len:560 episode reward: total was -11.510000. running mean: -26.656418\n",
      "ep 1126: ep_len:3 episode reward: total was 0.000000. running mean: -26.389854\n",
      "ep 1126: ep_len:575 episode reward: total was -29.620000. running mean: -26.422155\n",
      "ep 1126: ep_len:505 episode reward: total was -35.530000. running mean: -26.513234\n",
      "epsilon:0.246164 episode_count: 7889. steps_count: 3503490.000000\n",
      "ep 1127: ep_len:118 episode reward: total was -5.450000. running mean: -26.302601\n",
      "ep 1127: ep_len:620 episode reward: total was -20.040000. running mean: -26.239975\n",
      "ep 1127: ep_len:540 episode reward: total was -34.040000. running mean: -26.317976\n",
      "ep 1127: ep_len:560 episode reward: total was -28.620000. running mean: -26.340996\n",
      "ep 1127: ep_len:3 episode reward: total was 0.000000. running mean: -26.077586\n",
      "ep 1127: ep_len:675 episode reward: total was -38.930000. running mean: -26.206110\n",
      "ep 1127: ep_len:530 episode reward: total was -39.690000. running mean: -26.340949\n",
      "epsilon:0.246028 episode_count: 7896. steps_count: 3506536.000000\n",
      "ep 1128: ep_len:715 episode reward: total was -110.840000. running mean: -27.185939\n",
      "ep 1128: ep_len:665 episode reward: total was -66.610000. running mean: -27.580180\n",
      "ep 1128: ep_len:413 episode reward: total was -42.360000. running mean: -27.727978\n",
      "ep 1128: ep_len:56 episode reward: total was -1.960000. running mean: -27.470298\n",
      "ep 1128: ep_len:3 episode reward: total was 0.000000. running mean: -27.195595\n",
      "ep 1128: ep_len:570 episode reward: total was -23.830000. running mean: -27.161940\n",
      "ep 1128: ep_len:590 episode reward: total was -24.380000. running mean: -27.134120\n",
      "epsilon:0.245891 episode_count: 7903. steps_count: 3509548.000000\n",
      "ep 1129: ep_len:219 episode reward: total was -25.440000. running mean: -27.117179\n",
      "ep 1129: ep_len:535 episode reward: total was -29.590000. running mean: -27.141907\n",
      "ep 1129: ep_len:398 episode reward: total was -17.400000. running mean: -27.044488\n",
      "ep 1129: ep_len:565 episode reward: total was -9.620000. running mean: -26.870243\n",
      "ep 1129: ep_len:86 episode reward: total was -0.450000. running mean: -26.606041\n",
      "ep 1129: ep_len:720 episode reward: total was -35.330000. running mean: -26.693280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1129: ep_len:500 episode reward: total was -38.360000. running mean: -26.809948\n",
      "epsilon:0.245755 episode_count: 7910. steps_count: 3512571.000000\n",
      "ep 1130: ep_len:550 episode reward: total was -41.670000. running mean: -26.958548\n",
      "ep 1130: ep_len:565 episode reward: total was -24.150000. running mean: -26.930463\n",
      "ep 1130: ep_len:735 episode reward: total was -45.860000. running mean: -27.119758\n",
      "ep 1130: ep_len:500 episode reward: total was -23.270000. running mean: -27.081260\n",
      "ep 1130: ep_len:53 episode reward: total was 2.000000. running mean: -26.790448\n",
      "ep 1130: ep_len:505 episode reward: total was -21.700000. running mean: -26.739543\n",
      "ep 1130: ep_len:192 episode reward: total was -15.430000. running mean: -26.626448\n",
      "epsilon:0.245618 episode_count: 7917. steps_count: 3515671.000000\n",
      "ep 1131: ep_len:575 episode reward: total was -51.020000. running mean: -26.870383\n",
      "ep 1131: ep_len:580 episode reward: total was -26.590000. running mean: -26.867580\n",
      "ep 1131: ep_len:73 episode reward: total was -3.950000. running mean: -26.638404\n",
      "ep 1131: ep_len:615 episode reward: total was -18.670000. running mean: -26.558720\n",
      "ep 1131: ep_len:3 episode reward: total was 0.000000. running mean: -26.293133\n",
      "ep 1131: ep_len:535 episode reward: total was -30.820000. running mean: -26.338401\n",
      "ep 1131: ep_len:204 episode reward: total was -18.390000. running mean: -26.258917\n",
      "epsilon:0.245482 episode_count: 7924. steps_count: 3518256.000000\n",
      "ep 1132: ep_len:103 episode reward: total was 0.550000. running mean: -25.990828\n",
      "ep 1132: ep_len:263 episode reward: total was -9.880000. running mean: -25.829720\n",
      "ep 1132: ep_len:570 episode reward: total was -46.680000. running mean: -26.038223\n",
      "ep 1132: ep_len:555 episode reward: total was -15.700000. running mean: -25.934840\n",
      "ep 1132: ep_len:90 episode reward: total was 0.530000. running mean: -25.670192\n",
      "ep 1132: ep_len:545 episode reward: total was -11.470000. running mean: -25.528190\n",
      "ep 1132: ep_len:279 episode reward: total was -17.840000. running mean: -25.451308\n",
      "epsilon:0.245345 episode_count: 7931. steps_count: 3520661.000000\n",
      "ep 1133: ep_len:530 episode reward: total was -20.560000. running mean: -25.402395\n",
      "ep 1133: ep_len:500 episode reward: total was -9.880000. running mean: -25.247171\n",
      "ep 1133: ep_len:515 episode reward: total was -25.690000. running mean: -25.251599\n",
      "ep 1133: ep_len:595 episode reward: total was -20.190000. running mean: -25.200983\n",
      "ep 1133: ep_len:3 episode reward: total was 0.000000. running mean: -24.948974\n",
      "ep 1133: ep_len:530 episode reward: total was -28.460000. running mean: -24.984084\n",
      "ep 1133: ep_len:595 episode reward: total was -32.490000. running mean: -25.059143\n",
      "epsilon:0.245209 episode_count: 7938. steps_count: 3523929.000000\n",
      "ep 1134: ep_len:580 episode reward: total was -91.370000. running mean: -25.722252\n",
      "ep 1134: ep_len:505 episode reward: total was -58.730000. running mean: -26.052329\n",
      "ep 1134: ep_len:590 episode reward: total was -38.390000. running mean: -26.175706\n",
      "ep 1134: ep_len:560 episode reward: total was -34.030000. running mean: -26.254249\n",
      "ep 1134: ep_len:3 episode reward: total was 0.000000. running mean: -25.991706\n",
      "ep 1134: ep_len:550 episode reward: total was -25.400000. running mean: -25.985789\n",
      "ep 1134: ep_len:590 episode reward: total was -39.050000. running mean: -26.116431\n",
      "epsilon:0.245072 episode_count: 7945. steps_count: 3527307.000000\n",
      "ep 1135: ep_len:265 episode reward: total was 2.640000. running mean: -25.828867\n",
      "ep 1135: ep_len:500 episode reward: total was -41.880000. running mean: -25.989378\n",
      "ep 1135: ep_len:560 episode reward: total was -21.620000. running mean: -25.945684\n",
      "ep 1135: ep_len:161 episode reward: total was -4.430000. running mean: -25.730528\n",
      "ep 1135: ep_len:46 episode reward: total was 4.500000. running mean: -25.428222\n",
      "ep 1135: ep_len:845 episode reward: total was -85.780000. running mean: -26.031740\n",
      "ep 1135: ep_len:605 episode reward: total was -21.360000. running mean: -25.985023\n",
      "epsilon:0.244936 episode_count: 7952. steps_count: 3530289.000000\n",
      "ep 1136: ep_len:221 episode reward: total was -23.930000. running mean: -25.964472\n",
      "ep 1136: ep_len:525 episode reward: total was -38.760000. running mean: -26.092428\n",
      "ep 1136: ep_len:510 episode reward: total was -22.910000. running mean: -26.060603\n",
      "ep 1136: ep_len:710 episode reward: total was -54.210000. running mean: -26.342097\n",
      "ep 1136: ep_len:3 episode reward: total was 0.000000. running mean: -26.078676\n",
      "ep 1136: ep_len:224 episode reward: total was -5.900000. running mean: -25.876890\n",
      "ep 1136: ep_len:505 episode reward: total was -32.270000. running mean: -25.940821\n",
      "epsilon:0.244799 episode_count: 7959. steps_count: 3532987.000000\n",
      "ep 1137: ep_len:575 episode reward: total was -31.650000. running mean: -25.997913\n",
      "ep 1137: ep_len:520 episode reward: total was -33.300000. running mean: -26.070933\n",
      "ep 1137: ep_len:695 episode reward: total was -61.980000. running mean: -26.430024\n",
      "ep 1137: ep_len:156 episode reward: total was 5.600000. running mean: -26.109724\n",
      "ep 1137: ep_len:74 episode reward: total was -7.450000. running mean: -25.923127\n",
      "ep 1137: ep_len:500 episode reward: total was -28.090000. running mean: -25.944795\n",
      "ep 1137: ep_len:500 episode reward: total was -45.370000. running mean: -26.139047\n",
      "epsilon:0.244663 episode_count: 7966. steps_count: 3536007.000000\n",
      "ep 1138: ep_len:108 episode reward: total was -4.450000. running mean: -25.922157\n",
      "ep 1138: ep_len:500 episode reward: total was -58.330000. running mean: -26.246235\n",
      "ep 1138: ep_len:595 episode reward: total was -27.930000. running mean: -26.263073\n",
      "ep 1138: ep_len:560 episode reward: total was -24.660000. running mean: -26.247042\n",
      "ep 1138: ep_len:3 episode reward: total was 0.000000. running mean: -25.984572\n",
      "ep 1138: ep_len:615 episode reward: total was -27.170000. running mean: -25.996426\n",
      "ep 1138: ep_len:570 episode reward: total was -30.610000. running mean: -26.042562\n",
      "epsilon:0.244526 episode_count: 7973. steps_count: 3538958.000000\n",
      "ep 1139: ep_len:119 episode reward: total was 0.030000. running mean: -25.781836\n",
      "ep 1139: ep_len:545 episode reward: total was -12.100000. running mean: -25.645018\n",
      "ep 1139: ep_len:565 episode reward: total was -40.950000. running mean: -25.798068\n",
      "ep 1139: ep_len:111 episode reward: total was -6.380000. running mean: -25.603887\n",
      "ep 1139: ep_len:3 episode reward: total was 0.000000. running mean: -25.347848\n",
      "ep 1139: ep_len:625 episode reward: total was -29.200000. running mean: -25.386370\n",
      "ep 1139: ep_len:595 episode reward: total was -38.570000. running mean: -25.518206\n",
      "epsilon:0.244390 episode_count: 7980. steps_count: 3541521.000000\n",
      "ep 1140: ep_len:241 episode reward: total was -27.360000. running mean: -25.536624\n",
      "ep 1140: ep_len:535 episode reward: total was -36.620000. running mean: -25.647458\n",
      "ep 1140: ep_len:383 episode reward: total was -21.890000. running mean: -25.609883\n",
      "ep 1140: ep_len:403 episode reward: total was -39.220000. running mean: -25.745984\n",
      "ep 1140: ep_len:132 episode reward: total was -17.960000. running mean: -25.668124\n",
      "ep 1140: ep_len:590 episode reward: total was -17.140000. running mean: -25.582843\n",
      "ep 1140: ep_len:595 episode reward: total was -28.970000. running mean: -25.616715\n",
      "epsilon:0.244253 episode_count: 7987. steps_count: 3544400.000000\n",
      "ep 1141: ep_len:545 episode reward: total was -15.080000. running mean: -25.511348\n",
      "ep 1141: ep_len:500 episode reward: total was -7.360000. running mean: -25.329834\n",
      "ep 1141: ep_len:810 episode reward: total was -69.920000. running mean: -25.775736\n",
      "ep 1141: ep_len:432 episode reward: total was -4.660000. running mean: -25.564578\n",
      "ep 1141: ep_len:100 episode reward: total was 0.520000. running mean: -25.303733\n",
      "ep 1141: ep_len:605 episode reward: total was -17.620000. running mean: -25.226895\n",
      "ep 1141: ep_len:530 episode reward: total was -37.160000. running mean: -25.346226\n",
      "epsilon:0.244117 episode_count: 7994. steps_count: 3547922.000000\n",
      "ep 1142: ep_len:237 episode reward: total was -5.890000. running mean: -25.151664\n",
      "ep 1142: ep_len:510 episode reward: total was -11.720000. running mean: -25.017348\n",
      "ep 1142: ep_len:391 episode reward: total was -33.890000. running mean: -25.106074\n",
      "ep 1142: ep_len:500 episode reward: total was -26.650000. running mean: -25.121513\n",
      "ep 1142: ep_len:3 episode reward: total was 0.000000. running mean: -24.870298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1142: ep_len:505 episode reward: total was -39.580000. running mean: -25.017395\n",
      "ep 1142: ep_len:545 episode reward: total was -29.650000. running mean: -25.063721\n",
      "epsilon:0.243980 episode_count: 8001. steps_count: 3550613.000000\n",
      "ep 1143: ep_len:785 episode reward: total was -51.880000. running mean: -25.331884\n",
      "ep 1143: ep_len:520 episode reward: total was -25.980000. running mean: -25.338365\n",
      "ep 1143: ep_len:505 episode reward: total was -25.750000. running mean: -25.342482\n",
      "ep 1143: ep_len:120 episode reward: total was -7.950000. running mean: -25.168557\n",
      "ep 1143: ep_len:3 episode reward: total was 0.000000. running mean: -24.916871\n",
      "ep 1143: ep_len:520 episode reward: total was -32.490000. running mean: -24.992602\n",
      "ep 1143: ep_len:510 episode reward: total was -39.800000. running mean: -25.140676\n",
      "epsilon:0.243844 episode_count: 8008. steps_count: 3553576.000000\n",
      "ep 1144: ep_len:500 episode reward: total was -40.040000. running mean: -25.289670\n",
      "ep 1144: ep_len:500 episode reward: total was -18.050000. running mean: -25.217273\n",
      "ep 1144: ep_len:555 episode reward: total was -32.450000. running mean: -25.289600\n",
      "ep 1144: ep_len:500 episode reward: total was -21.110000. running mean: -25.247804\n",
      "ep 1144: ep_len:3 episode reward: total was 0.000000. running mean: -24.995326\n",
      "ep 1144: ep_len:565 episode reward: total was -31.860000. running mean: -25.063973\n",
      "ep 1144: ep_len:625 episode reward: total was -43.620000. running mean: -25.249533\n",
      "epsilon:0.243707 episode_count: 8015. steps_count: 3556824.000000\n",
      "ep 1145: ep_len:605 episode reward: total was -26.700000. running mean: -25.264038\n",
      "ep 1145: ep_len:555 episode reward: total was -8.260000. running mean: -25.093997\n",
      "ep 1145: ep_len:605 episode reward: total was -38.160000. running mean: -25.224657\n",
      "ep 1145: ep_len:505 episode reward: total was -19.730000. running mean: -25.169711\n",
      "ep 1145: ep_len:3 episode reward: total was 0.000000. running mean: -24.918014\n",
      "ep 1145: ep_len:500 episode reward: total was -21.580000. running mean: -24.884634\n",
      "ep 1145: ep_len:620 episode reward: total was -70.130000. running mean: -25.337087\n",
      "epsilon:0.243571 episode_count: 8022. steps_count: 3560217.000000\n",
      "ep 1146: ep_len:645 episode reward: total was -64.560000. running mean: -25.729316\n",
      "ep 1146: ep_len:500 episode reward: total was -28.340000. running mean: -25.755423\n",
      "ep 1146: ep_len:510 episode reward: total was -23.050000. running mean: -25.728369\n",
      "ep 1146: ep_len:500 episode reward: total was -25.650000. running mean: -25.727585\n",
      "ep 1146: ep_len:98 episode reward: total was -2.950000. running mean: -25.499810\n",
      "ep 1146: ep_len:505 episode reward: total was -53.290000. running mean: -25.777711\n",
      "ep 1146: ep_len:183 episode reward: total was -5.400000. running mean: -25.573934\n",
      "epsilon:0.243434 episode_count: 8029. steps_count: 3563158.000000\n",
      "ep 1147: ep_len:530 episode reward: total was -42.540000. running mean: -25.743595\n",
      "ep 1147: ep_len:550 episode reward: total was -7.820000. running mean: -25.564359\n",
      "ep 1147: ep_len:600 episode reward: total was -32.150000. running mean: -25.630215\n",
      "ep 1147: ep_len:505 episode reward: total was -23.080000. running mean: -25.604713\n",
      "ep 1147: ep_len:3 episode reward: total was 0.000000. running mean: -25.348666\n",
      "ep 1147: ep_len:630 episode reward: total was -47.090000. running mean: -25.566079\n",
      "ep 1147: ep_len:505 episode reward: total was -28.490000. running mean: -25.595319\n",
      "epsilon:0.243298 episode_count: 8036. steps_count: 3566481.000000\n",
      "ep 1148: ep_len:765 episode reward: total was -72.150000. running mean: -26.060865\n",
      "ep 1148: ep_len:515 episode reward: total was -23.810000. running mean: -26.038357\n",
      "ep 1148: ep_len:505 episode reward: total was -49.130000. running mean: -26.269273\n",
      "ep 1148: ep_len:500 episode reward: total was -37.690000. running mean: -26.383481\n",
      "ep 1148: ep_len:100 episode reward: total was -13.450000. running mean: -26.254146\n",
      "ep 1148: ep_len:540 episode reward: total was -45.670000. running mean: -26.448304\n",
      "ep 1148: ep_len:535 episode reward: total was -29.620000. running mean: -26.480021\n",
      "epsilon:0.243161 episode_count: 8043. steps_count: 3569941.000000\n",
      "ep 1149: ep_len:550 episode reward: total was -32.250000. running mean: -26.537721\n",
      "ep 1149: ep_len:575 episode reward: total was -25.650000. running mean: -26.528844\n",
      "ep 1149: ep_len:510 episode reward: total was -20.670000. running mean: -26.470255\n",
      "ep 1149: ep_len:56 episode reward: total was -6.470000. running mean: -26.270253\n",
      "ep 1149: ep_len:79 episode reward: total was -7.440000. running mean: -26.081950\n",
      "ep 1149: ep_len:505 episode reward: total was -21.050000. running mean: -26.031631\n",
      "ep 1149: ep_len:585 episode reward: total was -42.130000. running mean: -26.192614\n",
      "epsilon:0.243025 episode_count: 8050. steps_count: 3572801.000000\n",
      "ep 1150: ep_len:705 episode reward: total was -70.640000. running mean: -26.637088\n",
      "ep 1150: ep_len:525 episode reward: total was -23.860000. running mean: -26.609317\n",
      "ep 1150: ep_len:580 episode reward: total was -24.660000. running mean: -26.589824\n",
      "ep 1150: ep_len:560 episode reward: total was -10.070000. running mean: -26.424626\n",
      "ep 1150: ep_len:87 episode reward: total was -1.960000. running mean: -26.179980\n",
      "ep 1150: ep_len:308 episode reward: total was -25.910000. running mean: -26.177280\n",
      "ep 1150: ep_len:575 episode reward: total was -31.700000. running mean: -26.232507\n",
      "epsilon:0.242888 episode_count: 8057. steps_count: 3576141.000000\n",
      "ep 1151: ep_len:655 episode reward: total was -52.510000. running mean: -26.495282\n",
      "ep 1151: ep_len:500 episode reward: total was -54.370000. running mean: -26.774029\n",
      "ep 1151: ep_len:560 episode reward: total was -19.150000. running mean: -26.697789\n",
      "ep 1151: ep_len:170 episode reward: total was -8.410000. running mean: -26.514911\n",
      "ep 1151: ep_len:113 episode reward: total was -14.460000. running mean: -26.394362\n",
      "ep 1151: ep_len:535 episode reward: total was -25.690000. running mean: -26.387318\n",
      "ep 1151: ep_len:600 episode reward: total was -51.070000. running mean: -26.634145\n",
      "epsilon:0.242752 episode_count: 8064. steps_count: 3579274.000000\n",
      "ep 1152: ep_len:500 episode reward: total was -9.790000. running mean: -26.465704\n",
      "ep 1152: ep_len:540 episode reward: total was -34.490000. running mean: -26.545947\n",
      "ep 1152: ep_len:580 episode reward: total was -33.090000. running mean: -26.611387\n",
      "ep 1152: ep_len:515 episode reward: total was -10.120000. running mean: -26.446473\n",
      "ep 1152: ep_len:89 episode reward: total was -6.480000. running mean: -26.246809\n",
      "ep 1152: ep_len:540 episode reward: total was -22.710000. running mean: -26.211441\n",
      "ep 1152: ep_len:545 episode reward: total was -30.920000. running mean: -26.258526\n",
      "epsilon:0.242615 episode_count: 8071. steps_count: 3582583.000000\n",
      "ep 1153: ep_len:535 episode reward: total was -17.250000. running mean: -26.168441\n",
      "ep 1153: ep_len:515 episode reward: total was -16.780000. running mean: -26.074556\n",
      "ep 1153: ep_len:555 episode reward: total was -20.970000. running mean: -26.023511\n",
      "ep 1153: ep_len:605 episode reward: total was -47.030000. running mean: -26.233576\n",
      "ep 1153: ep_len:119 episode reward: total was 3.550000. running mean: -25.935740\n",
      "ep 1153: ep_len:535 episode reward: total was -31.600000. running mean: -25.992383\n",
      "ep 1153: ep_len:505 episode reward: total was -54.730000. running mean: -26.279759\n",
      "epsilon:0.242479 episode_count: 8078. steps_count: 3585952.000000\n",
      "ep 1154: ep_len:515 episode reward: total was -17.210000. running mean: -26.189061\n",
      "ep 1154: ep_len:565 episode reward: total was -6.750000. running mean: -25.994671\n",
      "ep 1154: ep_len:427 episode reward: total was -16.770000. running mean: -25.902424\n",
      "ep 1154: ep_len:500 episode reward: total was -19.580000. running mean: -25.839200\n",
      "ep 1154: ep_len:3 episode reward: total was 0.000000. running mean: -25.580808\n",
      "ep 1154: ep_len:585 episode reward: total was -43.250000. running mean: -25.757500\n",
      "ep 1154: ep_len:530 episode reward: total was -38.240000. running mean: -25.882325\n",
      "epsilon:0.242342 episode_count: 8085. steps_count: 3589077.000000\n",
      "ep 1155: ep_len:600 episode reward: total was -29.170000. running mean: -25.915201\n",
      "ep 1155: ep_len:715 episode reward: total was -38.040000. running mean: -26.036449\n",
      "ep 1155: ep_len:449 episode reward: total was -19.790000. running mean: -25.973985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1155: ep_len:605 episode reward: total was -25.070000. running mean: -25.964945\n",
      "ep 1155: ep_len:42 episode reward: total was 2.500000. running mean: -25.680296\n",
      "ep 1155: ep_len:500 episode reward: total was -30.810000. running mean: -25.731593\n",
      "ep 1155: ep_len:545 episode reward: total was -47.630000. running mean: -25.950577\n",
      "epsilon:0.242206 episode_count: 8092. steps_count: 3592533.000000\n",
      "ep 1156: ep_len:500 episode reward: total was -55.910000. running mean: -26.250171\n",
      "ep 1156: ep_len:368 episode reward: total was -31.410000. running mean: -26.301769\n",
      "ep 1156: ep_len:69 episode reward: total was -2.980000. running mean: -26.068551\n",
      "ep 1156: ep_len:535 episode reward: total was -18.670000. running mean: -25.994566\n",
      "ep 1156: ep_len:3 episode reward: total was 0.000000. running mean: -25.734620\n",
      "ep 1156: ep_len:540 episode reward: total was -25.220000. running mean: -25.729474\n",
      "ep 1156: ep_len:505 episode reward: total was -35.750000. running mean: -25.829679\n",
      "epsilon:0.242069 episode_count: 8099. steps_count: 3595053.000000\n",
      "ep 1157: ep_len:525 episode reward: total was -35.820000. running mean: -25.929583\n",
      "ep 1157: ep_len:555 episode reward: total was -20.190000. running mean: -25.872187\n",
      "ep 1157: ep_len:705 episode reward: total was -87.090000. running mean: -26.484365\n",
      "ep 1157: ep_len:500 episode reward: total was -48.040000. running mean: -26.699921\n",
      "ep 1157: ep_len:3 episode reward: total was 0.000000. running mean: -26.432922\n",
      "ep 1157: ep_len:585 episode reward: total was -31.710000. running mean: -26.485693\n",
      "ep 1157: ep_len:309 episode reward: total was -24.960000. running mean: -26.470436\n",
      "epsilon:0.241933 episode_count: 8106. steps_count: 3598235.000000\n",
      "ep 1158: ep_len:103 episode reward: total was -2.470000. running mean: -26.230432\n",
      "ep 1158: ep_len:500 episode reward: total was -1.400000. running mean: -25.982127\n",
      "ep 1158: ep_len:434 episode reward: total was -25.820000. running mean: -25.980506\n",
      "ep 1158: ep_len:500 episode reward: total was -18.620000. running mean: -25.906901\n",
      "ep 1158: ep_len:3 episode reward: total was 0.000000. running mean: -25.647832\n",
      "ep 1158: ep_len:505 episode reward: total was -26.180000. running mean: -25.653154\n",
      "ep 1158: ep_len:505 episode reward: total was -49.710000. running mean: -25.893722\n",
      "epsilon:0.241796 episode_count: 8113. steps_count: 3600785.000000\n",
      "ep 1159: ep_len:600 episode reward: total was -44.400000. running mean: -26.078785\n",
      "ep 1159: ep_len:500 episode reward: total was -13.890000. running mean: -25.956897\n",
      "ep 1159: ep_len:500 episode reward: total was -22.780000. running mean: -25.925128\n",
      "ep 1159: ep_len:510 episode reward: total was -28.590000. running mean: -25.951777\n",
      "ep 1159: ep_len:3 episode reward: total was 0.000000. running mean: -25.692259\n",
      "ep 1159: ep_len:520 episode reward: total was -31.340000. running mean: -25.748736\n",
      "ep 1159: ep_len:620 episode reward: total was -33.510000. running mean: -25.826349\n",
      "epsilon:0.241660 episode_count: 8120. steps_count: 3604038.000000\n",
      "ep 1160: ep_len:229 episode reward: total was -18.400000. running mean: -25.752085\n",
      "ep 1160: ep_len:530 episode reward: total was -10.220000. running mean: -25.596765\n",
      "ep 1160: ep_len:620 episode reward: total was -28.890000. running mean: -25.629697\n",
      "ep 1160: ep_len:500 episode reward: total was -11.610000. running mean: -25.489500\n",
      "ep 1160: ep_len:89 episode reward: total was -4.960000. running mean: -25.284205\n",
      "ep 1160: ep_len:610 episode reward: total was -19.830000. running mean: -25.229663\n",
      "ep 1160: ep_len:565 episode reward: total was -38.010000. running mean: -25.357466\n",
      "epsilon:0.241523 episode_count: 8127. steps_count: 3607181.000000\n",
      "ep 1161: ep_len:510 episode reward: total was -45.010000. running mean: -25.553992\n",
      "ep 1161: ep_len:500 episode reward: total was -9.340000. running mean: -25.391852\n",
      "ep 1161: ep_len:510 episode reward: total was -17.600000. running mean: -25.313933\n",
      "ep 1161: ep_len:510 episode reward: total was -31.620000. running mean: -25.376994\n",
      "ep 1161: ep_len:86 episode reward: total was -9.480000. running mean: -25.218024\n",
      "ep 1161: ep_len:590 episode reward: total was -22.110000. running mean: -25.186944\n",
      "ep 1161: ep_len:500 episode reward: total was -22.770000. running mean: -25.162774\n",
      "epsilon:0.241387 episode_count: 8134. steps_count: 3610387.000000\n",
      "ep 1162: ep_len:550 episode reward: total was -27.850000. running mean: -25.189647\n",
      "ep 1162: ep_len:500 episode reward: total was 3.160000. running mean: -24.906150\n",
      "ep 1162: ep_len:515 episode reward: total was -41.790000. running mean: -25.074989\n",
      "ep 1162: ep_len:510 episode reward: total was -25.300000. running mean: -25.077239\n",
      "ep 1162: ep_len:3 episode reward: total was 0.000000. running mean: -24.826466\n",
      "ep 1162: ep_len:540 episode reward: total was -23.310000. running mean: -24.811302\n",
      "ep 1162: ep_len:340 episode reward: total was -21.370000. running mean: -24.776889\n",
      "epsilon:0.241250 episode_count: 8141. steps_count: 3613345.000000\n",
      "ep 1163: ep_len:218 episode reward: total was -26.370000. running mean: -24.792820\n",
      "ep 1163: ep_len:367 episode reward: total was -15.370000. running mean: -24.698592\n",
      "ep 1163: ep_len:635 episode reward: total was -16.030000. running mean: -24.611906\n",
      "ep 1163: ep_len:570 episode reward: total was -17.240000. running mean: -24.538187\n",
      "ep 1163: ep_len:3 episode reward: total was 0.000000. running mean: -24.292805\n",
      "ep 1163: ep_len:650 episode reward: total was -42.190000. running mean: -24.471777\n",
      "ep 1163: ep_len:520 episode reward: total was -34.660000. running mean: -24.573659\n",
      "epsilon:0.241114 episode_count: 8148. steps_count: 3616308.000000\n",
      "ep 1164: ep_len:104 episode reward: total was -2.470000. running mean: -24.352622\n",
      "ep 1164: ep_len:500 episode reward: total was -42.090000. running mean: -24.529996\n",
      "ep 1164: ep_len:520 episode reward: total was -23.180000. running mean: -24.516496\n",
      "ep 1164: ep_len:525 episode reward: total was -31.130000. running mean: -24.582631\n",
      "ep 1164: ep_len:3 episode reward: total was 0.000000. running mean: -24.336805\n",
      "ep 1164: ep_len:257 episode reward: total was -12.890000. running mean: -24.222337\n",
      "ep 1164: ep_len:605 episode reward: total was -27.610000. running mean: -24.256213\n",
      "epsilon:0.240977 episode_count: 8155. steps_count: 3618822.000000\n",
      "ep 1165: ep_len:500 episode reward: total was -15.280000. running mean: -24.166451\n",
      "ep 1165: ep_len:500 episode reward: total was -34.220000. running mean: -24.266987\n",
      "ep 1165: ep_len:575 episode reward: total was -39.040000. running mean: -24.414717\n",
      "ep 1165: ep_len:535 episode reward: total was -11.130000. running mean: -24.281870\n",
      "ep 1165: ep_len:3 episode reward: total was 0.000000. running mean: -24.039051\n",
      "ep 1165: ep_len:500 episode reward: total was -19.690000. running mean: -23.995561\n",
      "ep 1165: ep_len:640 episode reward: total was -51.150000. running mean: -24.267105\n",
      "epsilon:0.240841 episode_count: 8162. steps_count: 3622075.000000\n",
      "ep 1166: ep_len:575 episode reward: total was -22.740000. running mean: -24.251834\n",
      "ep 1166: ep_len:585 episode reward: total was -49.670000. running mean: -24.506016\n",
      "ep 1166: ep_len:505 episode reward: total was -27.070000. running mean: -24.531655\n",
      "ep 1166: ep_len:730 episode reward: total was -84.120000. running mean: -25.127539\n",
      "ep 1166: ep_len:3 episode reward: total was 0.000000. running mean: -24.876263\n",
      "ep 1166: ep_len:500 episode reward: total was -27.530000. running mean: -24.902801\n",
      "ep 1166: ep_len:625 episode reward: total was -20.550000. running mean: -24.859273\n",
      "epsilon:0.240704 episode_count: 8169. steps_count: 3625598.000000\n",
      "ep 1167: ep_len:560 episode reward: total was -25.740000. running mean: -24.868080\n",
      "ep 1167: ep_len:505 episode reward: total was -44.670000. running mean: -25.066099\n",
      "ep 1167: ep_len:720 episode reward: total was -54.840000. running mean: -25.363838\n",
      "ep 1167: ep_len:515 episode reward: total was -28.610000. running mean: -25.396300\n",
      "ep 1167: ep_len:104 episode reward: total was -1.990000. running mean: -25.162237\n",
      "ep 1167: ep_len:505 episode reward: total was -50.850000. running mean: -25.419115\n",
      "ep 1167: ep_len:540 episode reward: total was -15.350000. running mean: -25.318423\n",
      "epsilon:0.240568 episode_count: 8176. steps_count: 3629047.000000\n",
      "ep 1168: ep_len:550 episode reward: total was -16.180000. running mean: -25.227039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1168: ep_len:620 episode reward: total was -56.650000. running mean: -25.541269\n",
      "ep 1168: ep_len:530 episode reward: total was -24.950000. running mean: -25.535356\n",
      "ep 1168: ep_len:520 episode reward: total was -22.580000. running mean: -25.505803\n",
      "ep 1168: ep_len:3 episode reward: total was 0.000000. running mean: -25.250744\n",
      "ep 1168: ep_len:535 episode reward: total was -16.130000. running mean: -25.159537\n",
      "ep 1168: ep_len:505 episode reward: total was -33.130000. running mean: -25.239242\n",
      "epsilon:0.240431 episode_count: 8183. steps_count: 3632310.000000\n",
      "ep 1169: ep_len:605 episode reward: total was -22.660000. running mean: -25.213449\n",
      "ep 1169: ep_len:635 episode reward: total was -15.490000. running mean: -25.116215\n",
      "ep 1169: ep_len:630 episode reward: total was -44.630000. running mean: -25.311353\n",
      "ep 1169: ep_len:565 episode reward: total was -44.090000. running mean: -25.499139\n",
      "ep 1169: ep_len:3 episode reward: total was 0.000000. running mean: -25.244148\n",
      "ep 1169: ep_len:530 episode reward: total was -23.190000. running mean: -25.223606\n",
      "ep 1169: ep_len:510 episode reward: total was -37.480000. running mean: -25.346170\n",
      "epsilon:0.240295 episode_count: 8190. steps_count: 3635788.000000\n",
      "ep 1170: ep_len:535 episode reward: total was -48.110000. running mean: -25.573808\n",
      "ep 1170: ep_len:700 episode reward: total was -74.610000. running mean: -26.064170\n",
      "ep 1170: ep_len:620 episode reward: total was -27.150000. running mean: -26.075029\n",
      "ep 1170: ep_len:500 episode reward: total was -42.700000. running mean: -26.241278\n",
      "ep 1170: ep_len:3 episode reward: total was 0.000000. running mean: -25.978866\n",
      "ep 1170: ep_len:535 episode reward: total was -30.190000. running mean: -26.020977\n",
      "ep 1170: ep_len:500 episode reward: total was -22.740000. running mean: -25.988167\n",
      "epsilon:0.240158 episode_count: 8197. steps_count: 3639181.000000\n",
      "ep 1171: ep_len:645 episode reward: total was -45.220000. running mean: -26.180485\n",
      "ep 1171: ep_len:590 episode reward: total was -9.570000. running mean: -26.014381\n",
      "ep 1171: ep_len:515 episode reward: total was -12.190000. running mean: -25.876137\n",
      "ep 1171: ep_len:500 episode reward: total was -21.210000. running mean: -25.829475\n",
      "ep 1171: ep_len:3 episode reward: total was 0.000000. running mean: -25.571181\n",
      "ep 1171: ep_len:605 episode reward: total was -29.630000. running mean: -25.611769\n",
      "ep 1171: ep_len:545 episode reward: total was -11.570000. running mean: -25.471351\n",
      "epsilon:0.240022 episode_count: 8204. steps_count: 3642584.000000\n",
      "ep 1172: ep_len:540 episode reward: total was -14.700000. running mean: -25.363638\n",
      "ep 1172: ep_len:560 episode reward: total was -32.130000. running mean: -25.431301\n",
      "ep 1172: ep_len:815 episode reward: total was -61.270000. running mean: -25.789688\n",
      "ep 1172: ep_len:127 episode reward: total was -2.430000. running mean: -25.556091\n",
      "ep 1172: ep_len:3 episode reward: total was 0.000000. running mean: -25.300531\n",
      "ep 1172: ep_len:510 episode reward: total was -60.170000. running mean: -25.649225\n",
      "ep 1172: ep_len:630 episode reward: total was -20.600000. running mean: -25.598733\n",
      "epsilon:0.239885 episode_count: 8211. steps_count: 3645769.000000\n",
      "ep 1173: ep_len:500 episode reward: total was -13.250000. running mean: -25.475246\n",
      "ep 1173: ep_len:615 episode reward: total was -39.990000. running mean: -25.620393\n",
      "ep 1173: ep_len:630 episode reward: total was -36.430000. running mean: -25.728489\n",
      "ep 1173: ep_len:560 episode reward: total was -13.700000. running mean: -25.608204\n",
      "ep 1173: ep_len:2 episode reward: total was 0.000000. running mean: -25.352122\n",
      "ep 1173: ep_len:164 episode reward: total was -17.420000. running mean: -25.272801\n",
      "ep 1173: ep_len:545 episode reward: total was -32.090000. running mean: -25.340973\n",
      "epsilon:0.239749 episode_count: 8218. steps_count: 3648785.000000\n",
      "ep 1174: ep_len:123 episode reward: total was -11.930000. running mean: -25.206863\n",
      "ep 1174: ep_len:520 episode reward: total was -37.790000. running mean: -25.332695\n",
      "ep 1174: ep_len:695 episode reward: total was -78.490000. running mean: -25.864268\n",
      "ep 1174: ep_len:132 episode reward: total was -0.910000. running mean: -25.614725\n",
      "ep 1174: ep_len:3 episode reward: total was 0.000000. running mean: -25.358578\n",
      "ep 1174: ep_len:620 episode reward: total was -35.940000. running mean: -25.464392\n",
      "ep 1174: ep_len:505 episode reward: total was -25.050000. running mean: -25.460248\n",
      "epsilon:0.239612 episode_count: 8225. steps_count: 3651383.000000\n",
      "ep 1175: ep_len:595 episode reward: total was -9.950000. running mean: -25.305146\n",
      "ep 1175: ep_len:575 episode reward: total was -41.130000. running mean: -25.463394\n",
      "ep 1175: ep_len:685 episode reward: total was -60.450000. running mean: -25.813260\n",
      "ep 1175: ep_len:545 episode reward: total was -37.150000. running mean: -25.926628\n",
      "ep 1175: ep_len:3 episode reward: total was 0.000000. running mean: -25.667361\n",
      "ep 1175: ep_len:585 episode reward: total was -30.890000. running mean: -25.719588\n",
      "ep 1175: ep_len:500 episode reward: total was -36.200000. running mean: -25.824392\n",
      "epsilon:0.239476 episode_count: 8232. steps_count: 3654871.000000\n",
      "ep 1176: ep_len:525 episode reward: total was -21.680000. running mean: -25.782948\n",
      "ep 1176: ep_len:515 episode reward: total was -41.140000. running mean: -25.936518\n",
      "ep 1176: ep_len:408 episode reward: total was -5.830000. running mean: -25.735453\n",
      "ep 1176: ep_len:550 episode reward: total was -36.600000. running mean: -25.844099\n",
      "ep 1176: ep_len:56 episode reward: total was -6.500000. running mean: -25.650658\n",
      "ep 1176: ep_len:525 episode reward: total was -20.670000. running mean: -25.600851\n",
      "ep 1176: ep_len:259 episode reward: total was -12.920000. running mean: -25.474043\n",
      "epsilon:0.239339 episode_count: 8239. steps_count: 3657709.000000\n",
      "ep 1177: ep_len:620 episode reward: total was -51.400000. running mean: -25.733302\n",
      "ep 1177: ep_len:383 episode reward: total was -34.330000. running mean: -25.819269\n",
      "ep 1177: ep_len:615 episode reward: total was -33.890000. running mean: -25.899977\n",
      "ep 1177: ep_len:575 episode reward: total was -26.660000. running mean: -25.907577\n",
      "ep 1177: ep_len:3 episode reward: total was 0.000000. running mean: -25.648501\n",
      "ep 1177: ep_len:560 episode reward: total was -31.490000. running mean: -25.706916\n",
      "ep 1177: ep_len:302 episode reward: total was -34.930000. running mean: -25.799147\n",
      "epsilon:0.239203 episode_count: 8246. steps_count: 3660767.000000\n",
      "ep 1178: ep_len:660 episode reward: total was -26.760000. running mean: -25.808755\n",
      "ep 1178: ep_len:605 episode reward: total was 3.320000. running mean: -25.517468\n",
      "ep 1178: ep_len:600 episode reward: total was -27.640000. running mean: -25.538693\n",
      "ep 1178: ep_len:565 episode reward: total was -17.110000. running mean: -25.454406\n",
      "ep 1178: ep_len:2 episode reward: total was 0.000000. running mean: -25.199862\n",
      "ep 1178: ep_len:620 episode reward: total was -29.640000. running mean: -25.244264\n",
      "ep 1178: ep_len:510 episode reward: total was -25.110000. running mean: -25.242921\n",
      "epsilon:0.239066 episode_count: 8253. steps_count: 3664329.000000\n",
      "ep 1179: ep_len:605 episode reward: total was -24.110000. running mean: -25.231592\n",
      "ep 1179: ep_len:585 episode reward: total was -15.590000. running mean: -25.135176\n",
      "ep 1179: ep_len:530 episode reward: total was -21.620000. running mean: -25.100024\n",
      "ep 1179: ep_len:510 episode reward: total was -29.680000. running mean: -25.145824\n",
      "ep 1179: ep_len:3 episode reward: total was 0.000000. running mean: -24.894366\n",
      "ep 1179: ep_len:600 episode reward: total was -19.760000. running mean: -24.843022\n",
      "ep 1179: ep_len:284 episode reward: total was -20.370000. running mean: -24.798292\n",
      "epsilon:0.238930 episode_count: 8260. steps_count: 3667446.000000\n",
      "ep 1180: ep_len:585 episode reward: total was -32.640000. running mean: -24.876709\n",
      "ep 1180: ep_len:500 episode reward: total was -7.270000. running mean: -24.700642\n",
      "ep 1180: ep_len:590 episode reward: total was -27.150000. running mean: -24.725135\n",
      "ep 1180: ep_len:500 episode reward: total was -21.570000. running mean: -24.693584\n",
      "ep 1180: ep_len:3 episode reward: total was 0.000000. running mean: -24.446648\n",
      "ep 1180: ep_len:302 episode reward: total was -27.370000. running mean: -24.475882\n",
      "ep 1180: ep_len:585 episode reward: total was -26.440000. running mean: -24.495523\n",
      "epsilon:0.238793 episode_count: 8267. steps_count: 3670511.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1181: ep_len:520 episode reward: total was -35.630000. running mean: -24.606868\n",
      "ep 1181: ep_len:500 episode reward: total was -6.360000. running mean: -24.424399\n",
      "ep 1181: ep_len:630 episode reward: total was -30.370000. running mean: -24.483855\n",
      "ep 1181: ep_len:515 episode reward: total was -8.670000. running mean: -24.325716\n",
      "ep 1181: ep_len:94 episode reward: total was 1.540000. running mean: -24.067059\n",
      "ep 1181: ep_len:505 episode reward: total was -17.810000. running mean: -24.004489\n",
      "ep 1181: ep_len:590 episode reward: total was -31.750000. running mean: -24.081944\n",
      "epsilon:0.238657 episode_count: 8274. steps_count: 3673865.000000\n",
      "ep 1182: ep_len:515 episode reward: total was -38.070000. running mean: -24.221824\n",
      "ep 1182: ep_len:500 episode reward: total was -30.670000. running mean: -24.286306\n",
      "ep 1182: ep_len:555 episode reward: total was -29.920000. running mean: -24.342643\n",
      "ep 1182: ep_len:515 episode reward: total was -24.600000. running mean: -24.345217\n",
      "ep 1182: ep_len:3 episode reward: total was 0.000000. running mean: -24.101764\n",
      "ep 1182: ep_len:550 episode reward: total was -19.130000. running mean: -24.052047\n",
      "ep 1182: ep_len:262 episode reward: total was -26.330000. running mean: -24.074826\n",
      "epsilon:0.238520 episode_count: 8281. steps_count: 3676765.000000\n",
      "ep 1183: ep_len:540 episode reward: total was -39.010000. running mean: -24.224178\n",
      "ep 1183: ep_len:560 episode reward: total was -34.980000. running mean: -24.331736\n",
      "ep 1183: ep_len:695 episode reward: total was -48.890000. running mean: -24.577319\n",
      "ep 1183: ep_len:500 episode reward: total was -19.610000. running mean: -24.527646\n",
      "ep 1183: ep_len:3 episode reward: total was 0.000000. running mean: -24.282369\n",
      "ep 1183: ep_len:500 episode reward: total was -18.750000. running mean: -24.227045\n",
      "ep 1183: ep_len:515 episode reward: total was -25.470000. running mean: -24.239475\n",
      "epsilon:0.238384 episode_count: 8288. steps_count: 3680078.000000\n",
      "ep 1184: ep_len:685 episode reward: total was -56.840000. running mean: -24.565480\n",
      "ep 1184: ep_len:500 episode reward: total was -25.560000. running mean: -24.575425\n",
      "ep 1184: ep_len:545 episode reward: total was -27.970000. running mean: -24.609371\n",
      "ep 1184: ep_len:540 episode reward: total was -55.690000. running mean: -24.920178\n",
      "ep 1184: ep_len:3 episode reward: total was 0.000000. running mean: -24.670976\n",
      "ep 1184: ep_len:560 episode reward: total was -27.140000. running mean: -24.695666\n",
      "ep 1184: ep_len:525 episode reward: total was -11.370000. running mean: -24.562409\n",
      "epsilon:0.238247 episode_count: 8295. steps_count: 3683436.000000\n",
      "ep 1185: ep_len:620 episode reward: total was -41.900000. running mean: -24.735785\n",
      "ep 1185: ep_len:500 episode reward: total was -27.120000. running mean: -24.759627\n",
      "ep 1185: ep_len:610 episode reward: total was -23.500000. running mean: -24.747031\n",
      "ep 1185: ep_len:540 episode reward: total was -21.150000. running mean: -24.711061\n",
      "ep 1185: ep_len:78 episode reward: total was -9.970000. running mean: -24.563650\n",
      "ep 1185: ep_len:550 episode reward: total was -24.160000. running mean: -24.559614\n",
      "ep 1185: ep_len:510 episode reward: total was -23.020000. running mean: -24.544218\n",
      "epsilon:0.238111 episode_count: 8302. steps_count: 3686844.000000\n",
      "ep 1186: ep_len:565 episode reward: total was -34.890000. running mean: -24.647675\n",
      "ep 1186: ep_len:585 episode reward: total was -28.560000. running mean: -24.686799\n",
      "ep 1186: ep_len:670 episode reward: total was -37.330000. running mean: -24.813231\n",
      "ep 1186: ep_len:396 episode reward: total was -15.190000. running mean: -24.716998\n",
      "ep 1186: ep_len:48 episode reward: total was 4.500000. running mean: -24.424828\n",
      "ep 1186: ep_len:695 episode reward: total was -31.890000. running mean: -24.499480\n",
      "ep 1186: ep_len:620 episode reward: total was -39.580000. running mean: -24.650285\n",
      "epsilon:0.237974 episode_count: 8309. steps_count: 3690423.000000\n",
      "ep 1187: ep_len:520 episode reward: total was -16.260000. running mean: -24.566382\n",
      "ep 1187: ep_len:505 episode reward: total was -34.190000. running mean: -24.662619\n",
      "ep 1187: ep_len:590 episode reward: total was -36.420000. running mean: -24.780192\n",
      "ep 1187: ep_len:520 episode reward: total was -20.190000. running mean: -24.734290\n",
      "ep 1187: ep_len:3 episode reward: total was 0.000000. running mean: -24.486948\n",
      "ep 1187: ep_len:610 episode reward: total was -38.470000. running mean: -24.626778\n",
      "ep 1187: ep_len:550 episode reward: total was -24.530000. running mean: -24.625810\n",
      "epsilon:0.237838 episode_count: 8316. steps_count: 3693721.000000\n",
      "ep 1188: ep_len:216 episode reward: total was -28.390000. running mean: -24.663452\n",
      "ep 1188: ep_len:505 episode reward: total was -28.640000. running mean: -24.703218\n",
      "ep 1188: ep_len:393 episode reward: total was -12.880000. running mean: -24.584986\n",
      "ep 1188: ep_len:500 episode reward: total was -6.090000. running mean: -24.400036\n",
      "ep 1188: ep_len:3 episode reward: total was 0.000000. running mean: -24.156035\n",
      "ep 1188: ep_len:610 episode reward: total was -35.640000. running mean: -24.270875\n",
      "ep 1188: ep_len:580 episode reward: total was -30.130000. running mean: -24.329466\n",
      "epsilon:0.237701 episode_count: 8323. steps_count: 3696528.000000\n",
      "ep 1189: ep_len:500 episode reward: total was -33.760000. running mean: -24.423772\n",
      "ep 1189: ep_len:510 episode reward: total was -23.640000. running mean: -24.415934\n",
      "ep 1189: ep_len:600 episode reward: total was -22.670000. running mean: -24.398474\n",
      "ep 1189: ep_len:135 episode reward: total was -3.430000. running mean: -24.188790\n",
      "ep 1189: ep_len:41 episode reward: total was 1.000000. running mean: -23.936902\n",
      "ep 1189: ep_len:500 episode reward: total was -19.030000. running mean: -23.887833\n",
      "ep 1189: ep_len:510 episode reward: total was -21.660000. running mean: -23.865554\n",
      "epsilon:0.237565 episode_count: 8330. steps_count: 3699324.000000\n",
      "ep 1190: ep_len:595 episode reward: total was -44.170000. running mean: -24.068599\n",
      "ep 1190: ep_len:500 episode reward: total was -53.660000. running mean: -24.364513\n",
      "ep 1190: ep_len:520 episode reward: total was -60.220000. running mean: -24.723068\n",
      "ep 1190: ep_len:620 episode reward: total was -27.200000. running mean: -24.747837\n",
      "ep 1190: ep_len:3 episode reward: total was 0.000000. running mean: -24.500359\n",
      "ep 1190: ep_len:500 episode reward: total was -21.320000. running mean: -24.468555\n",
      "ep 1190: ep_len:500 episode reward: total was -26.660000. running mean: -24.490470\n",
      "epsilon:0.237428 episode_count: 8337. steps_count: 3702562.000000\n",
      "ep 1191: ep_len:565 episode reward: total was -32.220000. running mean: -24.567765\n",
      "ep 1191: ep_len:555 episode reward: total was -16.110000. running mean: -24.483187\n",
      "ep 1191: ep_len:580 episode reward: total was -18.640000. running mean: -24.424755\n",
      "ep 1191: ep_len:595 episode reward: total was -27.580000. running mean: -24.456308\n",
      "ep 1191: ep_len:3 episode reward: total was 0.000000. running mean: -24.211745\n",
      "ep 1191: ep_len:515 episode reward: total was -25.610000. running mean: -24.225727\n",
      "ep 1191: ep_len:590 episode reward: total was -34.980000. running mean: -24.333270\n",
      "epsilon:0.237292 episode_count: 8344. steps_count: 3705965.000000\n",
      "ep 1192: ep_len:515 episode reward: total was -18.500000. running mean: -24.274937\n",
      "ep 1192: ep_len:500 episode reward: total was -11.830000. running mean: -24.150488\n",
      "ep 1192: ep_len:368 episode reward: total was -15.890000. running mean: -24.067883\n",
      "ep 1192: ep_len:500 episode reward: total was -17.840000. running mean: -24.005604\n",
      "ep 1192: ep_len:3 episode reward: total was 0.000000. running mean: -23.765548\n",
      "ep 1192: ep_len:505 episode reward: total was -24.710000. running mean: -23.774993\n",
      "ep 1192: ep_len:575 episode reward: total was -25.490000. running mean: -23.792143\n",
      "epsilon:0.237155 episode_count: 8351. steps_count: 3708931.000000\n",
      "ep 1193: ep_len:535 episode reward: total was -18.750000. running mean: -23.741721\n",
      "ep 1193: ep_len:610 episode reward: total was -24.690000. running mean: -23.751204\n",
      "ep 1193: ep_len:70 episode reward: total was -0.460000. running mean: -23.518292\n",
      "ep 1193: ep_len:500 episode reward: total was -34.690000. running mean: -23.630009\n",
      "ep 1193: ep_len:3 episode reward: total was 0.000000. running mean: -23.393709\n",
      "ep 1193: ep_len:565 episode reward: total was -22.610000. running mean: -23.385872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1193: ep_len:575 episode reward: total was -32.630000. running mean: -23.478313\n",
      "epsilon:0.237019 episode_count: 8358. steps_count: 3711789.000000\n",
      "ep 1194: ep_len:625 episode reward: total was -21.620000. running mean: -23.459730\n",
      "ep 1194: ep_len:515 episode reward: total was -45.290000. running mean: -23.678033\n",
      "ep 1194: ep_len:560 episode reward: total was -35.700000. running mean: -23.798253\n",
      "ep 1194: ep_len:49 episode reward: total was -3.460000. running mean: -23.594870\n",
      "ep 1194: ep_len:3 episode reward: total was 0.000000. running mean: -23.358921\n",
      "ep 1194: ep_len:505 episode reward: total was -34.780000. running mean: -23.473132\n",
      "ep 1194: ep_len:195 episode reward: total was -17.430000. running mean: -23.412701\n",
      "epsilon:0.236882 episode_count: 8365. steps_count: 3714241.000000\n",
      "ep 1195: ep_len:550 episode reward: total was -20.110000. running mean: -23.379674\n",
      "ep 1195: ep_len:590 episode reward: total was -46.610000. running mean: -23.611977\n",
      "ep 1195: ep_len:555 episode reward: total was -24.660000. running mean: -23.622457\n",
      "ep 1195: ep_len:560 episode reward: total was 2.530000. running mean: -23.360933\n",
      "ep 1195: ep_len:3 episode reward: total was 0.000000. running mean: -23.127323\n",
      "ep 1195: ep_len:505 episode reward: total was -30.740000. running mean: -23.203450\n",
      "ep 1195: ep_len:580 episode reward: total was -52.700000. running mean: -23.498416\n",
      "epsilon:0.236746 episode_count: 8372. steps_count: 3717584.000000\n",
      "ep 1196: ep_len:540 episode reward: total was -38.540000. running mean: -23.648831\n",
      "ep 1196: ep_len:585 episode reward: total was -28.670000. running mean: -23.699043\n",
      "ep 1196: ep_len:530 episode reward: total was -19.700000. running mean: -23.659053\n",
      "ep 1196: ep_len:525 episode reward: total was -11.610000. running mean: -23.538562\n",
      "ep 1196: ep_len:3 episode reward: total was 0.000000. running mean: -23.303177\n",
      "ep 1196: ep_len:500 episode reward: total was -46.640000. running mean: -23.536545\n",
      "ep 1196: ep_len:670 episode reward: total was -75.580000. running mean: -24.056979\n",
      "epsilon:0.236609 episode_count: 8379. steps_count: 3720937.000000\n",
      "ep 1197: ep_len:590 episode reward: total was -14.420000. running mean: -23.960610\n",
      "ep 1197: ep_len:535 episode reward: total was -24.390000. running mean: -23.964903\n",
      "ep 1197: ep_len:359 episode reward: total was -7.380000. running mean: -23.799054\n",
      "ep 1197: ep_len:126 episode reward: total was -0.400000. running mean: -23.565064\n",
      "ep 1197: ep_len:93 episode reward: total was -13.470000. running mean: -23.464113\n",
      "ep 1197: ep_len:610 episode reward: total was -29.870000. running mean: -23.528172\n",
      "ep 1197: ep_len:605 episode reward: total was -48.100000. running mean: -23.773890\n",
      "epsilon:0.236473 episode_count: 8386. steps_count: 3723855.000000\n",
      "ep 1198: ep_len:565 episode reward: total was -37.370000. running mean: -23.909852\n",
      "ep 1198: ep_len:535 episode reward: total was -20.590000. running mean: -23.876653\n",
      "ep 1198: ep_len:645 episode reward: total was -32.420000. running mean: -23.962086\n",
      "ep 1198: ep_len:605 episode reward: total was -39.650000. running mean: -24.118966\n",
      "ep 1198: ep_len:3 episode reward: total was 0.000000. running mean: -23.877776\n",
      "ep 1198: ep_len:590 episode reward: total was -31.930000. running mean: -23.958298\n",
      "ep 1198: ep_len:525 episode reward: total was -54.210000. running mean: -24.260815\n",
      "epsilon:0.236336 episode_count: 8393. steps_count: 3727323.000000\n",
      "ep 1199: ep_len:215 episode reward: total was -8.440000. running mean: -24.102607\n",
      "ep 1199: ep_len:550 episode reward: total was -44.300000. running mean: -24.304581\n",
      "ep 1199: ep_len:525 episode reward: total was -7.360000. running mean: -24.135135\n",
      "ep 1199: ep_len:500 episode reward: total was -16.100000. running mean: -24.054784\n",
      "ep 1199: ep_len:95 episode reward: total was -1.480000. running mean: -23.829036\n",
      "ep 1199: ep_len:685 episode reward: total was -60.960000. running mean: -24.200346\n",
      "ep 1199: ep_len:500 episode reward: total was -42.230000. running mean: -24.380642\n",
      "epsilon:0.236200 episode_count: 8400. steps_count: 3730393.000000\n",
      "ep 1200: ep_len:129 episode reward: total was -9.460000. running mean: -24.231436\n",
      "ep 1200: ep_len:625 episode reward: total was -37.730000. running mean: -24.366421\n",
      "ep 1200: ep_len:565 episode reward: total was -17.170000. running mean: -24.294457\n",
      "ep 1200: ep_len:505 episode reward: total was -14.720000. running mean: -24.198713\n",
      "ep 1200: ep_len:1 episode reward: total was 0.000000. running mean: -23.956725\n",
      "ep 1200: ep_len:625 episode reward: total was -54.200000. running mean: -24.259158\n",
      "ep 1200: ep_len:500 episode reward: total was -40.770000. running mean: -24.424267\n",
      "epsilon:0.236063 episode_count: 8407. steps_count: 3733343.000000\n",
      "ep 1201: ep_len:505 episode reward: total was -32.020000. running mean: -24.500224\n",
      "ep 1201: ep_len:620 episode reward: total was -13.490000. running mean: -24.390122\n",
      "ep 1201: ep_len:655 episode reward: total was -24.780000. running mean: -24.394021\n",
      "ep 1201: ep_len:56 episode reward: total was -5.460000. running mean: -24.204680\n",
      "ep 1201: ep_len:3 episode reward: total was 0.000000. running mean: -23.962634\n",
      "ep 1201: ep_len:625 episode reward: total was -21.670000. running mean: -23.939707\n",
      "ep 1201: ep_len:570 episode reward: total was -31.010000. running mean: -24.010410\n",
      "epsilon:0.235927 episode_count: 8414. steps_count: 3736377.000000\n",
      "ep 1202: ep_len:575 episode reward: total was -18.150000. running mean: -23.951806\n",
      "ep 1202: ep_len:525 episode reward: total was -22.220000. running mean: -23.934488\n",
      "ep 1202: ep_len:650 episode reward: total was -26.520000. running mean: -23.960343\n",
      "ep 1202: ep_len:515 episode reward: total was -12.220000. running mean: -23.842940\n",
      "ep 1202: ep_len:101 episode reward: total was -8.450000. running mean: -23.689010\n",
      "ep 1202: ep_len:291 episode reward: total was -29.910000. running mean: -23.751220\n",
      "ep 1202: ep_len:580 episode reward: total was -56.280000. running mean: -24.076508\n",
      "epsilon:0.235790 episode_count: 8421. steps_count: 3739614.000000\n",
      "ep 1203: ep_len:630 episode reward: total was -16.000000. running mean: -23.995743\n",
      "ep 1203: ep_len:500 episode reward: total was -14.920000. running mean: -23.904985\n",
      "ep 1203: ep_len:535 episode reward: total was -25.550000. running mean: -23.921436\n",
      "ep 1203: ep_len:56 episode reward: total was -3.950000. running mean: -23.721721\n",
      "ep 1203: ep_len:51 episode reward: total was 3.500000. running mean: -23.449504\n",
      "ep 1203: ep_len:520 episode reward: total was -21.110000. running mean: -23.426109\n",
      "ep 1203: ep_len:515 episode reward: total was -45.720000. running mean: -23.649048\n",
      "epsilon:0.235654 episode_count: 8428. steps_count: 3742421.000000\n",
      "ep 1204: ep_len:500 episode reward: total was -17.220000. running mean: -23.584757\n",
      "ep 1204: ep_len:595 episode reward: total was -3.230000. running mean: -23.381210\n",
      "ep 1204: ep_len:500 episode reward: total was -40.640000. running mean: -23.553798\n",
      "ep 1204: ep_len:510 episode reward: total was -22.780000. running mean: -23.546060\n",
      "ep 1204: ep_len:3 episode reward: total was 0.000000. running mean: -23.310599\n",
      "ep 1204: ep_len:570 episode reward: total was -24.610000. running mean: -23.323593\n",
      "ep 1204: ep_len:630 episode reward: total was -30.920000. running mean: -23.399557\n",
      "epsilon:0.235517 episode_count: 8435. steps_count: 3745729.000000\n",
      "ep 1205: ep_len:600 episode reward: total was -23.500000. running mean: -23.400562\n",
      "ep 1205: ep_len:545 episode reward: total was -19.490000. running mean: -23.361456\n",
      "ep 1205: ep_len:645 episode reward: total was -66.720000. running mean: -23.795041\n",
      "ep 1205: ep_len:123 episode reward: total was -8.960000. running mean: -23.646691\n",
      "ep 1205: ep_len:3 episode reward: total was 0.000000. running mean: -23.410224\n",
      "ep 1205: ep_len:585 episode reward: total was -26.370000. running mean: -23.439822\n",
      "ep 1205: ep_len:605 episode reward: total was -23.900000. running mean: -23.444424\n",
      "epsilon:0.235381 episode_count: 8442. steps_count: 3748835.000000\n",
      "ep 1206: ep_len:500 episode reward: total was -28.960000. running mean: -23.499579\n",
      "ep 1206: ep_len:660 episode reward: total was -22.540000. running mean: -23.489984\n",
      "ep 1206: ep_len:510 episode reward: total was -21.720000. running mean: -23.472284\n",
      "ep 1206: ep_len:525 episode reward: total was -19.520000. running mean: -23.432761\n",
      "ep 1206: ep_len:3 episode reward: total was 0.000000. running mean: -23.198433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1206: ep_len:620 episode reward: total was -47.990000. running mean: -23.446349\n",
      "ep 1206: ep_len:530 episode reward: total was -48.670000. running mean: -23.698586\n",
      "epsilon:0.235244 episode_count: 8449. steps_count: 3752183.000000\n",
      "ep 1207: ep_len:265 episode reward: total was -9.910000. running mean: -23.560700\n",
      "ep 1207: ep_len:530 episode reward: total was -40.150000. running mean: -23.726593\n",
      "ep 1207: ep_len:595 episode reward: total was -76.130000. running mean: -24.250627\n",
      "ep 1207: ep_len:500 episode reward: total was -6.220000. running mean: -24.070320\n",
      "ep 1207: ep_len:3 episode reward: total was 0.000000. running mean: -23.829617\n",
      "ep 1207: ep_len:171 episode reward: total was -3.440000. running mean: -23.625721\n",
      "ep 1207: ep_len:505 episode reward: total was -36.050000. running mean: -23.749964\n",
      "epsilon:0.235108 episode_count: 8456. steps_count: 3754752.000000\n",
      "ep 1208: ep_len:545 episode reward: total was -47.590000. running mean: -23.988364\n",
      "ep 1208: ep_len:585 episode reward: total was -12.230000. running mean: -23.870781\n",
      "ep 1208: ep_len:620 episode reward: total was -71.700000. running mean: -24.349073\n",
      "ep 1208: ep_len:500 episode reward: total was -24.080000. running mean: -24.346382\n",
      "ep 1208: ep_len:1 episode reward: total was 0.000000. running mean: -24.102918\n",
      "ep 1208: ep_len:555 episode reward: total was -59.670000. running mean: -24.458589\n",
      "ep 1208: ep_len:210 episode reward: total was -16.880000. running mean: -24.382803\n",
      "epsilon:0.234971 episode_count: 8463. steps_count: 3757768.000000\n",
      "ep 1209: ep_len:655 episode reward: total was -25.810000. running mean: -24.397075\n",
      "ep 1209: ep_len:525 episode reward: total was -10.020000. running mean: -24.253304\n",
      "ep 1209: ep_len:500 episode reward: total was -22.090000. running mean: -24.231671\n",
      "ep 1209: ep_len:500 episode reward: total was -16.570000. running mean: -24.155055\n",
      "ep 1209: ep_len:111 episode reward: total was -0.960000. running mean: -23.923104\n",
      "ep 1209: ep_len:520 episode reward: total was -18.990000. running mean: -23.873773\n",
      "ep 1209: ep_len:535 episode reward: total was -27.450000. running mean: -23.909535\n",
      "epsilon:0.234835 episode_count: 8470. steps_count: 3761114.000000\n",
      "ep 1210: ep_len:535 episode reward: total was -20.050000. running mean: -23.870940\n",
      "ep 1210: ep_len:500 episode reward: total was -33.370000. running mean: -23.965931\n",
      "ep 1210: ep_len:635 episode reward: total was -34.190000. running mean: -24.068171\n",
      "ep 1210: ep_len:500 episode reward: total was -18.560000. running mean: -24.013090\n",
      "ep 1210: ep_len:85 episode reward: total was -13.970000. running mean: -23.912659\n",
      "ep 1210: ep_len:640 episode reward: total was -51.730000. running mean: -24.190832\n",
      "ep 1210: ep_len:530 episode reward: total was -34.110000. running mean: -24.290024\n",
      "epsilon:0.234698 episode_count: 8477. steps_count: 3764539.000000\n",
      "ep 1211: ep_len:570 episode reward: total was -36.870000. running mean: -24.415824\n",
      "ep 1211: ep_len:620 episode reward: total was -9.530000. running mean: -24.266965\n",
      "ep 1211: ep_len:540 episode reward: total was -40.580000. running mean: -24.430096\n",
      "ep 1211: ep_len:500 episode reward: total was -18.600000. running mean: -24.371795\n",
      "ep 1211: ep_len:3 episode reward: total was 0.000000. running mean: -24.128077\n",
      "ep 1211: ep_len:505 episode reward: total was -36.810000. running mean: -24.254896\n",
      "ep 1211: ep_len:600 episode reward: total was -32.410000. running mean: -24.336447\n",
      "epsilon:0.234562 episode_count: 8484. steps_count: 3767877.000000\n",
      "ep 1212: ep_len:500 episode reward: total was -40.080000. running mean: -24.493883\n",
      "ep 1212: ep_len:540 episode reward: total was -1.160000. running mean: -24.260544\n",
      "ep 1212: ep_len:615 episode reward: total was -21.810000. running mean: -24.236038\n",
      "ep 1212: ep_len:500 episode reward: total was -16.600000. running mean: -24.159678\n",
      "ep 1212: ep_len:3 episode reward: total was 0.000000. running mean: -23.918081\n",
      "ep 1212: ep_len:245 episode reward: total was -7.410000. running mean: -23.753000\n",
      "ep 1212: ep_len:630 episode reward: total was -28.930000. running mean: -23.804770\n",
      "epsilon:0.234425 episode_count: 8491. steps_count: 3770910.000000\n",
      "ep 1213: ep_len:525 episode reward: total was -19.530000. running mean: -23.762023\n",
      "ep 1213: ep_len:500 episode reward: total was -25.920000. running mean: -23.783602\n",
      "ep 1213: ep_len:555 episode reward: total was -14.130000. running mean: -23.687066\n",
      "ep 1213: ep_len:361 episode reward: total was -27.720000. running mean: -23.727396\n",
      "ep 1213: ep_len:49 episode reward: total was 3.000000. running mean: -23.460122\n",
      "ep 1213: ep_len:595 episode reward: total was -40.630000. running mean: -23.631821\n",
      "ep 1213: ep_len:182 episode reward: total was -13.420000. running mean: -23.529702\n",
      "epsilon:0.234289 episode_count: 8498. steps_count: 3773677.000000\n",
      "ep 1214: ep_len:650 episode reward: total was -38.810000. running mean: -23.682505\n",
      "ep 1214: ep_len:185 episode reward: total was -8.900000. running mean: -23.534680\n",
      "ep 1214: ep_len:500 episode reward: total was -14.640000. running mean: -23.445733\n",
      "ep 1214: ep_len:545 episode reward: total was -21.530000. running mean: -23.426576\n",
      "ep 1214: ep_len:3 episode reward: total was 0.000000. running mean: -23.192310\n",
      "ep 1214: ep_len:640 episode reward: total was -45.060000. running mean: -23.410987\n",
      "ep 1214: ep_len:256 episode reward: total was -11.330000. running mean: -23.290177\n",
      "epsilon:0.234152 episode_count: 8505. steps_count: 3776456.000000\n",
      "ep 1215: ep_len:565 episode reward: total was -40.650000. running mean: -23.463776\n",
      "ep 1215: ep_len:585 episode reward: total was -92.830000. running mean: -24.157438\n",
      "ep 1215: ep_len:500 episode reward: total was -27.190000. running mean: -24.187763\n",
      "ep 1215: ep_len:600 episode reward: total was -18.080000. running mean: -24.126686\n",
      "ep 1215: ep_len:128 episode reward: total was 0.550000. running mean: -23.879919\n",
      "ep 1215: ep_len:545 episode reward: total was -39.460000. running mean: -24.035720\n",
      "ep 1215: ep_len:625 episode reward: total was -38.570000. running mean: -24.181063\n",
      "epsilon:0.234016 episode_count: 8512. steps_count: 3780004.000000\n",
      "ep 1216: ep_len:535 episode reward: total was -29.090000. running mean: -24.230152\n",
      "ep 1216: ep_len:625 episode reward: total was -113.940000. running mean: -25.127250\n",
      "ep 1216: ep_len:590 episode reward: total was -39.660000. running mean: -25.272578\n",
      "ep 1216: ep_len:605 episode reward: total was -20.050000. running mean: -25.220352\n",
      "ep 1216: ep_len:122 episode reward: total was -4.450000. running mean: -25.012649\n",
      "ep 1216: ep_len:550 episode reward: total was -19.140000. running mean: -24.953922\n",
      "ep 1216: ep_len:615 episode reward: total was -23.880000. running mean: -24.943183\n",
      "epsilon:0.233879 episode_count: 8519. steps_count: 3783646.000000\n",
      "ep 1217: ep_len:605 episode reward: total was -35.900000. running mean: -25.052751\n",
      "ep 1217: ep_len:500 episode reward: total was -19.390000. running mean: -24.996124\n",
      "ep 1217: ep_len:387 episode reward: total was -15.300000. running mean: -24.899162\n",
      "ep 1217: ep_len:505 episode reward: total was -20.490000. running mean: -24.855071\n",
      "ep 1217: ep_len:76 episode reward: total was -2.950000. running mean: -24.636020\n",
      "ep 1217: ep_len:520 episode reward: total was -21.560000. running mean: -24.605260\n",
      "ep 1217: ep_len:505 episode reward: total was -31.240000. running mean: -24.671607\n",
      "epsilon:0.233743 episode_count: 8526. steps_count: 3786744.000000\n",
      "ep 1218: ep_len:247 episode reward: total was -20.410000. running mean: -24.628991\n",
      "ep 1218: ep_len:535 episode reward: total was -37.750000. running mean: -24.760201\n",
      "ep 1218: ep_len:429 episode reward: total was -24.810000. running mean: -24.760699\n",
      "ep 1218: ep_len:520 episode reward: total was -34.680000. running mean: -24.859892\n",
      "ep 1218: ep_len:3 episode reward: total was 0.000000. running mean: -24.611293\n",
      "ep 1218: ep_len:570 episode reward: total was -43.250000. running mean: -24.797680\n",
      "ep 1218: ep_len:525 episode reward: total was -27.690000. running mean: -24.826604\n",
      "epsilon:0.233606 episode_count: 8533. steps_count: 3789573.000000\n",
      "ep 1219: ep_len:600 episode reward: total was -29.740000. running mean: -24.875738\n",
      "ep 1219: ep_len:625 episode reward: total was -23.160000. running mean: -24.858580\n",
      "ep 1219: ep_len:535 episode reward: total was -19.680000. running mean: -24.806794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1219: ep_len:500 episode reward: total was -14.550000. running mean: -24.704226\n",
      "ep 1219: ep_len:3 episode reward: total was 0.000000. running mean: -24.457184\n",
      "ep 1219: ep_len:164 episode reward: total was -11.930000. running mean: -24.331912\n",
      "ep 1219: ep_len:287 episode reward: total was -13.380000. running mean: -24.222393\n",
      "epsilon:0.233470 episode_count: 8540. steps_count: 3792287.000000\n",
      "ep 1220: ep_len:525 episode reward: total was -41.980000. running mean: -24.399969\n",
      "ep 1220: ep_len:560 episode reward: total was -35.690000. running mean: -24.512870\n",
      "ep 1220: ep_len:520 episode reward: total was -36.700000. running mean: -24.634741\n",
      "ep 1220: ep_len:580 episode reward: total was -11.050000. running mean: -24.498893\n",
      "ep 1220: ep_len:3 episode reward: total was 0.000000. running mean: -24.253905\n",
      "ep 1220: ep_len:545 episode reward: total was -38.480000. running mean: -24.396165\n",
      "ep 1220: ep_len:510 episode reward: total was -27.440000. running mean: -24.426604\n",
      "epsilon:0.233333 episode_count: 8547. steps_count: 3795530.000000\n",
      "ep 1221: ep_len:500 episode reward: total was -35.900000. running mean: -24.541338\n",
      "ep 1221: ep_len:189 episode reward: total was -8.420000. running mean: -24.380124\n",
      "ep 1221: ep_len:715 episode reward: total was -52.310000. running mean: -24.659423\n",
      "ep 1221: ep_len:124 episode reward: total was -8.960000. running mean: -24.502429\n",
      "ep 1221: ep_len:3 episode reward: total was 0.000000. running mean: -24.257405\n",
      "ep 1221: ep_len:169 episode reward: total was -6.970000. running mean: -24.084531\n",
      "ep 1221: ep_len:500 episode reward: total was -14.720000. running mean: -23.990885\n",
      "epsilon:0.233197 episode_count: 8554. steps_count: 3797730.000000\n",
      "ep 1222: ep_len:550 episode reward: total was -16.220000. running mean: -23.913176\n",
      "ep 1222: ep_len:510 episode reward: total was -9.370000. running mean: -23.767745\n",
      "ep 1222: ep_len:413 episode reward: total was -10.840000. running mean: -23.638467\n",
      "ep 1222: ep_len:158 episode reward: total was -5.430000. running mean: -23.456383\n",
      "ep 1222: ep_len:1 episode reward: total was 0.000000. running mean: -23.221819\n",
      "ep 1222: ep_len:575 episode reward: total was -38.200000. running mean: -23.371601\n",
      "ep 1222: ep_len:520 episode reward: total was -52.610000. running mean: -23.663985\n",
      "epsilon:0.233060 episode_count: 8561. steps_count: 3800457.000000\n",
      "ep 1223: ep_len:530 episode reward: total was -47.540000. running mean: -23.902745\n",
      "ep 1223: ep_len:510 episode reward: total was -24.280000. running mean: -23.906517\n",
      "ep 1223: ep_len:625 episode reward: total was -27.000000. running mean: -23.937452\n",
      "ep 1223: ep_len:505 episode reward: total was -25.600000. running mean: -23.954078\n",
      "ep 1223: ep_len:3 episode reward: total was 0.000000. running mean: -23.714537\n",
      "ep 1223: ep_len:560 episode reward: total was -39.950000. running mean: -23.876891\n",
      "ep 1223: ep_len:211 episode reward: total was -18.890000. running mean: -23.827022\n",
      "epsilon:0.232924 episode_count: 8568. steps_count: 3803401.000000\n",
      "ep 1224: ep_len:248 episode reward: total was -14.900000. running mean: -23.737752\n",
      "ep 1224: ep_len:530 episode reward: total was 7.780000. running mean: -23.422575\n",
      "ep 1224: ep_len:540 episode reward: total was -23.150000. running mean: -23.419849\n",
      "ep 1224: ep_len:520 episode reward: total was -30.990000. running mean: -23.495550\n",
      "ep 1224: ep_len:3 episode reward: total was 0.000000. running mean: -23.260595\n",
      "ep 1224: ep_len:615 episode reward: total was -29.060000. running mean: -23.318589\n",
      "ep 1224: ep_len:520 episode reward: total was -37.520000. running mean: -23.460603\n",
      "epsilon:0.232787 episode_count: 8575. steps_count: 3806377.000000\n",
      "ep 1225: ep_len:555 episode reward: total was -23.180000. running mean: -23.457797\n",
      "ep 1225: ep_len:500 episode reward: total was -19.840000. running mean: -23.421619\n",
      "ep 1225: ep_len:555 episode reward: total was -19.160000. running mean: -23.379003\n",
      "ep 1225: ep_len:590 episode reward: total was -23.570000. running mean: -23.380913\n",
      "ep 1225: ep_len:3 episode reward: total was 0.000000. running mean: -23.147104\n",
      "ep 1225: ep_len:500 episode reward: total was -25.680000. running mean: -23.172433\n",
      "ep 1225: ep_len:635 episode reward: total was -12.820000. running mean: -23.068908\n",
      "epsilon:0.232651 episode_count: 8582. steps_count: 3809715.000000\n",
      "ep 1226: ep_len:570 episode reward: total was -34.330000. running mean: -23.181519\n",
      "ep 1226: ep_len:500 episode reward: total was -23.570000. running mean: -23.185404\n",
      "ep 1226: ep_len:525 episode reward: total was -49.730000. running mean: -23.450850\n",
      "ep 1226: ep_len:500 episode reward: total was -27.210000. running mean: -23.488442\n",
      "ep 1226: ep_len:103 episode reward: total was 4.040000. running mean: -23.213157\n",
      "ep 1226: ep_len:595 episode reward: total was -36.130000. running mean: -23.342326\n",
      "ep 1226: ep_len:500 episode reward: total was -18.110000. running mean: -23.290002\n",
      "epsilon:0.232514 episode_count: 8589. steps_count: 3813008.000000\n",
      "ep 1227: ep_len:605 episode reward: total was -22.690000. running mean: -23.284002\n",
      "ep 1227: ep_len:655 episode reward: total was -47.140000. running mean: -23.522562\n",
      "ep 1227: ep_len:565 episode reward: total was -50.510000. running mean: -23.792437\n",
      "ep 1227: ep_len:595 episode reward: total was -27.180000. running mean: -23.826312\n",
      "ep 1227: ep_len:3 episode reward: total was 0.000000. running mean: -23.588049\n",
      "ep 1227: ep_len:336 episode reward: total was -8.870000. running mean: -23.440869\n",
      "ep 1227: ep_len:575 episode reward: total was -20.130000. running mean: -23.407760\n",
      "epsilon:0.232378 episode_count: 8596. steps_count: 3816342.000000\n",
      "ep 1228: ep_len:500 episode reward: total was -32.030000. running mean: -23.493982\n",
      "ep 1228: ep_len:510 episode reward: total was -7.710000. running mean: -23.336143\n",
      "ep 1228: ep_len:474 episode reward: total was -11.300000. running mean: -23.215781\n",
      "ep 1228: ep_len:126 episode reward: total was -8.950000. running mean: -23.073123\n",
      "ep 1228: ep_len:108 episode reward: total was 4.050000. running mean: -22.801892\n",
      "ep 1228: ep_len:540 episode reward: total was -36.600000. running mean: -22.939873\n",
      "ep 1228: ep_len:600 episode reward: total was -29.900000. running mean: -23.009474\n",
      "epsilon:0.232241 episode_count: 8603. steps_count: 3819200.000000\n",
      "ep 1229: ep_len:500 episode reward: total was -14.260000. running mean: -22.921980\n",
      "ep 1229: ep_len:172 episode reward: total was -8.920000. running mean: -22.781960\n",
      "ep 1229: ep_len:535 episode reward: total was -48.060000. running mean: -23.034740\n",
      "ep 1229: ep_len:159 episode reward: total was -3.430000. running mean: -22.838693\n",
      "ep 1229: ep_len:3 episode reward: total was 0.000000. running mean: -22.610306\n",
      "ep 1229: ep_len:580 episode reward: total was -34.890000. running mean: -22.733103\n",
      "ep 1229: ep_len:510 episode reward: total was -25.670000. running mean: -22.762472\n",
      "epsilon:0.232105 episode_count: 8610. steps_count: 3821659.000000\n",
      "ep 1230: ep_len:540 episode reward: total was -7.650000. running mean: -22.611347\n",
      "ep 1230: ep_len:175 episode reward: total was -13.980000. running mean: -22.525034\n",
      "ep 1230: ep_len:360 episode reward: total was -5.810000. running mean: -22.357883\n",
      "ep 1230: ep_len:38 episode reward: total was -0.470000. running mean: -22.139005\n",
      "ep 1230: ep_len:95 episode reward: total was -2.460000. running mean: -21.942215\n",
      "ep 1230: ep_len:319 episode reward: total was -8.880000. running mean: -21.811592\n",
      "ep 1230: ep_len:330 episode reward: total was -24.860000. running mean: -21.842076\n",
      "epsilon:0.231968 episode_count: 8617. steps_count: 3823516.000000\n",
      "ep 1231: ep_len:525 episode reward: total was -33.410000. running mean: -21.957756\n",
      "ep 1231: ep_len:595 episode reward: total was -21.960000. running mean: -21.957778\n",
      "ep 1231: ep_len:660 episode reward: total was -33.290000. running mean: -22.071100\n",
      "ep 1231: ep_len:121 episode reward: total was -3.440000. running mean: -21.884789\n",
      "ep 1231: ep_len:3 episode reward: total was 0.000000. running mean: -21.665941\n",
      "ep 1231: ep_len:530 episode reward: total was -21.850000. running mean: -21.667782\n",
      "ep 1231: ep_len:250 episode reward: total was -5.350000. running mean: -21.504604\n",
      "epsilon:0.231832 episode_count: 8624. steps_count: 3826200.000000\n",
      "ep 1232: ep_len:535 episode reward: total was -37.050000. running mean: -21.660058\n",
      "ep 1232: ep_len:555 episode reward: total was -34.540000. running mean: -21.788858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1232: ep_len:715 episode reward: total was -58.930000. running mean: -22.160269\n",
      "ep 1232: ep_len:505 episode reward: total was -9.720000. running mean: -22.035866\n",
      "ep 1232: ep_len:3 episode reward: total was 0.000000. running mean: -21.815508\n",
      "ep 1232: ep_len:500 episode reward: total was -18.830000. running mean: -21.785653\n",
      "ep 1232: ep_len:570 episode reward: total was -39.640000. running mean: -21.964196\n",
      "epsilon:0.231695 episode_count: 8631. steps_count: 3829583.000000\n",
      "ep 1233: ep_len:134 episode reward: total was -3.460000. running mean: -21.779154\n",
      "ep 1233: ep_len:500 episode reward: total was -28.630000. running mean: -21.847663\n",
      "ep 1233: ep_len:73 episode reward: total was 2.540000. running mean: -21.603786\n",
      "ep 1233: ep_len:505 episode reward: total was -47.170000. running mean: -21.859448\n",
      "ep 1233: ep_len:115 episode reward: total was -12.450000. running mean: -21.765354\n",
      "ep 1233: ep_len:540 episode reward: total was -41.820000. running mean: -21.965900\n",
      "ep 1233: ep_len:500 episode reward: total was -22.800000. running mean: -21.974241\n",
      "epsilon:0.231559 episode_count: 8638. steps_count: 3831950.000000\n",
      "ep 1234: ep_len:640 episode reward: total was -46.450000. running mean: -22.218999\n",
      "ep 1234: ep_len:510 episode reward: total was -28.800000. running mean: -22.284809\n",
      "ep 1234: ep_len:650 episode reward: total was -37.570000. running mean: -22.437661\n",
      "ep 1234: ep_len:500 episode reward: total was -20.050000. running mean: -22.413784\n",
      "ep 1234: ep_len:113 episode reward: total was -8.960000. running mean: -22.279246\n",
      "ep 1234: ep_len:318 episode reward: total was -14.860000. running mean: -22.205054\n",
      "ep 1234: ep_len:284 episode reward: total was -6.320000. running mean: -22.046203\n",
      "epsilon:0.231422 episode_count: 8645. steps_count: 3834965.000000\n",
      "ep 1235: ep_len:615 episode reward: total was -25.380000. running mean: -22.079541\n",
      "ep 1235: ep_len:560 episode reward: total was -24.180000. running mean: -22.100546\n",
      "ep 1235: ep_len:43 episode reward: total was 0.520000. running mean: -21.874340\n",
      "ep 1235: ep_len:55 episode reward: total was -2.430000. running mean: -21.679897\n",
      "ep 1235: ep_len:3 episode reward: total was 0.000000. running mean: -21.463098\n",
      "ep 1235: ep_len:520 episode reward: total was -10.630000. running mean: -21.354767\n",
      "ep 1235: ep_len:520 episode reward: total was -33.510000. running mean: -21.476319\n",
      "epsilon:0.231286 episode_count: 8652. steps_count: 3837281.000000\n",
      "ep 1236: ep_len:500 episode reward: total was -9.280000. running mean: -21.354356\n",
      "ep 1236: ep_len:189 episode reward: total was -16.470000. running mean: -21.305512\n",
      "ep 1236: ep_len:585 episode reward: total was -30.040000. running mean: -21.392857\n",
      "ep 1236: ep_len:505 episode reward: total was -59.330000. running mean: -21.772229\n",
      "ep 1236: ep_len:3 episode reward: total was 0.000000. running mean: -21.554506\n",
      "ep 1236: ep_len:595 episode reward: total was -30.550000. running mean: -21.644461\n",
      "ep 1236: ep_len:605 episode reward: total was -31.920000. running mean: -21.747217\n",
      "epsilon:0.231149 episode_count: 8659. steps_count: 3840263.000000\n",
      "ep 1237: ep_len:211 episode reward: total was -9.460000. running mean: -21.624345\n",
      "ep 1237: ep_len:535 episode reward: total was -15.080000. running mean: -21.558901\n",
      "ep 1237: ep_len:545 episode reward: total was -21.180000. running mean: -21.555112\n",
      "ep 1237: ep_len:500 episode reward: total was -19.200000. running mean: -21.531561\n",
      "ep 1237: ep_len:3 episode reward: total was 0.000000. running mean: -21.316245\n",
      "ep 1237: ep_len:560 episode reward: total was -26.130000. running mean: -21.364383\n",
      "ep 1237: ep_len:535 episode reward: total was -27.150000. running mean: -21.422239\n",
      "epsilon:0.231013 episode_count: 8666. steps_count: 3843152.000000\n",
      "ep 1238: ep_len:555 episode reward: total was -18.200000. running mean: -21.390017\n",
      "ep 1238: ep_len:181 episode reward: total was -8.390000. running mean: -21.260017\n",
      "ep 1238: ep_len:575 episode reward: total was -26.180000. running mean: -21.309216\n",
      "ep 1238: ep_len:500 episode reward: total was -13.750000. running mean: -21.233624\n",
      "ep 1238: ep_len:3 episode reward: total was 0.000000. running mean: -21.021288\n",
      "ep 1238: ep_len:520 episode reward: total was -16.670000. running mean: -20.977775\n",
      "ep 1238: ep_len:605 episode reward: total was -29.440000. running mean: -21.062397\n",
      "epsilon:0.230876 episode_count: 8673. steps_count: 3846091.000000\n",
      "ep 1239: ep_len:555 episode reward: total was -41.980000. running mean: -21.271573\n",
      "ep 1239: ep_len:540 episode reward: total was -9.710000. running mean: -21.155958\n",
      "ep 1239: ep_len:645 episode reward: total was -42.890000. running mean: -21.373298\n",
      "ep 1239: ep_len:595 episode reward: total was -29.680000. running mean: -21.456365\n",
      "ep 1239: ep_len:108 episode reward: total was 3.040000. running mean: -21.211401\n",
      "ep 1239: ep_len:515 episode reward: total was -15.540000. running mean: -21.154687\n",
      "ep 1239: ep_len:550 episode reward: total was -27.210000. running mean: -21.215241\n",
      "epsilon:0.230740 episode_count: 8680. steps_count: 3849599.000000\n",
      "ep 1240: ep_len:630 episode reward: total was -34.320000. running mean: -21.346288\n",
      "ep 1240: ep_len:585 episode reward: total was -54.610000. running mean: -21.678925\n",
      "ep 1240: ep_len:535 episode reward: total was -28.920000. running mean: -21.751336\n",
      "ep 1240: ep_len:170 episode reward: total was -4.920000. running mean: -21.583023\n",
      "ep 1240: ep_len:96 episode reward: total was 3.510000. running mean: -21.332092\n",
      "ep 1240: ep_len:505 episode reward: total was -38.090000. running mean: -21.499672\n",
      "ep 1240: ep_len:595 episode reward: total was -45.700000. running mean: -21.741675\n",
      "epsilon:0.230603 episode_count: 8687. steps_count: 3852715.000000\n",
      "ep 1241: ep_len:575 episode reward: total was -22.030000. running mean: -21.744558\n",
      "ep 1241: ep_len:505 episode reward: total was -19.230000. running mean: -21.719412\n",
      "ep 1241: ep_len:625 episode reward: total was -36.830000. running mean: -21.870518\n",
      "ep 1241: ep_len:500 episode reward: total was 0.360000. running mean: -21.648213\n",
      "ep 1241: ep_len:110 episode reward: total was -12.450000. running mean: -21.556231\n",
      "ep 1241: ep_len:545 episode reward: total was -29.120000. running mean: -21.631869\n",
      "ep 1241: ep_len:620 episode reward: total was -43.190000. running mean: -21.847450\n",
      "epsilon:0.230467 episode_count: 8694. steps_count: 3856195.000000\n",
      "ep 1242: ep_len:134 episode reward: total was -5.440000. running mean: -21.683376\n",
      "ep 1242: ep_len:550 episode reward: total was -2.180000. running mean: -21.488342\n",
      "ep 1242: ep_len:580 episode reward: total was -36.530000. running mean: -21.638758\n",
      "ep 1242: ep_len:585 episode reward: total was -50.110000. running mean: -21.923471\n",
      "ep 1242: ep_len:86 episode reward: total was -7.470000. running mean: -21.778936\n",
      "ep 1242: ep_len:505 episode reward: total was -29.830000. running mean: -21.859447\n",
      "ep 1242: ep_len:625 episode reward: total was -51.080000. running mean: -22.151652\n",
      "epsilon:0.230330 episode_count: 8701. steps_count: 3859260.000000\n",
      "ep 1243: ep_len:132 episode reward: total was -6.950000. running mean: -21.999636\n",
      "ep 1243: ep_len:500 episode reward: total was -3.870000. running mean: -21.818339\n",
      "ep 1243: ep_len:545 episode reward: total was -35.750000. running mean: -21.957656\n",
      "ep 1243: ep_len:560 episode reward: total was -15.150000. running mean: -21.889579\n",
      "ep 1243: ep_len:3 episode reward: total was 0.000000. running mean: -21.670684\n",
      "ep 1243: ep_len:590 episode reward: total was -11.980000. running mean: -21.573777\n",
      "ep 1243: ep_len:272 episode reward: total was -9.330000. running mean: -21.451339\n",
      "epsilon:0.230194 episode_count: 8708. steps_count: 3861862.000000\n",
      "ep 1244: ep_len:241 episode reward: total was 0.600000. running mean: -21.230826\n",
      "ep 1244: ep_len:605 episode reward: total was -43.080000. running mean: -21.449317\n",
      "ep 1244: ep_len:590 episode reward: total was -22.360000. running mean: -21.458424\n",
      "ep 1244: ep_len:610 episode reward: total was -15.500000. running mean: -21.398840\n",
      "ep 1244: ep_len:3 episode reward: total was 0.000000. running mean: -21.184852\n",
      "ep 1244: ep_len:505 episode reward: total was -30.700000. running mean: -21.280003\n",
      "ep 1244: ep_len:272 episode reward: total was -43.920000. running mean: -21.506403\n",
      "epsilon:0.230057 episode_count: 8715. steps_count: 3864688.000000\n",
      "ep 1245: ep_len:116 episode reward: total was -1.440000. running mean: -21.305739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1245: ep_len:610 episode reward: total was -46.080000. running mean: -21.553482\n",
      "ep 1245: ep_len:505 episode reward: total was -17.010000. running mean: -21.508047\n",
      "ep 1245: ep_len:56 episode reward: total was -0.930000. running mean: -21.302266\n",
      "ep 1245: ep_len:129 episode reward: total was -13.950000. running mean: -21.228744\n",
      "ep 1245: ep_len:530 episode reward: total was -35.300000. running mean: -21.369456\n",
      "ep 1245: ep_len:510 episode reward: total was -15.990000. running mean: -21.315662\n",
      "epsilon:0.229921 episode_count: 8722. steps_count: 3867144.000000\n",
      "ep 1246: ep_len:510 episode reward: total was -26.600000. running mean: -21.368505\n",
      "ep 1246: ep_len:585 episode reward: total was -32.410000. running mean: -21.478920\n",
      "ep 1246: ep_len:500 episode reward: total was -8.470000. running mean: -21.348831\n",
      "ep 1246: ep_len:565 episode reward: total was -32.980000. running mean: -21.465142\n",
      "ep 1246: ep_len:3 episode reward: total was 0.000000. running mean: -21.250491\n",
      "ep 1246: ep_len:550 episode reward: total was -16.960000. running mean: -21.207586\n",
      "ep 1246: ep_len:169 episode reward: total was -9.390000. running mean: -21.089410\n",
      "epsilon:0.229784 episode_count: 8729. steps_count: 3870026.000000\n",
      "ep 1247: ep_len:770 episode reward: total was -45.290000. running mean: -21.331416\n",
      "ep 1247: ep_len:585 episode reward: total was -35.480000. running mean: -21.472902\n",
      "ep 1247: ep_len:575 episode reward: total was -43.910000. running mean: -21.697273\n",
      "ep 1247: ep_len:500 episode reward: total was -9.670000. running mean: -21.577000\n",
      "ep 1247: ep_len:3 episode reward: total was 0.000000. running mean: -21.361230\n",
      "ep 1247: ep_len:565 episode reward: total was -28.400000. running mean: -21.431618\n",
      "ep 1247: ep_len:500 episode reward: total was -35.010000. running mean: -21.567402\n",
      "epsilon:0.229648 episode_count: 8736. steps_count: 3873524.000000\n",
      "ep 1248: ep_len:130 episode reward: total was -4.960000. running mean: -21.401328\n",
      "ep 1248: ep_len:585 episode reward: total was -26.560000. running mean: -21.452914\n",
      "ep 1248: ep_len:600 episode reward: total was -29.530000. running mean: -21.533685\n",
      "ep 1248: ep_len:500 episode reward: total was -31.650000. running mean: -21.634848\n",
      "ep 1248: ep_len:85 episode reward: total was -10.470000. running mean: -21.523200\n",
      "ep 1248: ep_len:500 episode reward: total was -14.660000. running mean: -21.454568\n",
      "ep 1248: ep_len:550 episode reward: total was -19.590000. running mean: -21.435922\n",
      "epsilon:0.229511 episode_count: 8743. steps_count: 3876474.000000\n",
      "ep 1249: ep_len:262 episode reward: total was -6.400000. running mean: -21.285563\n",
      "ep 1249: ep_len:605 episode reward: total was -21.970000. running mean: -21.292407\n",
      "ep 1249: ep_len:358 episode reward: total was -7.320000. running mean: -21.152683\n",
      "ep 1249: ep_len:500 episode reward: total was -3.990000. running mean: -20.981057\n",
      "ep 1249: ep_len:3 episode reward: total was 0.000000. running mean: -20.771246\n",
      "ep 1249: ep_len:305 episode reward: total was -9.360000. running mean: -20.657134\n",
      "ep 1249: ep_len:575 episode reward: total was -20.110000. running mean: -20.651662\n",
      "epsilon:0.229375 episode_count: 8750. steps_count: 3879082.000000\n",
      "ep 1250: ep_len:585 episode reward: total was -18.160000. running mean: -20.626746\n",
      "ep 1250: ep_len:610 episode reward: total was -19.060000. running mean: -20.611078\n",
      "ep 1250: ep_len:550 episode reward: total was -42.820000. running mean: -20.833167\n",
      "ep 1250: ep_len:379 episode reward: total was -7.240000. running mean: -20.697236\n",
      "ep 1250: ep_len:102 episode reward: total was 0.060000. running mean: -20.489663\n",
      "ep 1250: ep_len:580 episode reward: total was -29.710000. running mean: -20.581867\n",
      "ep 1250: ep_len:330 episode reward: total was -11.350000. running mean: -20.489548\n",
      "epsilon:0.229238 episode_count: 8757. steps_count: 3882218.000000\n",
      "ep 1251: ep_len:128 episode reward: total was -3.440000. running mean: -20.319053\n",
      "ep 1251: ep_len:363 episode reward: total was -66.980000. running mean: -20.785662\n",
      "ep 1251: ep_len:590 episode reward: total was -62.050000. running mean: -21.198305\n",
      "ep 1251: ep_len:560 episode reward: total was -41.070000. running mean: -21.397022\n",
      "ep 1251: ep_len:3 episode reward: total was 0.000000. running mean: -21.183052\n",
      "ep 1251: ep_len:545 episode reward: total was -25.480000. running mean: -21.226022\n",
      "ep 1251: ep_len:265 episode reward: total was -19.400000. running mean: -21.207761\n",
      "epsilon:0.229102 episode_count: 8764. steps_count: 3884672.000000\n",
      "ep 1252: ep_len:620 episode reward: total was -27.490000. running mean: -21.270584\n",
      "ep 1252: ep_len:570 episode reward: total was -34.520000. running mean: -21.403078\n",
      "ep 1252: ep_len:570 episode reward: total was -22.300000. running mean: -21.412047\n",
      "ep 1252: ep_len:575 episode reward: total was -8.630000. running mean: -21.284227\n",
      "ep 1252: ep_len:3 episode reward: total was 0.000000. running mean: -21.071384\n",
      "ep 1252: ep_len:500 episode reward: total was -37.570000. running mean: -21.236371\n",
      "ep 1252: ep_len:625 episode reward: total was -75.190000. running mean: -21.775907\n",
      "epsilon:0.228965 episode_count: 8771. steps_count: 3888135.000000\n",
      "ep 1253: ep_len:635 episode reward: total was -39.770000. running mean: -21.955848\n",
      "ep 1253: ep_len:505 episode reward: total was -20.520000. running mean: -21.941489\n",
      "ep 1253: ep_len:545 episode reward: total was -32.860000. running mean: -22.050674\n",
      "ep 1253: ep_len:510 episode reward: total was -32.600000. running mean: -22.156168\n",
      "ep 1253: ep_len:95 episode reward: total was 2.040000. running mean: -21.914206\n",
      "ep 1253: ep_len:505 episode reward: total was -21.000000. running mean: -21.905064\n",
      "ep 1253: ep_len:520 episode reward: total was -30.700000. running mean: -21.993013\n",
      "epsilon:0.228829 episode_count: 8778. steps_count: 3891450.000000\n",
      "ep 1254: ep_len:625 episode reward: total was -21.490000. running mean: -21.987983\n",
      "ep 1254: ep_len:615 episode reward: total was -51.770000. running mean: -22.285803\n",
      "ep 1254: ep_len:660 episode reward: total was -39.540000. running mean: -22.458345\n",
      "ep 1254: ep_len:520 episode reward: total was -41.690000. running mean: -22.650662\n",
      "ep 1254: ep_len:51 episode reward: total was 2.000000. running mean: -22.404155\n",
      "ep 1254: ep_len:570 episode reward: total was -20.630000. running mean: -22.386414\n",
      "ep 1254: ep_len:570 episode reward: total was -27.510000. running mean: -22.437650\n",
      "epsilon:0.228692 episode_count: 8785. steps_count: 3895061.000000\n",
      "ep 1255: ep_len:620 episode reward: total was -20.470000. running mean: -22.417973\n",
      "ep 1255: ep_len:510 episode reward: total was -12.320000. running mean: -22.316993\n",
      "ep 1255: ep_len:700 episode reward: total was -38.290000. running mean: -22.476723\n",
      "ep 1255: ep_len:570 episode reward: total was -15.130000. running mean: -22.403256\n",
      "ep 1255: ep_len:3 episode reward: total was 0.000000. running mean: -22.179224\n",
      "ep 1255: ep_len:500 episode reward: total was -36.810000. running mean: -22.325531\n",
      "ep 1255: ep_len:530 episode reward: total was -33.520000. running mean: -22.437476\n",
      "epsilon:0.228556 episode_count: 8792. steps_count: 3898494.000000\n",
      "ep 1256: ep_len:530 episode reward: total was -23.070000. running mean: -22.443801\n",
      "ep 1256: ep_len:565 episode reward: total was -33.060000. running mean: -22.549963\n",
      "ep 1256: ep_len:635 episode reward: total was -39.550000. running mean: -22.719964\n",
      "ep 1256: ep_len:132 episode reward: total was 2.090000. running mean: -22.471864\n",
      "ep 1256: ep_len:3 episode reward: total was 0.000000. running mean: -22.247145\n",
      "ep 1256: ep_len:585 episode reward: total was -25.880000. running mean: -22.283474\n",
      "ep 1256: ep_len:550 episode reward: total was -19.120000. running mean: -22.251839\n",
      "epsilon:0.228419 episode_count: 8799. steps_count: 3901494.000000\n",
      "ep 1257: ep_len:735 episode reward: total was -39.710000. running mean: -22.426421\n",
      "ep 1257: ep_len:560 episode reward: total was -32.600000. running mean: -22.528157\n",
      "ep 1257: ep_len:500 episode reward: total was -13.110000. running mean: -22.433975\n",
      "ep 1257: ep_len:500 episode reward: total was -28.160000. running mean: -22.491235\n",
      "ep 1257: ep_len:50 episode reward: total was 3.500000. running mean: -22.231323\n",
      "ep 1257: ep_len:555 episode reward: total was -36.630000. running mean: -22.375310\n",
      "ep 1257: ep_len:191 episode reward: total was -7.370000. running mean: -22.225257\n",
      "epsilon:0.228283 episode_count: 8806. steps_count: 3904585.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1258: ep_len:202 episode reward: total was 4.100000. running mean: -21.962004\n",
      "ep 1258: ep_len:560 episode reward: total was -32.100000. running mean: -22.063384\n",
      "ep 1258: ep_len:565 episode reward: total was -21.350000. running mean: -22.056250\n",
      "ep 1258: ep_len:148 episode reward: total was -9.450000. running mean: -21.930188\n",
      "ep 1258: ep_len:3 episode reward: total was 0.000000. running mean: -21.710886\n",
      "ep 1258: ep_len:500 episode reward: total was -13.270000. running mean: -21.626477\n",
      "ep 1258: ep_len:600 episode reward: total was -25.370000. running mean: -21.663912\n",
      "epsilon:0.228146 episode_count: 8813. steps_count: 3907163.000000\n",
      "ep 1259: ep_len:500 episode reward: total was -35.190000. running mean: -21.799173\n",
      "ep 1259: ep_len:655 episode reward: total was -46.180000. running mean: -22.042981\n",
      "ep 1259: ep_len:595 episode reward: total was -16.640000. running mean: -21.988951\n",
      "ep 1259: ep_len:419 episode reward: total was -20.230000. running mean: -21.971362\n",
      "ep 1259: ep_len:104 episode reward: total was -2.970000. running mean: -21.781348\n",
      "ep 1259: ep_len:525 episode reward: total was -24.670000. running mean: -21.810235\n",
      "ep 1259: ep_len:580 episode reward: total was -39.670000. running mean: -21.988832\n",
      "epsilon:0.228010 episode_count: 8820. steps_count: 3910541.000000\n",
      "ep 1260: ep_len:685 episode reward: total was -38.830000. running mean: -22.157244\n",
      "ep 1260: ep_len:510 episode reward: total was 1.790000. running mean: -21.917772\n",
      "ep 1260: ep_len:695 episode reward: total was -55.030000. running mean: -22.248894\n",
      "ep 1260: ep_len:525 episode reward: total was -27.490000. running mean: -22.301305\n",
      "ep 1260: ep_len:3 episode reward: total was 0.000000. running mean: -22.078292\n",
      "ep 1260: ep_len:595 episode reward: total was -33.230000. running mean: -22.189809\n",
      "ep 1260: ep_len:505 episode reward: total was -37.550000. running mean: -22.343411\n",
      "epsilon:0.227873 episode_count: 8827. steps_count: 3914059.000000\n",
      "ep 1261: ep_len:114 episode reward: total was 0.050000. running mean: -22.119477\n",
      "ep 1261: ep_len:505 episode reward: total was -24.790000. running mean: -22.146182\n",
      "ep 1261: ep_len:605 episode reward: total was -22.160000. running mean: -22.146320\n",
      "ep 1261: ep_len:500 episode reward: total was -28.190000. running mean: -22.206757\n",
      "ep 1261: ep_len:3 episode reward: total was 0.000000. running mean: -21.984690\n",
      "ep 1261: ep_len:510 episode reward: total was -41.150000. running mean: -22.176343\n",
      "ep 1261: ep_len:600 episode reward: total was -19.590000. running mean: -22.150479\n",
      "epsilon:0.227737 episode_count: 8834. steps_count: 3916896.000000\n",
      "ep 1262: ep_len:260 episode reward: total was -4.860000. running mean: -21.977574\n",
      "ep 1262: ep_len:565 episode reward: total was -45.210000. running mean: -22.209899\n",
      "ep 1262: ep_len:530 episode reward: total was -16.660000. running mean: -22.154400\n",
      "ep 1262: ep_len:500 episode reward: total was -36.210000. running mean: -22.294956\n",
      "ep 1262: ep_len:3 episode reward: total was 0.000000. running mean: -22.072006\n",
      "ep 1262: ep_len:550 episode reward: total was -28.160000. running mean: -22.132886\n",
      "ep 1262: ep_len:211 episode reward: total was -14.890000. running mean: -22.060457\n",
      "epsilon:0.227600 episode_count: 8841. steps_count: 3919515.000000\n",
      "ep 1263: ep_len:570 episode reward: total was -42.410000. running mean: -22.263953\n",
      "ep 1263: ep_len:565 episode reward: total was -25.670000. running mean: -22.298013\n",
      "ep 1263: ep_len:650 episode reward: total was -31.860000. running mean: -22.393633\n",
      "ep 1263: ep_len:56 episode reward: total was -2.950000. running mean: -22.199197\n",
      "ep 1263: ep_len:3 episode reward: total was 0.000000. running mean: -21.977205\n",
      "ep 1263: ep_len:530 episode reward: total was -39.580000. running mean: -22.153233\n",
      "ep 1263: ep_len:505 episode reward: total was -31.050000. running mean: -22.242200\n",
      "epsilon:0.227464 episode_count: 8848. steps_count: 3922394.000000\n",
      "ep 1264: ep_len:123 episode reward: total was -1.430000. running mean: -22.034078\n",
      "ep 1264: ep_len:650 episode reward: total was -36.150000. running mean: -22.175238\n",
      "ep 1264: ep_len:565 episode reward: total was -27.680000. running mean: -22.230285\n",
      "ep 1264: ep_len:510 episode reward: total was -45.170000. running mean: -22.459682\n",
      "ep 1264: ep_len:51 episode reward: total was 2.000000. running mean: -22.215085\n",
      "ep 1264: ep_len:510 episode reward: total was -10.020000. running mean: -22.093135\n",
      "ep 1264: ep_len:510 episode reward: total was -41.120000. running mean: -22.283403\n",
      "epsilon:0.227327 episode_count: 8855. steps_count: 3925313.000000\n",
      "ep 1265: ep_len:240 episode reward: total was -8.870000. running mean: -22.149269\n",
      "ep 1265: ep_len:630 episode reward: total was -35.690000. running mean: -22.284677\n",
      "ep 1265: ep_len:655 episode reward: total was -81.710000. running mean: -22.878930\n",
      "ep 1265: ep_len:500 episode reward: total was -39.230000. running mean: -23.042440\n",
      "ep 1265: ep_len:3 episode reward: total was 0.000000. running mean: -22.812016\n",
      "ep 1265: ep_len:500 episode reward: total was -26.280000. running mean: -22.846696\n",
      "ep 1265: ep_len:515 episode reward: total was -30.000000. running mean: -22.918229\n",
      "epsilon:0.227191 episode_count: 8862. steps_count: 3928356.000000\n",
      "ep 1266: ep_len:585 episode reward: total was -19.040000. running mean: -22.879447\n",
      "ep 1266: ep_len:505 episode reward: total was 8.280000. running mean: -22.567852\n",
      "ep 1266: ep_len:635 episode reward: total was -39.880000. running mean: -22.740974\n",
      "ep 1266: ep_len:500 episode reward: total was -29.130000. running mean: -22.804864\n",
      "ep 1266: ep_len:96 episode reward: total was -8.950000. running mean: -22.666315\n",
      "ep 1266: ep_len:615 episode reward: total was -41.890000. running mean: -22.858552\n",
      "ep 1266: ep_len:585 episode reward: total was -46.530000. running mean: -23.095267\n",
      "epsilon:0.227054 episode_count: 8869. steps_count: 3931877.000000\n",
      "ep 1267: ep_len:590 episode reward: total was -25.260000. running mean: -23.116914\n",
      "ep 1267: ep_len:540 episode reward: total was -54.740000. running mean: -23.433145\n",
      "ep 1267: ep_len:416 episode reward: total was -14.290000. running mean: -23.341713\n",
      "ep 1267: ep_len:397 episode reward: total was -17.240000. running mean: -23.280696\n",
      "ep 1267: ep_len:3 episode reward: total was 0.000000. running mean: -23.047889\n",
      "ep 1267: ep_len:715 episode reward: total was -58.480000. running mean: -23.402210\n",
      "ep 1267: ep_len:500 episode reward: total was -30.490000. running mean: -23.473088\n",
      "epsilon:0.226918 episode_count: 8876. steps_count: 3935038.000000\n",
      "ep 1268: ep_len:505 episode reward: total was -32.000000. running mean: -23.558357\n",
      "ep 1268: ep_len:500 episode reward: total was -20.160000. running mean: -23.524374\n",
      "ep 1268: ep_len:520 episode reward: total was -7.610000. running mean: -23.365230\n",
      "ep 1268: ep_len:500 episode reward: total was -25.550000. running mean: -23.387078\n",
      "ep 1268: ep_len:95 episode reward: total was 0.050000. running mean: -23.152707\n",
      "ep 1268: ep_len:303 episode reward: total was -18.880000. running mean: -23.109980\n",
      "ep 1268: ep_len:585 episode reward: total was -38.180000. running mean: -23.260680\n",
      "epsilon:0.226781 episode_count: 8883. steps_count: 3938046.000000\n",
      "ep 1269: ep_len:112 episode reward: total was -6.960000. running mean: -23.097673\n",
      "ep 1269: ep_len:570 episode reward: total was -27.580000. running mean: -23.142497\n",
      "ep 1269: ep_len:620 episode reward: total was -46.060000. running mean: -23.371672\n",
      "ep 1269: ep_len:645 episode reward: total was -74.700000. running mean: -23.884955\n",
      "ep 1269: ep_len:3 episode reward: total was 0.000000. running mean: -23.646105\n",
      "ep 1269: ep_len:186 episode reward: total was -4.420000. running mean: -23.453844\n",
      "ep 1269: ep_len:304 episode reward: total was -11.300000. running mean: -23.332306\n",
      "epsilon:0.226645 episode_count: 8890. steps_count: 3940486.000000\n",
      "ep 1270: ep_len:209 episode reward: total was -25.890000. running mean: -23.357883\n",
      "ep 1270: ep_len:500 episode reward: total was -30.060000. running mean: -23.424904\n",
      "ep 1270: ep_len:605 episode reward: total was -31.130000. running mean: -23.501955\n",
      "ep 1270: ep_len:610 episode reward: total was -43.110000. running mean: -23.698035\n",
      "ep 1270: ep_len:83 episode reward: total was -11.980000. running mean: -23.580855\n",
      "ep 1270: ep_len:137 episode reward: total was -1.430000. running mean: -23.359346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1270: ep_len:194 episode reward: total was -17.440000. running mean: -23.300153\n",
      "epsilon:0.226508 episode_count: 8897. steps_count: 3942824.000000\n",
      "ep 1271: ep_len:209 episode reward: total was -2.400000. running mean: -23.091151\n",
      "ep 1271: ep_len:500 episode reward: total was -14.050000. running mean: -23.000740\n",
      "ep 1271: ep_len:715 episode reward: total was -58.840000. running mean: -23.359133\n",
      "ep 1271: ep_len:500 episode reward: total was -25.640000. running mean: -23.381941\n",
      "ep 1271: ep_len:3 episode reward: total was 0.000000. running mean: -23.148122\n",
      "ep 1271: ep_len:645 episode reward: total was -54.550000. running mean: -23.462141\n",
      "ep 1271: ep_len:635 episode reward: total was -53.450000. running mean: -23.762019\n",
      "epsilon:0.226372 episode_count: 8904. steps_count: 3946031.000000\n",
      "ep 1272: ep_len:500 episode reward: total was -24.160000. running mean: -23.765999\n",
      "ep 1272: ep_len:625 episode reward: total was -39.010000. running mean: -23.918439\n",
      "ep 1272: ep_len:515 episode reward: total was -29.060000. running mean: -23.969855\n",
      "ep 1272: ep_len:140 episode reward: total was -8.880000. running mean: -23.818956\n",
      "ep 1272: ep_len:3 episode reward: total was 0.000000. running mean: -23.580767\n",
      "ep 1272: ep_len:565 episode reward: total was -19.640000. running mean: -23.541359\n",
      "ep 1272: ep_len:955 episode reward: total was -157.900000. running mean: -24.884945\n",
      "epsilon:0.226235 episode_count: 8911. steps_count: 3949334.000000\n",
      "ep 1273: ep_len:535 episode reward: total was -30.070000. running mean: -24.936796\n",
      "ep 1273: ep_len:510 episode reward: total was -25.270000. running mean: -24.940128\n",
      "ep 1273: ep_len:555 episode reward: total was -18.680000. running mean: -24.877527\n",
      "ep 1273: ep_len:368 episode reward: total was -31.220000. running mean: -24.940951\n",
      "ep 1273: ep_len:3 episode reward: total was 0.000000. running mean: -24.691542\n",
      "ep 1273: ep_len:530 episode reward: total was -28.150000. running mean: -24.726126\n",
      "ep 1273: ep_len:580 episode reward: total was -32.520000. running mean: -24.804065\n",
      "epsilon:0.226099 episode_count: 8918. steps_count: 3952415.000000\n",
      "ep 1274: ep_len:500 episode reward: total was -20.820000. running mean: -24.764224\n",
      "ep 1274: ep_len:270 episode reward: total was -15.350000. running mean: -24.670082\n",
      "ep 1274: ep_len:500 episode reward: total was -30.230000. running mean: -24.725681\n",
      "ep 1274: ep_len:505 episode reward: total was -13.080000. running mean: -24.609225\n",
      "ep 1274: ep_len:3 episode reward: total was 0.000000. running mean: -24.363132\n",
      "ep 1274: ep_len:860 episode reward: total was -79.480000. running mean: -24.914301\n",
      "ep 1274: ep_len:555 episode reward: total was -30.430000. running mean: -24.969458\n",
      "epsilon:0.225962 episode_count: 8925. steps_count: 3955608.000000\n",
      "ep 1275: ep_len:535 episode reward: total was -20.520000. running mean: -24.924963\n",
      "ep 1275: ep_len:500 episode reward: total was -31.120000. running mean: -24.986914\n",
      "ep 1275: ep_len:680 episode reward: total was -34.820000. running mean: -25.085245\n",
      "ep 1275: ep_len:585 episode reward: total was -1.050000. running mean: -24.844892\n",
      "ep 1275: ep_len:3 episode reward: total was 0.000000. running mean: -24.596443\n",
      "ep 1275: ep_len:570 episode reward: total was -17.040000. running mean: -24.520879\n",
      "ep 1275: ep_len:625 episode reward: total was -71.590000. running mean: -24.991570\n",
      "epsilon:0.225826 episode_count: 8932. steps_count: 3959106.000000\n",
      "ep 1276: ep_len:500 episode reward: total was -12.770000. running mean: -24.869354\n",
      "ep 1276: ep_len:181 episode reward: total was -10.420000. running mean: -24.724861\n",
      "ep 1276: ep_len:560 episode reward: total was -38.420000. running mean: -24.861812\n",
      "ep 1276: ep_len:530 episode reward: total was -26.160000. running mean: -24.874794\n",
      "ep 1276: ep_len:3 episode reward: total was 0.000000. running mean: -24.626046\n",
      "ep 1276: ep_len:640 episode reward: total was -35.630000. running mean: -24.736086\n",
      "ep 1276: ep_len:510 episode reward: total was -22.150000. running mean: -24.710225\n",
      "epsilon:0.225689 episode_count: 8939. steps_count: 3962030.000000\n",
      "ep 1277: ep_len:233 episode reward: total was -3.880000. running mean: -24.501923\n",
      "ep 1277: ep_len:500 episode reward: total was -39.750000. running mean: -24.654403\n",
      "ep 1277: ep_len:575 episode reward: total was -29.530000. running mean: -24.703159\n",
      "ep 1277: ep_len:515 episode reward: total was -37.170000. running mean: -24.827828\n",
      "ep 1277: ep_len:96 episode reward: total was -6.470000. running mean: -24.644249\n",
      "ep 1277: ep_len:520 episode reward: total was -44.670000. running mean: -24.844507\n",
      "ep 1277: ep_len:305 episode reward: total was -13.830000. running mean: -24.734362\n",
      "epsilon:0.225553 episode_count: 8946. steps_count: 3964774.000000\n",
      "ep 1278: ep_len:635 episode reward: total was -28.540000. running mean: -24.772418\n",
      "ep 1278: ep_len:590 episode reward: total was -23.080000. running mean: -24.755494\n",
      "ep 1278: ep_len:585 episode reward: total was -26.570000. running mean: -24.773639\n",
      "ep 1278: ep_len:500 episode reward: total was -14.650000. running mean: -24.672403\n",
      "ep 1278: ep_len:3 episode reward: total was 0.000000. running mean: -24.425679\n",
      "ep 1278: ep_len:500 episode reward: total was -28.610000. running mean: -24.467522\n",
      "ep 1278: ep_len:505 episode reward: total was -34.820000. running mean: -24.571047\n",
      "epsilon:0.225416 episode_count: 8953. steps_count: 3968092.000000\n",
      "ep 1279: ep_len:505 episode reward: total was -21.070000. running mean: -24.536036\n",
      "ep 1279: ep_len:515 episode reward: total was -32.070000. running mean: -24.611376\n",
      "ep 1279: ep_len:530 episode reward: total was -24.670000. running mean: -24.611962\n",
      "ep 1279: ep_len:510 episode reward: total was -36.100000. running mean: -24.726843\n",
      "ep 1279: ep_len:81 episode reward: total was -6.960000. running mean: -24.549174\n",
      "ep 1279: ep_len:540 episode reward: total was -17.480000. running mean: -24.478482\n",
      "ep 1279: ep_len:281 episode reward: total was -10.850000. running mean: -24.342198\n",
      "epsilon:0.225280 episode_count: 8960. steps_count: 3971054.000000\n",
      "ep 1280: ep_len:675 episode reward: total was -48.670000. running mean: -24.585476\n",
      "ep 1280: ep_len:515 episode reward: total was -44.340000. running mean: -24.783021\n",
      "ep 1280: ep_len:665 episode reward: total was -26.950000. running mean: -24.804691\n",
      "ep 1280: ep_len:500 episode reward: total was -11.270000. running mean: -24.669344\n",
      "ep 1280: ep_len:3 episode reward: total was 0.000000. running mean: -24.422650\n",
      "ep 1280: ep_len:515 episode reward: total was -29.710000. running mean: -24.475524\n",
      "ep 1280: ep_len:525 episode reward: total was -23.050000. running mean: -24.461269\n",
      "epsilon:0.225143 episode_count: 8967. steps_count: 3974452.000000\n",
      "ep 1281: ep_len:116 episode reward: total was 0.030000. running mean: -24.216356\n",
      "ep 1281: ep_len:500 episode reward: total was -38.890000. running mean: -24.363092\n",
      "ep 1281: ep_len:500 episode reward: total was -30.580000. running mean: -24.425261\n",
      "ep 1281: ep_len:520 episode reward: total was -25.110000. running mean: -24.432109\n",
      "ep 1281: ep_len:113 episode reward: total was -1.970000. running mean: -24.207488\n",
      "ep 1281: ep_len:600 episode reward: total was -24.200000. running mean: -24.207413\n",
      "ep 1281: ep_len:775 episode reward: total was -119.360000. running mean: -25.158939\n",
      "epsilon:0.225007 episode_count: 8974. steps_count: 3977576.000000\n",
      "ep 1282: ep_len:580 episode reward: total was -22.560000. running mean: -25.132949\n",
      "ep 1282: ep_len:505 episode reward: total was -21.890000. running mean: -25.100520\n",
      "ep 1282: ep_len:500 episode reward: total was -46.150000. running mean: -25.311015\n",
      "ep 1282: ep_len:805 episode reward: total was -60.080000. running mean: -25.658704\n",
      "ep 1282: ep_len:3 episode reward: total was 0.000000. running mean: -25.402117\n",
      "ep 1282: ep_len:500 episode reward: total was -46.260000. running mean: -25.610696\n",
      "ep 1282: ep_len:535 episode reward: total was -28.200000. running mean: -25.636589\n",
      "epsilon:0.224870 episode_count: 8981. steps_count: 3981004.000000\n",
      "ep 1283: ep_len:249 episode reward: total was 0.120000. running mean: -25.379023\n",
      "ep 1283: ep_len:625 episode reward: total was -11.460000. running mean: -25.239833\n",
      "ep 1283: ep_len:660 episode reward: total was -36.370000. running mean: -25.351135\n",
      "ep 1283: ep_len:500 episode reward: total was -22.520000. running mean: -25.322823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1283: ep_len:3 episode reward: total was 0.000000. running mean: -25.069595\n",
      "ep 1283: ep_len:505 episode reward: total was -44.610000. running mean: -25.264999\n",
      "ep 1283: ep_len:520 episode reward: total was -31.220000. running mean: -25.324549\n",
      "epsilon:0.224734 episode_count: 8988. steps_count: 3984066.000000\n",
      "ep 1284: ep_len:705 episode reward: total was -51.370000. running mean: -25.585004\n",
      "ep 1284: ep_len:520 episode reward: total was -32.010000. running mean: -25.649254\n",
      "ep 1284: ep_len:670 episode reward: total was -27.340000. running mean: -25.666161\n",
      "ep 1284: ep_len:505 episode reward: total was -34.760000. running mean: -25.757100\n",
      "ep 1284: ep_len:2 episode reward: total was 0.000000. running mean: -25.499529\n",
      "ep 1284: ep_len:530 episode reward: total was -28.640000. running mean: -25.530933\n",
      "ep 1284: ep_len:194 episode reward: total was -8.380000. running mean: -25.359424\n",
      "epsilon:0.224597 episode_count: 8995. steps_count: 3987192.000000\n",
      "ep 1285: ep_len:520 episode reward: total was -47.050000. running mean: -25.576330\n",
      "ep 1285: ep_len:500 episode reward: total was -17.370000. running mean: -25.494266\n",
      "ep 1285: ep_len:640 episode reward: total was -16.060000. running mean: -25.399924\n",
      "ep 1285: ep_len:505 episode reward: total was -16.680000. running mean: -25.312725\n",
      "ep 1285: ep_len:129 episode reward: total was -5.450000. running mean: -25.114097\n",
      "ep 1285: ep_len:302 episode reward: total was -14.870000. running mean: -25.011656\n",
      "ep 1285: ep_len:560 episode reward: total was -46.670000. running mean: -25.228240\n",
      "epsilon:0.224461 episode_count: 9002. steps_count: 3990348.000000\n",
      "ep 1286: ep_len:219 episode reward: total was -17.420000. running mean: -25.150157\n",
      "ep 1286: ep_len:179 episode reward: total was -20.470000. running mean: -25.103356\n",
      "ep 1286: ep_len:645 episode reward: total was -48.620000. running mean: -25.338522\n",
      "ep 1286: ep_len:600 episode reward: total was -23.640000. running mean: -25.321537\n",
      "ep 1286: ep_len:3 episode reward: total was 0.000000. running mean: -25.068322\n",
      "ep 1286: ep_len:500 episode reward: total was -31.340000. running mean: -25.131038\n",
      "ep 1286: ep_len:520 episode reward: total was -24.150000. running mean: -25.121228\n",
      "epsilon:0.224324 episode_count: 9009. steps_count: 3993014.000000\n",
      "ep 1287: ep_len:500 episode reward: total was -14.710000. running mean: -25.017116\n",
      "ep 1287: ep_len:530 episode reward: total was -25.990000. running mean: -25.026845\n",
      "ep 1287: ep_len:635 episode reward: total was -17.920000. running mean: -24.955776\n",
      "ep 1287: ep_len:590 episode reward: total was -106.910000. running mean: -25.775318\n",
      "ep 1287: ep_len:89 episode reward: total was -4.470000. running mean: -25.562265\n",
      "ep 1287: ep_len:535 episode reward: total was -33.680000. running mean: -25.643443\n",
      "ep 1287: ep_len:540 episode reward: total was -32.050000. running mean: -25.707508\n",
      "epsilon:0.224188 episode_count: 9016. steps_count: 3996433.000000\n",
      "ep 1288: ep_len:237 episode reward: total was -5.320000. running mean: -25.503633\n",
      "ep 1288: ep_len:630 episode reward: total was -35.700000. running mean: -25.605597\n",
      "ep 1288: ep_len:780 episode reward: total was -74.420000. running mean: -26.093741\n",
      "ep 1288: ep_len:570 episode reward: total was -42.650000. running mean: -26.259303\n",
      "ep 1288: ep_len:3 episode reward: total was 0.000000. running mean: -25.996710\n",
      "ep 1288: ep_len:535 episode reward: total was -27.330000. running mean: -26.010043\n",
      "ep 1288: ep_len:500 episode reward: total was -30.570000. running mean: -26.055643\n",
      "epsilon:0.224051 episode_count: 9023. steps_count: 3999688.000000\n",
      "ep 1289: ep_len:236 episode reward: total was -28.370000. running mean: -26.078786\n",
      "ep 1289: ep_len:500 episode reward: total was -13.390000. running mean: -25.951898\n",
      "ep 1289: ep_len:540 episode reward: total was -21.950000. running mean: -25.911879\n",
      "ep 1289: ep_len:500 episode reward: total was -6.740000. running mean: -25.720161\n",
      "ep 1289: ep_len:3 episode reward: total was 0.000000. running mean: -25.462959\n",
      "ep 1289: ep_len:510 episode reward: total was -19.820000. running mean: -25.406529\n",
      "ep 1289: ep_len:500 episode reward: total was -22.060000. running mean: -25.373064\n",
      "epsilon:0.223915 episode_count: 9030. steps_count: 4002477.000000\n",
      "ep 1290: ep_len:610 episode reward: total was -20.130000. running mean: -25.320634\n",
      "ep 1290: ep_len:500 episode reward: total was -22.010000. running mean: -25.287527\n",
      "ep 1290: ep_len:685 episode reward: total was -33.370000. running mean: -25.368352\n",
      "ep 1290: ep_len:605 episode reward: total was -12.640000. running mean: -25.241068\n",
      "ep 1290: ep_len:3 episode reward: total was 0.000000. running mean: -24.988658\n",
      "ep 1290: ep_len:585 episode reward: total was -15.080000. running mean: -24.889571\n",
      "ep 1290: ep_len:183 episode reward: total was -6.870000. running mean: -24.709375\n",
      "epsilon:0.223778 episode_count: 9037. steps_count: 4005648.000000\n",
      "ep 1291: ep_len:123 episode reward: total was -2.440000. running mean: -24.486682\n",
      "ep 1291: ep_len:625 episode reward: total was -9.190000. running mean: -24.333715\n",
      "ep 1291: ep_len:645 episode reward: total was -49.090000. running mean: -24.581278\n",
      "ep 1291: ep_len:670 episode reward: total was -62.060000. running mean: -24.956065\n",
      "ep 1291: ep_len:3 episode reward: total was 0.000000. running mean: -24.706504\n",
      "ep 1291: ep_len:530 episode reward: total was -22.050000. running mean: -24.679939\n",
      "ep 1291: ep_len:500 episode reward: total was -28.160000. running mean: -24.714740\n",
      "epsilon:0.223642 episode_count: 9044. steps_count: 4008744.000000\n",
      "ep 1292: ep_len:615 episode reward: total was -22.990000. running mean: -24.697492\n",
      "ep 1292: ep_len:505 episode reward: total was -40.090000. running mean: -24.851418\n",
      "ep 1292: ep_len:690 episode reward: total was -28.840000. running mean: -24.891303\n",
      "ep 1292: ep_len:368 episode reward: total was -21.760000. running mean: -24.859990\n",
      "ep 1292: ep_len:90 episode reward: total was -14.970000. running mean: -24.761090\n",
      "ep 1292: ep_len:314 episode reward: total was -37.360000. running mean: -24.887080\n",
      "ep 1292: ep_len:500 episode reward: total was -37.860000. running mean: -25.016809\n",
      "epsilon:0.223505 episode_count: 9051. steps_count: 4011826.000000\n",
      "ep 1293: ep_len:675 episode reward: total was -38.300000. running mean: -25.149641\n",
      "ep 1293: ep_len:252 episode reward: total was -25.440000. running mean: -25.152544\n",
      "ep 1293: ep_len:560 episode reward: total was -34.870000. running mean: -25.249719\n",
      "ep 1293: ep_len:132 episode reward: total was -9.910000. running mean: -25.096322\n",
      "ep 1293: ep_len:3 episode reward: total was 0.000000. running mean: -24.845358\n",
      "ep 1293: ep_len:660 episode reward: total was -18.730000. running mean: -24.784205\n",
      "ep 1293: ep_len:520 episode reward: total was -10.030000. running mean: -24.636663\n",
      "epsilon:0.223369 episode_count: 9058. steps_count: 4014628.000000\n",
      "ep 1294: ep_len:840 episode reward: total was -70.530000. running mean: -25.095596\n",
      "ep 1294: ep_len:540 episode reward: total was -50.770000. running mean: -25.352340\n",
      "ep 1294: ep_len:378 episode reward: total was -50.940000. running mean: -25.608217\n",
      "ep 1294: ep_len:418 episode reward: total was -20.240000. running mean: -25.554535\n",
      "ep 1294: ep_len:3 episode reward: total was 0.000000. running mean: -25.298989\n",
      "ep 1294: ep_len:585 episode reward: total was -23.630000. running mean: -25.282299\n",
      "ep 1294: ep_len:305 episode reward: total was -15.330000. running mean: -25.182776\n",
      "epsilon:0.223232 episode_count: 9065. steps_count: 4017697.000000\n",
      "ep 1295: ep_len:530 episode reward: total was -38.060000. running mean: -25.311549\n",
      "ep 1295: ep_len:570 episode reward: total was -21.000000. running mean: -25.268433\n",
      "ep 1295: ep_len:610 episode reward: total was -18.630000. running mean: -25.202049\n",
      "ep 1295: ep_len:500 episode reward: total was -17.590000. running mean: -25.125928\n",
      "ep 1295: ep_len:3 episode reward: total was 0.000000. running mean: -24.874669\n",
      "ep 1295: ep_len:530 episode reward: total was -33.230000. running mean: -24.958222\n",
      "ep 1295: ep_len:560 episode reward: total was -36.750000. running mean: -25.076140\n",
      "epsilon:0.223096 episode_count: 9072. steps_count: 4021000.000000\n",
      "ep 1296: ep_len:575 episode reward: total was -28.740000. running mean: -25.112779\n",
      "ep 1296: ep_len:650 episode reward: total was -66.670000. running mean: -25.528351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1296: ep_len:79 episode reward: total was -0.970000. running mean: -25.282767\n",
      "ep 1296: ep_len:377 episode reward: total was -15.260000. running mean: -25.182540\n",
      "ep 1296: ep_len:3 episode reward: total was 0.000000. running mean: -24.930714\n",
      "ep 1296: ep_len:555 episode reward: total was -26.140000. running mean: -24.942807\n",
      "ep 1296: ep_len:328 episode reward: total was -13.840000. running mean: -24.831779\n",
      "epsilon:0.222959 episode_count: 9079. steps_count: 4023567.000000\n",
      "ep 1297: ep_len:600 episode reward: total was -15.040000. running mean: -24.733861\n",
      "ep 1297: ep_len:500 episode reward: total was -28.790000. running mean: -24.774423\n",
      "ep 1297: ep_len:505 episode reward: total was -21.220000. running mean: -24.738878\n",
      "ep 1297: ep_len:540 episode reward: total was -26.020000. running mean: -24.751690\n",
      "ep 1297: ep_len:3 episode reward: total was 0.000000. running mean: -24.504173\n",
      "ep 1297: ep_len:510 episode reward: total was -17.460000. running mean: -24.433731\n",
      "ep 1297: ep_len:169 episode reward: total was -10.890000. running mean: -24.298294\n",
      "epsilon:0.222823 episode_count: 9086. steps_count: 4026394.000000\n",
      "ep 1298: ep_len:660 episode reward: total was -30.830000. running mean: -24.363611\n",
      "ep 1298: ep_len:500 episode reward: total was -31.340000. running mean: -24.433375\n",
      "ep 1298: ep_len:500 episode reward: total was -14.100000. running mean: -24.330041\n",
      "ep 1298: ep_len:500 episode reward: total was -10.140000. running mean: -24.188141\n",
      "ep 1298: ep_len:131 episode reward: total was -5.940000. running mean: -24.005659\n",
      "ep 1298: ep_len:525 episode reward: total was -17.660000. running mean: -23.942203\n",
      "ep 1298: ep_len:500 episode reward: total was -23.040000. running mean: -23.933181\n",
      "epsilon:0.222686 episode_count: 9093. steps_count: 4029710.000000\n",
      "ep 1299: ep_len:560 episode reward: total was -50.520000. running mean: -24.199049\n",
      "ep 1299: ep_len:500 episode reward: total was -49.860000. running mean: -24.455658\n",
      "ep 1299: ep_len:500 episode reward: total was -56.640000. running mean: -24.777502\n",
      "ep 1299: ep_len:505 episode reward: total was -11.470000. running mean: -24.644427\n",
      "ep 1299: ep_len:3 episode reward: total was 0.000000. running mean: -24.397982\n",
      "ep 1299: ep_len:186 episode reward: total was -0.410000. running mean: -24.158103\n",
      "ep 1299: ep_len:585 episode reward: total was -25.160000. running mean: -24.168122\n",
      "epsilon:0.222550 episode_count: 9100. steps_count: 4032549.000000\n",
      "ep 1300: ep_len:540 episode reward: total was -34.710000. running mean: -24.273540\n",
      "ep 1300: ep_len:189 episode reward: total was -13.930000. running mean: -24.170105\n",
      "ep 1300: ep_len:565 episode reward: total was -25.040000. running mean: -24.178804\n",
      "ep 1300: ep_len:500 episode reward: total was -43.270000. running mean: -24.369716\n",
      "ep 1300: ep_len:3 episode reward: total was 0.000000. running mean: -24.126019\n",
      "ep 1300: ep_len:505 episode reward: total was -21.020000. running mean: -24.094958\n",
      "ep 1300: ep_len:284 episode reward: total was -11.820000. running mean: -23.972209\n",
      "epsilon:0.222413 episode_count: 9107. steps_count: 4035135.000000\n",
      "ep 1301: ep_len:630 episode reward: total was -58.040000. running mean: -24.312887\n",
      "ep 1301: ep_len:595 episode reward: total was -13.820000. running mean: -24.207958\n",
      "ep 1301: ep_len:384 episode reward: total was -14.840000. running mean: -24.114278\n",
      "ep 1301: ep_len:525 episode reward: total was -30.080000. running mean: -24.173936\n",
      "ep 1301: ep_len:3 episode reward: total was 0.000000. running mean: -23.932196\n",
      "ep 1301: ep_len:545 episode reward: total was -26.910000. running mean: -23.961974\n",
      "ep 1301: ep_len:282 episode reward: total was -16.380000. running mean: -23.886155\n",
      "epsilon:0.222277 episode_count: 9114. steps_count: 4038099.000000\n",
      "ep 1302: ep_len:218 episode reward: total was -2.870000. running mean: -23.675993\n",
      "ep 1302: ep_len:515 episode reward: total was -18.890000. running mean: -23.628133\n",
      "ep 1302: ep_len:45 episode reward: total was -0.480000. running mean: -23.396652\n",
      "ep 1302: ep_len:500 episode reward: total was -3.180000. running mean: -23.194485\n",
      "ep 1302: ep_len:3 episode reward: total was 0.000000. running mean: -22.962540\n",
      "ep 1302: ep_len:505 episode reward: total was -28.490000. running mean: -23.017815\n",
      "ep 1302: ep_len:525 episode reward: total was -29.000000. running mean: -23.077637\n",
      "epsilon:0.222140 episode_count: 9121. steps_count: 4040410.000000\n",
      "ep 1303: ep_len:635 episode reward: total was -33.980000. running mean: -23.186660\n",
      "ep 1303: ep_len:540 episode reward: total was -24.600000. running mean: -23.200794\n",
      "ep 1303: ep_len:635 episode reward: total was -51.940000. running mean: -23.488186\n",
      "ep 1303: ep_len:510 episode reward: total was -24.530000. running mean: -23.498604\n",
      "ep 1303: ep_len:3 episode reward: total was 0.000000. running mean: -23.263618\n",
      "ep 1303: ep_len:625 episode reward: total was -23.480000. running mean: -23.265782\n",
      "ep 1303: ep_len:500 episode reward: total was -25.960000. running mean: -23.292724\n",
      "epsilon:0.222004 episode_count: 9128. steps_count: 4043858.000000\n",
      "ep 1304: ep_len:510 episode reward: total was -29.820000. running mean: -23.357997\n",
      "ep 1304: ep_len:550 episode reward: total was -0.110000. running mean: -23.125517\n",
      "ep 1304: ep_len:670 episode reward: total was -38.370000. running mean: -23.277962\n",
      "ep 1304: ep_len:535 episode reward: total was -22.100000. running mean: -23.266182\n",
      "ep 1304: ep_len:3 episode reward: total was 0.000000. running mean: -23.033520\n",
      "ep 1304: ep_len:600 episode reward: total was -25.110000. running mean: -23.054285\n",
      "ep 1304: ep_len:530 episode reward: total was -37.230000. running mean: -23.196042\n",
      "epsilon:0.221867 episode_count: 9135. steps_count: 4047256.000000\n",
      "ep 1305: ep_len:510 episode reward: total was -19.810000. running mean: -23.162182\n",
      "ep 1305: ep_len:515 episode reward: total was -18.500000. running mean: -23.115560\n",
      "ep 1305: ep_len:555 episode reward: total was -32.910000. running mean: -23.213504\n",
      "ep 1305: ep_len:585 episode reward: total was -44.710000. running mean: -23.428469\n",
      "ep 1305: ep_len:3 episode reward: total was 0.000000. running mean: -23.194185\n",
      "ep 1305: ep_len:500 episode reward: total was -23.270000. running mean: -23.194943\n",
      "ep 1305: ep_len:560 episode reward: total was -17.140000. running mean: -23.134393\n",
      "epsilon:0.221731 episode_count: 9142. steps_count: 4050484.000000\n",
      "ep 1306: ep_len:120 episode reward: total was -15.940000. running mean: -23.062449\n",
      "ep 1306: ep_len:500 episode reward: total was -33.760000. running mean: -23.169425\n",
      "ep 1306: ep_len:560 episode reward: total was -27.800000. running mean: -23.215731\n",
      "ep 1306: ep_len:530 episode reward: total was -26.090000. running mean: -23.244473\n",
      "ep 1306: ep_len:112 episode reward: total was -10.460000. running mean: -23.116629\n",
      "ep 1306: ep_len:500 episode reward: total was -19.290000. running mean: -23.078362\n",
      "ep 1306: ep_len:301 episode reward: total was -13.310000. running mean: -22.980679\n",
      "epsilon:0.221594 episode_count: 9149. steps_count: 4053107.000000\n",
      "ep 1307: ep_len:530 episode reward: total was -41.040000. running mean: -23.161272\n",
      "ep 1307: ep_len:610 episode reward: total was -26.160000. running mean: -23.191259\n",
      "ep 1307: ep_len:535 episode reward: total was -20.620000. running mean: -23.165547\n",
      "ep 1307: ep_len:570 episode reward: total was -17.060000. running mean: -23.104491\n",
      "ep 1307: ep_len:3 episode reward: total was 0.000000. running mean: -22.873446\n",
      "ep 1307: ep_len:520 episode reward: total was -29.600000. running mean: -22.940712\n",
      "ep 1307: ep_len:610 episode reward: total was -55.070000. running mean: -23.262005\n",
      "epsilon:0.221458 episode_count: 9156. steps_count: 4056485.000000\n",
      "ep 1308: ep_len:500 episode reward: total was -25.060000. running mean: -23.279985\n",
      "ep 1308: ep_len:500 episode reward: total was -17.050000. running mean: -23.217685\n",
      "ep 1308: ep_len:600 episode reward: total was -32.110000. running mean: -23.306608\n",
      "ep 1308: ep_len:575 episode reward: total was -22.640000. running mean: -23.299942\n",
      "ep 1308: ep_len:3 episode reward: total was 0.000000. running mean: -23.066942\n",
      "ep 1308: ep_len:525 episode reward: total was -27.030000. running mean: -23.106573\n",
      "ep 1308: ep_len:208 episode reward: total was -13.920000. running mean: -23.014707\n",
      "epsilon:0.221321 episode_count: 9163. steps_count: 4059396.000000\n",
      "ep 1309: ep_len:232 episode reward: total was -22.360000. running mean: -23.008160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1309: ep_len:505 episode reward: total was -19.320000. running mean: -22.971279\n",
      "ep 1309: ep_len:565 episode reward: total was -38.950000. running mean: -23.131066\n",
      "ep 1309: ep_len:640 episode reward: total was -31.030000. running mean: -23.210055\n",
      "ep 1309: ep_len:3 episode reward: total was 0.000000. running mean: -22.977955\n",
      "ep 1309: ep_len:545 episode reward: total was -45.690000. running mean: -23.205075\n",
      "ep 1309: ep_len:268 episode reward: total was -34.330000. running mean: -23.316324\n",
      "epsilon:0.221185 episode_count: 9170. steps_count: 4062154.000000\n",
      "ep 1310: ep_len:610 episode reward: total was -20.520000. running mean: -23.288361\n",
      "ep 1310: ep_len:600 episode reward: total was 1.380000. running mean: -23.041677\n",
      "ep 1310: ep_len:71 episode reward: total was -4.500000. running mean: -22.856261\n",
      "ep 1310: ep_len:51 episode reward: total was -4.980000. running mean: -22.677498\n",
      "ep 1310: ep_len:86 episode reward: total was -12.960000. running mean: -22.580323\n",
      "ep 1310: ep_len:560 episode reward: total was -33.100000. running mean: -22.685520\n",
      "ep 1310: ep_len:630 episode reward: total was -21.580000. running mean: -22.674465\n",
      "epsilon:0.221048 episode_count: 9177. steps_count: 4064762.000000\n",
      "ep 1311: ep_len:505 episode reward: total was -35.510000. running mean: -22.802820\n",
      "ep 1311: ep_len:705 episode reward: total was -54.940000. running mean: -23.124192\n",
      "ep 1311: ep_len:443 episode reward: total was -13.860000. running mean: -23.031550\n",
      "ep 1311: ep_len:510 episode reward: total was -34.090000. running mean: -23.142134\n",
      "ep 1311: ep_len:51 episode reward: total was 3.500000. running mean: -22.875713\n",
      "ep 1311: ep_len:570 episode reward: total was -26.690000. running mean: -22.913856\n",
      "ep 1311: ep_len:510 episode reward: total was -28.690000. running mean: -22.971617\n",
      "epsilon:0.220912 episode_count: 9184. steps_count: 4068056.000000\n",
      "ep 1312: ep_len:635 episode reward: total was -29.810000. running mean: -23.040001\n",
      "ep 1312: ep_len:515 episode reward: total was -15.870000. running mean: -22.968301\n",
      "ep 1312: ep_len:525 episode reward: total was -16.990000. running mean: -22.908518\n",
      "ep 1312: ep_len:480 episode reward: total was -29.510000. running mean: -22.974533\n",
      "ep 1312: ep_len:3 episode reward: total was 0.000000. running mean: -22.744788\n",
      "ep 1312: ep_len:335 episode reward: total was -19.360000. running mean: -22.710940\n",
      "ep 1312: ep_len:600 episode reward: total was -30.140000. running mean: -22.785230\n",
      "epsilon:0.220775 episode_count: 9191. steps_count: 4071149.000000\n",
      "ep 1313: ep_len:830 episode reward: total was -95.660000. running mean: -23.513978\n",
      "ep 1313: ep_len:595 episode reward: total was -57.540000. running mean: -23.854238\n",
      "ep 1313: ep_len:615 episode reward: total was -28.080000. running mean: -23.896496\n",
      "ep 1313: ep_len:56 episode reward: total was 0.550000. running mean: -23.652031\n",
      "ep 1313: ep_len:104 episode reward: total was 0.060000. running mean: -23.414911\n",
      "ep 1313: ep_len:545 episode reward: total was -28.210000. running mean: -23.462861\n",
      "ep 1313: ep_len:575 episode reward: total was -33.200000. running mean: -23.560233\n",
      "epsilon:0.220639 episode_count: 9198. steps_count: 4074469.000000\n",
      "ep 1314: ep_len:615 episode reward: total was -18.160000. running mean: -23.506231\n",
      "ep 1314: ep_len:595 episode reward: total was -23.950000. running mean: -23.510668\n",
      "ep 1314: ep_len:660 episode reward: total was -26.550000. running mean: -23.541062\n",
      "ep 1314: ep_len:560 episode reward: total was -19.220000. running mean: -23.497851\n",
      "ep 1314: ep_len:3 episode reward: total was 0.000000. running mean: -23.262872\n",
      "ep 1314: ep_len:515 episode reward: total was -31.030000. running mean: -23.340544\n",
      "ep 1314: ep_len:308 episode reward: total was -12.790000. running mean: -23.235038\n",
      "epsilon:0.220502 episode_count: 9205. steps_count: 4077725.000000\n",
      "ep 1315: ep_len:515 episode reward: total was -38.070000. running mean: -23.383388\n",
      "ep 1315: ep_len:620 episode reward: total was -40.650000. running mean: -23.556054\n",
      "ep 1315: ep_len:615 episode reward: total was -45.400000. running mean: -23.774493\n",
      "ep 1315: ep_len:575 episode reward: total was -14.160000. running mean: -23.678349\n",
      "ep 1315: ep_len:3 episode reward: total was 0.000000. running mean: -23.441565\n",
      "ep 1315: ep_len:620 episode reward: total was -31.650000. running mean: -23.523649\n",
      "ep 1315: ep_len:271 episode reward: total was -16.820000. running mean: -23.456613\n",
      "epsilon:0.220366 episode_count: 9212. steps_count: 4080944.000000\n",
      "ep 1316: ep_len:500 episode reward: total was -47.620000. running mean: -23.698247\n",
      "ep 1316: ep_len:500 episode reward: total was -36.290000. running mean: -23.824164\n",
      "ep 1316: ep_len:620 episode reward: total was -43.940000. running mean: -24.025323\n",
      "ep 1316: ep_len:605 episode reward: total was -15.460000. running mean: -23.939669\n",
      "ep 1316: ep_len:3 episode reward: total was 0.000000. running mean: -23.700273\n",
      "ep 1316: ep_len:620 episode reward: total was -33.710000. running mean: -23.800370\n",
      "ep 1316: ep_len:339 episode reward: total was -28.310000. running mean: -23.845466\n",
      "epsilon:0.220229 episode_count: 9219. steps_count: 4084131.000000\n",
      "ep 1317: ep_len:134 episode reward: total was -1.440000. running mean: -23.621412\n",
      "ep 1317: ep_len:363 episode reward: total was -16.780000. running mean: -23.552998\n",
      "ep 1317: ep_len:540 episode reward: total was -19.660000. running mean: -23.514068\n",
      "ep 1317: ep_len:585 episode reward: total was -48.200000. running mean: -23.760927\n",
      "ep 1317: ep_len:91 episode reward: total was -13.960000. running mean: -23.662918\n",
      "ep 1317: ep_len:595 episode reward: total was -11.420000. running mean: -23.540488\n",
      "ep 1317: ep_len:575 episode reward: total was -51.130000. running mean: -23.816384\n",
      "epsilon:0.220093 episode_count: 9226. steps_count: 4087014.000000\n",
      "ep 1318: ep_len:580 episode reward: total was -18.010000. running mean: -23.758320\n",
      "ep 1318: ep_len:530 episode reward: total was -55.100000. running mean: -24.071737\n",
      "ep 1318: ep_len:515 episode reward: total was -46.010000. running mean: -24.291119\n",
      "ep 1318: ep_len:132 episode reward: total was -5.410000. running mean: -24.102308\n",
      "ep 1318: ep_len:99 episode reward: total was 3.050000. running mean: -23.830785\n",
      "ep 1318: ep_len:505 episode reward: total was -20.780000. running mean: -23.800277\n",
      "ep 1318: ep_len:510 episode reward: total was -17.610000. running mean: -23.738374\n",
      "epsilon:0.219956 episode_count: 9233. steps_count: 4089885.000000\n",
      "ep 1319: ep_len:615 episode reward: total was -46.500000. running mean: -23.965991\n",
      "ep 1319: ep_len:278 episode reward: total was -18.410000. running mean: -23.910431\n",
      "ep 1319: ep_len:372 episode reward: total was -20.900000. running mean: -23.880326\n",
      "ep 1319: ep_len:620 episode reward: total was -28.130000. running mean: -23.922823\n",
      "ep 1319: ep_len:3 episode reward: total was 0.000000. running mean: -23.683595\n",
      "ep 1319: ep_len:237 episode reward: total was -1.860000. running mean: -23.465359\n",
      "ep 1319: ep_len:525 episode reward: total was -39.210000. running mean: -23.622805\n",
      "epsilon:0.219820 episode_count: 9240. steps_count: 4092535.000000\n",
      "ep 1320: ep_len:117 episode reward: total was -2.970000. running mean: -23.416277\n",
      "ep 1320: ep_len:585 episode reward: total was -27.770000. running mean: -23.459814\n",
      "ep 1320: ep_len:665 episode reward: total was -18.200000. running mean: -23.407216\n",
      "ep 1320: ep_len:525 episode reward: total was -15.070000. running mean: -23.323844\n",
      "ep 1320: ep_len:3 episode reward: total was 0.000000. running mean: -23.090606\n",
      "ep 1320: ep_len:600 episode reward: total was -37.650000. running mean: -23.236200\n",
      "ep 1320: ep_len:510 episode reward: total was -32.880000. running mean: -23.332638\n",
      "epsilon:0.219683 episode_count: 9247. steps_count: 4095540.000000\n",
      "ep 1321: ep_len:685 episode reward: total was -21.750000. running mean: -23.316811\n",
      "ep 1321: ep_len:345 episode reward: total was -33.360000. running mean: -23.417243\n",
      "ep 1321: ep_len:585 episode reward: total was -41.570000. running mean: -23.598771\n",
      "ep 1321: ep_len:510 episode reward: total was -16.670000. running mean: -23.529483\n",
      "ep 1321: ep_len:42 episode reward: total was -5.000000. running mean: -23.344188\n",
      "ep 1321: ep_len:500 episode reward: total was -19.170000. running mean: -23.302446\n",
      "ep 1321: ep_len:300 episode reward: total was -13.290000. running mean: -23.202322\n",
      "epsilon:0.219547 episode_count: 9254. steps_count: 4098507.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1322: ep_len:525 episode reward: total was -30.190000. running mean: -23.272199\n",
      "ep 1322: ep_len:500 episode reward: total was -25.200000. running mean: -23.291477\n",
      "ep 1322: ep_len:443 episode reward: total was -30.820000. running mean: -23.366762\n",
      "ep 1322: ep_len:550 episode reward: total was -30.230000. running mean: -23.435394\n",
      "ep 1322: ep_len:3 episode reward: total was 0.000000. running mean: -23.201040\n",
      "ep 1322: ep_len:605 episode reward: total was -8.380000. running mean: -23.052830\n",
      "ep 1322: ep_len:199 episode reward: total was -10.890000. running mean: -22.931202\n",
      "epsilon:0.219410 episode_count: 9261. steps_count: 4101332.000000\n",
      "ep 1323: ep_len:590 episode reward: total was -34.610000. running mean: -23.047990\n",
      "ep 1323: ep_len:600 episode reward: total was -50.750000. running mean: -23.325010\n",
      "ep 1323: ep_len:675 episode reward: total was -11.760000. running mean: -23.209360\n",
      "ep 1323: ep_len:383 episode reward: total was -23.740000. running mean: -23.214666\n",
      "ep 1323: ep_len:101 episode reward: total was -12.960000. running mean: -23.112119\n",
      "ep 1323: ep_len:500 episode reward: total was -38.590000. running mean: -23.266898\n",
      "ep 1323: ep_len:595 episode reward: total was -32.050000. running mean: -23.354729\n",
      "epsilon:0.219274 episode_count: 9268. steps_count: 4104776.000000\n",
      "ep 1324: ep_len:640 episode reward: total was -31.530000. running mean: -23.436482\n",
      "ep 1324: ep_len:610 episode reward: total was -29.000000. running mean: -23.492117\n",
      "ep 1324: ep_len:595 episode reward: total was -17.090000. running mean: -23.428096\n",
      "ep 1324: ep_len:500 episode reward: total was -6.740000. running mean: -23.261215\n",
      "ep 1324: ep_len:3 episode reward: total was 0.000000. running mean: -23.028603\n",
      "ep 1324: ep_len:565 episode reward: total was -14.120000. running mean: -22.939517\n",
      "ep 1324: ep_len:625 episode reward: total was -32.400000. running mean: -23.034122\n",
      "epsilon:0.219137 episode_count: 9275. steps_count: 4108314.000000\n",
      "ep 1325: ep_len:515 episode reward: total was -38.540000. running mean: -23.189180\n",
      "ep 1325: ep_len:500 episode reward: total was -15.230000. running mean: -23.109589\n",
      "ep 1325: ep_len:364 episode reward: total was -9.850000. running mean: -22.976993\n",
      "ep 1325: ep_len:500 episode reward: total was -35.130000. running mean: -23.098523\n",
      "ep 1325: ep_len:3 episode reward: total was 0.000000. running mean: -22.867538\n",
      "ep 1325: ep_len:500 episode reward: total was -20.170000. running mean: -22.840562\n",
      "ep 1325: ep_len:333 episode reward: total was -20.350000. running mean: -22.815657\n",
      "epsilon:0.219001 episode_count: 9282. steps_count: 4111029.000000\n",
      "ep 1326: ep_len:650 episode reward: total was -54.970000. running mean: -23.137200\n",
      "ep 1326: ep_len:535 episode reward: total was -18.330000. running mean: -23.089128\n",
      "ep 1326: ep_len:565 episode reward: total was -24.890000. running mean: -23.107137\n",
      "ep 1326: ep_len:575 episode reward: total was -42.630000. running mean: -23.302365\n",
      "ep 1326: ep_len:95 episode reward: total was -13.460000. running mean: -23.203942\n",
      "ep 1326: ep_len:505 episode reward: total was -31.540000. running mean: -23.287302\n",
      "ep 1326: ep_len:297 episode reward: total was -27.360000. running mean: -23.328029\n",
      "epsilon:0.218864 episode_count: 9289. steps_count: 4114251.000000\n",
      "ep 1327: ep_len:560 episode reward: total was -36.920000. running mean: -23.463949\n",
      "ep 1327: ep_len:195 episode reward: total was -10.390000. running mean: -23.333209\n",
      "ep 1327: ep_len:585 episode reward: total was -30.580000. running mean: -23.405677\n",
      "ep 1327: ep_len:505 episode reward: total was -18.060000. running mean: -23.352221\n",
      "ep 1327: ep_len:78 episode reward: total was -1.950000. running mean: -23.138198\n",
      "ep 1327: ep_len:620 episode reward: total was -23.570000. running mean: -23.142516\n",
      "ep 1327: ep_len:605 episode reward: total was -36.170000. running mean: -23.272791\n",
      "epsilon:0.218728 episode_count: 9296. steps_count: 4117399.000000\n",
      "ep 1328: ep_len:615 episode reward: total was -28.810000. running mean: -23.328163\n",
      "ep 1328: ep_len:670 episode reward: total was -58.240000. running mean: -23.677282\n",
      "ep 1328: ep_len:500 episode reward: total was -17.920000. running mean: -23.619709\n",
      "ep 1328: ep_len:549 episode reward: total was -28.440000. running mean: -23.667912\n",
      "ep 1328: ep_len:3 episode reward: total was 0.000000. running mean: -23.431233\n",
      "ep 1328: ep_len:312 episode reward: total was -15.870000. running mean: -23.355620\n",
      "ep 1328: ep_len:695 episode reward: total was -65.550000. running mean: -23.777564\n",
      "epsilon:0.218591 episode_count: 9303. steps_count: 4120743.000000\n",
      "ep 1329: ep_len:196 episode reward: total was -13.880000. running mean: -23.678588\n",
      "ep 1329: ep_len:500 episode reward: total was -12.650000. running mean: -23.568303\n",
      "ep 1329: ep_len:555 episode reward: total was -40.970000. running mean: -23.742320\n",
      "ep 1329: ep_len:370 episode reward: total was -11.760000. running mean: -23.622496\n",
      "ep 1329: ep_len:93 episode reward: total was -9.460000. running mean: -23.480871\n",
      "ep 1329: ep_len:500 episode reward: total was -27.110000. running mean: -23.517163\n",
      "ep 1329: ep_len:620 episode reward: total was -30.910000. running mean: -23.591091\n",
      "epsilon:0.218455 episode_count: 9310. steps_count: 4123577.000000\n",
      "ep 1330: ep_len:635 episode reward: total was -29.630000. running mean: -23.651480\n",
      "ep 1330: ep_len:500 episode reward: total was -19.880000. running mean: -23.613765\n",
      "ep 1330: ep_len:630 episode reward: total was -26.000000. running mean: -23.637628\n",
      "ep 1330: ep_len:535 episode reward: total was -9.520000. running mean: -23.496451\n",
      "ep 1330: ep_len:48 episode reward: total was 4.500000. running mean: -23.216487\n",
      "ep 1330: ep_len:500 episode reward: total was -20.830000. running mean: -23.192622\n",
      "ep 1330: ep_len:260 episode reward: total was -11.820000. running mean: -23.078896\n",
      "epsilon:0.218318 episode_count: 9317. steps_count: 4126685.000000\n",
      "ep 1331: ep_len:570 episode reward: total was -64.770000. running mean: -23.495807\n",
      "ep 1331: ep_len:590 episode reward: total was -42.590000. running mean: -23.686749\n",
      "ep 1331: ep_len:376 episode reward: total was -12.330000. running mean: -23.573181\n",
      "ep 1331: ep_len:408 episode reward: total was -13.700000. running mean: -23.474450\n",
      "ep 1331: ep_len:41 episode reward: total was -0.500000. running mean: -23.244705\n",
      "ep 1331: ep_len:610 episode reward: total was -14.530000. running mean: -23.157558\n",
      "ep 1331: ep_len:545 episode reward: total was -42.020000. running mean: -23.346182\n",
      "epsilon:0.218182 episode_count: 9324. steps_count: 4129825.000000\n",
      "ep 1332: ep_len:126 episode reward: total was -3.440000. running mean: -23.147121\n",
      "ep 1332: ep_len:500 episode reward: total was -28.750000. running mean: -23.203149\n",
      "ep 1332: ep_len:500 episode reward: total was -50.240000. running mean: -23.473518\n",
      "ep 1332: ep_len:500 episode reward: total was -28.580000. running mean: -23.524583\n",
      "ep 1332: ep_len:99 episode reward: total was 2.530000. running mean: -23.264037\n",
      "ep 1332: ep_len:605 episode reward: total was -43.200000. running mean: -23.463396\n",
      "ep 1332: ep_len:190 episode reward: total was -13.390000. running mean: -23.362663\n",
      "epsilon:0.218045 episode_count: 9331. steps_count: 4132345.000000\n",
      "ep 1333: ep_len:570 episode reward: total was -21.190000. running mean: -23.340936\n",
      "ep 1333: ep_len:500 episode reward: total was -17.860000. running mean: -23.286127\n",
      "ep 1333: ep_len:555 episode reward: total was -53.560000. running mean: -23.588865\n",
      "ep 1333: ep_len:129 episode reward: total was 2.080000. running mean: -23.332177\n",
      "ep 1333: ep_len:50 episode reward: total was 2.000000. running mean: -23.078855\n",
      "ep 1333: ep_len:500 episode reward: total was -28.720000. running mean: -23.135266\n",
      "ep 1333: ep_len:520 episode reward: total was -33.150000. running mean: -23.235414\n",
      "epsilon:0.217909 episode_count: 9338. steps_count: 4135169.000000\n",
      "ep 1334: ep_len:252 episode reward: total was -14.420000. running mean: -23.147260\n",
      "ep 1334: ep_len:540 episode reward: total was -14.050000. running mean: -23.056287\n",
      "ep 1334: ep_len:580 episode reward: total was -51.510000. running mean: -23.340824\n",
      "ep 1334: ep_len:500 episode reward: total was -17.010000. running mean: -23.277516\n",
      "ep 1334: ep_len:115 episode reward: total was -8.940000. running mean: -23.134141\n",
      "ep 1334: ep_len:550 episode reward: total was -27.880000. running mean: -23.181599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1334: ep_len:595 episode reward: total was -42.060000. running mean: -23.370383\n",
      "epsilon:0.217772 episode_count: 9345. steps_count: 4138301.000000\n",
      "ep 1335: ep_len:585 episode reward: total was -25.040000. running mean: -23.387079\n",
      "ep 1335: ep_len:520 episode reward: total was -27.530000. running mean: -23.428509\n",
      "ep 1335: ep_len:625 episode reward: total was -27.610000. running mean: -23.470324\n",
      "ep 1335: ep_len:500 episode reward: total was -16.540000. running mean: -23.401020\n",
      "ep 1335: ep_len:3 episode reward: total was 0.000000. running mean: -23.167010\n",
      "ep 1335: ep_len:500 episode reward: total was -30.550000. running mean: -23.240840\n",
      "ep 1335: ep_len:525 episode reward: total was -27.200000. running mean: -23.280432\n",
      "epsilon:0.217636 episode_count: 9352. steps_count: 4141559.000000\n",
      "ep 1336: ep_len:600 episode reward: total was -9.600000. running mean: -23.143627\n",
      "ep 1336: ep_len:555 episode reward: total was -3.740000. running mean: -22.949591\n",
      "ep 1336: ep_len:580 episode reward: total was -41.960000. running mean: -23.139695\n",
      "ep 1336: ep_len:56 episode reward: total was -5.950000. running mean: -22.967798\n",
      "ep 1336: ep_len:3 episode reward: total was 0.000000. running mean: -22.738120\n",
      "ep 1336: ep_len:565 episode reward: total was -13.610000. running mean: -22.646839\n",
      "ep 1336: ep_len:500 episode reward: total was -25.180000. running mean: -22.672171\n",
      "epsilon:0.217499 episode_count: 9359. steps_count: 4144418.000000\n",
      "ep 1337: ep_len:570 episode reward: total was -20.180000. running mean: -22.647249\n",
      "ep 1337: ep_len:550 episode reward: total was -19.950000. running mean: -22.620276\n",
      "ep 1337: ep_len:545 episode reward: total was -36.020000. running mean: -22.754274\n",
      "ep 1337: ep_len:585 episode reward: total was -8.730000. running mean: -22.614031\n",
      "ep 1337: ep_len:73 episode reward: total was -0.980000. running mean: -22.397691\n",
      "ep 1337: ep_len:580 episode reward: total was -22.500000. running mean: -22.398714\n",
      "ep 1337: ep_len:595 episode reward: total was -23.130000. running mean: -22.406027\n",
      "epsilon:0.217363 episode_count: 9366. steps_count: 4147916.000000\n",
      "ep 1338: ep_len:500 episode reward: total was -50.310000. running mean: -22.685066\n",
      "ep 1338: ep_len:590 episode reward: total was -5.200000. running mean: -22.510216\n",
      "ep 1338: ep_len:540 episode reward: total was -32.620000. running mean: -22.611313\n",
      "ep 1338: ep_len:595 episode reward: total was -31.100000. running mean: -22.696200\n",
      "ep 1338: ep_len:106 episode reward: total was 2.030000. running mean: -22.448938\n",
      "ep 1338: ep_len:620 episode reward: total was -16.600000. running mean: -22.390449\n",
      "ep 1338: ep_len:500 episode reward: total was -25.090000. running mean: -22.417444\n",
      "epsilon:0.217226 episode_count: 9373. steps_count: 4151367.000000\n",
      "ep 1339: ep_len:600 episode reward: total was -36.860000. running mean: -22.561870\n",
      "ep 1339: ep_len:600 episode reward: total was -25.950000. running mean: -22.595751\n",
      "ep 1339: ep_len:355 episode reward: total was -27.380000. running mean: -22.643594\n",
      "ep 1339: ep_len:515 episode reward: total was -13.690000. running mean: -22.554058\n",
      "ep 1339: ep_len:3 episode reward: total was 0.000000. running mean: -22.328517\n",
      "ep 1339: ep_len:500 episode reward: total was -27.830000. running mean: -22.383532\n",
      "ep 1339: ep_len:500 episode reward: total was -31.270000. running mean: -22.472397\n",
      "epsilon:0.217090 episode_count: 9380. steps_count: 4154440.000000\n",
      "ep 1340: ep_len:500 episode reward: total was -31.720000. running mean: -22.564873\n",
      "ep 1340: ep_len:359 episode reward: total was -35.880000. running mean: -22.698024\n",
      "ep 1340: ep_len:575 episode reward: total was -40.440000. running mean: -22.875444\n",
      "ep 1340: ep_len:56 episode reward: total was -2.950000. running mean: -22.676189\n",
      "ep 1340: ep_len:89 episode reward: total was 3.550000. running mean: -22.413927\n",
      "ep 1340: ep_len:505 episode reward: total was -15.670000. running mean: -22.346488\n",
      "ep 1340: ep_len:525 episode reward: total was -30.080000. running mean: -22.423823\n",
      "epsilon:0.216953 episode_count: 9387. steps_count: 4157049.000000\n",
      "ep 1341: ep_len:610 episode reward: total was -47.950000. running mean: -22.679085\n",
      "ep 1341: ep_len:525 episode reward: total was -32.050000. running mean: -22.772794\n",
      "ep 1341: ep_len:620 episode reward: total was -13.090000. running mean: -22.675966\n",
      "ep 1341: ep_len:56 episode reward: total was -1.960000. running mean: -22.468807\n",
      "ep 1341: ep_len:3 episode reward: total was 0.000000. running mean: -22.244119\n",
      "ep 1341: ep_len:510 episode reward: total was -42.290000. running mean: -22.444577\n",
      "ep 1341: ep_len:323 episode reward: total was -14.370000. running mean: -22.363832\n",
      "epsilon:0.216817 episode_count: 9394. steps_count: 4159696.000000\n",
      "ep 1342: ep_len:585 episode reward: total was -48.690000. running mean: -22.627093\n",
      "ep 1342: ep_len:515 episode reward: total was -31.470000. running mean: -22.715522\n",
      "ep 1342: ep_len:66 episode reward: total was -0.960000. running mean: -22.497967\n",
      "ep 1342: ep_len:530 episode reward: total was -12.640000. running mean: -22.399387\n",
      "ep 1342: ep_len:3 episode reward: total was 0.000000. running mean: -22.175394\n",
      "ep 1342: ep_len:323 episode reward: total was -5.350000. running mean: -22.007140\n",
      "ep 1342: ep_len:515 episode reward: total was -23.150000. running mean: -22.018568\n",
      "epsilon:0.216680 episode_count: 9401. steps_count: 4162233.000000\n",
      "ep 1343: ep_len:242 episode reward: total was -32.410000. running mean: -22.122483\n",
      "ep 1343: ep_len:1110 episode reward: total was -148.320000. running mean: -23.384458\n",
      "ep 1343: ep_len:443 episode reward: total was -3.270000. running mean: -23.183313\n",
      "ep 1343: ep_len:580 episode reward: total was -19.600000. running mean: -23.147480\n",
      "ep 1343: ep_len:3 episode reward: total was 0.000000. running mean: -22.916005\n",
      "ep 1343: ep_len:685 episode reward: total was -77.550000. running mean: -23.462345\n",
      "ep 1343: ep_len:580 episode reward: total was -36.990000. running mean: -23.597622\n",
      "epsilon:0.216544 episode_count: 9408. steps_count: 4165876.000000\n",
      "ep 1344: ep_len:560 episode reward: total was -24.020000. running mean: -23.601846\n",
      "ep 1344: ep_len:510 episode reward: total was -32.220000. running mean: -23.688027\n",
      "ep 1344: ep_len:439 episode reward: total was -20.730000. running mean: -23.658447\n",
      "ep 1344: ep_len:565 episode reward: total was -81.790000. running mean: -24.239762\n",
      "ep 1344: ep_len:3 episode reward: total was 0.000000. running mean: -23.997365\n",
      "ep 1344: ep_len:670 episode reward: total was -37.970000. running mean: -24.137091\n",
      "ep 1344: ep_len:550 episode reward: total was -48.130000. running mean: -24.377020\n",
      "epsilon:0.216407 episode_count: 9415. steps_count: 4169173.000000\n",
      "ep 1345: ep_len:675 episode reward: total was -19.740000. running mean: -24.330650\n",
      "ep 1345: ep_len:280 episode reward: total was -10.880000. running mean: -24.196143\n",
      "ep 1345: ep_len:525 episode reward: total was -26.910000. running mean: -24.223282\n",
      "ep 1345: ep_len:510 episode reward: total was -36.640000. running mean: -24.347449\n",
      "ep 1345: ep_len:99 episode reward: total was 3.540000. running mean: -24.068575\n",
      "ep 1345: ep_len:645 episode reward: total was -38.860000. running mean: -24.216489\n",
      "ep 1345: ep_len:555 episode reward: total was -28.580000. running mean: -24.260124\n",
      "epsilon:0.216271 episode_count: 9422. steps_count: 4172462.000000\n",
      "ep 1346: ep_len:229 episode reward: total was -24.850000. running mean: -24.266023\n",
      "ep 1346: ep_len:500 episode reward: total was -19.610000. running mean: -24.219463\n",
      "ep 1346: ep_len:455 episode reward: total was -20.310000. running mean: -24.180368\n",
      "ep 1346: ep_len:615 episode reward: total was -5.120000. running mean: -23.989764\n",
      "ep 1346: ep_len:3 episode reward: total was 0.000000. running mean: -23.749867\n",
      "ep 1346: ep_len:500 episode reward: total was -25.530000. running mean: -23.767668\n",
      "ep 1346: ep_len:540 episode reward: total was -37.180000. running mean: -23.901791\n",
      "epsilon:0.216134 episode_count: 9429. steps_count: 4175304.000000\n",
      "ep 1347: ep_len:505 episode reward: total was -77.320000. running mean: -24.435973\n",
      "ep 1347: ep_len:510 episode reward: total was -26.090000. running mean: -24.452514\n",
      "ep 1347: ep_len:715 episode reward: total was -44.820000. running mean: -24.656189\n",
      "ep 1347: ep_len:550 episode reward: total was -10.110000. running mean: -24.510727\n",
      "ep 1347: ep_len:48 episode reward: total was 1.500000. running mean: -24.250619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1347: ep_len:610 episode reward: total was -24.350000. running mean: -24.251613\n",
      "ep 1347: ep_len:333 episode reward: total was -16.340000. running mean: -24.172497\n",
      "epsilon:0.215998 episode_count: 9436. steps_count: 4178575.000000\n",
      "ep 1348: ep_len:595 episode reward: total was -6.610000. running mean: -23.996872\n",
      "ep 1348: ep_len:510 episode reward: total was -42.350000. running mean: -24.180403\n",
      "ep 1348: ep_len:755 episode reward: total was -41.810000. running mean: -24.356699\n",
      "ep 1348: ep_len:570 episode reward: total was -32.230000. running mean: -24.435432\n",
      "ep 1348: ep_len:3 episode reward: total was 0.000000. running mean: -24.191078\n",
      "ep 1348: ep_len:580 episode reward: total was -20.540000. running mean: -24.154567\n",
      "ep 1348: ep_len:605 episode reward: total was -31.670000. running mean: -24.229722\n",
      "epsilon:0.215861 episode_count: 9443. steps_count: 4182193.000000\n",
      "ep 1349: ep_len:625 episode reward: total was -25.030000. running mean: -24.237724\n",
      "ep 1349: ep_len:500 episode reward: total was -17.250000. running mean: -24.167847\n",
      "ep 1349: ep_len:610 episode reward: total was -22.280000. running mean: -24.148969\n",
      "ep 1349: ep_len:500 episode reward: total was -4.220000. running mean: -23.949679\n",
      "ep 1349: ep_len:3 episode reward: total was 0.000000. running mean: -23.710182\n",
      "ep 1349: ep_len:303 episode reward: total was -12.850000. running mean: -23.601580\n",
      "ep 1349: ep_len:605 episode reward: total was -24.690000. running mean: -23.612465\n",
      "epsilon:0.215725 episode_count: 9450. steps_count: 4185339.000000\n",
      "ep 1350: ep_len:700 episode reward: total was -55.870000. running mean: -23.935040\n",
      "ep 1350: ep_len:515 episode reward: total was -33.580000. running mean: -24.031489\n",
      "ep 1350: ep_len:595 episode reward: total was -22.250000. running mean: -24.013675\n",
      "ep 1350: ep_len:387 episode reward: total was -29.690000. running mean: -24.070438\n",
      "ep 1350: ep_len:3 episode reward: total was 0.000000. running mean: -23.829733\n",
      "ep 1350: ep_len:520 episode reward: total was -16.050000. running mean: -23.751936\n",
      "ep 1350: ep_len:615 episode reward: total was -25.900000. running mean: -23.773417\n",
      "epsilon:0.215588 episode_count: 9457. steps_count: 4188674.000000\n",
      "ep 1351: ep_len:226 episode reward: total was -10.930000. running mean: -23.644983\n",
      "ep 1351: ep_len:500 episode reward: total was -34.880000. running mean: -23.757333\n",
      "ep 1351: ep_len:610 episode reward: total was -38.640000. running mean: -23.906159\n",
      "ep 1351: ep_len:510 episode reward: total was -27.180000. running mean: -23.938898\n",
      "ep 1351: ep_len:3 episode reward: total was 0.000000. running mean: -23.699509\n",
      "ep 1351: ep_len:500 episode reward: total was -24.320000. running mean: -23.705714\n",
      "ep 1351: ep_len:545 episode reward: total was -22.460000. running mean: -23.693257\n",
      "epsilon:0.215452 episode_count: 9464. steps_count: 4191568.000000\n",
      "ep 1352: ep_len:500 episode reward: total was -42.580000. running mean: -23.882124\n",
      "ep 1352: ep_len:540 episode reward: total was -33.110000. running mean: -23.974403\n",
      "ep 1352: ep_len:620 episode reward: total was -27.360000. running mean: -24.008259\n",
      "ep 1352: ep_len:565 episode reward: total was -15.140000. running mean: -23.919576\n",
      "ep 1352: ep_len:3 episode reward: total was 0.000000. running mean: -23.680380\n",
      "ep 1352: ep_len:234 episode reward: total was -12.430000. running mean: -23.567877\n",
      "ep 1352: ep_len:500 episode reward: total was -36.750000. running mean: -23.699698\n",
      "epsilon:0.215315 episode_count: 9471. steps_count: 4194530.000000\n",
      "ep 1353: ep_len:660 episode reward: total was -41.450000. running mean: -23.877201\n",
      "ep 1353: ep_len:580 episode reward: total was -15.070000. running mean: -23.789129\n",
      "ep 1353: ep_len:655 episode reward: total was -59.140000. running mean: -24.142638\n",
      "ep 1353: ep_len:56 episode reward: total was -5.480000. running mean: -23.956011\n",
      "ep 1353: ep_len:86 episode reward: total was 4.530000. running mean: -23.671151\n",
      "ep 1353: ep_len:560 episode reward: total was -17.680000. running mean: -23.611240\n",
      "ep 1353: ep_len:530 episode reward: total was -28.570000. running mean: -23.660827\n",
      "epsilon:0.215179 episode_count: 9478. steps_count: 4197657.000000\n",
      "ep 1354: ep_len:585 episode reward: total was -16.110000. running mean: -23.585319\n",
      "ep 1354: ep_len:515 episode reward: total was -8.700000. running mean: -23.436466\n",
      "ep 1354: ep_len:448 episode reward: total was -29.840000. running mean: -23.500501\n",
      "ep 1354: ep_len:386 episode reward: total was -17.730000. running mean: -23.442796\n",
      "ep 1354: ep_len:3 episode reward: total was 0.000000. running mean: -23.208368\n",
      "ep 1354: ep_len:505 episode reward: total was -23.080000. running mean: -23.207084\n",
      "ep 1354: ep_len:705 episode reward: total was -33.890000. running mean: -23.313914\n",
      "epsilon:0.215042 episode_count: 9485. steps_count: 4200804.000000\n",
      "ep 1355: ep_len:615 episode reward: total was -34.480000. running mean: -23.425574\n",
      "ep 1355: ep_len:273 episode reward: total was -19.910000. running mean: -23.390419\n",
      "ep 1355: ep_len:580 episode reward: total was -40.220000. running mean: -23.558715\n",
      "ep 1355: ep_len:535 episode reward: total was -15.170000. running mean: -23.474827\n",
      "ep 1355: ep_len:3 episode reward: total was 0.000000. running mean: -23.240079\n",
      "ep 1355: ep_len:725 episode reward: total was -33.210000. running mean: -23.339778\n",
      "ep 1355: ep_len:580 episode reward: total was -17.150000. running mean: -23.277881\n",
      "epsilon:0.214906 episode_count: 9492. steps_count: 4204115.000000\n",
      "ep 1356: ep_len:685 episode reward: total was -29.240000. running mean: -23.337502\n",
      "ep 1356: ep_len:505 episode reward: total was -43.150000. running mean: -23.535627\n",
      "ep 1356: ep_len:510 episode reward: total was -20.660000. running mean: -23.506870\n",
      "ep 1356: ep_len:500 episode reward: total was -15.240000. running mean: -23.424202\n",
      "ep 1356: ep_len:88 episode reward: total was -11.460000. running mean: -23.304560\n",
      "ep 1356: ep_len:620 episode reward: total was -22.510000. running mean: -23.296614\n",
      "ep 1356: ep_len:565 episode reward: total was -27.170000. running mean: -23.335348\n",
      "epsilon:0.214769 episode_count: 9499. steps_count: 4207588.000000\n",
      "ep 1357: ep_len:665 episode reward: total was -29.780000. running mean: -23.399795\n",
      "ep 1357: ep_len:555 episode reward: total was -20.920000. running mean: -23.374997\n",
      "ep 1357: ep_len:56 episode reward: total was -0.980000. running mean: -23.151047\n",
      "ep 1357: ep_len:46 episode reward: total was -0.970000. running mean: -22.929236\n",
      "ep 1357: ep_len:45 episode reward: total was 0.000000. running mean: -22.699944\n",
      "ep 1357: ep_len:280 episode reward: total was -6.380000. running mean: -22.536744\n",
      "ep 1357: ep_len:615 episode reward: total was -29.580000. running mean: -22.607177\n",
      "epsilon:0.214633 episode_count: 9506. steps_count: 4209850.000000\n",
      "ep 1358: ep_len:505 episode reward: total was -20.200000. running mean: -22.583105\n",
      "ep 1358: ep_len:500 episode reward: total was -12.000000. running mean: -22.477274\n",
      "ep 1358: ep_len:595 episode reward: total was -30.650000. running mean: -22.559001\n",
      "ep 1358: ep_len:550 episode reward: total was -12.570000. running mean: -22.459111\n",
      "ep 1358: ep_len:3 episode reward: total was 0.000000. running mean: -22.234520\n",
      "ep 1358: ep_len:500 episode reward: total was -30.780000. running mean: -22.319975\n",
      "ep 1358: ep_len:286 episode reward: total was -9.800000. running mean: -22.194775\n",
      "epsilon:0.214496 episode_count: 9513. steps_count: 4212789.000000\n",
      "ep 1359: ep_len:605 episode reward: total was -31.860000. running mean: -22.291427\n",
      "ep 1359: ep_len:530 episode reward: total was -31.690000. running mean: -22.385413\n",
      "ep 1359: ep_len:500 episode reward: total was -24.200000. running mean: -22.403559\n",
      "ep 1359: ep_len:555 episode reward: total was -19.100000. running mean: -22.370523\n",
      "ep 1359: ep_len:95 episode reward: total was -3.460000. running mean: -22.181418\n",
      "ep 1359: ep_len:640 episode reward: total was -57.590000. running mean: -22.535504\n",
      "ep 1359: ep_len:615 episode reward: total was -22.870000. running mean: -22.538849\n",
      "epsilon:0.214360 episode_count: 9520. steps_count: 4216329.000000\n",
      "ep 1360: ep_len:545 episode reward: total was -2.650000. running mean: -22.339961\n",
      "ep 1360: ep_len:630 episode reward: total was -17.110000. running mean: -22.287661\n",
      "ep 1360: ep_len:585 episode reward: total was -44.660000. running mean: -22.511384\n",
      "ep 1360: ep_len:515 episode reward: total was -27.080000. running mean: -22.557070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1360: ep_len:91 episode reward: total was -1.450000. running mean: -22.346000\n",
      "ep 1360: ep_len:500 episode reward: total was -5.120000. running mean: -22.173740\n",
      "ep 1360: ep_len:340 episode reward: total was -20.330000. running mean: -22.155302\n",
      "epsilon:0.214223 episode_count: 9527. steps_count: 4219535.000000\n",
      "ep 1361: ep_len:500 episode reward: total was -20.670000. running mean: -22.140449\n",
      "ep 1361: ep_len:550 episode reward: total was -2.710000. running mean: -21.946145\n",
      "ep 1361: ep_len:690 episode reward: total was -36.740000. running mean: -22.094083\n",
      "ep 1361: ep_len:515 episode reward: total was -33.110000. running mean: -22.204243\n",
      "ep 1361: ep_len:3 episode reward: total was 0.000000. running mean: -21.982200\n",
      "ep 1361: ep_len:590 episode reward: total was -24.830000. running mean: -22.010678\n",
      "ep 1361: ep_len:555 episode reward: total was -38.010000. running mean: -22.170671\n",
      "epsilon:0.214087 episode_count: 9534. steps_count: 4222938.000000\n",
      "ep 1362: ep_len:565 episode reward: total was -35.380000. running mean: -22.302765\n",
      "ep 1362: ep_len:570 episode reward: total was -42.180000. running mean: -22.501537\n",
      "ep 1362: ep_len:343 episode reward: total was -15.810000. running mean: -22.434622\n",
      "ep 1362: ep_len:580 episode reward: total was -30.680000. running mean: -22.517075\n",
      "ep 1362: ep_len:95 episode reward: total was -1.440000. running mean: -22.306305\n",
      "ep 1362: ep_len:595 episode reward: total was -11.580000. running mean: -22.199042\n",
      "ep 1362: ep_len:188 episode reward: total was -5.330000. running mean: -22.030351\n",
      "epsilon:0.213950 episode_count: 9541. steps_count: 4225874.000000\n",
      "ep 1363: ep_len:500 episode reward: total was -38.590000. running mean: -22.195948\n",
      "ep 1363: ep_len:625 episode reward: total was -5.490000. running mean: -22.028888\n",
      "ep 1363: ep_len:555 episode reward: total was -24.840000. running mean: -22.056999\n",
      "ep 1363: ep_len:500 episode reward: total was -18.040000. running mean: -22.016829\n",
      "ep 1363: ep_len:86 episode reward: total was 2.540000. running mean: -21.771261\n",
      "ep 1363: ep_len:710 episode reward: total was -37.310000. running mean: -21.926648\n",
      "ep 1363: ep_len:575 episode reward: total was -21.420000. running mean: -21.921582\n",
      "epsilon:0.213814 episode_count: 9548. steps_count: 4229425.000000\n",
      "ep 1364: ep_len:640 episode reward: total was -46.020000. running mean: -22.162566\n",
      "ep 1364: ep_len:500 episode reward: total was -16.730000. running mean: -22.108240\n",
      "ep 1364: ep_len:620 episode reward: total was -17.950000. running mean: -22.066658\n",
      "ep 1364: ep_len:127 episode reward: total was 0.580000. running mean: -21.840191\n",
      "ep 1364: ep_len:89 episode reward: total was 1.040000. running mean: -21.611390\n",
      "ep 1364: ep_len:605 episode reward: total was -47.210000. running mean: -21.867376\n",
      "ep 1364: ep_len:580 episode reward: total was -24.920000. running mean: -21.897902\n",
      "epsilon:0.213677 episode_count: 9555. steps_count: 4232586.000000\n",
      "ep 1365: ep_len:625 episode reward: total was -17.460000. running mean: -21.853523\n",
      "ep 1365: ep_len:550 episode reward: total was -16.430000. running mean: -21.799288\n",
      "ep 1365: ep_len:665 episode reward: total was -22.770000. running mean: -21.808995\n",
      "ep 1365: ep_len:123 episode reward: total was 0.080000. running mean: -21.590105\n",
      "ep 1365: ep_len:85 episode reward: total was -4.000000. running mean: -21.414204\n",
      "ep 1365: ep_len:575 episode reward: total was -19.370000. running mean: -21.393762\n",
      "ep 1365: ep_len:188 episode reward: total was -11.890000. running mean: -21.298724\n",
      "epsilon:0.213541 episode_count: 9562. steps_count: 4235397.000000\n",
      "ep 1366: ep_len:134 episode reward: total was -4.950000. running mean: -21.135237\n",
      "ep 1366: ep_len:640 episode reward: total was -51.230000. running mean: -21.436185\n",
      "ep 1366: ep_len:570 episode reward: total was -26.110000. running mean: -21.482923\n",
      "ep 1366: ep_len:500 episode reward: total was -16.110000. running mean: -21.429193\n",
      "ep 1366: ep_len:3 episode reward: total was 0.000000. running mean: -21.214902\n",
      "ep 1366: ep_len:685 episode reward: total was -37.390000. running mean: -21.376653\n",
      "ep 1366: ep_len:530 episode reward: total was -22.410000. running mean: -21.386986\n",
      "epsilon:0.213404 episode_count: 9569. steps_count: 4238459.000000\n",
      "ep 1367: ep_len:500 episode reward: total was -35.510000. running mean: -21.528216\n",
      "ep 1367: ep_len:710 episode reward: total was -61.500000. running mean: -21.927934\n",
      "ep 1367: ep_len:510 episode reward: total was -33.550000. running mean: -22.044155\n",
      "ep 1367: ep_len:500 episode reward: total was -24.590000. running mean: -22.069613\n",
      "ep 1367: ep_len:103 episode reward: total was 2.020000. running mean: -21.828717\n",
      "ep 1367: ep_len:230 episode reward: total was -20.820000. running mean: -21.818630\n",
      "ep 1367: ep_len:580 episode reward: total was -28.660000. running mean: -21.887043\n",
      "epsilon:0.213268 episode_count: 9576. steps_count: 4241592.000000\n",
      "ep 1368: ep_len:123 episode reward: total was -2.960000. running mean: -21.697773\n",
      "ep 1368: ep_len:630 episode reward: total was -36.610000. running mean: -21.846895\n",
      "ep 1368: ep_len:363 episode reward: total was -19.350000. running mean: -21.821926\n",
      "ep 1368: ep_len:525 episode reward: total was -9.160000. running mean: -21.695307\n",
      "ep 1368: ep_len:3 episode reward: total was 0.000000. running mean: -21.478354\n",
      "ep 1368: ep_len:520 episode reward: total was -36.780000. running mean: -21.631370\n",
      "ep 1368: ep_len:505 episode reward: total was -17.000000. running mean: -21.585057\n",
      "epsilon:0.213131 episode_count: 9583. steps_count: 4244261.000000\n",
      "ep 1369: ep_len:545 episode reward: total was -23.450000. running mean: -21.603706\n",
      "ep 1369: ep_len:335 episode reward: total was -18.870000. running mean: -21.576369\n",
      "ep 1369: ep_len:700 episode reward: total was -57.440000. running mean: -21.935005\n",
      "ep 1369: ep_len:500 episode reward: total was -38.160000. running mean: -22.097255\n",
      "ep 1369: ep_len:3 episode reward: total was 0.000000. running mean: -21.876283\n",
      "ep 1369: ep_len:183 episode reward: total was -3.900000. running mean: -21.696520\n",
      "ep 1369: ep_len:530 episode reward: total was -31.960000. running mean: -21.799155\n",
      "epsilon:0.212995 episode_count: 9590. steps_count: 4247057.000000\n",
      "ep 1370: ep_len:715 episode reward: total was -28.700000. running mean: -21.868163\n",
      "ep 1370: ep_len:525 episode reward: total was -40.030000. running mean: -22.049782\n",
      "ep 1370: ep_len:555 episode reward: total was -19.180000. running mean: -22.021084\n",
      "ep 1370: ep_len:735 episode reward: total was -66.030000. running mean: -22.461173\n",
      "ep 1370: ep_len:3 episode reward: total was 0.000000. running mean: -22.236561\n",
      "ep 1370: ep_len:540 episode reward: total was -19.990000. running mean: -22.214096\n",
      "ep 1370: ep_len:610 episode reward: total was -28.340000. running mean: -22.275355\n",
      "epsilon:0.212858 episode_count: 9597. steps_count: 4250740.000000\n",
      "ep 1371: ep_len:550 episode reward: total was -19.050000. running mean: -22.243101\n",
      "ep 1371: ep_len:505 episode reward: total was -28.130000. running mean: -22.301970\n",
      "ep 1371: ep_len:402 episode reward: total was -8.810000. running mean: -22.167050\n",
      "ep 1371: ep_len:505 episode reward: total was -23.070000. running mean: -22.176080\n",
      "ep 1371: ep_len:49 episode reward: total was 1.500000. running mean: -21.939319\n",
      "ep 1371: ep_len:525 episode reward: total was -33.980000. running mean: -22.059726\n",
      "ep 1371: ep_len:530 episode reward: total was -32.940000. running mean: -22.168529\n",
      "epsilon:0.212722 episode_count: 9604. steps_count: 4253806.000000\n",
      "ep 1372: ep_len:675 episode reward: total was -55.440000. running mean: -22.501243\n",
      "ep 1372: ep_len:505 episode reward: total was -15.510000. running mean: -22.431331\n",
      "ep 1372: ep_len:79 episode reward: total was 0.040000. running mean: -22.206618\n",
      "ep 1372: ep_len:153 episode reward: total was -5.900000. running mean: -22.043551\n",
      "ep 1372: ep_len:3 episode reward: total was 0.000000. running mean: -21.823116\n",
      "ep 1372: ep_len:500 episode reward: total was -43.820000. running mean: -22.043085\n",
      "ep 1372: ep_len:550 episode reward: total was -28.510000. running mean: -22.107754\n",
      "epsilon:0.212585 episode_count: 9611. steps_count: 4256271.000000\n",
      "ep 1373: ep_len:500 episode reward: total was -5.270000. running mean: -21.939376\n",
      "ep 1373: ep_len:535 episode reward: total was -24.840000. running mean: -21.968383\n",
      "ep 1373: ep_len:570 episode reward: total was -30.410000. running mean: -22.052799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1373: ep_len:510 episode reward: total was -42.700000. running mean: -22.259271\n",
      "ep 1373: ep_len:3 episode reward: total was 0.000000. running mean: -22.036678\n",
      "ep 1373: ep_len:620 episode reward: total was -33.680000. running mean: -22.153111\n",
      "ep 1373: ep_len:630 episode reward: total was -23.880000. running mean: -22.170380\n",
      "epsilon:0.212449 episode_count: 9618. steps_count: 4259639.000000\n",
      "ep 1374: ep_len:229 episode reward: total was -9.930000. running mean: -22.047976\n",
      "ep 1374: ep_len:660 episode reward: total was -43.260000. running mean: -22.260097\n",
      "ep 1374: ep_len:670 episode reward: total was -16.730000. running mean: -22.204796\n",
      "ep 1374: ep_len:500 episode reward: total was -12.720000. running mean: -22.109948\n",
      "ep 1374: ep_len:3 episode reward: total was 0.000000. running mean: -21.888848\n",
      "ep 1374: ep_len:710 episode reward: total was -24.180000. running mean: -21.911760\n",
      "ep 1374: ep_len:505 episode reward: total was -19.190000. running mean: -21.884542\n",
      "epsilon:0.212312 episode_count: 9625. steps_count: 4262916.000000\n",
      "ep 1375: ep_len:600 episode reward: total was -16.670000. running mean: -21.832397\n",
      "ep 1375: ep_len:530 episode reward: total was -13.700000. running mean: -21.751073\n",
      "ep 1375: ep_len:565 episode reward: total was -14.630000. running mean: -21.679862\n",
      "ep 1375: ep_len:526 episode reward: total was -43.510000. running mean: -21.898163\n",
      "ep 1375: ep_len:87 episode reward: total was 3.520000. running mean: -21.643982\n",
      "ep 1375: ep_len:535 episode reward: total was -15.470000. running mean: -21.582242\n",
      "ep 1375: ep_len:625 episode reward: total was -40.100000. running mean: -21.767420\n",
      "epsilon:0.212176 episode_count: 9632. steps_count: 4266384.000000\n",
      "ep 1376: ep_len:190 episode reward: total was -2.930000. running mean: -21.579045\n",
      "ep 1376: ep_len:285 episode reward: total was -17.910000. running mean: -21.542355\n",
      "ep 1376: ep_len:550 episode reward: total was -44.960000. running mean: -21.776531\n",
      "ep 1376: ep_len:545 episode reward: total was -20.690000. running mean: -21.765666\n",
      "ep 1376: ep_len:76 episode reward: total was -0.480000. running mean: -21.552809\n",
      "ep 1376: ep_len:176 episode reward: total was -1.440000. running mean: -21.351681\n",
      "ep 1376: ep_len:560 episode reward: total was -23.500000. running mean: -21.373164\n",
      "epsilon:0.212039 episode_count: 9639. steps_count: 4268766.000000\n",
      "ep 1377: ep_len:615 episode reward: total was -30.650000. running mean: -21.465933\n",
      "ep 1377: ep_len:565 episode reward: total was -24.080000. running mean: -21.492074\n",
      "ep 1377: ep_len:720 episode reward: total was -37.340000. running mean: -21.650553\n",
      "ep 1377: ep_len:500 episode reward: total was -22.090000. running mean: -21.654947\n",
      "ep 1377: ep_len:3 episode reward: total was 0.000000. running mean: -21.438398\n",
      "ep 1377: ep_len:520 episode reward: total was -29.440000. running mean: -21.518414\n",
      "ep 1377: ep_len:525 episode reward: total was -17.100000. running mean: -21.474230\n",
      "epsilon:0.211903 episode_count: 9646. steps_count: 4272214.000000\n",
      "ep 1378: ep_len:550 episode reward: total was -33.170000. running mean: -21.591187\n",
      "ep 1378: ep_len:645 episode reward: total was -6.490000. running mean: -21.440175\n",
      "ep 1378: ep_len:570 episode reward: total was -41.740000. running mean: -21.643174\n",
      "ep 1378: ep_len:503 episode reward: total was -31.480000. running mean: -21.741542\n",
      "ep 1378: ep_len:3 episode reward: total was 0.000000. running mean: -21.524127\n",
      "ep 1378: ep_len:500 episode reward: total was -25.350000. running mean: -21.562385\n",
      "ep 1378: ep_len:510 episode reward: total was -21.080000. running mean: -21.557561\n",
      "epsilon:0.211766 episode_count: 9653. steps_count: 4275495.000000\n",
      "ep 1379: ep_len:565 episode reward: total was -25.660000. running mean: -21.598586\n",
      "ep 1379: ep_len:555 episode reward: total was -36.610000. running mean: -21.748700\n",
      "ep 1379: ep_len:555 episode reward: total was -23.050000. running mean: -21.761713\n",
      "ep 1379: ep_len:560 episode reward: total was -2.690000. running mean: -21.570996\n",
      "ep 1379: ep_len:94 episode reward: total was -10.960000. running mean: -21.464886\n",
      "ep 1379: ep_len:565 episode reward: total was -30.470000. running mean: -21.554937\n",
      "ep 1379: ep_len:600 episode reward: total was -76.730000. running mean: -22.106688\n",
      "epsilon:0.211630 episode_count: 9660. steps_count: 4278989.000000\n",
      "ep 1380: ep_len:610 episode reward: total was -36.010000. running mean: -22.245721\n",
      "ep 1380: ep_len:505 episode reward: total was -32.580000. running mean: -22.349064\n",
      "ep 1380: ep_len:560 episode reward: total was -20.110000. running mean: -22.326673\n",
      "ep 1380: ep_len:565 episode reward: total was -20.670000. running mean: -22.310106\n",
      "ep 1380: ep_len:3 episode reward: total was 0.000000. running mean: -22.087005\n",
      "ep 1380: ep_len:500 episode reward: total was -19.180000. running mean: -22.057935\n",
      "ep 1380: ep_len:575 episode reward: total was -38.700000. running mean: -22.224356\n",
      "epsilon:0.211493 episode_count: 9667. steps_count: 4282307.000000\n",
      "ep 1381: ep_len:510 episode reward: total was -37.570000. running mean: -22.377812\n",
      "ep 1381: ep_len:505 episode reward: total was -18.210000. running mean: -22.336134\n",
      "ep 1381: ep_len:670 episode reward: total was -74.680000. running mean: -22.859573\n",
      "ep 1381: ep_len:565 episode reward: total was -15.080000. running mean: -22.781777\n",
      "ep 1381: ep_len:3 episode reward: total was 0.000000. running mean: -22.553959\n",
      "ep 1381: ep_len:540 episode reward: total was -48.040000. running mean: -22.808820\n",
      "ep 1381: ep_len:595 episode reward: total was -50.090000. running mean: -23.081631\n",
      "epsilon:0.211357 episode_count: 9674. steps_count: 4285695.000000\n",
      "ep 1382: ep_len:635 episode reward: total was -53.990000. running mean: -23.390715\n",
      "ep 1382: ep_len:500 episode reward: total was -24.330000. running mean: -23.400108\n",
      "ep 1382: ep_len:610 episode reward: total was -31.040000. running mean: -23.476507\n",
      "ep 1382: ep_len:520 episode reward: total was -27.070000. running mean: -23.512442\n",
      "ep 1382: ep_len:3 episode reward: total was 0.000000. running mean: -23.277317\n",
      "ep 1382: ep_len:555 episode reward: total was -39.630000. running mean: -23.440844\n",
      "ep 1382: ep_len:610 episode reward: total was -52.140000. running mean: -23.727836\n",
      "epsilon:0.211220 episode_count: 9681. steps_count: 4289128.000000\n",
      "ep 1383: ep_len:500 episode reward: total was -13.810000. running mean: -23.628657\n",
      "ep 1383: ep_len:580 episode reward: total was -33.020000. running mean: -23.722571\n",
      "ep 1383: ep_len:565 episode reward: total was -33.620000. running mean: -23.821545\n",
      "ep 1383: ep_len:500 episode reward: total was -26.640000. running mean: -23.849730\n",
      "ep 1383: ep_len:48 episode reward: total was 3.000000. running mean: -23.581232\n",
      "ep 1383: ep_len:710 episode reward: total was -31.340000. running mean: -23.658820\n",
      "ep 1383: ep_len:515 episode reward: total was -30.190000. running mean: -23.724132\n",
      "epsilon:0.211084 episode_count: 9688. steps_count: 4292546.000000\n",
      "ep 1384: ep_len:560 episode reward: total was -30.860000. running mean: -23.795491\n",
      "ep 1384: ep_len:575 episode reward: total was -29.560000. running mean: -23.853136\n",
      "ep 1384: ep_len:715 episode reward: total was -83.010000. running mean: -24.444704\n",
      "ep 1384: ep_len:124 episode reward: total was 2.560000. running mean: -24.174657\n",
      "ep 1384: ep_len:3 episode reward: total was 0.000000. running mean: -23.932911\n",
      "ep 1384: ep_len:500 episode reward: total was -6.540000. running mean: -23.758982\n",
      "ep 1384: ep_len:505 episode reward: total was -29.090000. running mean: -23.812292\n",
      "epsilon:0.210947 episode_count: 9695. steps_count: 4295528.000000\n",
      "ep 1385: ep_len:620 episode reward: total was -31.370000. running mean: -23.887869\n",
      "ep 1385: ep_len:545 episode reward: total was -26.010000. running mean: -23.909090\n",
      "ep 1385: ep_len:645 episode reward: total was -27.340000. running mean: -23.943399\n",
      "ep 1385: ep_len:383 episode reward: total was -36.160000. running mean: -24.065565\n",
      "ep 1385: ep_len:52 episode reward: total was 2.000000. running mean: -23.804910\n",
      "ep 1385: ep_len:665 episode reward: total was -26.750000. running mean: -23.834361\n",
      "ep 1385: ep_len:555 episode reward: total was -26.650000. running mean: -23.862517\n",
      "epsilon:0.210811 episode_count: 9702. steps_count: 4298993.000000\n",
      "ep 1386: ep_len:620 episode reward: total was -32.410000. running mean: -23.947992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1386: ep_len:500 episode reward: total was -19.470000. running mean: -23.903212\n",
      "ep 1386: ep_len:625 episode reward: total was -35.700000. running mean: -24.021180\n",
      "ep 1386: ep_len:42 episode reward: total was -1.990000. running mean: -23.800868\n",
      "ep 1386: ep_len:76 episode reward: total was -2.500000. running mean: -23.587859\n",
      "ep 1386: ep_len:650 episode reward: total was -39.560000. running mean: -23.747581\n",
      "ep 1386: ep_len:260 episode reward: total was -13.360000. running mean: -23.643705\n",
      "epsilon:0.210674 episode_count: 9709. steps_count: 4301766.000000\n",
      "ep 1387: ep_len:640 episode reward: total was -26.370000. running mean: -23.670968\n",
      "ep 1387: ep_len:500 episode reward: total was -29.540000. running mean: -23.729658\n",
      "ep 1387: ep_len:585 episode reward: total was -17.540000. running mean: -23.667762\n",
      "ep 1387: ep_len:52 episode reward: total was -0.960000. running mean: -23.440684\n",
      "ep 1387: ep_len:3 episode reward: total was 0.000000. running mean: -23.206277\n",
      "ep 1387: ep_len:705 episode reward: total was -29.760000. running mean: -23.271814\n",
      "ep 1387: ep_len:500 episode reward: total was -23.840000. running mean: -23.277496\n",
      "epsilon:0.210538 episode_count: 9716. steps_count: 4304751.000000\n",
      "ep 1388: ep_len:218 episode reward: total was 4.100000. running mean: -23.003721\n",
      "ep 1388: ep_len:279 episode reward: total was -15.420000. running mean: -22.927884\n",
      "ep 1388: ep_len:560 episode reward: total was -25.650000. running mean: -22.955105\n",
      "ep 1388: ep_len:500 episode reward: total was -43.650000. running mean: -23.162054\n",
      "ep 1388: ep_len:99 episode reward: total was 0.540000. running mean: -22.925034\n",
      "ep 1388: ep_len:505 episode reward: total was -21.300000. running mean: -22.908783\n",
      "ep 1388: ep_len:187 episode reward: total was -11.940000. running mean: -22.799095\n",
      "epsilon:0.210401 episode_count: 9723. steps_count: 4307099.000000\n",
      "ep 1389: ep_len:595 episode reward: total was -41.220000. running mean: -22.983304\n",
      "ep 1389: ep_len:620 episode reward: total was -36.620000. running mean: -23.119671\n",
      "ep 1389: ep_len:595 episode reward: total was -27.130000. running mean: -23.159775\n",
      "ep 1389: ep_len:122 episode reward: total was -8.470000. running mean: -23.012877\n",
      "ep 1389: ep_len:86 episode reward: total was 2.020000. running mean: -22.762548\n",
      "ep 1389: ep_len:510 episode reward: total was -18.500000. running mean: -22.719923\n",
      "ep 1389: ep_len:510 episode reward: total was -37.100000. running mean: -22.863723\n",
      "epsilon:0.210265 episode_count: 9730. steps_count: 4310137.000000\n",
      "ep 1390: ep_len:530 episode reward: total was -68.730000. running mean: -23.322386\n",
      "ep 1390: ep_len:510 episode reward: total was -21.440000. running mean: -23.303562\n",
      "ep 1390: ep_len:590 episode reward: total was -18.220000. running mean: -23.252727\n",
      "ep 1390: ep_len:56 episode reward: total was -3.980000. running mean: -23.059999\n",
      "ep 1390: ep_len:98 episode reward: total was -4.450000. running mean: -22.873899\n",
      "ep 1390: ep_len:635 episode reward: total was -54.020000. running mean: -23.185360\n",
      "ep 1390: ep_len:580 episode reward: total was -18.900000. running mean: -23.142507\n",
      "epsilon:0.210128 episode_count: 9737. steps_count: 4313136.000000\n",
      "ep 1391: ep_len:200 episode reward: total was -4.930000. running mean: -22.960382\n",
      "ep 1391: ep_len:525 episode reward: total was 1.780000. running mean: -22.712978\n",
      "ep 1391: ep_len:580 episode reward: total was -23.380000. running mean: -22.719648\n",
      "ep 1391: ep_len:505 episode reward: total was -34.070000. running mean: -22.833152\n",
      "ep 1391: ep_len:3 episode reward: total was 0.000000. running mean: -22.604820\n",
      "ep 1391: ep_len:235 episode reward: total was -0.420000. running mean: -22.382972\n",
      "ep 1391: ep_len:595 episode reward: total was -26.640000. running mean: -22.425542\n",
      "epsilon:0.209992 episode_count: 9744. steps_count: 4315779.000000\n",
      "ep 1392: ep_len:600 episode reward: total was -27.370000. running mean: -22.474987\n",
      "ep 1392: ep_len:585 episode reward: total was -20.580000. running mean: -22.456037\n",
      "ep 1392: ep_len:377 episode reward: total was -26.930000. running mean: -22.500777\n",
      "ep 1392: ep_len:505 episode reward: total was -27.160000. running mean: -22.547369\n",
      "ep 1392: ep_len:94 episode reward: total was -9.950000. running mean: -22.421395\n",
      "ep 1392: ep_len:600 episode reward: total was -32.370000. running mean: -22.520881\n",
      "ep 1392: ep_len:197 episode reward: total was -29.460000. running mean: -22.590272\n",
      "epsilon:0.209855 episode_count: 9751. steps_count: 4318737.000000\n",
      "ep 1393: ep_len:565 episode reward: total was -17.160000. running mean: -22.535970\n",
      "ep 1393: ep_len:500 episode reward: total was -33.170000. running mean: -22.642310\n",
      "ep 1393: ep_len:427 episode reward: total was -31.340000. running mean: -22.729287\n",
      "ep 1393: ep_len:500 episode reward: total was -19.620000. running mean: -22.698194\n",
      "ep 1393: ep_len:82 episode reward: total was -11.470000. running mean: -22.585912\n",
      "ep 1393: ep_len:675 episode reward: total was -16.260000. running mean: -22.522653\n",
      "ep 1393: ep_len:282 episode reward: total was -15.380000. running mean: -22.451226\n",
      "epsilon:0.209719 episode_count: 9758. steps_count: 4321768.000000\n",
      "ep 1394: ep_len:510 episode reward: total was -21.520000. running mean: -22.441914\n",
      "ep 1394: ep_len:580 episode reward: total was -41.670000. running mean: -22.634195\n",
      "ep 1394: ep_len:610 episode reward: total was -46.110000. running mean: -22.868953\n",
      "ep 1394: ep_len:670 episode reward: total was -79.170000. running mean: -23.431964\n",
      "ep 1394: ep_len:3 episode reward: total was 0.000000. running mean: -23.197644\n",
      "ep 1394: ep_len:665 episode reward: total was -64.550000. running mean: -23.611167\n",
      "ep 1394: ep_len:585 episode reward: total was -33.730000. running mean: -23.712356\n",
      "epsilon:0.209582 episode_count: 9765. steps_count: 4325391.000000\n",
      "ep 1395: ep_len:570 episode reward: total was -48.960000. running mean: -23.964832\n",
      "ep 1395: ep_len:500 episode reward: total was -12.360000. running mean: -23.848784\n",
      "ep 1395: ep_len:635 episode reward: total was -12.580000. running mean: -23.736096\n",
      "ep 1395: ep_len:555 episode reward: total was -8.200000. running mean: -23.580735\n",
      "ep 1395: ep_len:3 episode reward: total was 0.000000. running mean: -23.344928\n",
      "ep 1395: ep_len:560 episode reward: total was -24.860000. running mean: -23.360078\n",
      "ep 1395: ep_len:182 episode reward: total was -16.970000. running mean: -23.296178\n",
      "epsilon:0.209446 episode_count: 9772. steps_count: 4328396.000000\n",
      "ep 1396: ep_len:107 episode reward: total was -7.970000. running mean: -23.142916\n",
      "ep 1396: ep_len:500 episode reward: total was -20.600000. running mean: -23.117487\n",
      "ep 1396: ep_len:710 episode reward: total was -25.740000. running mean: -23.143712\n",
      "ep 1396: ep_len:132 episode reward: total was 0.560000. running mean: -22.906675\n",
      "ep 1396: ep_len:3 episode reward: total was 0.000000. running mean: -22.677608\n",
      "ep 1396: ep_len:535 episode reward: total was -45.080000. running mean: -22.901632\n",
      "ep 1396: ep_len:590 episode reward: total was -36.070000. running mean: -23.033316\n",
      "epsilon:0.209309 episode_count: 9779. steps_count: 4330973.000000\n",
      "ep 1397: ep_len:250 episode reward: total was -20.920000. running mean: -23.012182\n",
      "ep 1397: ep_len:500 episode reward: total was -18.090000. running mean: -22.962961\n",
      "ep 1397: ep_len:680 episode reward: total was -23.780000. running mean: -22.971131\n",
      "ep 1397: ep_len:500 episode reward: total was -28.710000. running mean: -23.028520\n",
      "ep 1397: ep_len:52 episode reward: total was -1.000000. running mean: -22.808235\n",
      "ep 1397: ep_len:500 episode reward: total was -43.310000. running mean: -23.013252\n",
      "ep 1397: ep_len:520 episode reward: total was -42.620000. running mean: -23.209320\n",
      "epsilon:0.209173 episode_count: 9786. steps_count: 4333975.000000\n",
      "ep 1398: ep_len:202 episode reward: total was -12.920000. running mean: -23.106426\n",
      "ep 1398: ep_len:314 episode reward: total was -29.840000. running mean: -23.173762\n",
      "ep 1398: ep_len:570 episode reward: total was -41.180000. running mean: -23.353825\n",
      "ep 1398: ep_len:40 episode reward: total was -2.480000. running mean: -23.145086\n",
      "ep 1398: ep_len:3 episode reward: total was 0.000000. running mean: -22.913635\n",
      "ep 1398: ep_len:231 episode reward: total was -6.950000. running mean: -22.753999\n",
      "ep 1398: ep_len:595 episode reward: total was -26.700000. running mean: -22.793459\n",
      "epsilon:0.209036 episode_count: 9793. steps_count: 4335930.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1399: ep_len:845 episode reward: total was -96.610000. running mean: -23.531625\n",
      "ep 1399: ep_len:785 episode reward: total was -84.080000. running mean: -24.137108\n",
      "ep 1399: ep_len:605 episode reward: total was -19.310000. running mean: -24.088837\n",
      "ep 1399: ep_len:505 episode reward: total was -3.130000. running mean: -23.879249\n",
      "ep 1399: ep_len:89 episode reward: total was -1.960000. running mean: -23.660056\n",
      "ep 1399: ep_len:500 episode reward: total was -41.310000. running mean: -23.836556\n",
      "ep 1399: ep_len:545 episode reward: total was -26.500000. running mean: -23.863190\n",
      "epsilon:0.208900 episode_count: 9800. steps_count: 4339804.000000\n",
      "ep 1400: ep_len:134 episode reward: total was -19.990000. running mean: -23.824458\n",
      "ep 1400: ep_len:510 episode reward: total was -5.700000. running mean: -23.643214\n",
      "ep 1400: ep_len:595 episode reward: total was -47.960000. running mean: -23.886382\n",
      "ep 1400: ep_len:500 episode reward: total was -30.720000. running mean: -23.954718\n",
      "ep 1400: ep_len:104 episode reward: total was 3.550000. running mean: -23.679671\n",
      "ep 1400: ep_len:515 episode reward: total was -10.150000. running mean: -23.544374\n",
      "ep 1400: ep_len:294 episode reward: total was -12.320000. running mean: -23.432130\n",
      "epsilon:0.208763 episode_count: 9807. steps_count: 4342456.000000\n",
      "ep 1401: ep_len:229 episode reward: total was -9.920000. running mean: -23.297009\n",
      "ep 1401: ep_len:520 episode reward: total was -26.180000. running mean: -23.325839\n",
      "ep 1401: ep_len:500 episode reward: total was -22.780000. running mean: -23.320380\n",
      "ep 1401: ep_len:520 episode reward: total was -18.560000. running mean: -23.272777\n",
      "ep 1401: ep_len:3 episode reward: total was 0.000000. running mean: -23.040049\n",
      "ep 1401: ep_len:535 episode reward: total was -19.400000. running mean: -23.003648\n",
      "ep 1401: ep_len:500 episode reward: total was -35.710000. running mean: -23.130712\n",
      "epsilon:0.208627 episode_count: 9814. steps_count: 4345263.000000\n",
      "ep 1402: ep_len:585 episode reward: total was -18.690000. running mean: -23.086305\n",
      "ep 1402: ep_len:500 episode reward: total was -51.920000. running mean: -23.374642\n",
      "ep 1402: ep_len:525 episode reward: total was -30.960000. running mean: -23.450495\n",
      "ep 1402: ep_len:520 episode reward: total was -31.620000. running mean: -23.532190\n",
      "ep 1402: ep_len:33 episode reward: total was 1.500000. running mean: -23.281868\n",
      "ep 1402: ep_len:500 episode reward: total was -51.300000. running mean: -23.562050\n",
      "ep 1402: ep_len:186 episode reward: total was -10.900000. running mean: -23.435429\n",
      "epsilon:0.208490 episode_count: 9821. steps_count: 4348112.000000\n",
      "ep 1403: ep_len:500 episode reward: total was -30.530000. running mean: -23.506375\n",
      "ep 1403: ep_len:570 episode reward: total was -23.460000. running mean: -23.505911\n",
      "ep 1403: ep_len:500 episode reward: total was -26.540000. running mean: -23.536252\n",
      "ep 1403: ep_len:584 episode reward: total was -45.000000. running mean: -23.750890\n",
      "ep 1403: ep_len:89 episode reward: total was 2.050000. running mean: -23.492881\n",
      "ep 1403: ep_len:675 episode reward: total was -24.800000. running mean: -23.505952\n",
      "ep 1403: ep_len:620 episode reward: total was -24.180000. running mean: -23.512692\n",
      "epsilon:0.208354 episode_count: 9828. steps_count: 4351650.000000\n",
      "ep 1404: ep_len:500 episode reward: total was -25.220000. running mean: -23.529765\n",
      "ep 1404: ep_len:500 episode reward: total was -17.080000. running mean: -23.465268\n",
      "ep 1404: ep_len:615 episode reward: total was -26.700000. running mean: -23.497615\n",
      "ep 1404: ep_len:535 episode reward: total was -12.670000. running mean: -23.389339\n",
      "ep 1404: ep_len:108 episode reward: total was -9.430000. running mean: -23.249746\n",
      "ep 1404: ep_len:600 episode reward: total was -18.160000. running mean: -23.198848\n",
      "ep 1404: ep_len:510 episode reward: total was -22.700000. running mean: -23.193860\n",
      "epsilon:0.208217 episode_count: 9835. steps_count: 4355018.000000\n",
      "ep 1405: ep_len:610 episode reward: total was -14.190000. running mean: -23.103821\n",
      "ep 1405: ep_len:545 episode reward: total was -23.230000. running mean: -23.105083\n",
      "ep 1405: ep_len:580 episode reward: total was -62.110000. running mean: -23.495132\n",
      "ep 1405: ep_len:550 episode reward: total was -31.100000. running mean: -23.571181\n",
      "ep 1405: ep_len:3 episode reward: total was 0.000000. running mean: -23.335469\n",
      "ep 1405: ep_len:258 episode reward: total was -3.410000. running mean: -23.136214\n",
      "ep 1405: ep_len:515 episode reward: total was -24.400000. running mean: -23.148852\n",
      "epsilon:0.208081 episode_count: 9842. steps_count: 4358079.000000\n",
      "ep 1406: ep_len:132 episode reward: total was -2.970000. running mean: -22.947063\n",
      "ep 1406: ep_len:535 episode reward: total was -31.230000. running mean: -23.029893\n",
      "ep 1406: ep_len:625 episode reward: total was -27.050000. running mean: -23.070094\n",
      "ep 1406: ep_len:38 episode reward: total was -1.970000. running mean: -22.859093\n",
      "ep 1406: ep_len:3 episode reward: total was 0.000000. running mean: -22.630502\n",
      "ep 1406: ep_len:620 episode reward: total was -15.580000. running mean: -22.559997\n",
      "ep 1406: ep_len:510 episode reward: total was -21.720000. running mean: -22.551597\n",
      "epsilon:0.207944 episode_count: 9849. steps_count: 4360542.000000\n",
      "ep 1407: ep_len:620 episode reward: total was -28.130000. running mean: -22.607381\n",
      "ep 1407: ep_len:585 episode reward: total was -18.480000. running mean: -22.566107\n",
      "ep 1407: ep_len:540 episode reward: total was -18.090000. running mean: -22.521346\n",
      "ep 1407: ep_len:515 episode reward: total was -26.620000. running mean: -22.562333\n",
      "ep 1407: ep_len:47 episode reward: total was -9.000000. running mean: -22.426709\n",
      "ep 1407: ep_len:590 episode reward: total was -27.560000. running mean: -22.478042\n",
      "ep 1407: ep_len:330 episode reward: total was -19.400000. running mean: -22.447262\n",
      "epsilon:0.207808 episode_count: 9856. steps_count: 4363769.000000\n",
      "ep 1408: ep_len:640 episode reward: total was -39.380000. running mean: -22.616589\n",
      "ep 1408: ep_len:575 episode reward: total was -22.060000. running mean: -22.611023\n",
      "ep 1408: ep_len:605 episode reward: total was -43.900000. running mean: -22.823913\n",
      "ep 1408: ep_len:500 episode reward: total was -21.170000. running mean: -22.807374\n",
      "ep 1408: ep_len:88 episode reward: total was -9.960000. running mean: -22.678900\n",
      "ep 1408: ep_len:505 episode reward: total was -9.770000. running mean: -22.549811\n",
      "ep 1408: ep_len:555 episode reward: total was -15.370000. running mean: -22.478013\n",
      "epsilon:0.207671 episode_count: 9863. steps_count: 4367237.000000\n",
      "ep 1409: ep_len:535 episode reward: total was -10.480000. running mean: -22.358033\n",
      "ep 1409: ep_len:595 episode reward: total was -38.990000. running mean: -22.524353\n",
      "ep 1409: ep_len:510 episode reward: total was -34.970000. running mean: -22.648809\n",
      "ep 1409: ep_len:500 episode reward: total was -23.730000. running mean: -22.659621\n",
      "ep 1409: ep_len:3 episode reward: total was 0.000000. running mean: -22.433025\n",
      "ep 1409: ep_len:550 episode reward: total was -15.520000. running mean: -22.363895\n",
      "ep 1409: ep_len:296 episode reward: total was -11.320000. running mean: -22.253456\n",
      "epsilon:0.207535 episode_count: 9870. steps_count: 4370226.000000\n",
      "ep 1410: ep_len:263 episode reward: total was -5.390000. running mean: -22.084821\n",
      "ep 1410: ep_len:530 episode reward: total was -14.020000. running mean: -22.004173\n",
      "ep 1410: ep_len:367 episode reward: total was -12.890000. running mean: -21.913031\n",
      "ep 1410: ep_len:635 episode reward: total was -42.090000. running mean: -22.114801\n",
      "ep 1410: ep_len:3 episode reward: total was 0.000000. running mean: -21.893653\n",
      "ep 1410: ep_len:500 episode reward: total was -12.980000. running mean: -21.804516\n",
      "ep 1410: ep_len:500 episode reward: total was -26.240000. running mean: -21.848871\n",
      "epsilon:0.207398 episode_count: 9877. steps_count: 4373024.000000\n",
      "ep 1411: ep_len:500 episode reward: total was 1.250000. running mean: -21.617882\n",
      "ep 1411: ep_len:500 episode reward: total was -20.360000. running mean: -21.605304\n",
      "ep 1411: ep_len:550 episode reward: total was -34.740000. running mean: -21.736651\n",
      "ep 1411: ep_len:56 episode reward: total was -2.970000. running mean: -21.548984\n",
      "ep 1411: ep_len:74 episode reward: total was -0.490000. running mean: -21.338394\n",
      "ep 1411: ep_len:685 episode reward: total was -15.230000. running mean: -21.277310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1411: ep_len:500 episode reward: total was -26.560000. running mean: -21.330137\n",
      "epsilon:0.207262 episode_count: 9884. steps_count: 4375889.000000\n",
      "ep 1412: ep_len:505 episode reward: total was -17.290000. running mean: -21.289736\n",
      "ep 1412: ep_len:515 episode reward: total was -20.580000. running mean: -21.282638\n",
      "ep 1412: ep_len:500 episode reward: total was -15.630000. running mean: -21.226112\n",
      "ep 1412: ep_len:505 episode reward: total was -16.620000. running mean: -21.180051\n",
      "ep 1412: ep_len:3 episode reward: total was 0.000000. running mean: -20.968250\n",
      "ep 1412: ep_len:530 episode reward: total was -39.660000. running mean: -21.155168\n",
      "ep 1412: ep_len:620 episode reward: total was -42.600000. running mean: -21.369616\n",
      "epsilon:0.207125 episode_count: 9891. steps_count: 4379067.000000\n",
      "ep 1413: ep_len:134 episode reward: total was -13.950000. running mean: -21.295420\n",
      "ep 1413: ep_len:500 episode reward: total was -22.880000. running mean: -21.311266\n",
      "ep 1413: ep_len:625 episode reward: total was -42.190000. running mean: -21.520053\n",
      "ep 1413: ep_len:640 episode reward: total was -9.480000. running mean: -21.399653\n",
      "ep 1413: ep_len:95 episode reward: total was 0.540000. running mean: -21.180256\n",
      "ep 1413: ep_len:595 episode reward: total was -22.800000. running mean: -21.196454\n",
      "ep 1413: ep_len:293 episode reward: total was -16.380000. running mean: -21.148289\n",
      "epsilon:0.206989 episode_count: 9898. steps_count: 4381949.000000\n",
      "ep 1414: ep_len:500 episode reward: total was -10.290000. running mean: -21.039706\n",
      "ep 1414: ep_len:187 episode reward: total was -18.440000. running mean: -21.013709\n",
      "ep 1414: ep_len:425 episode reward: total was -28.870000. running mean: -21.092272\n",
      "ep 1414: ep_len:545 episode reward: total was -6.670000. running mean: -20.948049\n",
      "ep 1414: ep_len:128 episode reward: total was -9.440000. running mean: -20.832969\n",
      "ep 1414: ep_len:560 episode reward: total was -29.180000. running mean: -20.916439\n",
      "ep 1414: ep_len:199 episode reward: total was -9.870000. running mean: -20.805975\n",
      "epsilon:0.206852 episode_count: 9905. steps_count: 4384493.000000\n",
      "ep 1415: ep_len:510 episode reward: total was -6.460000. running mean: -20.662515\n",
      "ep 1415: ep_len:615 episode reward: total was -7.210000. running mean: -20.527990\n",
      "ep 1415: ep_len:525 episode reward: total was -12.140000. running mean: -20.444110\n",
      "ep 1415: ep_len:510 episode reward: total was -31.620000. running mean: -20.555869\n",
      "ep 1415: ep_len:3 episode reward: total was 0.000000. running mean: -20.350310\n",
      "ep 1415: ep_len:565 episode reward: total was -8.970000. running mean: -20.236507\n",
      "ep 1415: ep_len:610 episode reward: total was -39.690000. running mean: -20.431042\n",
      "epsilon:0.206716 episode_count: 9912. steps_count: 4387831.000000\n",
      "ep 1416: ep_len:505 episode reward: total was -11.810000. running mean: -20.344832\n",
      "ep 1416: ep_len:500 episode reward: total was -22.930000. running mean: -20.370683\n",
      "ep 1416: ep_len:740 episode reward: total was -78.920000. running mean: -20.956176\n",
      "ep 1416: ep_len:620 episode reward: total was -32.020000. running mean: -21.066815\n",
      "ep 1416: ep_len:3 episode reward: total was 0.000000. running mean: -20.856147\n",
      "ep 1416: ep_len:615 episode reward: total was -36.150000. running mean: -21.009085\n",
      "ep 1416: ep_len:610 episode reward: total was -38.030000. running mean: -21.179294\n",
      "epsilon:0.206579 episode_count: 9919. steps_count: 4391424.000000\n",
      "ep 1417: ep_len:113 episode reward: total was -1.940000. running mean: -20.986901\n",
      "ep 1417: ep_len:570 episode reward: total was -36.660000. running mean: -21.143632\n",
      "ep 1417: ep_len:570 episode reward: total was -19.100000. running mean: -21.123196\n",
      "ep 1417: ep_len:520 episode reward: total was -36.720000. running mean: -21.279164\n",
      "ep 1417: ep_len:113 episode reward: total was -10.940000. running mean: -21.175772\n",
      "ep 1417: ep_len:590 episode reward: total was -23.100000. running mean: -21.195015\n",
      "ep 1417: ep_len:580 episode reward: total was -23.590000. running mean: -21.218964\n",
      "epsilon:0.206443 episode_count: 9926. steps_count: 4394480.000000\n",
      "ep 1418: ep_len:645 episode reward: total was -28.580000. running mean: -21.292575\n",
      "ep 1418: ep_len:500 episode reward: total was -39.120000. running mean: -21.470849\n",
      "ep 1418: ep_len:79 episode reward: total was -1.950000. running mean: -21.275641\n",
      "ep 1418: ep_len:515 episode reward: total was -35.620000. running mean: -21.419084\n",
      "ep 1418: ep_len:3 episode reward: total was 0.000000. running mean: -21.204893\n",
      "ep 1418: ep_len:500 episode reward: total was -14.160000. running mean: -21.134444\n",
      "ep 1418: ep_len:500 episode reward: total was -18.650000. running mean: -21.109600\n",
      "epsilon:0.206306 episode_count: 9933. steps_count: 4397222.000000\n",
      "ep 1419: ep_len:265 episode reward: total was -7.910000. running mean: -20.977604\n",
      "ep 1419: ep_len:555 episode reward: total was -28.480000. running mean: -21.052628\n",
      "ep 1419: ep_len:79 episode reward: total was -1.460000. running mean: -20.856702\n",
      "ep 1419: ep_len:130 episode reward: total was -9.430000. running mean: -20.742435\n",
      "ep 1419: ep_len:3 episode reward: total was 0.000000. running mean: -20.535010\n",
      "ep 1419: ep_len:158 episode reward: total was 4.100000. running mean: -20.288660\n",
      "ep 1419: ep_len:540 episode reward: total was -26.680000. running mean: -20.352574\n",
      "epsilon:0.206170 episode_count: 9940. steps_count: 4398952.000000\n",
      "ep 1420: ep_len:580 episode reward: total was -49.460000. running mean: -20.643648\n",
      "ep 1420: ep_len:675 episode reward: total was -56.970000. running mean: -21.006911\n",
      "ep 1420: ep_len:580 episode reward: total was -44.430000. running mean: -21.241142\n",
      "ep 1420: ep_len:168 episode reward: total was -3.900000. running mean: -21.067731\n",
      "ep 1420: ep_len:3 episode reward: total was 0.000000. running mean: -20.857054\n",
      "ep 1420: ep_len:505 episode reward: total was -27.780000. running mean: -20.926283\n",
      "ep 1420: ep_len:590 episode reward: total was -17.080000. running mean: -20.887820\n",
      "epsilon:0.206033 episode_count: 9947. steps_count: 4402053.000000\n",
      "ep 1421: ep_len:640 episode reward: total was -54.090000. running mean: -21.219842\n",
      "ep 1421: ep_len:166 episode reward: total was -9.470000. running mean: -21.102344\n",
      "ep 1421: ep_len:445 episode reward: total was -18.780000. running mean: -21.079120\n",
      "ep 1421: ep_len:510 episode reward: total was -13.690000. running mean: -21.005229\n",
      "ep 1421: ep_len:93 episode reward: total was -6.470000. running mean: -20.859877\n",
      "ep 1421: ep_len:630 episode reward: total was -34.450000. running mean: -20.995778\n",
      "ep 1421: ep_len:600 episode reward: total was -49.590000. running mean: -21.281720\n",
      "epsilon:0.205897 episode_count: 9954. steps_count: 4405137.000000\n",
      "ep 1422: ep_len:855 episode reward: total was -86.110000. running mean: -21.930003\n",
      "ep 1422: ep_len:505 episode reward: total was -39.410000. running mean: -22.104803\n",
      "ep 1422: ep_len:565 episode reward: total was -18.150000. running mean: -22.065255\n",
      "ep 1422: ep_len:150 episode reward: total was -19.930000. running mean: -22.043902\n",
      "ep 1422: ep_len:87 episode reward: total was 1.530000. running mean: -21.808163\n",
      "ep 1422: ep_len:540 episode reward: total was -21.140000. running mean: -21.801482\n",
      "ep 1422: ep_len:326 episode reward: total was -9.830000. running mean: -21.681767\n",
      "epsilon:0.205760 episode_count: 9961. steps_count: 4408165.000000\n",
      "ep 1423: ep_len:580 episode reward: total was -23.190000. running mean: -21.696849\n",
      "ep 1423: ep_len:500 episode reward: total was -28.120000. running mean: -21.761081\n",
      "ep 1423: ep_len:625 episode reward: total was -49.200000. running mean: -22.035470\n",
      "ep 1423: ep_len:510 episode reward: total was -12.090000. running mean: -21.936015\n",
      "ep 1423: ep_len:56 episode reward: total was 4.000000. running mean: -21.676655\n",
      "ep 1423: ep_len:298 episode reward: total was -12.840000. running mean: -21.588288\n",
      "ep 1423: ep_len:505 episode reward: total was -39.650000. running mean: -21.768906\n",
      "epsilon:0.205624 episode_count: 9968. steps_count: 4411239.000000\n",
      "ep 1424: ep_len:510 episode reward: total was -16.010000. running mean: -21.711316\n",
      "ep 1424: ep_len:500 episode reward: total was -18.320000. running mean: -21.677403\n",
      "ep 1424: ep_len:580 episode reward: total was -48.170000. running mean: -21.942329\n",
      "ep 1424: ep_len:545 episode reward: total was -15.730000. running mean: -21.880206\n",
      "ep 1424: ep_len:3 episode reward: total was 0.000000. running mean: -21.661404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1424: ep_len:174 episode reward: total was 0.600000. running mean: -21.438790\n",
      "ep 1424: ep_len:500 episode reward: total was -20.740000. running mean: -21.431802\n",
      "epsilon:0.205487 episode_count: 9975. steps_count: 4414051.000000\n",
      "ep 1425: ep_len:500 episode reward: total was -15.770000. running mean: -21.375184\n",
      "ep 1425: ep_len:500 episode reward: total was -31.690000. running mean: -21.478332\n",
      "ep 1425: ep_len:585 episode reward: total was -37.360000. running mean: -21.637149\n",
      "ep 1425: ep_len:500 episode reward: total was -31.160000. running mean: -21.732377\n",
      "ep 1425: ep_len:3 episode reward: total was 0.000000. running mean: -21.515054\n",
      "ep 1425: ep_len:326 episode reward: total was -7.300000. running mean: -21.372903\n",
      "ep 1425: ep_len:525 episode reward: total was -35.100000. running mean: -21.510174\n",
      "epsilon:0.205351 episode_count: 9982. steps_count: 4416990.000000\n",
      "ep 1426: ep_len:242 episode reward: total was -1.850000. running mean: -21.313572\n",
      "ep 1426: ep_len:545 episode reward: total was -14.150000. running mean: -21.241937\n",
      "ep 1426: ep_len:510 episode reward: total was -21.670000. running mean: -21.246217\n",
      "ep 1426: ep_len:525 episode reward: total was -43.650000. running mean: -21.470255\n",
      "ep 1426: ep_len:3 episode reward: total was 0.000000. running mean: -21.255552\n",
      "ep 1426: ep_len:530 episode reward: total was -44.150000. running mean: -21.484497\n",
      "ep 1426: ep_len:279 episode reward: total was -12.870000. running mean: -21.398352\n",
      "epsilon:0.205214 episode_count: 9989. steps_count: 4419624.000000\n",
      "ep 1427: ep_len:112 episode reward: total was -1.940000. running mean: -21.203768\n",
      "ep 1427: ep_len:313 episode reward: total was -32.860000. running mean: -21.320331\n",
      "ep 1427: ep_len:605 episode reward: total was -24.560000. running mean: -21.352727\n",
      "ep 1427: ep_len:500 episode reward: total was -28.100000. running mean: -21.420200\n",
      "ep 1427: ep_len:120 episode reward: total was -2.930000. running mean: -21.235298\n",
      "ep 1427: ep_len:620 episode reward: total was -42.570000. running mean: -21.448645\n",
      "ep 1427: ep_len:201 episode reward: total was -14.400000. running mean: -21.378159\n",
      "epsilon:0.205078 episode_count: 9996. steps_count: 4422095.000000\n",
      "ep 1428: ep_len:675 episode reward: total was -54.430000. running mean: -21.708677\n",
      "ep 1428: ep_len:500 episode reward: total was -0.600000. running mean: -21.497590\n",
      "ep 1428: ep_len:595 episode reward: total was -34.080000. running mean: -21.623414\n",
      "ep 1428: ep_len:127 episode reward: total was -2.910000. running mean: -21.436280\n",
      "ep 1428: ep_len:95 episode reward: total was -13.950000. running mean: -21.361418\n",
      "ep 1428: ep_len:660 episode reward: total was -36.380000. running mean: -21.511603\n",
      "ep 1428: ep_len:580 episode reward: total was -42.130000. running mean: -21.717787\n",
      "epsilon:0.204941 episode_count: 10003. steps_count: 4425327.000000\n",
      "ep 1429: ep_len:705 episode reward: total was -43.350000. running mean: -21.934109\n",
      "ep 1429: ep_len:277 episode reward: total was -20.880000. running mean: -21.923568\n",
      "ep 1429: ep_len:510 episode reward: total was -25.000000. running mean: -21.954333\n",
      "ep 1429: ep_len:500 episode reward: total was -26.130000. running mean: -21.996089\n",
      "ep 1429: ep_len:3 episode reward: total was 0.000000. running mean: -21.776128\n",
      "ep 1429: ep_len:540 episode reward: total was -23.100000. running mean: -21.789367\n",
      "ep 1429: ep_len:500 episode reward: total was -30.130000. running mean: -21.872773\n",
      "epsilon:0.204805 episode_count: 10010. steps_count: 4428362.000000\n",
      "ep 1430: ep_len:134 episode reward: total was -3.430000. running mean: -21.688346\n",
      "ep 1430: ep_len:500 episode reward: total was -30.070000. running mean: -21.772162\n",
      "ep 1430: ep_len:373 episode reward: total was -3.830000. running mean: -21.592741\n",
      "ep 1430: ep_len:170 episode reward: total was -1.900000. running mean: -21.395813\n",
      "ep 1430: ep_len:46 episode reward: total was 0.000000. running mean: -21.181855\n",
      "ep 1430: ep_len:328 episode reward: total was -12.350000. running mean: -21.093537\n",
      "ep 1430: ep_len:600 episode reward: total was -28.960000. running mean: -21.172201\n",
      "epsilon:0.204668 episode_count: 10017. steps_count: 4430513.000000\n",
      "ep 1431: ep_len:570 episode reward: total was -15.650000. running mean: -21.116979\n",
      "ep 1431: ep_len:500 episode reward: total was -14.810000. running mean: -21.053909\n",
      "ep 1431: ep_len:500 episode reward: total was -54.850000. running mean: -21.391870\n",
      "ep 1431: ep_len:510 episode reward: total was -40.080000. running mean: -21.578752\n",
      "ep 1431: ep_len:98 episode reward: total was 1.030000. running mean: -21.352664\n",
      "ep 1431: ep_len:510 episode reward: total was -15.060000. running mean: -21.289737\n",
      "ep 1431: ep_len:605 episode reward: total was -42.660000. running mean: -21.503440\n",
      "epsilon:0.204532 episode_count: 10024. steps_count: 4433806.000000\n",
      "ep 1432: ep_len:600 episode reward: total was -12.590000. running mean: -21.414306\n",
      "ep 1432: ep_len:201 episode reward: total was -7.380000. running mean: -21.273963\n",
      "ep 1432: ep_len:550 episode reward: total was -28.240000. running mean: -21.343623\n",
      "ep 1432: ep_len:397 episode reward: total was -11.190000. running mean: -21.242087\n",
      "ep 1432: ep_len:3 episode reward: total was 0.000000. running mean: -21.029666\n",
      "ep 1432: ep_len:520 episode reward: total was -51.600000. running mean: -21.335369\n",
      "ep 1432: ep_len:535 episode reward: total was -44.740000. running mean: -21.569416\n",
      "epsilon:0.204395 episode_count: 10031. steps_count: 4436612.000000\n",
      "ep 1433: ep_len:515 episode reward: total was -43.020000. running mean: -21.783921\n",
      "ep 1433: ep_len:176 episode reward: total was -14.400000. running mean: -21.710082\n",
      "ep 1433: ep_len:419 episode reward: total was -13.290000. running mean: -21.625881\n",
      "ep 1433: ep_len:605 episode reward: total was -8.090000. running mean: -21.490523\n",
      "ep 1433: ep_len:3 episode reward: total was 0.000000. running mean: -21.275617\n",
      "ep 1433: ep_len:580 episode reward: total was -53.670000. running mean: -21.599561\n",
      "ep 1433: ep_len:610 episode reward: total was -43.910000. running mean: -21.822666\n",
      "epsilon:0.204259 episode_count: 10038. steps_count: 4439520.000000\n",
      "ep 1434: ep_len:655 episode reward: total was -28.970000. running mean: -21.894139\n",
      "ep 1434: ep_len:500 episode reward: total was 0.200000. running mean: -21.673197\n",
      "ep 1434: ep_len:505 episode reward: total was -27.570000. running mean: -21.732166\n",
      "ep 1434: ep_len:515 episode reward: total was -36.600000. running mean: -21.880844\n",
      "ep 1434: ep_len:3 episode reward: total was 0.000000. running mean: -21.662035\n",
      "ep 1434: ep_len:650 episode reward: total was -40.080000. running mean: -21.846215\n",
      "ep 1434: ep_len:730 episode reward: total was -49.880000. running mean: -22.126553\n",
      "epsilon:0.204122 episode_count: 10045. steps_count: 4443078.000000\n",
      "ep 1435: ep_len:210 episode reward: total was -21.440000. running mean: -22.119687\n",
      "ep 1435: ep_len:550 episode reward: total was -26.980000. running mean: -22.168291\n",
      "ep 1435: ep_len:500 episode reward: total was -23.000000. running mean: -22.176608\n",
      "ep 1435: ep_len:505 episode reward: total was -36.700000. running mean: -22.321842\n",
      "ep 1435: ep_len:3 episode reward: total was 0.000000. running mean: -22.098623\n",
      "ep 1435: ep_len:530 episode reward: total was -44.240000. running mean: -22.320037\n",
      "ep 1435: ep_len:500 episode reward: total was -30.860000. running mean: -22.405437\n",
      "epsilon:0.203986 episode_count: 10052. steps_count: 4445876.000000\n",
      "ep 1436: ep_len:555 episode reward: total was -19.200000. running mean: -22.373382\n",
      "ep 1436: ep_len:570 episode reward: total was -38.250000. running mean: -22.532148\n",
      "ep 1436: ep_len:550 episode reward: total was -47.150000. running mean: -22.778327\n",
      "ep 1436: ep_len:168 episode reward: total was -2.870000. running mean: -22.579244\n",
      "ep 1436: ep_len:109 episode reward: total was -2.960000. running mean: -22.383051\n",
      "ep 1436: ep_len:510 episode reward: total was -36.260000. running mean: -22.521821\n",
      "ep 1436: ep_len:505 episode reward: total was -54.250000. running mean: -22.839102\n",
      "epsilon:0.203849 episode_count: 10059. steps_count: 4448843.000000\n",
      "ep 1437: ep_len:665 episode reward: total was -26.750000. running mean: -22.878211\n",
      "ep 1437: ep_len:183 episode reward: total was -12.900000. running mean: -22.778429\n",
      "ep 1437: ep_len:72 episode reward: total was -1.440000. running mean: -22.565045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1437: ep_len:56 episode reward: total was -2.950000. running mean: -22.368895\n",
      "ep 1437: ep_len:3 episode reward: total was 0.000000. running mean: -22.145206\n",
      "ep 1437: ep_len:246 episode reward: total was -15.880000. running mean: -22.082554\n",
      "ep 1437: ep_len:545 episode reward: total was -25.640000. running mean: -22.118128\n",
      "epsilon:0.203713 episode_count: 10066. steps_count: 4450613.000000\n",
      "ep 1438: ep_len:580 episode reward: total was -27.270000. running mean: -22.169647\n",
      "ep 1438: ep_len:500 episode reward: total was -9.730000. running mean: -22.045250\n",
      "ep 1438: ep_len:595 episode reward: total was -19.510000. running mean: -22.019898\n",
      "ep 1438: ep_len:510 episode reward: total was -15.510000. running mean: -21.954799\n",
      "ep 1438: ep_len:59 episode reward: total was -3.460000. running mean: -21.769851\n",
      "ep 1438: ep_len:565 episode reward: total was -17.650000. running mean: -21.728652\n",
      "ep 1438: ep_len:635 episode reward: total was -35.990000. running mean: -21.871266\n",
      "epsilon:0.203576 episode_count: 10073. steps_count: 4454057.000000\n",
      "ep 1439: ep_len:535 episode reward: total was -21.230000. running mean: -21.864853\n",
      "ep 1439: ep_len:500 episode reward: total was -38.570000. running mean: -22.031905\n",
      "ep 1439: ep_len:560 episode reward: total was -34.070000. running mean: -22.152286\n",
      "ep 1439: ep_len:500 episode reward: total was -16.610000. running mean: -22.096863\n",
      "ep 1439: ep_len:130 episode reward: total was -0.430000. running mean: -21.880194\n",
      "ep 1439: ep_len:580 episode reward: total was -24.660000. running mean: -21.907992\n",
      "ep 1439: ep_len:600 episode reward: total was -31.440000. running mean: -22.003312\n",
      "epsilon:0.203440 episode_count: 10080. steps_count: 4457462.000000\n",
      "ep 1440: ep_len:570 episode reward: total was -49.730000. running mean: -22.280579\n",
      "ep 1440: ep_len:760 episode reward: total was -59.920000. running mean: -22.656973\n",
      "ep 1440: ep_len:535 episode reward: total was -22.220000. running mean: -22.652604\n",
      "ep 1440: ep_len:510 episode reward: total was -30.670000. running mean: -22.732777\n",
      "ep 1440: ep_len:122 episode reward: total was -15.430000. running mean: -22.659750\n",
      "ep 1440: ep_len:261 episode reward: total was -8.390000. running mean: -22.517052\n",
      "ep 1440: ep_len:358 episode reward: total was -11.790000. running mean: -22.409782\n",
      "epsilon:0.203303 episode_count: 10087. steps_count: 4460578.000000\n",
      "ep 1441: ep_len:610 episode reward: total was -51.150000. running mean: -22.697184\n",
      "ep 1441: ep_len:383 episode reward: total was -30.810000. running mean: -22.778312\n",
      "ep 1441: ep_len:500 episode reward: total was -38.580000. running mean: -22.936329\n",
      "ep 1441: ep_len:515 episode reward: total was -23.070000. running mean: -22.937666\n",
      "ep 1441: ep_len:117 episode reward: total was 3.550000. running mean: -22.672789\n",
      "ep 1441: ep_len:605 episode reward: total was -32.420000. running mean: -22.770261\n",
      "ep 1441: ep_len:560 episode reward: total was -45.680000. running mean: -22.999358\n",
      "epsilon:0.203167 episode_count: 10094. steps_count: 4463868.000000\n",
      "ep 1442: ep_len:615 episode reward: total was -9.680000. running mean: -22.866165\n",
      "ep 1442: ep_len:625 episode reward: total was -21.070000. running mean: -22.848203\n",
      "ep 1442: ep_len:540 episode reward: total was -46.050000. running mean: -23.080221\n",
      "ep 1442: ep_len:625 episode reward: total was -24.510000. running mean: -23.094519\n",
      "ep 1442: ep_len:3 episode reward: total was 0.000000. running mean: -22.863574\n",
      "ep 1442: ep_len:225 episode reward: total was -8.920000. running mean: -22.724138\n",
      "ep 1442: ep_len:605 episode reward: total was -34.530000. running mean: -22.842197\n",
      "epsilon:0.203030 episode_count: 10101. steps_count: 4467106.000000\n",
      "ep 1443: ep_len:575 episode reward: total was -53.020000. running mean: -23.143975\n",
      "ep 1443: ep_len:505 episode reward: total was -38.670000. running mean: -23.299235\n",
      "ep 1443: ep_len:575 episode reward: total was -19.610000. running mean: -23.262343\n",
      "ep 1443: ep_len:500 episode reward: total was -16.170000. running mean: -23.191419\n",
      "ep 1443: ep_len:81 episode reward: total was -11.960000. running mean: -23.079105\n",
      "ep 1443: ep_len:500 episode reward: total was -27.610000. running mean: -23.124414\n",
      "ep 1443: ep_len:585 episode reward: total was -22.590000. running mean: -23.119070\n",
      "epsilon:0.202894 episode_count: 10108. steps_count: 4470427.000000\n",
      "ep 1444: ep_len:530 episode reward: total was -23.760000. running mean: -23.125479\n",
      "ep 1444: ep_len:555 episode reward: total was -12.770000. running mean: -23.021924\n",
      "ep 1444: ep_len:640 episode reward: total was -30.810000. running mean: -23.099805\n",
      "ep 1444: ep_len:590 episode reward: total was -9.010000. running mean: -22.958907\n",
      "ep 1444: ep_len:48 episode reward: total was -7.500000. running mean: -22.804318\n",
      "ep 1444: ep_len:296 episode reward: total was -4.880000. running mean: -22.625075\n",
      "ep 1444: ep_len:570 episode reward: total was -37.410000. running mean: -22.772924\n",
      "epsilon:0.202757 episode_count: 10115. steps_count: 4473656.000000\n",
      "ep 1445: ep_len:500 episode reward: total was -19.280000. running mean: -22.737995\n",
      "ep 1445: ep_len:535 episode reward: total was -16.580000. running mean: -22.676415\n",
      "ep 1445: ep_len:427 episode reward: total was -17.770000. running mean: -22.627351\n",
      "ep 1445: ep_len:540 episode reward: total was -0.160000. running mean: -22.402677\n",
      "ep 1445: ep_len:3 episode reward: total was 0.000000. running mean: -22.178650\n",
      "ep 1445: ep_len:228 episode reward: total was 2.620000. running mean: -21.930664\n",
      "ep 1445: ep_len:555 episode reward: total was -36.990000. running mean: -22.081257\n",
      "epsilon:0.202621 episode_count: 10122. steps_count: 4476444.000000\n",
      "ep 1446: ep_len:655 episode reward: total was -43.790000. running mean: -22.298345\n",
      "ep 1446: ep_len:241 episode reward: total was -23.400000. running mean: -22.309361\n",
      "ep 1446: ep_len:505 episode reward: total was -28.730000. running mean: -22.373568\n",
      "ep 1446: ep_len:560 episode reward: total was -8.090000. running mean: -22.230732\n",
      "ep 1446: ep_len:113 episode reward: total was -14.960000. running mean: -22.158025\n",
      "ep 1446: ep_len:500 episode reward: total was -8.780000. running mean: -22.024244\n",
      "ep 1446: ep_len:555 episode reward: total was -45.100000. running mean: -22.255002\n",
      "epsilon:0.202484 episode_count: 10129. steps_count: 4479573.000000\n",
      "ep 1447: ep_len:125 episode reward: total was -1.450000. running mean: -22.046952\n",
      "ep 1447: ep_len:540 episode reward: total was -26.260000. running mean: -22.089082\n",
      "ep 1447: ep_len:570 episode reward: total was -13.120000. running mean: -21.999392\n",
      "ep 1447: ep_len:530 episode reward: total was -30.650000. running mean: -22.085898\n",
      "ep 1447: ep_len:110 episode reward: total was 1.060000. running mean: -21.854439\n",
      "ep 1447: ep_len:590 episode reward: total was -6.120000. running mean: -21.697094\n",
      "ep 1447: ep_len:580 episode reward: total was -27.110000. running mean: -21.751223\n",
      "epsilon:0.202348 episode_count: 10136. steps_count: 4482618.000000\n",
      "ep 1448: ep_len:680 episode reward: total was -11.630000. running mean: -21.650011\n",
      "ep 1448: ep_len:500 episode reward: total was -20.050000. running mean: -21.634011\n",
      "ep 1448: ep_len:610 episode reward: total was -10.640000. running mean: -21.524071\n",
      "ep 1448: ep_len:500 episode reward: total was -37.200000. running mean: -21.680830\n",
      "ep 1448: ep_len:98 episode reward: total was -10.950000. running mean: -21.573522\n",
      "ep 1448: ep_len:500 episode reward: total was -21.460000. running mean: -21.572387\n",
      "ep 1448: ep_len:294 episode reward: total was -14.840000. running mean: -21.505063\n",
      "epsilon:0.202211 episode_count: 10143. steps_count: 4485800.000000\n",
      "ep 1449: ep_len:192 episode reward: total was -16.840000. running mean: -21.458412\n",
      "ep 1449: ep_len:590 episode reward: total was -9.750000. running mean: -21.341328\n",
      "ep 1449: ep_len:575 episode reward: total was -35.450000. running mean: -21.482415\n",
      "ep 1449: ep_len:535 episode reward: total was -22.040000. running mean: -21.487991\n",
      "ep 1449: ep_len:3 episode reward: total was 0.000000. running mean: -21.273111\n",
      "ep 1449: ep_len:565 episode reward: total was -32.670000. running mean: -21.387080\n",
      "ep 1449: ep_len:615 episode reward: total was -23.880000. running mean: -21.412009\n",
      "epsilon:0.202075 episode_count: 10150. steps_count: 4488875.000000\n",
      "ep 1450: ep_len:207 episode reward: total was -3.940000. running mean: -21.237289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1450: ep_len:500 episode reward: total was -16.710000. running mean: -21.192016\n",
      "ep 1450: ep_len:600 episode reward: total was -40.140000. running mean: -21.381496\n",
      "ep 1450: ep_len:170 episode reward: total was -3.900000. running mean: -21.206681\n",
      "ep 1450: ep_len:38 episode reward: total was -7.000000. running mean: -21.064614\n",
      "ep 1450: ep_len:525 episode reward: total was -20.580000. running mean: -21.059768\n",
      "ep 1450: ep_len:327 episode reward: total was -35.430000. running mean: -21.203470\n",
      "epsilon:0.201938 episode_count: 10157. steps_count: 4491242.000000\n",
      "ep 1451: ep_len:217 episode reward: total was -3.890000. running mean: -21.030335\n",
      "ep 1451: ep_len:300 episode reward: total was -44.900000. running mean: -21.269032\n",
      "ep 1451: ep_len:69 episode reward: total was -4.480000. running mean: -21.101142\n",
      "ep 1451: ep_len:610 episode reward: total was -21.650000. running mean: -21.106630\n",
      "ep 1451: ep_len:35 episode reward: total was -1.490000. running mean: -20.910464\n",
      "ep 1451: ep_len:595 episode reward: total was -29.260000. running mean: -20.993959\n",
      "ep 1451: ep_len:745 episode reward: total was -38.740000. running mean: -21.171420\n",
      "epsilon:0.201802 episode_count: 10164. steps_count: 4493813.000000\n",
      "ep 1452: ep_len:590 episode reward: total was -32.820000. running mean: -21.287906\n",
      "ep 1452: ep_len:340 episode reward: total was -52.880000. running mean: -21.603827\n",
      "ep 1452: ep_len:79 episode reward: total was -1.950000. running mean: -21.407288\n",
      "ep 1452: ep_len:565 episode reward: total was -4.520000. running mean: -21.238415\n",
      "ep 1452: ep_len:3 episode reward: total was 0.000000. running mean: -21.026031\n",
      "ep 1452: ep_len:575 episode reward: total was -19.650000. running mean: -21.012271\n",
      "ep 1452: ep_len:500 episode reward: total was -25.520000. running mean: -21.057348\n",
      "epsilon:0.201665 episode_count: 10171. steps_count: 4496465.000000\n",
      "ep 1453: ep_len:515 episode reward: total was -29.420000. running mean: -21.140975\n",
      "ep 1453: ep_len:500 episode reward: total was -15.320000. running mean: -21.082765\n",
      "ep 1453: ep_len:540 episode reward: total was -26.600000. running mean: -21.137937\n",
      "ep 1453: ep_len:152 episode reward: total was -5.890000. running mean: -20.985458\n",
      "ep 1453: ep_len:91 episode reward: total was -3.450000. running mean: -20.810103\n",
      "ep 1453: ep_len:545 episode reward: total was -28.130000. running mean: -20.883302\n",
      "ep 1453: ep_len:500 episode reward: total was -32.750000. running mean: -21.001969\n",
      "epsilon:0.201529 episode_count: 10178. steps_count: 4499308.000000\n",
      "ep 1454: ep_len:600 episode reward: total was -13.670000. running mean: -20.928650\n",
      "ep 1454: ep_len:520 episode reward: total was -21.510000. running mean: -20.934463\n",
      "ep 1454: ep_len:500 episode reward: total was -39.610000. running mean: -21.121218\n",
      "ep 1454: ep_len:535 episode reward: total was -16.630000. running mean: -21.076306\n",
      "ep 1454: ep_len:3 episode reward: total was 0.000000. running mean: -20.865543\n",
      "ep 1454: ep_len:510 episode reward: total was -19.520000. running mean: -20.852088\n",
      "ep 1454: ep_len:301 episode reward: total was -18.860000. running mean: -20.832167\n",
      "epsilon:0.201392 episode_count: 10185. steps_count: 4502277.000000\n",
      "ep 1455: ep_len:134 episode reward: total was -2.450000. running mean: -20.648345\n",
      "ep 1455: ep_len:645 episode reward: total was -50.050000. running mean: -20.942362\n",
      "ep 1455: ep_len:655 episode reward: total was -21.720000. running mean: -20.950138\n",
      "ep 1455: ep_len:590 episode reward: total was -25.460000. running mean: -20.995237\n",
      "ep 1455: ep_len:75 episode reward: total was -9.950000. running mean: -20.884784\n",
      "ep 1455: ep_len:520 episode reward: total was -46.090000. running mean: -21.136837\n",
      "ep 1455: ep_len:565 episode reward: total was -19.180000. running mean: -21.117268\n",
      "epsilon:0.201256 episode_count: 10192. steps_count: 4505461.000000\n",
      "ep 1456: ep_len:640 episode reward: total was -28.850000. running mean: -21.194596\n",
      "ep 1456: ep_len:535 episode reward: total was -36.210000. running mean: -21.344750\n",
      "ep 1456: ep_len:645 episode reward: total was -109.860000. running mean: -22.229902\n",
      "ep 1456: ep_len:525 episode reward: total was -18.720000. running mean: -22.194803\n",
      "ep 1456: ep_len:3 episode reward: total was 0.000000. running mean: -21.972855\n",
      "ep 1456: ep_len:500 episode reward: total was -29.360000. running mean: -22.046727\n",
      "ep 1456: ep_len:520 episode reward: total was -23.410000. running mean: -22.060359\n",
      "epsilon:0.201119 episode_count: 10199. steps_count: 4508829.000000\n",
      "ep 1457: ep_len:134 episode reward: total was -7.470000. running mean: -21.914456\n",
      "ep 1457: ep_len:525 episode reward: total was -18.610000. running mean: -21.881411\n",
      "ep 1457: ep_len:640 episode reward: total was -38.370000. running mean: -22.046297\n",
      "ep 1457: ep_len:530 episode reward: total was -9.160000. running mean: -21.917434\n",
      "ep 1457: ep_len:99 episode reward: total was -2.460000. running mean: -21.722860\n",
      "ep 1457: ep_len:500 episode reward: total was -40.850000. running mean: -21.914131\n",
      "ep 1457: ep_len:525 episode reward: total was -56.840000. running mean: -22.263390\n",
      "epsilon:0.200983 episode_count: 10206. steps_count: 4511782.000000\n",
      "ep 1458: ep_len:500 episode reward: total was -11.330000. running mean: -22.154056\n",
      "ep 1458: ep_len:500 episode reward: total was -3.850000. running mean: -21.971015\n",
      "ep 1458: ep_len:349 episode reward: total was -30.880000. running mean: -22.060105\n",
      "ep 1458: ep_len:163 episode reward: total was -4.400000. running mean: -21.883504\n",
      "ep 1458: ep_len:94 episode reward: total was 5.550000. running mean: -21.609169\n",
      "ep 1458: ep_len:595 episode reward: total was -19.640000. running mean: -21.589477\n",
      "ep 1458: ep_len:500 episode reward: total was -30.070000. running mean: -21.674283\n",
      "epsilon:0.200846 episode_count: 10213. steps_count: 4514483.000000\n",
      "ep 1459: ep_len:600 episode reward: total was -33.650000. running mean: -21.794040\n",
      "ep 1459: ep_len:585 episode reward: total was -29.540000. running mean: -21.871499\n",
      "ep 1459: ep_len:500 episode reward: total was -19.490000. running mean: -21.847684\n",
      "ep 1459: ep_len:537 episode reward: total was -44.110000. running mean: -22.070308\n",
      "ep 1459: ep_len:3 episode reward: total was 0.000000. running mean: -21.849604\n",
      "ep 1459: ep_len:585 episode reward: total was -44.240000. running mean: -22.073508\n",
      "ep 1459: ep_len:505 episode reward: total was -29.230000. running mean: -22.145073\n",
      "epsilon:0.200710 episode_count: 10220. steps_count: 4517798.000000\n",
      "ep 1460: ep_len:124 episode reward: total was -2.930000. running mean: -21.952923\n",
      "ep 1460: ep_len:500 episode reward: total was -35.680000. running mean: -22.090193\n",
      "ep 1460: ep_len:540 episode reward: total was -41.740000. running mean: -22.286691\n",
      "ep 1460: ep_len:515 episode reward: total was -24.590000. running mean: -22.309725\n",
      "ep 1460: ep_len:3 episode reward: total was 0.000000. running mean: -22.086627\n",
      "ep 1460: ep_len:610 episode reward: total was -32.580000. running mean: -22.191561\n",
      "ep 1460: ep_len:585 episode reward: total was -44.210000. running mean: -22.411745\n",
      "epsilon:0.200573 episode_count: 10227. steps_count: 4520675.000000\n",
      "ep 1461: ep_len:229 episode reward: total was -24.870000. running mean: -22.436328\n",
      "ep 1461: ep_len:550 episode reward: total was -29.090000. running mean: -22.502865\n",
      "ep 1461: ep_len:79 episode reward: total was -5.960000. running mean: -22.337436\n",
      "ep 1461: ep_len:46 episode reward: total was -1.440000. running mean: -22.128462\n",
      "ep 1461: ep_len:110 episode reward: total was -14.470000. running mean: -22.051877\n",
      "ep 1461: ep_len:261 episode reward: total was 1.160000. running mean: -21.819758\n",
      "ep 1461: ep_len:510 episode reward: total was -13.120000. running mean: -21.732761\n",
      "epsilon:0.200437 episode_count: 10234. steps_count: 4522460.000000\n",
      "ep 1462: ep_len:600 episode reward: total was -15.670000. running mean: -21.672133\n",
      "ep 1462: ep_len:201 episode reward: total was -5.360000. running mean: -21.509012\n",
      "ep 1462: ep_len:500 episode reward: total was -31.830000. running mean: -21.612222\n",
      "ep 1462: ep_len:500 episode reward: total was -32.660000. running mean: -21.722699\n",
      "ep 1462: ep_len:77 episode reward: total was 1.540000. running mean: -21.490072\n",
      "ep 1462: ep_len:515 episode reward: total was -22.010000. running mean: -21.495272\n",
      "ep 1462: ep_len:650 episode reward: total was -17.810000. running mean: -21.458419\n",
      "epsilon:0.200300 episode_count: 10241. steps_count: 4525503.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1463: ep_len:540 episode reward: total was -18.140000. running mean: -21.425235\n",
      "ep 1463: ep_len:510 episode reward: total was -27.500000. running mean: -21.485982\n",
      "ep 1463: ep_len:715 episode reward: total was -58.430000. running mean: -21.855423\n",
      "ep 1463: ep_len:132 episode reward: total was -1.410000. running mean: -21.650968\n",
      "ep 1463: ep_len:3 episode reward: total was 0.000000. running mean: -21.434459\n",
      "ep 1463: ep_len:525 episode reward: total was -29.740000. running mean: -21.517514\n",
      "ep 1463: ep_len:315 episode reward: total was -14.850000. running mean: -21.450839\n",
      "epsilon:0.200164 episode_count: 10248. steps_count: 4528243.000000\n",
      "ep 1464: ep_len:250 episode reward: total was -5.380000. running mean: -21.290131\n",
      "ep 1464: ep_len:368 episode reward: total was -25.850000. running mean: -21.335729\n",
      "ep 1464: ep_len:75 episode reward: total was -5.470000. running mean: -21.177072\n",
      "ep 1464: ep_len:505 episode reward: total was -9.210000. running mean: -21.057401\n",
      "ep 1464: ep_len:3 episode reward: total was 0.000000. running mean: -20.846827\n",
      "ep 1464: ep_len:565 episode reward: total was -11.290000. running mean: -20.751259\n",
      "ep 1464: ep_len:347 episode reward: total was -10.310000. running mean: -20.646846\n",
      "epsilon:0.200027 episode_count: 10255. steps_count: 4530356.000000\n",
      "ep 1465: ep_len:550 episode reward: total was -37.950000. running mean: -20.819878\n",
      "ep 1465: ep_len:535 episode reward: total was -29.700000. running mean: -20.908679\n",
      "ep 1465: ep_len:615 episode reward: total was -6.560000. running mean: -20.765192\n",
      "ep 1465: ep_len:129 episode reward: total was 1.590000. running mean: -20.541640\n",
      "ep 1465: ep_len:97 episode reward: total was 2.040000. running mean: -20.315824\n",
      "ep 1465: ep_len:505 episode reward: total was -13.100000. running mean: -20.243666\n",
      "ep 1465: ep_len:575 episode reward: total was -19.580000. running mean: -20.237029\n",
      "epsilon:0.199891 episode_count: 10262. steps_count: 4533362.000000\n",
      "ep 1466: ep_len:500 episode reward: total was -13.670000. running mean: -20.171359\n",
      "ep 1466: ep_len:500 episode reward: total was -34.150000. running mean: -20.311145\n",
      "ep 1466: ep_len:365 episode reward: total was -15.900000. running mean: -20.267034\n",
      "ep 1466: ep_len:550 episode reward: total was -54.320000. running mean: -20.607563\n",
      "ep 1466: ep_len:3 episode reward: total was 0.000000. running mean: -20.401488\n",
      "ep 1466: ep_len:515 episode reward: total was -41.050000. running mean: -20.607973\n",
      "ep 1466: ep_len:605 episode reward: total was -35.440000. running mean: -20.756293\n",
      "epsilon:0.199754 episode_count: 10269. steps_count: 4536400.000000\n",
      "ep 1467: ep_len:134 episode reward: total was -0.950000. running mean: -20.558230\n",
      "ep 1467: ep_len:505 episode reward: total was -4.880000. running mean: -20.401448\n",
      "ep 1467: ep_len:545 episode reward: total was -31.500000. running mean: -20.512434\n",
      "ep 1467: ep_len:144 episode reward: total was -3.420000. running mean: -20.341509\n",
      "ep 1467: ep_len:85 episode reward: total was -3.950000. running mean: -20.177594\n",
      "ep 1467: ep_len:695 episode reward: total was -68.440000. running mean: -20.660218\n",
      "ep 1467: ep_len:575 episode reward: total was -39.100000. running mean: -20.844616\n",
      "epsilon:0.199618 episode_count: 10276. steps_count: 4539083.000000\n",
      "ep 1468: ep_len:500 episode reward: total was -17.670000. running mean: -20.812870\n",
      "ep 1468: ep_len:540 episode reward: total was -37.180000. running mean: -20.976541\n",
      "ep 1468: ep_len:695 episode reward: total was -51.940000. running mean: -21.286176\n",
      "ep 1468: ep_len:500 episode reward: total was -21.110000. running mean: -21.284414\n",
      "ep 1468: ep_len:3 episode reward: total was 0.000000. running mean: -21.071570\n",
      "ep 1468: ep_len:535 episode reward: total was -19.140000. running mean: -21.052254\n",
      "ep 1468: ep_len:520 episode reward: total was -19.890000. running mean: -21.040632\n",
      "epsilon:0.199481 episode_count: 10283. steps_count: 4542376.000000\n",
      "ep 1469: ep_len:206 episode reward: total was -1.430000. running mean: -20.844525\n",
      "ep 1469: ep_len:505 episode reward: total was -26.690000. running mean: -20.902980\n",
      "ep 1469: ep_len:610 episode reward: total was -52.110000. running mean: -21.215050\n",
      "ep 1469: ep_len:515 episode reward: total was -40.670000. running mean: -21.409600\n",
      "ep 1469: ep_len:109 episode reward: total was -11.450000. running mean: -21.310004\n",
      "ep 1469: ep_len:540 episode reward: total was -34.580000. running mean: -21.442704\n",
      "ep 1469: ep_len:500 episode reward: total was -15.650000. running mean: -21.384777\n",
      "epsilon:0.199345 episode_count: 10290. steps_count: 4545361.000000\n",
      "ep 1470: ep_len:600 episode reward: total was -34.630000. running mean: -21.517229\n",
      "ep 1470: ep_len:575 episode reward: total was -17.210000. running mean: -21.474157\n",
      "ep 1470: ep_len:555 episode reward: total was -21.650000. running mean: -21.475915\n",
      "ep 1470: ep_len:585 episode reward: total was -10.170000. running mean: -21.362856\n",
      "ep 1470: ep_len:3 episode reward: total was 0.000000. running mean: -21.149227\n",
      "ep 1470: ep_len:650 episode reward: total was -33.620000. running mean: -21.273935\n",
      "ep 1470: ep_len:595 episode reward: total was -14.020000. running mean: -21.201396\n",
      "epsilon:0.199208 episode_count: 10297. steps_count: 4548924.000000\n",
      "ep 1471: ep_len:615 episode reward: total was -44.540000. running mean: -21.434782\n",
      "ep 1471: ep_len:600 episode reward: total was -29.210000. running mean: -21.512534\n",
      "ep 1471: ep_len:451 episode reward: total was -10.700000. running mean: -21.404409\n",
      "ep 1471: ep_len:505 episode reward: total was -2.630000. running mean: -21.216664\n",
      "ep 1471: ep_len:3 episode reward: total was 0.000000. running mean: -21.004498\n",
      "ep 1471: ep_len:550 episode reward: total was -20.340000. running mean: -20.997853\n",
      "ep 1471: ep_len:515 episode reward: total was -47.660000. running mean: -21.264474\n",
      "epsilon:0.199072 episode_count: 10304. steps_count: 4552163.000000\n",
      "ep 1472: ep_len:655 episode reward: total was -11.950000. running mean: -21.171330\n",
      "ep 1472: ep_len:605 episode reward: total was -11.230000. running mean: -21.071916\n",
      "ep 1472: ep_len:530 episode reward: total was -21.180000. running mean: -21.072997\n",
      "ep 1472: ep_len:605 episode reward: total was -34.760000. running mean: -21.209867\n",
      "ep 1472: ep_len:3 episode reward: total was 0.000000. running mean: -20.997768\n",
      "ep 1472: ep_len:635 episode reward: total was -14.600000. running mean: -20.933791\n",
      "ep 1472: ep_len:565 episode reward: total was -31.450000. running mean: -21.038953\n",
      "epsilon:0.198935 episode_count: 10311. steps_count: 4555761.000000\n",
      "ep 1473: ep_len:515 episode reward: total was -20.310000. running mean: -21.031663\n",
      "ep 1473: ep_len:500 episode reward: total was -12.510000. running mean: -20.946447\n",
      "ep 1473: ep_len:500 episode reward: total was -24.300000. running mean: -20.979982\n",
      "ep 1473: ep_len:530 episode reward: total was -2.560000. running mean: -20.795782\n",
      "ep 1473: ep_len:3 episode reward: total was 0.000000. running mean: -20.587825\n",
      "ep 1473: ep_len:545 episode reward: total was -20.620000. running mean: -20.588146\n",
      "ep 1473: ep_len:500 episode reward: total was -27.350000. running mean: -20.655765\n",
      "epsilon:0.198799 episode_count: 10318. steps_count: 4558854.000000\n",
      "ep 1474: ep_len:500 episode reward: total was -15.410000. running mean: -20.603307\n",
      "ep 1474: ep_len:525 episode reward: total was -5.820000. running mean: -20.455474\n",
      "ep 1474: ep_len:640 episode reward: total was -36.570000. running mean: -20.616619\n",
      "ep 1474: ep_len:520 episode reward: total was -24.040000. running mean: -20.650853\n",
      "ep 1474: ep_len:82 episode reward: total was -0.960000. running mean: -20.453945\n",
      "ep 1474: ep_len:570 episode reward: total was -14.800000. running mean: -20.397405\n",
      "ep 1474: ep_len:545 episode reward: total was -13.100000. running mean: -20.324431\n",
      "epsilon:0.198662 episode_count: 10325. steps_count: 4562236.000000\n",
      "ep 1475: ep_len:560 episode reward: total was -27.670000. running mean: -20.397887\n",
      "ep 1475: ep_len:500 episode reward: total was -16.840000. running mean: -20.362308\n",
      "ep 1475: ep_len:535 episode reward: total was -55.140000. running mean: -20.710085\n",
      "ep 1475: ep_len:500 episode reward: total was -12.210000. running mean: -20.625084\n",
      "ep 1475: ep_len:3 episode reward: total was 0.000000. running mean: -20.418833\n",
      "ep 1475: ep_len:595 episode reward: total was -52.030000. running mean: -20.734945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1475: ep_len:555 episode reward: total was -53.090000. running mean: -21.058495\n",
      "epsilon:0.198526 episode_count: 10332. steps_count: 4565484.000000\n",
      "ep 1476: ep_len:134 episode reward: total was -2.450000. running mean: -20.872411\n",
      "ep 1476: ep_len:690 episode reward: total was -12.490000. running mean: -20.788586\n",
      "ep 1476: ep_len:388 episode reward: total was -33.400000. running mean: -20.914701\n",
      "ep 1476: ep_len:530 episode reward: total was -28.060000. running mean: -20.986154\n",
      "ep 1476: ep_len:103 episode reward: total was 3.030000. running mean: -20.745992\n",
      "ep 1476: ep_len:600 episode reward: total was -7.020000. running mean: -20.608732\n",
      "ep 1476: ep_len:505 episode reward: total was -19.590000. running mean: -20.598545\n",
      "epsilon:0.198389 episode_count: 10339. steps_count: 4568434.000000\n",
      "ep 1477: ep_len:540 episode reward: total was -10.680000. running mean: -20.499359\n",
      "ep 1477: ep_len:535 episode reward: total was -24.450000. running mean: -20.538866\n",
      "ep 1477: ep_len:570 episode reward: total was -23.940000. running mean: -20.572877\n",
      "ep 1477: ep_len:500 episode reward: total was -38.020000. running mean: -20.747348\n",
      "ep 1477: ep_len:3 episode reward: total was 0.000000. running mean: -20.539875\n",
      "ep 1477: ep_len:580 episode reward: total was -17.660000. running mean: -20.511076\n",
      "ep 1477: ep_len:283 episode reward: total was -34.340000. running mean: -20.649365\n",
      "epsilon:0.198253 episode_count: 10346. steps_count: 4571445.000000\n",
      "ep 1478: ep_len:555 episode reward: total was -12.950000. running mean: -20.572372\n",
      "ep 1478: ep_len:505 episode reward: total was -4.170000. running mean: -20.408348\n",
      "ep 1478: ep_len:565 episode reward: total was -49.010000. running mean: -20.694364\n",
      "ep 1478: ep_len:419 episode reward: total was -8.200000. running mean: -20.569421\n",
      "ep 1478: ep_len:90 episode reward: total was 2.550000. running mean: -20.338227\n",
      "ep 1478: ep_len:625 episode reward: total was -16.760000. running mean: -20.302444\n",
      "ep 1478: ep_len:500 episode reward: total was -28.640000. running mean: -20.385820\n",
      "epsilon:0.198116 episode_count: 10353. steps_count: 4574704.000000\n",
      "ep 1479: ep_len:595 episode reward: total was -20.720000. running mean: -20.389162\n",
      "ep 1479: ep_len:660 episode reward: total was -24.090000. running mean: -20.426170\n",
      "ep 1479: ep_len:575 episode reward: total was -27.890000. running mean: -20.500808\n",
      "ep 1479: ep_len:560 episode reward: total was -22.630000. running mean: -20.522100\n",
      "ep 1479: ep_len:3 episode reward: total was 0.000000. running mean: -20.316879\n",
      "ep 1479: ep_len:615 episode reward: total was -10.920000. running mean: -20.222910\n",
      "ep 1479: ep_len:540 episode reward: total was -14.400000. running mean: -20.164681\n",
      "epsilon:0.197980 episode_count: 10360. steps_count: 4578252.000000\n",
      "ep 1480: ep_len:640 episode reward: total was -31.300000. running mean: -20.276035\n",
      "ep 1480: ep_len:595 episode reward: total was -23.810000. running mean: -20.311374\n",
      "ep 1480: ep_len:385 episode reward: total was -17.330000. running mean: -20.281560\n",
      "ep 1480: ep_len:56 episode reward: total was -1.470000. running mean: -20.093445\n",
      "ep 1480: ep_len:88 episode reward: total was 0.550000. running mean: -19.887010\n",
      "ep 1480: ep_len:620 episode reward: total was -19.140000. running mean: -19.879540\n",
      "ep 1480: ep_len:615 episode reward: total was -19.100000. running mean: -19.871745\n",
      "epsilon:0.197843 episode_count: 10367. steps_count: 4581251.000000\n",
      "ep 1481: ep_len:213 episode reward: total was -6.900000. running mean: -19.742027\n",
      "ep 1481: ep_len:660 episode reward: total was -40.840000. running mean: -19.953007\n",
      "ep 1481: ep_len:645 episode reward: total was -16.980000. running mean: -19.923277\n",
      "ep 1481: ep_len:545 episode reward: total was -14.680000. running mean: -19.870844\n",
      "ep 1481: ep_len:3 episode reward: total was 0.000000. running mean: -19.672136\n",
      "ep 1481: ep_len:570 episode reward: total was -31.500000. running mean: -19.790415\n",
      "ep 1481: ep_len:530 episode reward: total was -36.540000. running mean: -19.957910\n",
      "epsilon:0.197707 episode_count: 10374. steps_count: 4584417.000000\n",
      "ep 1482: ep_len:510 episode reward: total was -22.680000. running mean: -19.985131\n",
      "ep 1482: ep_len:500 episode reward: total was -18.640000. running mean: -19.971680\n",
      "ep 1482: ep_len:610 episode reward: total was -30.660000. running mean: -20.078563\n",
      "ep 1482: ep_len:500 episode reward: total was -48.790000. running mean: -20.365678\n",
      "ep 1482: ep_len:3 episode reward: total was 0.000000. running mean: -20.162021\n",
      "ep 1482: ep_len:610 episode reward: total was -22.250000. running mean: -20.182901\n",
      "ep 1482: ep_len:291 episode reward: total was -16.870000. running mean: -20.149772\n",
      "epsilon:0.197570 episode_count: 10381. steps_count: 4587441.000000\n",
      "ep 1483: ep_len:555 episode reward: total was -39.390000. running mean: -20.342174\n",
      "ep 1483: ep_len:530 episode reward: total was -15.470000. running mean: -20.293452\n",
      "ep 1483: ep_len:595 episode reward: total was -42.380000. running mean: -20.514318\n",
      "ep 1483: ep_len:102 episode reward: total was -1.950000. running mean: -20.328674\n",
      "ep 1483: ep_len:3 episode reward: total was 0.000000. running mean: -20.125388\n",
      "ep 1483: ep_len:655 episode reward: total was -22.880000. running mean: -20.152934\n",
      "ep 1483: ep_len:545 episode reward: total was -34.440000. running mean: -20.295804\n",
      "epsilon:0.197434 episode_count: 10388. steps_count: 4590426.000000\n",
      "ep 1484: ep_len:229 episode reward: total was -3.930000. running mean: -20.132146\n",
      "ep 1484: ep_len:500 episode reward: total was -23.510000. running mean: -20.165925\n",
      "ep 1484: ep_len:585 episode reward: total was -2.080000. running mean: -19.985066\n",
      "ep 1484: ep_len:157 episode reward: total was -3.390000. running mean: -19.819115\n",
      "ep 1484: ep_len:3 episode reward: total was 0.000000. running mean: -19.620924\n",
      "ep 1484: ep_len:500 episode reward: total was -32.850000. running mean: -19.753215\n",
      "ep 1484: ep_len:615 episode reward: total was -29.110000. running mean: -19.846783\n",
      "epsilon:0.197297 episode_count: 10395. steps_count: 4593015.000000\n",
      "ep 1485: ep_len:900 episode reward: total was -60.670000. running mean: -20.255015\n",
      "ep 1485: ep_len:332 episode reward: total was -16.350000. running mean: -20.215965\n",
      "ep 1485: ep_len:530 episode reward: total was -28.530000. running mean: -20.299105\n",
      "ep 1485: ep_len:585 episode reward: total was -44.180000. running mean: -20.537914\n",
      "ep 1485: ep_len:3 episode reward: total was 0.000000. running mean: -20.332535\n",
      "ep 1485: ep_len:530 episode reward: total was -27.220000. running mean: -20.401409\n",
      "ep 1485: ep_len:297 episode reward: total was -18.880000. running mean: -20.386195\n",
      "epsilon:0.197161 episode_count: 10402. steps_count: 4596192.000000\n",
      "ep 1486: ep_len:565 episode reward: total was -17.700000. running mean: -20.359333\n",
      "ep 1486: ep_len:262 episode reward: total was -12.910000. running mean: -20.284840\n",
      "ep 1486: ep_len:505 episode reward: total was -65.720000. running mean: -20.739192\n",
      "ep 1486: ep_len:167 episode reward: total was -0.920000. running mean: -20.541000\n",
      "ep 1486: ep_len:3 episode reward: total was 0.000000. running mean: -20.335590\n",
      "ep 1486: ep_len:168 episode reward: total was -4.460000. running mean: -20.176834\n",
      "ep 1486: ep_len:600 episode reward: total was -37.500000. running mean: -20.350065\n",
      "epsilon:0.197024 episode_count: 10409. steps_count: 4598462.000000\n",
      "ep 1487: ep_len:229 episode reward: total was -5.430000. running mean: -20.200865\n",
      "ep 1487: ep_len:500 episode reward: total was -42.650000. running mean: -20.425356\n",
      "ep 1487: ep_len:740 episode reward: total was -38.340000. running mean: -20.604503\n",
      "ep 1487: ep_len:615 episode reward: total was -3.100000. running mean: -20.429458\n",
      "ep 1487: ep_len:40 episode reward: total was 2.500000. running mean: -20.200163\n",
      "ep 1487: ep_len:585 episode reward: total was -31.690000. running mean: -20.315061\n",
      "ep 1487: ep_len:535 episode reward: total was -17.350000. running mean: -20.285411\n",
      "epsilon:0.196888 episode_count: 10416. steps_count: 4601706.000000\n",
      "ep 1488: ep_len:690 episode reward: total was -26.240000. running mean: -20.344957\n",
      "ep 1488: ep_len:545 episode reward: total was -28.960000. running mean: -20.431107\n",
      "ep 1488: ep_len:585 episode reward: total was -40.420000. running mean: -20.630996\n",
      "ep 1488: ep_len:520 episode reward: total was -18.070000. running mean: -20.605386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1488: ep_len:3 episode reward: total was 0.000000. running mean: -20.399332\n",
      "ep 1488: ep_len:500 episode reward: total was -20.700000. running mean: -20.402339\n",
      "ep 1488: ep_len:560 episode reward: total was -21.560000. running mean: -20.413915\n",
      "epsilon:0.196751 episode_count: 10423. steps_count: 4605109.000000\n",
      "ep 1489: ep_len:132 episode reward: total was -7.470000. running mean: -20.284476\n",
      "ep 1489: ep_len:545 episode reward: total was -36.000000. running mean: -20.441632\n",
      "ep 1489: ep_len:79 episode reward: total was -4.980000. running mean: -20.287015\n",
      "ep 1489: ep_len:131 episode reward: total was -1.950000. running mean: -20.103645\n",
      "ep 1489: ep_len:3 episode reward: total was 0.000000. running mean: -19.902609\n",
      "ep 1489: ep_len:630 episode reward: total was -23.330000. running mean: -19.936883\n",
      "ep 1489: ep_len:600 episode reward: total was -23.560000. running mean: -19.973114\n",
      "epsilon:0.196615 episode_count: 10430. steps_count: 4607229.000000\n",
      "ep 1490: ep_len:610 episode reward: total was -14.110000. running mean: -19.914483\n",
      "ep 1490: ep_len:500 episode reward: total was -23.540000. running mean: -19.950738\n",
      "ep 1490: ep_len:775 episode reward: total was -63.810000. running mean: -20.389330\n",
      "ep 1490: ep_len:53 episode reward: total was -0.960000. running mean: -20.195037\n",
      "ep 1490: ep_len:44 episode reward: total was 2.500000. running mean: -19.968087\n",
      "ep 1490: ep_len:575 episode reward: total was -45.520000. running mean: -20.223606\n",
      "ep 1490: ep_len:500 episode reward: total was -19.580000. running mean: -20.217170\n",
      "epsilon:0.196478 episode_count: 10437. steps_count: 4610286.000000\n",
      "ep 1491: ep_len:500 episode reward: total was -27.500000. running mean: -20.289998\n",
      "ep 1491: ep_len:945 episode reward: total was -93.790000. running mean: -21.024998\n",
      "ep 1491: ep_len:510 episode reward: total was -16.220000. running mean: -20.976948\n",
      "ep 1491: ep_len:505 episode reward: total was -29.120000. running mean: -21.058379\n",
      "ep 1491: ep_len:3 episode reward: total was 0.000000. running mean: -20.847795\n",
      "ep 1491: ep_len:580 episode reward: total was -28.340000. running mean: -20.922717\n",
      "ep 1491: ep_len:575 episode reward: total was -22.060000. running mean: -20.934090\n",
      "epsilon:0.196342 episode_count: 10444. steps_count: 4613904.000000\n",
      "ep 1492: ep_len:210 episode reward: total was -0.410000. running mean: -20.728849\n",
      "ep 1492: ep_len:530 episode reward: total was -18.690000. running mean: -20.708460\n",
      "ep 1492: ep_len:585 episode reward: total was -15.460000. running mean: -20.655976\n",
      "ep 1492: ep_len:640 episode reward: total was -46.500000. running mean: -20.914416\n",
      "ep 1492: ep_len:106 episode reward: total was 1.540000. running mean: -20.689872\n",
      "ep 1492: ep_len:500 episode reward: total was -52.630000. running mean: -21.009273\n",
      "ep 1492: ep_len:540 episode reward: total was -18.900000. running mean: -20.988180\n",
      "epsilon:0.196205 episode_count: 10451. steps_count: 4617015.000000\n",
      "ep 1493: ep_len:575 episode reward: total was -17.640000. running mean: -20.954699\n",
      "ep 1493: ep_len:795 episode reward: total was -94.680000. running mean: -21.691952\n",
      "ep 1493: ep_len:635 episode reward: total was -29.790000. running mean: -21.772932\n",
      "ep 1493: ep_len:555 episode reward: total was -0.130000. running mean: -21.556503\n",
      "ep 1493: ep_len:105 episode reward: total was -6.960000. running mean: -21.410538\n",
      "ep 1493: ep_len:735 episode reward: total was -47.360000. running mean: -21.670032\n",
      "ep 1493: ep_len:570 episode reward: total was -30.440000. running mean: -21.757732\n",
      "epsilon:0.196069 episode_count: 10458. steps_count: 4620985.000000\n",
      "ep 1494: ep_len:670 episode reward: total was -33.260000. running mean: -21.872755\n",
      "ep 1494: ep_len:500 episode reward: total was -33.130000. running mean: -21.985327\n",
      "ep 1494: ep_len:550 episode reward: total was -29.030000. running mean: -22.055774\n",
      "ep 1494: ep_len:580 episode reward: total was -4.090000. running mean: -21.876116\n",
      "ep 1494: ep_len:3 episode reward: total was 0.000000. running mean: -21.657355\n",
      "ep 1494: ep_len:570 episode reward: total was -21.580000. running mean: -21.656581\n",
      "ep 1494: ep_len:500 episode reward: total was -20.940000. running mean: -21.649416\n",
      "epsilon:0.195932 episode_count: 10465. steps_count: 4624358.000000\n",
      "ep 1495: ep_len:620 episode reward: total was -21.290000. running mean: -21.645821\n",
      "ep 1495: ep_len:545 episode reward: total was -6.190000. running mean: -21.491263\n",
      "ep 1495: ep_len:655 episode reward: total was -17.250000. running mean: -21.448851\n",
      "ep 1495: ep_len:152 episode reward: total was -4.910000. running mean: -21.283462\n",
      "ep 1495: ep_len:121 episode reward: total was -11.940000. running mean: -21.190027\n",
      "ep 1495: ep_len:170 episode reward: total was -6.960000. running mean: -21.047727\n",
      "ep 1495: ep_len:505 episode reward: total was -27.110000. running mean: -21.108350\n",
      "epsilon:0.195796 episode_count: 10472. steps_count: 4627126.000000\n",
      "ep 1496: ep_len:520 episode reward: total was -15.040000. running mean: -21.047666\n",
      "ep 1496: ep_len:615 episode reward: total was -28.410000. running mean: -21.121290\n",
      "ep 1496: ep_len:600 episode reward: total was -30.180000. running mean: -21.211877\n",
      "ep 1496: ep_len:155 episode reward: total was -7.410000. running mean: -21.073858\n",
      "ep 1496: ep_len:3 episode reward: total was 0.000000. running mean: -20.863120\n",
      "ep 1496: ep_len:169 episode reward: total was -9.970000. running mean: -20.754188\n",
      "ep 1496: ep_len:560 episode reward: total was -26.530000. running mean: -20.811946\n",
      "epsilon:0.195659 episode_count: 10479. steps_count: 4629748.000000\n",
      "ep 1497: ep_len:540 episode reward: total was -10.410000. running mean: -20.707927\n",
      "ep 1497: ep_len:840 episode reward: total was -109.210000. running mean: -21.592948\n",
      "ep 1497: ep_len:525 episode reward: total was -17.080000. running mean: -21.547818\n",
      "ep 1497: ep_len:595 episode reward: total was -34.130000. running mean: -21.673640\n",
      "ep 1497: ep_len:3 episode reward: total was 0.000000. running mean: -21.456904\n",
      "ep 1497: ep_len:545 episode reward: total was -23.530000. running mean: -21.477635\n",
      "ep 1497: ep_len:500 episode reward: total was -40.150000. running mean: -21.664358\n",
      "epsilon:0.195523 episode_count: 10486. steps_count: 4633296.000000\n",
      "ep 1498: ep_len:600 episode reward: total was -19.720000. running mean: -21.644915\n",
      "ep 1498: ep_len:500 episode reward: total was -38.240000. running mean: -21.810866\n",
      "ep 1498: ep_len:655 episode reward: total was -23.210000. running mean: -21.824857\n",
      "ep 1498: ep_len:145 episode reward: total was -2.910000. running mean: -21.635708\n",
      "ep 1498: ep_len:3 episode reward: total was 0.000000. running mean: -21.419351\n",
      "ep 1498: ep_len:665 episode reward: total was -48.420000. running mean: -21.689358\n",
      "ep 1498: ep_len:530 episode reward: total was -25.060000. running mean: -21.723064\n",
      "epsilon:0.195386 episode_count: 10493. steps_count: 4636394.000000\n",
      "ep 1499: ep_len:123 episode reward: total was -14.440000. running mean: -21.650233\n",
      "ep 1499: ep_len:555 episode reward: total was -11.000000. running mean: -21.543731\n",
      "ep 1499: ep_len:645 episode reward: total was -30.800000. running mean: -21.636294\n",
      "ep 1499: ep_len:126 episode reward: total was 4.590000. running mean: -21.374031\n",
      "ep 1499: ep_len:3 episode reward: total was 0.000000. running mean: -21.160291\n",
      "ep 1499: ep_len:570 episode reward: total was -27.870000. running mean: -21.227388\n",
      "ep 1499: ep_len:505 episode reward: total was -33.040000. running mean: -21.345514\n",
      "epsilon:0.195250 episode_count: 10500. steps_count: 4638921.000000\n",
      "ep 1500: ep_len:640 episode reward: total was -44.370000. running mean: -21.575759\n",
      "ep 1500: ep_len:560 episode reward: total was 0.810000. running mean: -21.351901\n",
      "ep 1500: ep_len:580 episode reward: total was -25.820000. running mean: -21.396582\n",
      "ep 1500: ep_len:151 episode reward: total was -9.900000. running mean: -21.281616\n",
      "ep 1500: ep_len:3 episode reward: total was 0.000000. running mean: -21.068800\n",
      "ep 1500: ep_len:500 episode reward: total was -19.120000. running mean: -21.049312\n",
      "ep 1500: ep_len:500 episode reward: total was -33.140000. running mean: -21.170219\n",
      "epsilon:0.195113 episode_count: 10507. steps_count: 4641855.000000\n",
      "ep 1501: ep_len:525 episode reward: total was -50.740000. running mean: -21.465917\n",
      "ep 1501: ep_len:805 episode reward: total was -100.640000. running mean: -22.257658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1501: ep_len:550 episode reward: total was -19.670000. running mean: -22.231781\n",
      "ep 1501: ep_len:396 episode reward: total was -7.160000. running mean: -22.081063\n",
      "ep 1501: ep_len:99 episode reward: total was -3.960000. running mean: -21.899853\n",
      "ep 1501: ep_len:570 episode reward: total was -18.320000. running mean: -21.864054\n",
      "ep 1501: ep_len:505 episode reward: total was -22.000000. running mean: -21.865414\n",
      "epsilon:0.194977 episode_count: 10514. steps_count: 4645305.000000\n",
      "ep 1502: ep_len:530 episode reward: total was -33.670000. running mean: -21.983459\n",
      "ep 1502: ep_len:166 episode reward: total was -3.930000. running mean: -21.802925\n",
      "ep 1502: ep_len:715 episode reward: total was -39.790000. running mean: -21.982796\n",
      "ep 1502: ep_len:500 episode reward: total was -14.530000. running mean: -21.908268\n",
      "ep 1502: ep_len:3 episode reward: total was 0.000000. running mean: -21.689185\n",
      "ep 1502: ep_len:525 episode reward: total was -19.360000. running mean: -21.665893\n",
      "ep 1502: ep_len:520 episode reward: total was -31.000000. running mean: -21.759234\n",
      "epsilon:0.194840 episode_count: 10521. steps_count: 4648264.000000\n",
      "ep 1503: ep_len:198 episode reward: total was -6.410000. running mean: -21.605742\n",
      "ep 1503: ep_len:565 episode reward: total was -20.720000. running mean: -21.596884\n",
      "ep 1503: ep_len:446 episode reward: total was -8.860000. running mean: -21.469516\n",
      "ep 1503: ep_len:500 episode reward: total was -20.020000. running mean: -21.455020\n",
      "ep 1503: ep_len:87 episode reward: total was 2.050000. running mean: -21.219970\n",
      "ep 1503: ep_len:505 episode reward: total was -34.350000. running mean: -21.351270\n",
      "ep 1503: ep_len:500 episode reward: total was -14.350000. running mean: -21.281258\n",
      "epsilon:0.194704 episode_count: 10528. steps_count: 4651065.000000\n",
      "ep 1504: ep_len:500 episode reward: total was -37.880000. running mean: -21.447245\n",
      "ep 1504: ep_len:550 episode reward: total was -15.540000. running mean: -21.388173\n",
      "ep 1504: ep_len:615 episode reward: total was -14.500000. running mean: -21.319291\n",
      "ep 1504: ep_len:530 episode reward: total was -10.520000. running mean: -21.211298\n",
      "ep 1504: ep_len:3 episode reward: total was 0.000000. running mean: -20.999185\n",
      "ep 1504: ep_len:605 episode reward: total was -15.580000. running mean: -20.944993\n",
      "ep 1504: ep_len:550 episode reward: total was -16.150000. running mean: -20.897043\n",
      "epsilon:0.194567 episode_count: 10535. steps_count: 4654418.000000\n",
      "ep 1505: ep_len:615 episode reward: total was -16.080000. running mean: -20.848873\n",
      "ep 1505: ep_len:163 episode reward: total was -6.970000. running mean: -20.710084\n",
      "ep 1505: ep_len:595 episode reward: total was -16.820000. running mean: -20.671183\n",
      "ep 1505: ep_len:500 episode reward: total was -30.170000. running mean: -20.766172\n",
      "ep 1505: ep_len:3 episode reward: total was 0.000000. running mean: -20.558510\n",
      "ep 1505: ep_len:575 episode reward: total was -9.030000. running mean: -20.443225\n",
      "ep 1505: ep_len:550 episode reward: total was -27.060000. running mean: -20.509392\n",
      "epsilon:0.194431 episode_count: 10542. steps_count: 4657419.000000\n",
      "ep 1506: ep_len:575 episode reward: total was -101.910000. running mean: -21.323399\n",
      "ep 1506: ep_len:344 episode reward: total was -19.380000. running mean: -21.303965\n",
      "ep 1506: ep_len:600 episode reward: total was -45.440000. running mean: -21.545325\n",
      "ep 1506: ep_len:510 episode reward: total was -31.190000. running mean: -21.641772\n",
      "ep 1506: ep_len:3 episode reward: total was 0.000000. running mean: -21.425354\n",
      "ep 1506: ep_len:525 episode reward: total was -27.150000. running mean: -21.482600\n",
      "ep 1506: ep_len:550 episode reward: total was -12.870000. running mean: -21.396474\n",
      "epsilon:0.194294 episode_count: 10549. steps_count: 4660526.000000\n",
      "ep 1507: ep_len:118 episode reward: total was -17.480000. running mean: -21.357310\n",
      "ep 1507: ep_len:565 episode reward: total was -12.020000. running mean: -21.263937\n",
      "ep 1507: ep_len:525 episode reward: total was -24.700000. running mean: -21.298297\n",
      "ep 1507: ep_len:610 episode reward: total was -21.090000. running mean: -21.296214\n",
      "ep 1507: ep_len:3 episode reward: total was 0.000000. running mean: -21.083252\n",
      "ep 1507: ep_len:680 episode reward: total was -62.990000. running mean: -21.502320\n",
      "ep 1507: ep_len:505 episode reward: total was -22.740000. running mean: -21.514696\n",
      "epsilon:0.194158 episode_count: 10556. steps_count: 4663532.000000\n",
      "ep 1508: ep_len:590 episode reward: total was -19.200000. running mean: -21.491549\n",
      "ep 1508: ep_len:605 episode reward: total was -34.200000. running mean: -21.618634\n",
      "ep 1508: ep_len:585 episode reward: total was -18.020000. running mean: -21.582648\n",
      "ep 1508: ep_len:500 episode reward: total was -17.580000. running mean: -21.542621\n",
      "ep 1508: ep_len:87 episode reward: total was -11.950000. running mean: -21.446695\n",
      "ep 1508: ep_len:505 episode reward: total was -40.360000. running mean: -21.635828\n",
      "ep 1508: ep_len:520 episode reward: total was -34.220000. running mean: -21.761670\n",
      "epsilon:0.194021 episode_count: 10563. steps_count: 4666924.000000\n",
      "ep 1509: ep_len:261 episode reward: total was -8.420000. running mean: -21.628253\n",
      "ep 1509: ep_len:605 episode reward: total was -14.500000. running mean: -21.556970\n",
      "ep 1509: ep_len:605 episode reward: total was -33.430000. running mean: -21.675701\n",
      "ep 1509: ep_len:565 episode reward: total was -9.060000. running mean: -21.549544\n",
      "ep 1509: ep_len:98 episode reward: total was -1.440000. running mean: -21.348448\n",
      "ep 1509: ep_len:308 episode reward: total was -9.420000. running mean: -21.229164\n",
      "ep 1509: ep_len:500 episode reward: total was -14.910000. running mean: -21.165972\n",
      "epsilon:0.193885 episode_count: 10570. steps_count: 4669866.000000\n",
      "ep 1510: ep_len:535 episode reward: total was -70.220000. running mean: -21.656512\n",
      "ep 1510: ep_len:550 episode reward: total was -16.920000. running mean: -21.609147\n",
      "ep 1510: ep_len:630 episode reward: total was -18.280000. running mean: -21.575856\n",
      "ep 1510: ep_len:510 episode reward: total was -20.800000. running mean: -21.568097\n",
      "ep 1510: ep_len:3 episode reward: total was 0.000000. running mean: -21.352416\n",
      "ep 1510: ep_len:695 episode reward: total was -18.730000. running mean: -21.326192\n",
      "ep 1510: ep_len:530 episode reward: total was -18.130000. running mean: -21.294230\n",
      "epsilon:0.193748 episode_count: 10577. steps_count: 4673319.000000\n",
      "ep 1511: ep_len:250 episode reward: total was -5.920000. running mean: -21.140488\n",
      "ep 1511: ep_len:755 episode reward: total was -63.950000. running mean: -21.568583\n",
      "ep 1511: ep_len:660 episode reward: total was -30.340000. running mean: -21.656297\n",
      "ep 1511: ep_len:510 episode reward: total was -17.180000. running mean: -21.611534\n",
      "ep 1511: ep_len:93 episode reward: total was -9.460000. running mean: -21.490019\n",
      "ep 1511: ep_len:575 episode reward: total was -31.100000. running mean: -21.586119\n",
      "ep 1511: ep_len:535 episode reward: total was -21.170000. running mean: -21.581958\n",
      "epsilon:0.193612 episode_count: 10584. steps_count: 4676697.000000\n",
      "ep 1512: ep_len:580 episode reward: total was -32.140000. running mean: -21.687538\n",
      "ep 1512: ep_len:550 episode reward: total was -51.580000. running mean: -21.986463\n",
      "ep 1512: ep_len:625 episode reward: total was -34.170000. running mean: -22.108298\n",
      "ep 1512: ep_len:500 episode reward: total was -16.660000. running mean: -22.053815\n",
      "ep 1512: ep_len:3 episode reward: total was 0.000000. running mean: -21.833277\n",
      "ep 1512: ep_len:500 episode reward: total was -27.120000. running mean: -21.886144\n",
      "ep 1512: ep_len:515 episode reward: total was -24.920000. running mean: -21.916483\n",
      "epsilon:0.193475 episode_count: 10591. steps_count: 4679970.000000\n",
      "ep 1513: ep_len:635 episode reward: total was -30.540000. running mean: -22.002718\n",
      "ep 1513: ep_len:555 episode reward: total was -3.950000. running mean: -21.822191\n",
      "ep 1513: ep_len:426 episode reward: total was -3.280000. running mean: -21.636769\n",
      "ep 1513: ep_len:540 episode reward: total was -16.050000. running mean: -21.580901\n",
      "ep 1513: ep_len:44 episode reward: total was 2.500000. running mean: -21.340092\n",
      "ep 1513: ep_len:236 episode reward: total was -2.900000. running mean: -21.155691\n",
      "ep 1513: ep_len:525 episode reward: total was -25.130000. running mean: -21.195434\n",
      "epsilon:0.193339 episode_count: 10598. steps_count: 4682931.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1514: ep_len:610 episode reward: total was -24.310000. running mean: -21.226580\n",
      "ep 1514: ep_len:605 episode reward: total was -57.110000. running mean: -21.585414\n",
      "ep 1514: ep_len:420 episode reward: total was -21.350000. running mean: -21.583060\n",
      "ep 1514: ep_len:401 episode reward: total was -11.700000. running mean: -21.484229\n",
      "ep 1514: ep_len:3 episode reward: total was 0.000000. running mean: -21.269387\n",
      "ep 1514: ep_len:515 episode reward: total was -64.690000. running mean: -21.703593\n",
      "ep 1514: ep_len:245 episode reward: total was -15.380000. running mean: -21.640357\n",
      "epsilon:0.193202 episode_count: 10605. steps_count: 4685730.000000\n",
      "ep 1515: ep_len:570 episode reward: total was -20.510000. running mean: -21.629054\n",
      "ep 1515: ep_len:500 episode reward: total was -28.330000. running mean: -21.696063\n",
      "ep 1515: ep_len:500 episode reward: total was -32.130000. running mean: -21.800402\n",
      "ep 1515: ep_len:510 episode reward: total was -10.670000. running mean: -21.689098\n",
      "ep 1515: ep_len:3 episode reward: total was 0.000000. running mean: -21.472207\n",
      "ep 1515: ep_len:685 episode reward: total was -10.240000. running mean: -21.359885\n",
      "ep 1515: ep_len:550 episode reward: total was -27.200000. running mean: -21.418287\n",
      "epsilon:0.193066 episode_count: 10612. steps_count: 4689048.000000\n",
      "ep 1516: ep_len:590 episode reward: total was -30.720000. running mean: -21.511304\n",
      "ep 1516: ep_len:515 episode reward: total was -34.130000. running mean: -21.637491\n",
      "ep 1516: ep_len:665 episode reward: total was -59.630000. running mean: -22.017416\n",
      "ep 1516: ep_len:500 episode reward: total was -30.110000. running mean: -22.098342\n",
      "ep 1516: ep_len:85 episode reward: total was -1.990000. running mean: -21.897258\n",
      "ep 1516: ep_len:244 episode reward: total was -4.390000. running mean: -21.722186\n",
      "ep 1516: ep_len:500 episode reward: total was -39.200000. running mean: -21.896964\n",
      "epsilon:0.192929 episode_count: 10619. steps_count: 4692147.000000\n",
      "ep 1517: ep_len:555 episode reward: total was -40.910000. running mean: -22.087094\n",
      "ep 1517: ep_len:580 episode reward: total was -33.560000. running mean: -22.201823\n",
      "ep 1517: ep_len:725 episode reward: total was -84.060000. running mean: -22.820405\n",
      "ep 1517: ep_len:45 episode reward: total was -1.960000. running mean: -22.611801\n",
      "ep 1517: ep_len:3 episode reward: total was 0.000000. running mean: -22.385683\n",
      "ep 1517: ep_len:645 episode reward: total was -31.520000. running mean: -22.477026\n",
      "ep 1517: ep_len:560 episode reward: total was -39.980000. running mean: -22.652056\n",
      "epsilon:0.192793 episode_count: 10626. steps_count: 4695260.000000\n",
      "ep 1518: ep_len:580 episode reward: total was -25.660000. running mean: -22.682135\n",
      "ep 1518: ep_len:574 episode reward: total was -67.240000. running mean: -23.127714\n",
      "ep 1518: ep_len:645 episode reward: total was -18.460000. running mean: -23.081037\n",
      "ep 1518: ep_len:605 episode reward: total was -14.550000. running mean: -22.995726\n",
      "ep 1518: ep_len:103 episode reward: total was -9.460000. running mean: -22.860369\n",
      "ep 1518: ep_len:610 episode reward: total was -43.020000. running mean: -23.061965\n",
      "ep 1518: ep_len:605 episode reward: total was -22.890000. running mean: -23.060246\n",
      "epsilon:0.192656 episode_count: 10633. steps_count: 4698982.000000\n",
      "ep 1519: ep_len:565 episode reward: total was -75.210000. running mean: -23.581743\n",
      "ep 1519: ep_len:610 episode reward: total was -21.920000. running mean: -23.565126\n",
      "ep 1519: ep_len:545 episode reward: total was -22.380000. running mean: -23.553275\n",
      "ep 1519: ep_len:515 episode reward: total was -10.040000. running mean: -23.418142\n",
      "ep 1519: ep_len:3 episode reward: total was 0.000000. running mean: -23.183960\n",
      "ep 1519: ep_len:640 episode reward: total was -55.640000. running mean: -23.508521\n",
      "ep 1519: ep_len:500 episode reward: total was -37.120000. running mean: -23.644636\n",
      "epsilon:0.192520 episode_count: 10640. steps_count: 4702360.000000\n",
      "ep 1520: ep_len:580 episode reward: total was -30.520000. running mean: -23.713389\n",
      "ep 1520: ep_len:530 episode reward: total was -26.510000. running mean: -23.741355\n",
      "ep 1520: ep_len:605 episode reward: total was -15.590000. running mean: -23.659842\n",
      "ep 1520: ep_len:525 episode reward: total was -19.610000. running mean: -23.619343\n",
      "ep 1520: ep_len:108 episode reward: total was -2.960000. running mean: -23.412750\n",
      "ep 1520: ep_len:520 episode reward: total was -21.530000. running mean: -23.393922\n",
      "ep 1520: ep_len:555 episode reward: total was -6.840000. running mean: -23.228383\n",
      "epsilon:0.192383 episode_count: 10647. steps_count: 4705783.000000\n",
      "ep 1521: ep_len:650 episode reward: total was -22.930000. running mean: -23.225399\n",
      "ep 1521: ep_len:545 episode reward: total was -79.380000. running mean: -23.786945\n",
      "ep 1521: ep_len:660 episode reward: total was -37.600000. running mean: -23.925076\n",
      "ep 1521: ep_len:595 episode reward: total was -27.020000. running mean: -23.956025\n",
      "ep 1521: ep_len:3 episode reward: total was 0.000000. running mean: -23.716465\n",
      "ep 1521: ep_len:610 episode reward: total was -28.180000. running mean: -23.761100\n",
      "ep 1521: ep_len:565 episode reward: total was -33.600000. running mean: -23.859489\n",
      "epsilon:0.192247 episode_count: 10654. steps_count: 4709411.000000\n",
      "ep 1522: ep_len:700 episode reward: total was -46.970000. running mean: -24.090594\n",
      "ep 1522: ep_len:600 episode reward: total was -17.050000. running mean: -24.020188\n",
      "ep 1522: ep_len:545 episode reward: total was -20.210000. running mean: -23.982087\n",
      "ep 1522: ep_len:385 episode reward: total was -16.230000. running mean: -23.904566\n",
      "ep 1522: ep_len:3 episode reward: total was 0.000000. running mean: -23.665520\n",
      "ep 1522: ep_len:500 episode reward: total was -18.780000. running mean: -23.616665\n",
      "ep 1522: ep_len:600 episode reward: total was -23.200000. running mean: -23.612498\n",
      "epsilon:0.192110 episode_count: 10661. steps_count: 4712744.000000\n",
      "ep 1523: ep_len:570 episode reward: total was -24.540000. running mean: -23.621773\n",
      "ep 1523: ep_len:500 episode reward: total was -21.520000. running mean: -23.600755\n",
      "ep 1523: ep_len:615 episode reward: total was -49.680000. running mean: -23.861548\n",
      "ep 1523: ep_len:550 episode reward: total was -97.900000. running mean: -24.601932\n",
      "ep 1523: ep_len:3 episode reward: total was 0.000000. running mean: -24.355913\n",
      "ep 1523: ep_len:311 episode reward: total was -10.390000. running mean: -24.216254\n",
      "ep 1523: ep_len:575 episode reward: total was -55.120000. running mean: -24.525291\n",
      "epsilon:0.191974 episode_count: 10668. steps_count: 4715868.000000\n",
      "ep 1524: ep_len:630 episode reward: total was -38.250000. running mean: -24.662539\n",
      "ep 1524: ep_len:575 episode reward: total was -26.740000. running mean: -24.683313\n",
      "ep 1524: ep_len:505 episode reward: total was -23.690000. running mean: -24.673380\n",
      "ep 1524: ep_len:540 episode reward: total was -41.750000. running mean: -24.844146\n",
      "ep 1524: ep_len:3 episode reward: total was 0.000000. running mean: -24.595705\n",
      "ep 1524: ep_len:625 episode reward: total was -27.520000. running mean: -24.624948\n",
      "ep 1524: ep_len:186 episode reward: total was -7.440000. running mean: -24.453098\n",
      "epsilon:0.191837 episode_count: 10675. steps_count: 4718932.000000\n",
      "ep 1525: ep_len:120 episode reward: total was -3.450000. running mean: -24.243067\n",
      "ep 1525: ep_len:595 episode reward: total was -18.070000. running mean: -24.181337\n",
      "ep 1525: ep_len:535 episode reward: total was -27.230000. running mean: -24.211823\n",
      "ep 1525: ep_len:162 episode reward: total was -5.420000. running mean: -24.023905\n",
      "ep 1525: ep_len:3 episode reward: total was 0.000000. running mean: -23.783666\n",
      "ep 1525: ep_len:645 episode reward: total was -41.150000. running mean: -23.957329\n",
      "ep 1525: ep_len:600 episode reward: total was -40.680000. running mean: -24.124556\n",
      "epsilon:0.191701 episode_count: 10682. steps_count: 4721592.000000\n",
      "ep 1526: ep_len:232 episode reward: total was -25.880000. running mean: -24.142110\n",
      "ep 1526: ep_len:580 episode reward: total was -41.610000. running mean: -24.316789\n",
      "ep 1526: ep_len:600 episode reward: total was -32.900000. running mean: -24.402621\n",
      "ep 1526: ep_len:515 episode reward: total was -3.130000. running mean: -24.189895\n",
      "ep 1526: ep_len:3 episode reward: total was 0.000000. running mean: -23.947996\n",
      "ep 1526: ep_len:610 episode reward: total was -30.300000. running mean: -24.011516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1526: ep_len:505 episode reward: total was -33.670000. running mean: -24.108101\n",
      "epsilon:0.191564 episode_count: 10689. steps_count: 4724637.000000\n",
      "ep 1527: ep_len:600 episode reward: total was -21.120000. running mean: -24.078220\n",
      "ep 1527: ep_len:545 episode reward: total was -31.690000. running mean: -24.154338\n",
      "ep 1527: ep_len:505 episode reward: total was -43.690000. running mean: -24.349695\n",
      "ep 1527: ep_len:40 episode reward: total was -3.490000. running mean: -24.141098\n",
      "ep 1527: ep_len:3 episode reward: total was 0.000000. running mean: -23.899687\n",
      "ep 1527: ep_len:510 episode reward: total was -57.170000. running mean: -24.232390\n",
      "ep 1527: ep_len:565 episode reward: total was -28.180000. running mean: -24.271866\n",
      "epsilon:0.191428 episode_count: 10696. steps_count: 4727405.000000\n",
      "ep 1528: ep_len:500 episode reward: total was -25.190000. running mean: -24.281047\n",
      "ep 1528: ep_len:535 episode reward: total was -41.640000. running mean: -24.454637\n",
      "ep 1528: ep_len:570 episode reward: total was -31.590000. running mean: -24.525990\n",
      "ep 1528: ep_len:520 episode reward: total was -4.690000. running mean: -24.327630\n",
      "ep 1528: ep_len:3 episode reward: total was 0.000000. running mean: -24.084354\n",
      "ep 1528: ep_len:291 episode reward: total was -15.380000. running mean: -23.997311\n",
      "ep 1528: ep_len:515 episode reward: total was -32.040000. running mean: -24.077737\n",
      "epsilon:0.191291 episode_count: 10703. steps_count: 4730339.000000\n",
      "ep 1529: ep_len:580 episode reward: total was -22.880000. running mean: -24.065760\n",
      "ep 1529: ep_len:525 episode reward: total was -10.720000. running mean: -23.932303\n",
      "ep 1529: ep_len:620 episode reward: total was -25.310000. running mean: -23.946079\n",
      "ep 1529: ep_len:505 episode reward: total was -32.640000. running mean: -24.033019\n",
      "ep 1529: ep_len:111 episode reward: total was -10.460000. running mean: -23.897289\n",
      "ep 1529: ep_len:540 episode reward: total was -14.430000. running mean: -23.802616\n",
      "ep 1529: ep_len:500 episode reward: total was -20.290000. running mean: -23.767489\n",
      "epsilon:0.191155 episode_count: 10710. steps_count: 4733720.000000\n",
      "ep 1530: ep_len:105 episode reward: total was -3.970000. running mean: -23.569515\n",
      "ep 1530: ep_len:545 episode reward: total was -20.480000. running mean: -23.538619\n",
      "ep 1530: ep_len:500 episode reward: total was -11.160000. running mean: -23.414833\n",
      "ep 1530: ep_len:550 episode reward: total was -14.200000. running mean: -23.322685\n",
      "ep 1530: ep_len:69 episode reward: total was -10.980000. running mean: -23.199258\n",
      "ep 1530: ep_len:500 episode reward: total was -14.320000. running mean: -23.110465\n",
      "ep 1530: ep_len:505 episode reward: total was -15.420000. running mean: -23.033561\n",
      "epsilon:0.191018 episode_count: 10717. steps_count: 4736494.000000\n",
      "ep 1531: ep_len:520 episode reward: total was -35.460000. running mean: -23.157825\n",
      "ep 1531: ep_len:500 episode reward: total was -3.980000. running mean: -22.966047\n",
      "ep 1531: ep_len:690 episode reward: total was -23.700000. running mean: -22.973386\n",
      "ep 1531: ep_len:500 episode reward: total was -21.600000. running mean: -22.959653\n",
      "ep 1531: ep_len:3 episode reward: total was 0.000000. running mean: -22.730056\n",
      "ep 1531: ep_len:594 episode reward: total was -54.010000. running mean: -23.042856\n",
      "ep 1531: ep_len:500 episode reward: total was -13.900000. running mean: -22.951427\n",
      "epsilon:0.190882 episode_count: 10724. steps_count: 4739801.000000\n",
      "ep 1532: ep_len:560 episode reward: total was -41.450000. running mean: -23.136413\n",
      "ep 1532: ep_len:530 episode reward: total was -34.560000. running mean: -23.250649\n",
      "ep 1532: ep_len:500 episode reward: total was -15.690000. running mean: -23.175042\n",
      "ep 1532: ep_len:605 episode reward: total was -13.450000. running mean: -23.077792\n",
      "ep 1532: ep_len:84 episode reward: total was -2.950000. running mean: -22.876514\n",
      "ep 1532: ep_len:620 episode reward: total was -25.680000. running mean: -22.904549\n",
      "ep 1532: ep_len:321 episode reward: total was -21.360000. running mean: -22.889103\n",
      "epsilon:0.190745 episode_count: 10731. steps_count: 4743021.000000\n",
      "ep 1533: ep_len:500 episode reward: total was -23.300000. running mean: -22.893212\n",
      "ep 1533: ep_len:590 episode reward: total was -28.580000. running mean: -22.950080\n",
      "ep 1533: ep_len:575 episode reward: total was -21.070000. running mean: -22.931279\n",
      "ep 1533: ep_len:515 episode reward: total was -38.670000. running mean: -23.088666\n",
      "ep 1533: ep_len:3 episode reward: total was 0.000000. running mean: -22.857780\n",
      "ep 1533: ep_len:635 episode reward: total was -24.450000. running mean: -22.873702\n",
      "ep 1533: ep_len:595 episode reward: total was -22.940000. running mean: -22.874365\n",
      "epsilon:0.190609 episode_count: 10738. steps_count: 4746434.000000\n",
      "ep 1534: ep_len:525 episode reward: total was -37.000000. running mean: -23.015621\n",
      "ep 1534: ep_len:500 episode reward: total was -32.900000. running mean: -23.114465\n",
      "ep 1534: ep_len:605 episode reward: total was -21.510000. running mean: -23.098420\n",
      "ep 1534: ep_len:500 episode reward: total was -20.650000. running mean: -23.073936\n",
      "ep 1534: ep_len:3 episode reward: total was 0.000000. running mean: -22.843197\n",
      "ep 1534: ep_len:500 episode reward: total was -9.450000. running mean: -22.709265\n",
      "ep 1534: ep_len:500 episode reward: total was -27.080000. running mean: -22.752972\n",
      "epsilon:0.190472 episode_count: 10745. steps_count: 4749567.000000\n",
      "ep 1535: ep_len:610 episode reward: total was -31.700000. running mean: -22.842442\n",
      "ep 1535: ep_len:505 episode reward: total was -46.880000. running mean: -23.082818\n",
      "ep 1535: ep_len:389 episode reward: total was -22.930000. running mean: -23.081290\n",
      "ep 1535: ep_len:500 episode reward: total was -32.160000. running mean: -23.172077\n",
      "ep 1535: ep_len:96 episode reward: total was -13.950000. running mean: -23.079856\n",
      "ep 1535: ep_len:172 episode reward: total was -24.980000. running mean: -23.098858\n",
      "ep 1535: ep_len:187 episode reward: total was -13.430000. running mean: -23.002169\n",
      "epsilon:0.190336 episode_count: 10752. steps_count: 4752026.000000\n",
      "ep 1536: ep_len:244 episode reward: total was -2.420000. running mean: -22.796347\n",
      "ep 1536: ep_len:500 episode reward: total was -11.900000. running mean: -22.687384\n",
      "ep 1536: ep_len:79 episode reward: total was -0.970000. running mean: -22.470210\n",
      "ep 1536: ep_len:515 episode reward: total was -6.570000. running mean: -22.311208\n",
      "ep 1536: ep_len:3 episode reward: total was 0.000000. running mean: -22.088096\n",
      "ep 1536: ep_len:650 episode reward: total was -53.440000. running mean: -22.401615\n",
      "ep 1536: ep_len:605 episode reward: total was -27.530000. running mean: -22.452899\n",
      "epsilon:0.190199 episode_count: 10759. steps_count: 4754622.000000\n",
      "ep 1537: ep_len:575 episode reward: total was -38.770000. running mean: -22.616070\n",
      "ep 1537: ep_len:244 episode reward: total was -7.410000. running mean: -22.464009\n",
      "ep 1537: ep_len:645 episode reward: total was -16.230000. running mean: -22.401669\n",
      "ep 1537: ep_len:112 episode reward: total was 5.570000. running mean: -22.121952\n",
      "ep 1537: ep_len:52 episode reward: total was -7.000000. running mean: -21.970733\n",
      "ep 1537: ep_len:173 episode reward: total was -3.470000. running mean: -21.785725\n",
      "ep 1537: ep_len:500 episode reward: total was -18.390000. running mean: -21.751768\n",
      "epsilon:0.190063 episode_count: 10766. steps_count: 4756923.000000\n",
      "ep 1538: ep_len:635 episode reward: total was -24.360000. running mean: -21.777851\n",
      "ep 1538: ep_len:525 episode reward: total was -7.680000. running mean: -21.636872\n",
      "ep 1538: ep_len:297 episode reward: total was -16.900000. running mean: -21.589503\n",
      "ep 1538: ep_len:515 episode reward: total was -26.580000. running mean: -21.639408\n",
      "ep 1538: ep_len:3 episode reward: total was 0.000000. running mean: -21.423014\n",
      "ep 1538: ep_len:535 episode reward: total was -13.160000. running mean: -21.340384\n",
      "ep 1538: ep_len:640 episode reward: total was -15.350000. running mean: -21.280480\n",
      "epsilon:0.189926 episode_count: 10773. steps_count: 4760073.000000\n",
      "ep 1539: ep_len:625 episode reward: total was -34.840000. running mean: -21.416075\n",
      "ep 1539: ep_len:520 episode reward: total was -12.350000. running mean: -21.325415\n",
      "ep 1539: ep_len:500 episode reward: total was -16.960000. running mean: -21.281761\n",
      "ep 1539: ep_len:500 episode reward: total was -22.610000. running mean: -21.295043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1539: ep_len:110 episode reward: total was -10.430000. running mean: -21.186392\n",
      "ep 1539: ep_len:615 episode reward: total was -26.130000. running mean: -21.235829\n",
      "ep 1539: ep_len:500 episode reward: total was -26.610000. running mean: -21.289570\n",
      "epsilon:0.189790 episode_count: 10780. steps_count: 4763443.000000\n",
      "ep 1540: ep_len:215 episode reward: total was -19.860000. running mean: -21.275275\n",
      "ep 1540: ep_len:515 episode reward: total was -13.930000. running mean: -21.201822\n",
      "ep 1540: ep_len:600 episode reward: total was -30.090000. running mean: -21.290704\n",
      "ep 1540: ep_len:540 episode reward: total was -14.260000. running mean: -21.220397\n",
      "ep 1540: ep_len:3 episode reward: total was 0.000000. running mean: -21.008193\n",
      "ep 1540: ep_len:575 episode reward: total was -18.770000. running mean: -20.985811\n",
      "ep 1540: ep_len:500 episode reward: total was -30.600000. running mean: -21.081953\n",
      "epsilon:0.189653 episode_count: 10787. steps_count: 4766391.000000\n",
      "ep 1541: ep_len:600 episode reward: total was -33.180000. running mean: -21.202933\n",
      "ep 1541: ep_len:376 episode reward: total was -57.860000. running mean: -21.569504\n",
      "ep 1541: ep_len:545 episode reward: total was -32.230000. running mean: -21.676109\n",
      "ep 1541: ep_len:610 episode reward: total was -27.220000. running mean: -21.731548\n",
      "ep 1541: ep_len:3 episode reward: total was 0.000000. running mean: -21.514232\n",
      "ep 1541: ep_len:175 episode reward: total was -23.990000. running mean: -21.538990\n",
      "ep 1541: ep_len:835 episode reward: total was -67.980000. running mean: -22.003400\n",
      "epsilon:0.189517 episode_count: 10794. steps_count: 4769535.000000\n",
      "ep 1542: ep_len:250 episode reward: total was -7.430000. running mean: -21.857666\n",
      "ep 1542: ep_len:347 episode reward: total was -31.440000. running mean: -21.953489\n",
      "ep 1542: ep_len:500 episode reward: total was -24.070000. running mean: -21.974654\n",
      "ep 1542: ep_len:525 episode reward: total was -10.110000. running mean: -21.856008\n",
      "ep 1542: ep_len:3 episode reward: total was 0.000000. running mean: -21.637448\n",
      "ep 1542: ep_len:505 episode reward: total was -25.110000. running mean: -21.672173\n",
      "ep 1542: ep_len:550 episode reward: total was -27.560000. running mean: -21.731052\n",
      "epsilon:0.189380 episode_count: 10801. steps_count: 4772215.000000\n",
      "ep 1543: ep_len:520 episode reward: total was -11.650000. running mean: -21.630241\n",
      "ep 1543: ep_len:500 episode reward: total was -4.340000. running mean: -21.457339\n",
      "ep 1543: ep_len:505 episode reward: total was -26.520000. running mean: -21.507965\n",
      "ep 1543: ep_len:505 episode reward: total was -20.610000. running mean: -21.498986\n",
      "ep 1543: ep_len:95 episode reward: total was 2.040000. running mean: -21.263596\n",
      "ep 1543: ep_len:580 episode reward: total was -36.970000. running mean: -21.420660\n",
      "ep 1543: ep_len:535 episode reward: total was -13.180000. running mean: -21.338253\n",
      "epsilon:0.189244 episode_count: 10808. steps_count: 4775455.000000\n",
      "ep 1544: ep_len:640 episode reward: total was -40.510000. running mean: -21.529971\n",
      "ep 1544: ep_len:605 episode reward: total was -58.140000. running mean: -21.896071\n",
      "ep 1544: ep_len:550 episode reward: total was -19.400000. running mean: -21.871110\n",
      "ep 1544: ep_len:151 episode reward: total was 5.620000. running mean: -21.596199\n",
      "ep 1544: ep_len:87 episode reward: total was -8.450000. running mean: -21.464737\n",
      "ep 1544: ep_len:500 episode reward: total was -19.690000. running mean: -21.446990\n",
      "ep 1544: ep_len:600 episode reward: total was -51.640000. running mean: -21.748920\n",
      "epsilon:0.189107 episode_count: 10815. steps_count: 4778588.000000\n",
      "ep 1545: ep_len:570 episode reward: total was -24.290000. running mean: -21.774331\n",
      "ep 1545: ep_len:585 episode reward: total was -28.090000. running mean: -21.837487\n",
      "ep 1545: ep_len:580 episode reward: total was -52.650000. running mean: -22.145612\n",
      "ep 1545: ep_len:505 episode reward: total was -22.670000. running mean: -22.150856\n",
      "ep 1545: ep_len:3 episode reward: total was 0.000000. running mean: -21.929348\n",
      "ep 1545: ep_len:525 episode reward: total was -30.610000. running mean: -22.016154\n",
      "ep 1545: ep_len:302 episode reward: total was -6.800000. running mean: -21.863993\n",
      "epsilon:0.188971 episode_count: 10822. steps_count: 4781658.000000\n",
      "ep 1546: ep_len:560 episode reward: total was -5.060000. running mean: -21.695953\n",
      "ep 1546: ep_len:500 episode reward: total was 6.190000. running mean: -21.417093\n",
      "ep 1546: ep_len:500 episode reward: total was -33.400000. running mean: -21.536922\n",
      "ep 1546: ep_len:403 episode reward: total was -20.750000. running mean: -21.529053\n",
      "ep 1546: ep_len:3 episode reward: total was 0.000000. running mean: -21.313763\n",
      "ep 1546: ep_len:182 episode reward: total was -6.940000. running mean: -21.170025\n",
      "ep 1546: ep_len:565 episode reward: total was -29.190000. running mean: -21.250225\n",
      "epsilon:0.188834 episode_count: 10829. steps_count: 4784371.000000\n",
      "ep 1547: ep_len:595 episode reward: total was -51.040000. running mean: -21.548122\n",
      "ep 1547: ep_len:530 episode reward: total was -29.130000. running mean: -21.623941\n",
      "ep 1547: ep_len:590 episode reward: total was -20.620000. running mean: -21.613902\n",
      "ep 1547: ep_len:595 episode reward: total was -16.640000. running mean: -21.564163\n",
      "ep 1547: ep_len:3 episode reward: total was 0.000000. running mean: -21.348521\n",
      "ep 1547: ep_len:336 episode reward: total was -14.380000. running mean: -21.278836\n",
      "ep 1547: ep_len:540 episode reward: total was -27.030000. running mean: -21.336348\n",
      "epsilon:0.188698 episode_count: 10836. steps_count: 4787560.000000\n",
      "ep 1548: ep_len:545 episode reward: total was -23.070000. running mean: -21.353684\n",
      "ep 1548: ep_len:645 episode reward: total was -27.160000. running mean: -21.411747\n",
      "ep 1548: ep_len:500 episode reward: total was -15.050000. running mean: -21.348130\n",
      "ep 1548: ep_len:505 episode reward: total was -23.030000. running mean: -21.364949\n",
      "ep 1548: ep_len:3 episode reward: total was 0.000000. running mean: -21.151299\n",
      "ep 1548: ep_len:500 episode reward: total was -41.860000. running mean: -21.358386\n",
      "ep 1548: ep_len:545 episode reward: total was -21.940000. running mean: -21.364202\n",
      "epsilon:0.188561 episode_count: 10843. steps_count: 4790803.000000\n",
      "ep 1549: ep_len:1045 episode reward: total was -159.880000. running mean: -22.749360\n",
      "ep 1549: ep_len:565 episode reward: total was -25.040000. running mean: -22.772267\n",
      "ep 1549: ep_len:575 episode reward: total was -30.450000. running mean: -22.849044\n",
      "ep 1549: ep_len:505 episode reward: total was -8.710000. running mean: -22.707653\n",
      "ep 1549: ep_len:3 episode reward: total was 0.000000. running mean: -22.480577\n",
      "ep 1549: ep_len:600 episode reward: total was -31.870000. running mean: -22.574471\n",
      "ep 1549: ep_len:515 episode reward: total was -44.140000. running mean: -22.790126\n",
      "epsilon:0.188425 episode_count: 10850. steps_count: 4794611.000000\n",
      "ep 1550: ep_len:650 episode reward: total was -39.330000. running mean: -22.955525\n",
      "ep 1550: ep_len:635 episode reward: total was -47.480000. running mean: -23.200770\n",
      "ep 1550: ep_len:433 episode reward: total was -18.340000. running mean: -23.152162\n",
      "ep 1550: ep_len:56 episode reward: total was -1.470000. running mean: -22.935341\n",
      "ep 1550: ep_len:3 episode reward: total was 0.000000. running mean: -22.705987\n",
      "ep 1550: ep_len:302 episode reward: total was -10.380000. running mean: -22.582727\n",
      "ep 1550: ep_len:500 episode reward: total was -26.350000. running mean: -22.620400\n",
      "epsilon:0.188288 episode_count: 10857. steps_count: 4797190.000000\n",
      "ep 1551: ep_len:575 episode reward: total was -20.670000. running mean: -22.600896\n",
      "ep 1551: ep_len:535 episode reward: total was -51.120000. running mean: -22.886087\n",
      "ep 1551: ep_len:630 episode reward: total was -11.580000. running mean: -22.773026\n",
      "ep 1551: ep_len:565 episode reward: total was -44.020000. running mean: -22.985496\n",
      "ep 1551: ep_len:3 episode reward: total was 0.000000. running mean: -22.755641\n",
      "ep 1551: ep_len:500 episode reward: total was -13.640000. running mean: -22.664485\n",
      "ep 1551: ep_len:590 episode reward: total was -16.480000. running mean: -22.602640\n",
      "epsilon:0.188152 episode_count: 10864. steps_count: 4800588.000000\n",
      "ep 1552: ep_len:575 episode reward: total was -10.070000. running mean: -22.477313\n",
      "ep 1552: ep_len:675 episode reward: total was -13.040000. running mean: -22.382940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1552: ep_len:374 episode reward: total was -24.890000. running mean: -22.408011\n",
      "ep 1552: ep_len:520 episode reward: total was -20.640000. running mean: -22.390331\n",
      "ep 1552: ep_len:130 episode reward: total was 4.070000. running mean: -22.125727\n",
      "ep 1552: ep_len:500 episode reward: total was -29.760000. running mean: -22.202070\n",
      "ep 1552: ep_len:299 episode reward: total was -3.750000. running mean: -22.017549\n",
      "epsilon:0.188015 episode_count: 10871. steps_count: 4803661.000000\n",
      "ep 1553: ep_len:570 episode reward: total was -33.840000. running mean: -22.135774\n",
      "ep 1553: ep_len:525 episode reward: total was -29.840000. running mean: -22.212816\n",
      "ep 1553: ep_len:560 episode reward: total was -22.320000. running mean: -22.213888\n",
      "ep 1553: ep_len:505 episode reward: total was -4.020000. running mean: -22.031949\n",
      "ep 1553: ep_len:3 episode reward: total was 0.000000. running mean: -21.811630\n",
      "ep 1553: ep_len:595 episode reward: total was -1.970000. running mean: -21.613213\n",
      "ep 1553: ep_len:605 episode reward: total was -29.520000. running mean: -21.692281\n",
      "epsilon:0.187879 episode_count: 10878. steps_count: 4807024.000000\n",
      "ep 1554: ep_len:249 episode reward: total was -1.380000. running mean: -21.489158\n",
      "ep 1554: ep_len:505 episode reward: total was -22.520000. running mean: -21.499467\n",
      "ep 1554: ep_len:560 episode reward: total was -45.020000. running mean: -21.734672\n",
      "ep 1554: ep_len:500 episode reward: total was -14.190000. running mean: -21.659225\n",
      "ep 1554: ep_len:92 episode reward: total was -11.970000. running mean: -21.562333\n",
      "ep 1554: ep_len:595 episode reward: total was -30.360000. running mean: -21.650310\n",
      "ep 1554: ep_len:190 episode reward: total was -8.830000. running mean: -21.522107\n",
      "epsilon:0.187742 episode_count: 10885. steps_count: 4809715.000000\n",
      "ep 1555: ep_len:520 episode reward: total was -20.060000. running mean: -21.507486\n",
      "ep 1555: ep_len:505 episode reward: total was -7.310000. running mean: -21.365511\n",
      "ep 1555: ep_len:445 episode reward: total was -11.850000. running mean: -21.270356\n",
      "ep 1555: ep_len:560 episode reward: total was -15.060000. running mean: -21.208252\n",
      "ep 1555: ep_len:3 episode reward: total was 0.000000. running mean: -20.996170\n",
      "ep 1555: ep_len:610 episode reward: total was -51.540000. running mean: -21.301608\n",
      "ep 1555: ep_len:525 episode reward: total was -26.650000. running mean: -21.355092\n",
      "epsilon:0.187606 episode_count: 10892. steps_count: 4812883.000000\n",
      "ep 1556: ep_len:107 episode reward: total was -6.960000. running mean: -21.211141\n",
      "ep 1556: ep_len:575 episode reward: total was -41.050000. running mean: -21.409530\n",
      "ep 1556: ep_len:515 episode reward: total was -32.990000. running mean: -21.525334\n",
      "ep 1556: ep_len:545 episode reward: total was -5.660000. running mean: -21.366681\n",
      "ep 1556: ep_len:3 episode reward: total was 0.000000. running mean: -21.153014\n",
      "ep 1556: ep_len:600 episode reward: total was -35.730000. running mean: -21.298784\n",
      "ep 1556: ep_len:331 episode reward: total was -14.870000. running mean: -21.234496\n",
      "epsilon:0.187469 episode_count: 10899. steps_count: 4815559.000000\n",
      "ep 1557: ep_len:570 episode reward: total was -47.980000. running mean: -21.501951\n",
      "ep 1557: ep_len:530 episode reward: total was -28.630000. running mean: -21.573232\n",
      "ep 1557: ep_len:575 episode reward: total was -52.200000. running mean: -21.879499\n",
      "ep 1557: ep_len:500 episode reward: total was -12.060000. running mean: -21.781304\n",
      "ep 1557: ep_len:129 episode reward: total was 3.060000. running mean: -21.532891\n",
      "ep 1557: ep_len:500 episode reward: total was -42.350000. running mean: -21.741062\n",
      "ep 1557: ep_len:555 episode reward: total was -24.190000. running mean: -21.765552\n",
      "epsilon:0.187333 episode_count: 10906. steps_count: 4818918.000000\n",
      "ep 1558: ep_len:645 episode reward: total was -37.870000. running mean: -21.926596\n",
      "ep 1558: ep_len:600 episode reward: total was -24.540000. running mean: -21.952730\n",
      "ep 1558: ep_len:500 episode reward: total was -7.550000. running mean: -21.808703\n",
      "ep 1558: ep_len:500 episode reward: total was -71.920000. running mean: -22.309816\n",
      "ep 1558: ep_len:3 episode reward: total was 0.000000. running mean: -22.086718\n",
      "ep 1558: ep_len:505 episode reward: total was -27.840000. running mean: -22.144251\n",
      "ep 1558: ep_len:590 episode reward: total was -37.980000. running mean: -22.302608\n",
      "epsilon:0.187196 episode_count: 10913. steps_count: 4822261.000000\n",
      "ep 1559: ep_len:635 episode reward: total was -31.620000. running mean: -22.395782\n",
      "ep 1559: ep_len:505 episode reward: total was -22.800000. running mean: -22.399824\n",
      "ep 1559: ep_len:550 episode reward: total was -48.670000. running mean: -22.662526\n",
      "ep 1559: ep_len:56 episode reward: total was 2.570000. running mean: -22.410201\n",
      "ep 1559: ep_len:119 episode reward: total was -12.950000. running mean: -22.315599\n",
      "ep 1559: ep_len:680 episode reward: total was -44.760000. running mean: -22.540043\n",
      "ep 1559: ep_len:605 episode reward: total was -39.210000. running mean: -22.706742\n",
      "epsilon:0.187060 episode_count: 10920. steps_count: 4825411.000000\n",
      "ep 1560: ep_len:545 episode reward: total was -29.200000. running mean: -22.771675\n",
      "ep 1560: ep_len:500 episode reward: total was -17.850000. running mean: -22.722458\n",
      "ep 1560: ep_len:660 episode reward: total was -36.540000. running mean: -22.860634\n",
      "ep 1560: ep_len:505 episode reward: total was -31.050000. running mean: -22.942527\n",
      "ep 1560: ep_len:3 episode reward: total was 0.000000. running mean: -22.713102\n",
      "ep 1560: ep_len:565 episode reward: total was -90.850000. running mean: -23.394471\n",
      "ep 1560: ep_len:590 episode reward: total was -17.380000. running mean: -23.334326\n",
      "epsilon:0.186923 episode_count: 10927. steps_count: 4828779.000000\n",
      "ep 1561: ep_len:213 episode reward: total was 2.100000. running mean: -23.079983\n",
      "ep 1561: ep_len:254 episode reward: total was -10.850000. running mean: -22.957683\n",
      "ep 1561: ep_len:545 episode reward: total was -30.970000. running mean: -23.037806\n",
      "ep 1561: ep_len:565 episode reward: total was -8.540000. running mean: -22.892828\n",
      "ep 1561: ep_len:3 episode reward: total was 0.000000. running mean: -22.663900\n",
      "ep 1561: ep_len:765 episode reward: total was -101.870000. running mean: -23.455961\n",
      "ep 1561: ep_len:555 episode reward: total was -73.210000. running mean: -23.953501\n",
      "epsilon:0.186787 episode_count: 10934. steps_count: 4831679.000000\n",
      "ep 1562: ep_len:500 episode reward: total was -13.800000. running mean: -23.851966\n",
      "ep 1562: ep_len:590 episode reward: total was -13.580000. running mean: -23.749247\n",
      "ep 1562: ep_len:600 episode reward: total was -55.260000. running mean: -24.064354\n",
      "ep 1562: ep_len:397 episode reward: total was -16.230000. running mean: -23.986011\n",
      "ep 1562: ep_len:96 episode reward: total was 3.050000. running mean: -23.715651\n",
      "ep 1562: ep_len:590 episode reward: total was -37.550000. running mean: -23.853994\n",
      "ep 1562: ep_len:585 episode reward: total was -39.210000. running mean: -24.007554\n",
      "epsilon:0.186650 episode_count: 10941. steps_count: 4835037.000000\n",
      "ep 1563: ep_len:500 episode reward: total was -31.030000. running mean: -24.077779\n",
      "ep 1563: ep_len:560 episode reward: total was -2.650000. running mean: -23.863501\n",
      "ep 1563: ep_len:570 episode reward: total was -24.860000. running mean: -23.873466\n",
      "ep 1563: ep_len:590 episode reward: total was -33.690000. running mean: -23.971631\n",
      "ep 1563: ep_len:3 episode reward: total was 0.000000. running mean: -23.731915\n",
      "ep 1563: ep_len:565 episode reward: total was -32.890000. running mean: -23.823496\n",
      "ep 1563: ep_len:325 episode reward: total was -17.830000. running mean: -23.763561\n",
      "epsilon:0.186514 episode_count: 10948. steps_count: 4838150.000000\n",
      "ep 1564: ep_len:595 episode reward: total was -19.610000. running mean: -23.722025\n",
      "ep 1564: ep_len:645 episode reward: total was -19.430000. running mean: -23.679105\n",
      "ep 1564: ep_len:590 episode reward: total was -45.100000. running mean: -23.893314\n",
      "ep 1564: ep_len:520 episode reward: total was -11.960000. running mean: -23.773981\n",
      "ep 1564: ep_len:88 episode reward: total was 1.530000. running mean: -23.520941\n",
      "ep 1564: ep_len:500 episode reward: total was -21.780000. running mean: -23.503531\n",
      "ep 1564: ep_len:615 episode reward: total was -65.630000. running mean: -23.924796\n",
      "epsilon:0.186377 episode_count: 10955. steps_count: 4841703.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1565: ep_len:600 episode reward: total was -16.130000. running mean: -23.846848\n",
      "ep 1565: ep_len:785 episode reward: total was -89.300000. running mean: -24.501380\n",
      "ep 1565: ep_len:535 episode reward: total was -39.070000. running mean: -24.647066\n",
      "ep 1565: ep_len:371 episode reward: total was -34.690000. running mean: -24.747495\n",
      "ep 1565: ep_len:3 episode reward: total was 0.000000. running mean: -24.500020\n",
      "ep 1565: ep_len:301 episode reward: total was -8.360000. running mean: -24.338620\n",
      "ep 1565: ep_len:200 episode reward: total was -6.340000. running mean: -24.158634\n",
      "epsilon:0.186241 episode_count: 10962. steps_count: 4844498.000000\n",
      "ep 1566: ep_len:580 episode reward: total was -15.470000. running mean: -24.071748\n",
      "ep 1566: ep_len:530 episode reward: total was -32.510000. running mean: -24.156130\n",
      "ep 1566: ep_len:575 episode reward: total was -34.640000. running mean: -24.260969\n",
      "ep 1566: ep_len:520 episode reward: total was -39.710000. running mean: -24.415459\n",
      "ep 1566: ep_len:3 episode reward: total was 0.000000. running mean: -24.171304\n",
      "ep 1566: ep_len:585 episode reward: total was -16.790000. running mean: -24.097491\n",
      "ep 1566: ep_len:600 episode reward: total was -31.470000. running mean: -24.171216\n",
      "epsilon:0.186104 episode_count: 10969. steps_count: 4847891.000000\n",
      "ep 1567: ep_len:550 episode reward: total was -7.600000. running mean: -24.005504\n",
      "ep 1567: ep_len:590 episode reward: total was -26.680000. running mean: -24.032249\n",
      "ep 1567: ep_len:650 episode reward: total was -39.940000. running mean: -24.191327\n",
      "ep 1567: ep_len:110 episode reward: total was -0.400000. running mean: -23.953414\n",
      "ep 1567: ep_len:3 episode reward: total was 0.000000. running mean: -23.713879\n",
      "ep 1567: ep_len:645 episode reward: total was -25.660000. running mean: -23.733341\n",
      "ep 1567: ep_len:620 episode reward: total was -26.080000. running mean: -23.756807\n",
      "epsilon:0.185968 episode_count: 10976. steps_count: 4851059.000000\n",
      "ep 1568: ep_len:255 episode reward: total was -7.380000. running mean: -23.593039\n",
      "ep 1568: ep_len:324 episode reward: total was -12.870000. running mean: -23.485809\n",
      "ep 1568: ep_len:425 episode reward: total was -5.730000. running mean: -23.308251\n",
      "ep 1568: ep_len:500 episode reward: total was -36.660000. running mean: -23.441768\n",
      "ep 1568: ep_len:3 episode reward: total was 0.000000. running mean: -23.207350\n",
      "ep 1568: ep_len:168 episode reward: total was -4.390000. running mean: -23.019177\n",
      "ep 1568: ep_len:198 episode reward: total was -7.360000. running mean: -22.862585\n",
      "epsilon:0.185831 episode_count: 10983. steps_count: 4852932.000000\n",
      "ep 1569: ep_len:520 episode reward: total was -65.720000. running mean: -23.291159\n",
      "ep 1569: ep_len:635 episode reward: total was -9.130000. running mean: -23.149548\n",
      "ep 1569: ep_len:650 episode reward: total was -31.860000. running mean: -23.236652\n",
      "ep 1569: ep_len:500 episode reward: total was -6.570000. running mean: -23.069986\n",
      "ep 1569: ep_len:56 episode reward: total was 4.000000. running mean: -22.799286\n",
      "ep 1569: ep_len:500 episode reward: total was -31.620000. running mean: -22.887493\n",
      "ep 1569: ep_len:525 episode reward: total was -34.050000. running mean: -22.999118\n",
      "epsilon:0.185695 episode_count: 10990. steps_count: 4856318.000000\n",
      "ep 1570: ep_len:680 episode reward: total was -31.310000. running mean: -23.082227\n",
      "ep 1570: ep_len:575 episode reward: total was -30.500000. running mean: -23.156405\n",
      "ep 1570: ep_len:45 episode reward: total was -2.990000. running mean: -22.954741\n",
      "ep 1570: ep_len:500 episode reward: total was -27.080000. running mean: -22.995993\n",
      "ep 1570: ep_len:90 episode reward: total was 4.540000. running mean: -22.720633\n",
      "ep 1570: ep_len:500 episode reward: total was -28.680000. running mean: -22.780227\n",
      "ep 1570: ep_len:500 episode reward: total was -22.930000. running mean: -22.781725\n",
      "epsilon:0.185558 episode_count: 10997. steps_count: 4859208.000000\n",
      "ep 1571: ep_len:262 episode reward: total was -1.870000. running mean: -22.572607\n",
      "ep 1571: ep_len:500 episode reward: total was -37.930000. running mean: -22.726181\n",
      "ep 1571: ep_len:620 episode reward: total was -34.600000. running mean: -22.844920\n",
      "ep 1571: ep_len:510 episode reward: total was -40.190000. running mean: -23.018370\n",
      "ep 1571: ep_len:125 episode reward: total was 3.540000. running mean: -22.752787\n",
      "ep 1571: ep_len:645 episode reward: total was -35.420000. running mean: -22.879459\n",
      "ep 1571: ep_len:580 episode reward: total was -24.110000. running mean: -22.891764\n",
      "epsilon:0.185422 episode_count: 11004. steps_count: 4862450.000000\n",
      "ep 1572: ep_len:525 episode reward: total was -25.990000. running mean: -22.922747\n",
      "ep 1572: ep_len:276 episode reward: total was -19.870000. running mean: -22.892219\n",
      "ep 1572: ep_len:460 episode reward: total was -18.790000. running mean: -22.851197\n",
      "ep 1572: ep_len:56 episode reward: total was -2.950000. running mean: -22.652185\n",
      "ep 1572: ep_len:3 episode reward: total was 0.000000. running mean: -22.425663\n",
      "ep 1572: ep_len:675 episode reward: total was -44.970000. running mean: -22.651106\n",
      "ep 1572: ep_len:625 episode reward: total was -38.000000. running mean: -22.804595\n",
      "epsilon:0.185285 episode_count: 11011. steps_count: 4865070.000000\n",
      "ep 1573: ep_len:500 episode reward: total was -26.780000. running mean: -22.844349\n",
      "ep 1573: ep_len:1160 episode reward: total was -68.550000. running mean: -23.301406\n",
      "ep 1573: ep_len:575 episode reward: total was -25.020000. running mean: -23.318592\n",
      "ep 1573: ep_len:500 episode reward: total was -32.680000. running mean: -23.412206\n",
      "ep 1573: ep_len:3 episode reward: total was 0.000000. running mean: -23.178084\n",
      "ep 1573: ep_len:540 episode reward: total was -35.270000. running mean: -23.299003\n",
      "ep 1573: ep_len:575 episode reward: total was -31.640000. running mean: -23.382413\n",
      "epsilon:0.185149 episode_count: 11018. steps_count: 4868923.000000\n",
      "ep 1574: ep_len:500 episode reward: total was -32.520000. running mean: -23.473789\n",
      "ep 1574: ep_len:300 episode reward: total was -22.330000. running mean: -23.462351\n",
      "ep 1574: ep_len:510 episode reward: total was -24.720000. running mean: -23.474927\n",
      "ep 1574: ep_len:535 episode reward: total was -5.110000. running mean: -23.291278\n",
      "ep 1574: ep_len:3 episode reward: total was 0.000000. running mean: -23.058365\n",
      "ep 1574: ep_len:279 episode reward: total was 0.160000. running mean: -22.826182\n",
      "ep 1574: ep_len:288 episode reward: total was -19.380000. running mean: -22.791720\n",
      "epsilon:0.185012 episode_count: 11025. steps_count: 4871338.000000\n",
      "ep 1575: ep_len:575 episode reward: total was -26.070000. running mean: -22.824503\n",
      "ep 1575: ep_len:585 episode reward: total was -15.660000. running mean: -22.752858\n",
      "ep 1575: ep_len:645 episode reward: total was -46.100000. running mean: -22.986329\n",
      "ep 1575: ep_len:605 episode reward: total was -38.040000. running mean: -23.136866\n",
      "ep 1575: ep_len:52 episode reward: total was 2.000000. running mean: -22.885497\n",
      "ep 1575: ep_len:640 episode reward: total was -46.650000. running mean: -23.123142\n",
      "ep 1575: ep_len:500 episode reward: total was -41.080000. running mean: -23.302711\n",
      "epsilon:0.184876 episode_count: 11032. steps_count: 4874940.000000\n",
      "ep 1576: ep_len:615 episode reward: total was -24.510000. running mean: -23.314784\n",
      "ep 1576: ep_len:500 episode reward: total was -9.060000. running mean: -23.172236\n",
      "ep 1576: ep_len:660 episode reward: total was -22.970000. running mean: -23.170213\n",
      "ep 1576: ep_len:500 episode reward: total was 2.390000. running mean: -22.914611\n",
      "ep 1576: ep_len:108 episode reward: total was -2.950000. running mean: -22.714965\n",
      "ep 1576: ep_len:186 episode reward: total was -2.420000. running mean: -22.512016\n",
      "ep 1576: ep_len:600 episode reward: total was -20.910000. running mean: -22.495995\n",
      "epsilon:0.184739 episode_count: 11039. steps_count: 4878109.000000\n",
      "ep 1577: ep_len:555 episode reward: total was -11.450000. running mean: -22.385535\n",
      "ep 1577: ep_len:500 episode reward: total was -45.630000. running mean: -22.617980\n",
      "ep 1577: ep_len:500 episode reward: total was -36.290000. running mean: -22.754700\n",
      "ep 1577: ep_len:500 episode reward: total was -11.580000. running mean: -22.642953\n",
      "ep 1577: ep_len:3 episode reward: total was 0.000000. running mean: -22.416524\n",
      "ep 1577: ep_len:550 episode reward: total was -38.120000. running mean: -22.573559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1577: ep_len:595 episode reward: total was -24.110000. running mean: -22.588923\n",
      "epsilon:0.184603 episode_count: 11046. steps_count: 4881312.000000\n",
      "ep 1578: ep_len:515 episode reward: total was -36.050000. running mean: -22.723534\n",
      "ep 1578: ep_len:500 episode reward: total was -12.270000. running mean: -22.618998\n",
      "ep 1578: ep_len:393 episode reward: total was -4.340000. running mean: -22.436208\n",
      "ep 1578: ep_len:510 episode reward: total was -30.690000. running mean: -22.518746\n",
      "ep 1578: ep_len:3 episode reward: total was 0.000000. running mean: -22.293559\n",
      "ep 1578: ep_len:336 episode reward: total was -6.360000. running mean: -22.134223\n",
      "ep 1578: ep_len:315 episode reward: total was -15.340000. running mean: -22.066281\n",
      "epsilon:0.184466 episode_count: 11053. steps_count: 4883884.000000\n",
      "ep 1579: ep_len:510 episode reward: total was -41.610000. running mean: -22.261718\n",
      "ep 1579: ep_len:520 episode reward: total was -37.200000. running mean: -22.411101\n",
      "ep 1579: ep_len:540 episode reward: total was -21.190000. running mean: -22.398890\n",
      "ep 1579: ep_len:540 episode reward: total was -53.700000. running mean: -22.711901\n",
      "ep 1579: ep_len:3 episode reward: total was 0.000000. running mean: -22.484782\n",
      "ep 1579: ep_len:600 episode reward: total was -28.650000. running mean: -22.546434\n",
      "ep 1579: ep_len:525 episode reward: total was -25.690000. running mean: -22.577870\n",
      "epsilon:0.184330 episode_count: 11060. steps_count: 4887122.000000\n",
      "ep 1580: ep_len:575 episode reward: total was -5.150000. running mean: -22.403591\n",
      "ep 1580: ep_len:590 episode reward: total was -14.040000. running mean: -22.319955\n",
      "ep 1580: ep_len:575 episode reward: total was -11.810000. running mean: -22.214856\n",
      "ep 1580: ep_len:500 episode reward: total was -1.030000. running mean: -22.003007\n",
      "ep 1580: ep_len:3 episode reward: total was 0.000000. running mean: -21.782977\n",
      "ep 1580: ep_len:635 episode reward: total was -10.430000. running mean: -21.669447\n",
      "ep 1580: ep_len:535 episode reward: total was -13.990000. running mean: -21.592653\n",
      "epsilon:0.184193 episode_count: 11067. steps_count: 4890535.000000\n",
      "ep 1581: ep_len:265 episode reward: total was -5.380000. running mean: -21.430526\n",
      "ep 1581: ep_len:515 episode reward: total was -5.670000. running mean: -21.272921\n",
      "ep 1581: ep_len:600 episode reward: total was -17.630000. running mean: -21.236492\n",
      "ep 1581: ep_len:56 episode reward: total was -0.460000. running mean: -21.028727\n",
      "ep 1581: ep_len:3 episode reward: total was 0.000000. running mean: -20.818440\n",
      "ep 1581: ep_len:535 episode reward: total was -29.490000. running mean: -20.905155\n",
      "ep 1581: ep_len:301 episode reward: total was -20.860000. running mean: -20.904704\n",
      "epsilon:0.184057 episode_count: 11074. steps_count: 4892810.000000\n",
      "ep 1582: ep_len:680 episode reward: total was -38.410000. running mean: -21.079757\n",
      "ep 1582: ep_len:645 episode reward: total was -15.000000. running mean: -21.018959\n",
      "ep 1582: ep_len:500 episode reward: total was -30.260000. running mean: -21.111370\n",
      "ep 1582: ep_len:530 episode reward: total was -4.140000. running mean: -20.941656\n",
      "ep 1582: ep_len:3 episode reward: total was 0.000000. running mean: -20.732239\n",
      "ep 1582: ep_len:580 episode reward: total was -9.080000. running mean: -20.615717\n",
      "ep 1582: ep_len:211 episode reward: total was -7.850000. running mean: -20.488060\n",
      "epsilon:0.183920 episode_count: 11081. steps_count: 4895959.000000\n",
      "ep 1583: ep_len:560 episode reward: total was -9.940000. running mean: -20.382579\n",
      "ep 1583: ep_len:288 episode reward: total was -28.360000. running mean: -20.462353\n",
      "ep 1583: ep_len:505 episode reward: total was -7.970000. running mean: -20.337430\n",
      "ep 1583: ep_len:565 episode reward: total was -50.090000. running mean: -20.634956\n",
      "ep 1583: ep_len:3 episode reward: total was 0.000000. running mean: -20.428606\n",
      "ep 1583: ep_len:535 episode reward: total was -15.500000. running mean: -20.379320\n",
      "ep 1583: ep_len:525 episode reward: total was -21.380000. running mean: -20.389327\n",
      "epsilon:0.183784 episode_count: 11088. steps_count: 4898940.000000\n",
      "ep 1584: ep_len:206 episode reward: total was 1.600000. running mean: -20.169433\n",
      "ep 1584: ep_len:775 episode reward: total was -63.380000. running mean: -20.601539\n",
      "ep 1584: ep_len:580 episode reward: total was -16.250000. running mean: -20.558024\n",
      "ep 1584: ep_len:402 episode reward: total was -6.150000. running mean: -20.413943\n",
      "ep 1584: ep_len:3 episode reward: total was 0.000000. running mean: -20.209804\n",
      "ep 1584: ep_len:720 episode reward: total was -41.360000. running mean: -20.421306\n",
      "ep 1584: ep_len:590 episode reward: total was -31.650000. running mean: -20.533593\n",
      "epsilon:0.183647 episode_count: 11095. steps_count: 4902216.000000\n",
      "ep 1585: ep_len:500 episode reward: total was -34.550000. running mean: -20.673757\n",
      "ep 1585: ep_len:630 episode reward: total was -1.640000. running mean: -20.483419\n",
      "ep 1585: ep_len:605 episode reward: total was -25.490000. running mean: -20.533485\n",
      "ep 1585: ep_len:500 episode reward: total was -18.630000. running mean: -20.514450\n",
      "ep 1585: ep_len:3 episode reward: total was 0.000000. running mean: -20.309306\n",
      "ep 1585: ep_len:545 episode reward: total was -25.640000. running mean: -20.362613\n",
      "ep 1585: ep_len:500 episode reward: total was -29.810000. running mean: -20.457087\n",
      "epsilon:0.183511 episode_count: 11102. steps_count: 4905499.000000\n",
      "ep 1586: ep_len:525 episode reward: total was -35.190000. running mean: -20.604416\n",
      "ep 1586: ep_len:555 episode reward: total was -7.220000. running mean: -20.470572\n",
      "ep 1586: ep_len:650 episode reward: total was -50.900000. running mean: -20.774866\n",
      "ep 1586: ep_len:408 episode reward: total was -10.180000. running mean: -20.668917\n",
      "ep 1586: ep_len:80 episode reward: total was -7.950000. running mean: -20.541728\n",
      "ep 1586: ep_len:620 episode reward: total was -9.520000. running mean: -20.431511\n",
      "ep 1586: ep_len:194 episode reward: total was -3.880000. running mean: -20.265996\n",
      "epsilon:0.183374 episode_count: 11109. steps_count: 4908531.000000\n",
      "ep 1587: ep_len:500 episode reward: total was -21.740000. running mean: -20.280736\n",
      "ep 1587: ep_len:555 episode reward: total was -17.550000. running mean: -20.253428\n",
      "ep 1587: ep_len:445 episode reward: total was -24.350000. running mean: -20.294394\n",
      "ep 1587: ep_len:540 episode reward: total was -30.580000. running mean: -20.397250\n",
      "ep 1587: ep_len:3 episode reward: total was 0.000000. running mean: -20.193278\n",
      "ep 1587: ep_len:655 episode reward: total was -31.790000. running mean: -20.309245\n",
      "ep 1587: ep_len:570 episode reward: total was -24.460000. running mean: -20.350752\n",
      "epsilon:0.183238 episode_count: 11116. steps_count: 4911799.000000\n",
      "ep 1588: ep_len:575 episode reward: total was -36.400000. running mean: -20.511245\n",
      "ep 1588: ep_len:585 episode reward: total was -20.170000. running mean: -20.507833\n",
      "ep 1588: ep_len:510 episode reward: total was -33.470000. running mean: -20.637454\n",
      "ep 1588: ep_len:420 episode reward: total was -19.710000. running mean: -20.628180\n",
      "ep 1588: ep_len:3 episode reward: total was 0.000000. running mean: -20.421898\n",
      "ep 1588: ep_len:550 episode reward: total was -43.590000. running mean: -20.653579\n",
      "ep 1588: ep_len:580 episode reward: total was -17.620000. running mean: -20.623243\n",
      "epsilon:0.183101 episode_count: 11123. steps_count: 4915022.000000\n",
      "ep 1589: ep_len:550 episode reward: total was -16.510000. running mean: -20.582111\n",
      "ep 1589: ep_len:575 episode reward: total was -46.060000. running mean: -20.836890\n",
      "ep 1589: ep_len:79 episode reward: total was 1.050000. running mean: -20.618021\n",
      "ep 1589: ep_len:595 episode reward: total was -52.710000. running mean: -20.938940\n",
      "ep 1589: ep_len:48 episode reward: total was 1.500000. running mean: -20.714551\n",
      "ep 1589: ep_len:500 episode reward: total was -11.300000. running mean: -20.620406\n",
      "ep 1589: ep_len:530 episode reward: total was -36.550000. running mean: -20.779701\n",
      "epsilon:0.182965 episode_count: 11130. steps_count: 4917899.000000\n",
      "ep 1590: ep_len:595 episode reward: total was -25.700000. running mean: -20.828904\n",
      "ep 1590: ep_len:810 episode reward: total was -69.700000. running mean: -21.317615\n",
      "ep 1590: ep_len:355 episode reward: total was -12.910000. running mean: -21.233539\n",
      "ep 1590: ep_len:50 episode reward: total was -2.960000. running mean: -21.050804\n",
      "ep 1590: ep_len:3 episode reward: total was 0.000000. running mean: -20.840296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1590: ep_len:500 episode reward: total was -41.760000. running mean: -21.049493\n",
      "ep 1590: ep_len:525 episode reward: total was -34.960000. running mean: -21.188598\n",
      "epsilon:0.182828 episode_count: 11137. steps_count: 4920737.000000\n",
      "ep 1591: ep_len:560 episode reward: total was -36.430000. running mean: -21.341012\n",
      "ep 1591: ep_len:500 episode reward: total was -35.190000. running mean: -21.479502\n",
      "ep 1591: ep_len:500 episode reward: total was -12.690000. running mean: -21.391607\n",
      "ep 1591: ep_len:615 episode reward: total was -7.010000. running mean: -21.247791\n",
      "ep 1591: ep_len:3 episode reward: total was 0.000000. running mean: -21.035313\n",
      "ep 1591: ep_len:530 episode reward: total was -33.510000. running mean: -21.160060\n",
      "ep 1591: ep_len:620 episode reward: total was -27.350000. running mean: -21.221959\n",
      "epsilon:0.182692 episode_count: 11144. steps_count: 4924065.000000\n",
      "ep 1592: ep_len:630 episode reward: total was -50.390000. running mean: -21.513640\n",
      "ep 1592: ep_len:500 episode reward: total was -13.630000. running mean: -21.434803\n",
      "ep 1592: ep_len:600 episode reward: total was -22.160000. running mean: -21.442055\n",
      "ep 1592: ep_len:500 episode reward: total was -22.610000. running mean: -21.453735\n",
      "ep 1592: ep_len:3 episode reward: total was 0.000000. running mean: -21.239197\n",
      "ep 1592: ep_len:675 episode reward: total was -41.970000. running mean: -21.446505\n",
      "ep 1592: ep_len:505 episode reward: total was -47.750000. running mean: -21.709540\n",
      "epsilon:0.182555 episode_count: 11151. steps_count: 4927478.000000\n",
      "ep 1593: ep_len:635 episode reward: total was -26.810000. running mean: -21.760545\n",
      "ep 1593: ep_len:540 episode reward: total was -42.090000. running mean: -21.963839\n",
      "ep 1593: ep_len:555 episode reward: total was -25.930000. running mean: -22.003501\n",
      "ep 1593: ep_len:500 episode reward: total was -20.230000. running mean: -21.985766\n",
      "ep 1593: ep_len:3 episode reward: total was 0.000000. running mean: -21.765908\n",
      "ep 1593: ep_len:630 episode reward: total was -28.070000. running mean: -21.828949\n",
      "ep 1593: ep_len:525 episode reward: total was -19.420000. running mean: -21.804860\n",
      "epsilon:0.182419 episode_count: 11158. steps_count: 4930866.000000\n",
      "ep 1594: ep_len:234 episode reward: total was -13.930000. running mean: -21.726111\n",
      "ep 1594: ep_len:550 episode reward: total was -2.140000. running mean: -21.530250\n",
      "ep 1594: ep_len:545 episode reward: total was -37.470000. running mean: -21.689647\n",
      "ep 1594: ep_len:585 episode reward: total was -15.210000. running mean: -21.624851\n",
      "ep 1594: ep_len:80 episode reward: total was -9.450000. running mean: -21.503103\n",
      "ep 1594: ep_len:535 episode reward: total was -10.120000. running mean: -21.389271\n",
      "ep 1594: ep_len:262 episode reward: total was -11.870000. running mean: -21.294079\n",
      "epsilon:0.182282 episode_count: 11165. steps_count: 4933657.000000\n",
      "ep 1595: ep_len:580 episode reward: total was -24.010000. running mean: -21.321238\n",
      "ep 1595: ep_len:695 episode reward: total was -74.610000. running mean: -21.854126\n",
      "ep 1595: ep_len:434 episode reward: total was -14.810000. running mean: -21.783684\n",
      "ep 1595: ep_len:540 episode reward: total was -13.610000. running mean: -21.701947\n",
      "ep 1595: ep_len:100 episode reward: total was -5.940000. running mean: -21.544328\n",
      "ep 1595: ep_len:515 episode reward: total was -55.680000. running mean: -21.885685\n",
      "ep 1595: ep_len:515 episode reward: total was -39.230000. running mean: -22.059128\n",
      "epsilon:0.182146 episode_count: 11172. steps_count: 4937036.000000\n",
      "ep 1596: ep_len:505 episode reward: total was -50.590000. running mean: -22.344437\n",
      "ep 1596: ep_len:580 episode reward: total was -31.100000. running mean: -22.431992\n",
      "ep 1596: ep_len:620 episode reward: total was -34.920000. running mean: -22.556872\n",
      "ep 1596: ep_len:119 episode reward: total was 2.090000. running mean: -22.310404\n",
      "ep 1596: ep_len:3 episode reward: total was 0.000000. running mean: -22.087300\n",
      "ep 1596: ep_len:605 episode reward: total was -37.660000. running mean: -22.243027\n",
      "ep 1596: ep_len:176 episode reward: total was -10.390000. running mean: -22.124496\n",
      "epsilon:0.182009 episode_count: 11179. steps_count: 4939644.000000\n",
      "ep 1597: ep_len:107 episode reward: total was -3.480000. running mean: -21.938051\n",
      "ep 1597: ep_len:525 episode reward: total was -28.340000. running mean: -22.002071\n",
      "ep 1597: ep_len:545 episode reward: total was -26.440000. running mean: -22.046450\n",
      "ep 1597: ep_len:525 episode reward: total was -32.120000. running mean: -22.147186\n",
      "ep 1597: ep_len:3 episode reward: total was 0.000000. running mean: -21.925714\n",
      "ep 1597: ep_len:560 episode reward: total was -25.210000. running mean: -21.958557\n",
      "ep 1597: ep_len:285 episode reward: total was -15.400000. running mean: -21.892971\n",
      "epsilon:0.181873 episode_count: 11186. steps_count: 4942194.000000\n",
      "ep 1598: ep_len:134 episode reward: total was 1.560000. running mean: -21.658441\n",
      "ep 1598: ep_len:500 episode reward: total was -23.530000. running mean: -21.677157\n",
      "ep 1598: ep_len:381 episode reward: total was -40.890000. running mean: -21.869285\n",
      "ep 1598: ep_len:515 episode reward: total was -11.520000. running mean: -21.765793\n",
      "ep 1598: ep_len:3 episode reward: total was 0.000000. running mean: -21.548135\n",
      "ep 1598: ep_len:640 episode reward: total was -29.880000. running mean: -21.631453\n",
      "ep 1598: ep_len:177 episode reward: total was -3.390000. running mean: -21.449039\n",
      "epsilon:0.181736 episode_count: 11193. steps_count: 4944544.000000\n",
      "ep 1599: ep_len:505 episode reward: total was -21.090000. running mean: -21.445448\n",
      "ep 1599: ep_len:500 episode reward: total was -5.390000. running mean: -21.284894\n",
      "ep 1599: ep_len:540 episode reward: total was -40.490000. running mean: -21.476945\n",
      "ep 1599: ep_len:510 episode reward: total was -1.610000. running mean: -21.278275\n",
      "ep 1599: ep_len:109 episode reward: total was -8.450000. running mean: -21.149993\n",
      "ep 1599: ep_len:535 episode reward: total was -24.270000. running mean: -21.181193\n",
      "ep 1599: ep_len:303 episode reward: total was -11.820000. running mean: -21.087581\n",
      "epsilon:0.181600 episode_count: 11200. steps_count: 4947546.000000\n",
      "ep 1600: ep_len:595 episode reward: total was -16.040000. running mean: -21.037105\n",
      "ep 1600: ep_len:580 episode reward: total was -20.690000. running mean: -21.033634\n",
      "ep 1600: ep_len:805 episode reward: total was -52.750000. running mean: -21.350798\n",
      "ep 1600: ep_len:545 episode reward: total was -18.180000. running mean: -21.319090\n",
      "ep 1600: ep_len:110 episode reward: total was 4.030000. running mean: -21.065599\n",
      "ep 1600: ep_len:600 episode reward: total was -73.150000. running mean: -21.586443\n",
      "ep 1600: ep_len:206 episode reward: total was -16.400000. running mean: -21.534578\n",
      "epsilon:0.181463 episode_count: 11207. steps_count: 4950987.000000\n",
      "ep 1601: ep_len:246 episode reward: total was -2.940000. running mean: -21.348633\n",
      "ep 1601: ep_len:346 episode reward: total was -22.420000. running mean: -21.359346\n",
      "ep 1601: ep_len:535 episode reward: total was -8.420000. running mean: -21.229953\n",
      "ep 1601: ep_len:500 episode reward: total was -30.650000. running mean: -21.324153\n",
      "ep 1601: ep_len:3 episode reward: total was 0.000000. running mean: -21.110912\n",
      "ep 1601: ep_len:500 episode reward: total was -8.710000. running mean: -20.986903\n",
      "ep 1601: ep_len:605 episode reward: total was -28.680000. running mean: -21.063834\n",
      "epsilon:0.181327 episode_count: 11214. steps_count: 4953722.000000\n",
      "ep 1602: ep_len:530 episode reward: total was -27.110000. running mean: -21.124295\n",
      "ep 1602: ep_len:540 episode reward: total was -0.700000. running mean: -20.920052\n",
      "ep 1602: ep_len:555 episode reward: total was -9.610000. running mean: -20.806952\n",
      "ep 1602: ep_len:580 episode reward: total was -11.680000. running mean: -20.715682\n",
      "ep 1602: ep_len:82 episode reward: total was 0.540000. running mean: -20.503125\n",
      "ep 1602: ep_len:500 episode reward: total was -5.130000. running mean: -20.349394\n",
      "ep 1602: ep_len:555 episode reward: total was -22.090000. running mean: -20.366800\n",
      "epsilon:0.181190 episode_count: 11221. steps_count: 4957064.000000\n",
      "ep 1603: ep_len:585 episode reward: total was -20.970000. running mean: -20.372832\n",
      "ep 1603: ep_len:525 episode reward: total was -13.090000. running mean: -20.300004\n",
      "ep 1603: ep_len:505 episode reward: total was -20.020000. running mean: -20.297204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1603: ep_len:505 episode reward: total was -26.150000. running mean: -20.355732\n",
      "ep 1603: ep_len:96 episode reward: total was 2.530000. running mean: -20.126875\n",
      "ep 1603: ep_len:500 episode reward: total was -34.820000. running mean: -20.273806\n",
      "ep 1603: ep_len:520 episode reward: total was -19.620000. running mean: -20.267268\n",
      "epsilon:0.181054 episode_count: 11228. steps_count: 4960300.000000\n",
      "ep 1604: ep_len:630 episode reward: total was -19.810000. running mean: -20.262695\n",
      "ep 1604: ep_len:500 episode reward: total was -31.200000. running mean: -20.372068\n",
      "ep 1604: ep_len:540 episode reward: total was -18.290000. running mean: -20.351247\n",
      "ep 1604: ep_len:505 episode reward: total was -13.200000. running mean: -20.279735\n",
      "ep 1604: ep_len:79 episode reward: total was 0.040000. running mean: -20.076538\n",
      "ep 1604: ep_len:650 episode reward: total was -18.270000. running mean: -20.058472\n",
      "ep 1604: ep_len:650 episode reward: total was -42.940000. running mean: -20.287287\n",
      "epsilon:0.180917 episode_count: 11235. steps_count: 4963854.000000\n",
      "ep 1605: ep_len:575 episode reward: total was -9.450000. running mean: -20.178915\n",
      "ep 1605: ep_len:277 episode reward: total was -13.920000. running mean: -20.116325\n",
      "ep 1605: ep_len:575 episode reward: total was -22.490000. running mean: -20.140062\n",
      "ep 1605: ep_len:525 episode reward: total was -28.000000. running mean: -20.218662\n",
      "ep 1605: ep_len:3 episode reward: total was 0.000000. running mean: -20.016475\n",
      "ep 1605: ep_len:505 episode reward: total was -44.370000. running mean: -20.260010\n",
      "ep 1605: ep_len:500 episode reward: total was -24.850000. running mean: -20.305910\n",
      "epsilon:0.180781 episode_count: 11242. steps_count: 4966814.000000\n",
      "ep 1606: ep_len:525 episode reward: total was -20.920000. running mean: -20.312051\n",
      "ep 1606: ep_len:580 episode reward: total was -42.500000. running mean: -20.533931\n",
      "ep 1606: ep_len:665 episode reward: total was -40.020000. running mean: -20.728791\n",
      "ep 1606: ep_len:510 episode reward: total was -17.540000. running mean: -20.696903\n",
      "ep 1606: ep_len:3 episode reward: total was 0.000000. running mean: -20.489934\n",
      "ep 1606: ep_len:630 episode reward: total was -31.200000. running mean: -20.597035\n",
      "ep 1606: ep_len:555 episode reward: total was -26.170000. running mean: -20.652765\n",
      "epsilon:0.180644 episode_count: 11249. steps_count: 4970282.000000\n",
      "ep 1607: ep_len:104 episode reward: total was -9.950000. running mean: -20.545737\n",
      "ep 1607: ep_len:565 episode reward: total was -40.430000. running mean: -20.744580\n",
      "ep 1607: ep_len:530 episode reward: total was -25.170000. running mean: -20.788834\n",
      "ep 1607: ep_len:525 episode reward: total was -13.490000. running mean: -20.715845\n",
      "ep 1607: ep_len:91 episode reward: total was -12.950000. running mean: -20.638187\n",
      "ep 1607: ep_len:615 episode reward: total was -29.610000. running mean: -20.727905\n",
      "ep 1607: ep_len:206 episode reward: total was -9.870000. running mean: -20.619326\n",
      "epsilon:0.180508 episode_count: 11256. steps_count: 4972918.000000\n",
      "ep 1608: ep_len:246 episode reward: total was -29.850000. running mean: -20.711633\n",
      "ep 1608: ep_len:580 episode reward: total was -8.560000. running mean: -20.590116\n",
      "ep 1608: ep_len:540 episode reward: total was -19.640000. running mean: -20.580615\n",
      "ep 1608: ep_len:585 episode reward: total was -11.500000. running mean: -20.489809\n",
      "ep 1608: ep_len:3 episode reward: total was 0.000000. running mean: -20.284911\n",
      "ep 1608: ep_len:525 episode reward: total was -13.910000. running mean: -20.221162\n",
      "ep 1608: ep_len:510 episode reward: total was -27.000000. running mean: -20.288950\n",
      "epsilon:0.180371 episode_count: 11263. steps_count: 4975907.000000\n",
      "ep 1609: ep_len:625 episode reward: total was -36.830000. running mean: -20.454361\n",
      "ep 1609: ep_len:500 episode reward: total was -8.730000. running mean: -20.337117\n",
      "ep 1609: ep_len:855 episode reward: total was -74.390000. running mean: -20.877646\n",
      "ep 1609: ep_len:115 episode reward: total was 1.590000. running mean: -20.652970\n",
      "ep 1609: ep_len:110 episode reward: total was -5.950000. running mean: -20.505940\n",
      "ep 1609: ep_len:505 episode reward: total was -15.490000. running mean: -20.455780\n",
      "ep 1609: ep_len:575 episode reward: total was -21.340000. running mean: -20.464623\n",
      "epsilon:0.180235 episode_count: 11270. steps_count: 4979192.000000\n",
      "ep 1610: ep_len:265 episode reward: total was -6.360000. running mean: -20.323576\n",
      "ep 1610: ep_len:590 episode reward: total was -19.660000. running mean: -20.316941\n",
      "ep 1610: ep_len:580 episode reward: total was -32.900000. running mean: -20.442771\n",
      "ep 1610: ep_len:500 episode reward: total was -3.650000. running mean: -20.274844\n",
      "ep 1610: ep_len:3 episode reward: total was 0.000000. running mean: -20.072095\n",
      "ep 1610: ep_len:555 episode reward: total was -22.850000. running mean: -20.099874\n",
      "ep 1610: ep_len:560 episode reward: total was -26.120000. running mean: -20.160075\n",
      "epsilon:0.180098 episode_count: 11277. steps_count: 4982245.000000\n",
      "ep 1611: ep_len:530 episode reward: total was -13.180000. running mean: -20.090275\n",
      "ep 1611: ep_len:505 episode reward: total was -32.850000. running mean: -20.217872\n",
      "ep 1611: ep_len:635 episode reward: total was -28.530000. running mean: -20.300993\n",
      "ep 1611: ep_len:510 episode reward: total was -23.600000. running mean: -20.333983\n",
      "ep 1611: ep_len:3 episode reward: total was 0.000000. running mean: -20.130643\n",
      "ep 1611: ep_len:620 episode reward: total was -18.650000. running mean: -20.115837\n",
      "ep 1611: ep_len:565 episode reward: total was -25.090000. running mean: -20.165579\n",
      "epsilon:0.179962 episode_count: 11284. steps_count: 4985613.000000\n",
      "ep 1612: ep_len:600 episode reward: total was -24.830000. running mean: -20.212223\n",
      "ep 1612: ep_len:500 episode reward: total was -10.130000. running mean: -20.111401\n",
      "ep 1612: ep_len:635 episode reward: total was -43.190000. running mean: -20.342187\n",
      "ep 1612: ep_len:515 episode reward: total was -17.530000. running mean: -20.314065\n",
      "ep 1612: ep_len:3 episode reward: total was 0.000000. running mean: -20.110924\n",
      "ep 1612: ep_len:565 episode reward: total was -20.610000. running mean: -20.115915\n",
      "ep 1612: ep_len:505 episode reward: total was -27.290000. running mean: -20.187656\n",
      "epsilon:0.179825 episode_count: 11291. steps_count: 4988936.000000\n",
      "ep 1613: ep_len:600 episode reward: total was -26.360000. running mean: -20.249379\n",
      "ep 1613: ep_len:162 episode reward: total was -9.390000. running mean: -20.140785\n",
      "ep 1613: ep_len:560 episode reward: total was -27.910000. running mean: -20.218478\n",
      "ep 1613: ep_len:525 episode reward: total was -33.640000. running mean: -20.352693\n",
      "ep 1613: ep_len:3 episode reward: total was 0.000000. running mean: -20.149166\n",
      "ep 1613: ep_len:575 episode reward: total was -24.640000. running mean: -20.194074\n",
      "ep 1613: ep_len:500 episode reward: total was -25.780000. running mean: -20.249933\n",
      "epsilon:0.179689 episode_count: 11298. steps_count: 4991861.000000\n",
      "ep 1614: ep_len:204 episode reward: total was -10.900000. running mean: -20.156434\n",
      "ep 1614: ep_len:655 episode reward: total was -21.030000. running mean: -20.165170\n",
      "ep 1614: ep_len:650 episode reward: total was -17.430000. running mean: -20.137818\n",
      "ep 1614: ep_len:530 episode reward: total was -22.160000. running mean: -20.158040\n",
      "ep 1614: ep_len:3 episode reward: total was 0.000000. running mean: -19.956459\n",
      "ep 1614: ep_len:295 episode reward: total was -12.370000. running mean: -19.880595\n",
      "ep 1614: ep_len:715 episode reward: total was -63.440000. running mean: -20.316189\n",
      "epsilon:0.179552 episode_count: 11305. steps_count: 4994913.000000\n",
      "ep 1615: ep_len:565 episode reward: total was -28.570000. running mean: -20.398727\n",
      "ep 1615: ep_len:595 episode reward: total was -32.000000. running mean: -20.514740\n",
      "ep 1615: ep_len:565 episode reward: total was -39.910000. running mean: -20.708692\n",
      "ep 1615: ep_len:505 episode reward: total was -16.710000. running mean: -20.668705\n",
      "ep 1615: ep_len:126 episode reward: total was -1.450000. running mean: -20.476518\n",
      "ep 1615: ep_len:500 episode reward: total was -7.030000. running mean: -20.342053\n",
      "ep 1615: ep_len:535 episode reward: total was -13.620000. running mean: -20.274833\n",
      "epsilon:0.179416 episode_count: 11312. steps_count: 4998304.000000\n",
      "ep 1616: ep_len:535 episode reward: total was -13.590000. running mean: -20.207984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1616: ep_len:505 episode reward: total was -46.940000. running mean: -20.475304\n",
      "ep 1616: ep_len:500 episode reward: total was -16.750000. running mean: -20.438051\n",
      "ep 1616: ep_len:500 episode reward: total was -26.590000. running mean: -20.499571\n",
      "ep 1616: ep_len:3 episode reward: total was 0.000000. running mean: -20.294575\n",
      "ep 1616: ep_len:186 episode reward: total was 2.590000. running mean: -20.065729\n",
      "ep 1616: ep_len:565 episode reward: total was -38.610000. running mean: -20.251172\n",
      "epsilon:0.179279 episode_count: 11319. steps_count: 5001098.000000\n",
      "ep 1617: ep_len:500 episode reward: total was -12.170000. running mean: -20.170360\n",
      "ep 1617: ep_len:580 episode reward: total was -12.880000. running mean: -20.097457\n",
      "ep 1617: ep_len:675 episode reward: total was -26.820000. running mean: -20.164682\n",
      "ep 1617: ep_len:515 episode reward: total was -19.610000. running mean: -20.159135\n",
      "ep 1617: ep_len:3 episode reward: total was 0.000000. running mean: -19.957544\n",
      "ep 1617: ep_len:665 episode reward: total was -22.300000. running mean: -19.980969\n",
      "ep 1617: ep_len:610 episode reward: total was -24.510000. running mean: -20.026259\n",
      "epsilon:0.179143 episode_count: 11326. steps_count: 5004646.000000\n",
      "ep 1618: ep_len:570 episode reward: total was -16.090000. running mean: -19.986896\n",
      "ep 1618: ep_len:525 episode reward: total was -18.100000. running mean: -19.968027\n",
      "ep 1618: ep_len:610 episode reward: total was -27.410000. running mean: -20.042447\n",
      "ep 1618: ep_len:132 episode reward: total was -7.440000. running mean: -19.916423\n",
      "ep 1618: ep_len:3 episode reward: total was 0.000000. running mean: -19.717258\n",
      "ep 1618: ep_len:710 episode reward: total was -32.400000. running mean: -19.844086\n",
      "ep 1618: ep_len:565 episode reward: total was -30.120000. running mean: -19.946845\n",
      "epsilon:0.179006 episode_count: 11333. steps_count: 5007761.000000\n",
      "ep 1619: ep_len:226 episode reward: total was -0.930000. running mean: -19.756677\n",
      "ep 1619: ep_len:590 episode reward: total was -14.640000. running mean: -19.705510\n",
      "ep 1619: ep_len:373 episode reward: total was -8.330000. running mean: -19.591755\n",
      "ep 1619: ep_len:575 episode reward: total was -23.160000. running mean: -19.627437\n",
      "ep 1619: ep_len:3 episode reward: total was 0.000000. running mean: -19.431163\n",
      "ep 1619: ep_len:685 episode reward: total was -24.750000. running mean: -19.484351\n",
      "ep 1619: ep_len:290 episode reward: total was -11.840000. running mean: -19.407908\n",
      "epsilon:0.178870 episode_count: 11340. steps_count: 5010503.000000\n",
      "ep 1620: ep_len:134 episode reward: total was -2.940000. running mean: -19.243229\n",
      "ep 1620: ep_len:500 episode reward: total was -13.880000. running mean: -19.189596\n",
      "ep 1620: ep_len:500 episode reward: total was -23.990000. running mean: -19.237600\n",
      "ep 1620: ep_len:540 episode reward: total was -15.650000. running mean: -19.201724\n",
      "ep 1620: ep_len:3 episode reward: total was 0.000000. running mean: -19.009707\n",
      "ep 1620: ep_len:595 episode reward: total was -38.320000. running mean: -19.202810\n",
      "ep 1620: ep_len:585 episode reward: total was -14.910000. running mean: -19.159882\n",
      "epsilon:0.178733 episode_count: 11347. steps_count: 5013360.000000\n",
      "ep 1621: ep_len:605 episode reward: total was -28.190000. running mean: -19.250183\n",
      "ep 1621: ep_len:525 episode reward: total was -16.170000. running mean: -19.219381\n",
      "ep 1621: ep_len:645 episode reward: total was -33.510000. running mean: -19.362287\n",
      "ep 1621: ep_len:505 episode reward: total was -15.940000. running mean: -19.328065\n",
      "ep 1621: ep_len:72 episode reward: total was -3.460000. running mean: -19.169384\n",
      "ep 1621: ep_len:545 episode reward: total was -23.680000. running mean: -19.214490\n",
      "ep 1621: ep_len:545 episode reward: total was -24.140000. running mean: -19.263745\n",
      "epsilon:0.178597 episode_count: 11354. steps_count: 5016802.000000\n",
      "ep 1622: ep_len:620 episode reward: total was -19.770000. running mean: -19.268808\n",
      "ep 1622: ep_len:525 episode reward: total was 3.810000. running mean: -19.038020\n",
      "ep 1622: ep_len:680 episode reward: total was -28.800000. running mean: -19.135639\n",
      "ep 1622: ep_len:520 episode reward: total was -18.240000. running mean: -19.126683\n",
      "ep 1622: ep_len:3 episode reward: total was 0.000000. running mean: -18.935416\n",
      "ep 1622: ep_len:555 episode reward: total was -15.590000. running mean: -18.901962\n",
      "ep 1622: ep_len:520 episode reward: total was -22.540000. running mean: -18.938342\n",
      "epsilon:0.178460 episode_count: 11361. steps_count: 5020225.000000\n",
      "ep 1623: ep_len:223 episode reward: total was -26.360000. running mean: -19.012559\n",
      "ep 1623: ep_len:500 episode reward: total was -10.730000. running mean: -18.929733\n",
      "ep 1623: ep_len:620 episode reward: total was -11.600000. running mean: -18.856436\n",
      "ep 1623: ep_len:500 episode reward: total was -34.160000. running mean: -19.009472\n",
      "ep 1623: ep_len:107 episode reward: total was -2.470000. running mean: -18.844077\n",
      "ep 1623: ep_len:500 episode reward: total was -11.650000. running mean: -18.772136\n",
      "ep 1623: ep_len:605 episode reward: total was -14.870000. running mean: -18.733115\n",
      "epsilon:0.178324 episode_count: 11368. steps_count: 5023280.000000\n",
      "ep 1624: ep_len:515 episode reward: total was -30.200000. running mean: -18.847784\n",
      "ep 1624: ep_len:565 episode reward: total was -14.590000. running mean: -18.805206\n",
      "ep 1624: ep_len:805 episode reward: total was -132.330000. running mean: -19.940454\n",
      "ep 1624: ep_len:515 episode reward: total was -27.110000. running mean: -20.012149\n",
      "ep 1624: ep_len:3 episode reward: total was 0.000000. running mean: -19.812028\n",
      "ep 1624: ep_len:505 episode reward: total was -23.520000. running mean: -19.849108\n",
      "ep 1624: ep_len:600 episode reward: total was -26.530000. running mean: -19.915916\n",
      "epsilon:0.178187 episode_count: 11375. steps_count: 5026788.000000\n",
      "ep 1625: ep_len:695 episode reward: total was -49.430000. running mean: -20.211057\n",
      "ep 1625: ep_len:650 episode reward: total was -26.060000. running mean: -20.269547\n",
      "ep 1625: ep_len:515 episode reward: total was -40.030000. running mean: -20.467151\n",
      "ep 1625: ep_len:565 episode reward: total was -4.010000. running mean: -20.302580\n",
      "ep 1625: ep_len:3 episode reward: total was 0.000000. running mean: -20.099554\n",
      "ep 1625: ep_len:525 episode reward: total was -27.030000. running mean: -20.168858\n",
      "ep 1625: ep_len:585 episode reward: total was -27.650000. running mean: -20.243670\n",
      "epsilon:0.178051 episode_count: 11382. steps_count: 5030326.000000\n",
      "ep 1626: ep_len:253 episode reward: total was 0.600000. running mean: -20.035233\n",
      "ep 1626: ep_len:630 episode reward: total was -4.510000. running mean: -19.879981\n",
      "ep 1626: ep_len:685 episode reward: total was -33.350000. running mean: -20.014681\n",
      "ep 1626: ep_len:164 episode reward: total was -1.890000. running mean: -19.833434\n",
      "ep 1626: ep_len:3 episode reward: total was 0.000000. running mean: -19.635100\n",
      "ep 1626: ep_len:590 episode reward: total was -46.550000. running mean: -19.904249\n",
      "ep 1626: ep_len:575 episode reward: total was -19.100000. running mean: -19.896206\n",
      "epsilon:0.177914 episode_count: 11389. steps_count: 5033226.000000\n",
      "ep 1627: ep_len:540 episode reward: total was -22.240000. running mean: -19.919644\n",
      "ep 1627: ep_len:585 episode reward: total was -22.710000. running mean: -19.947548\n",
      "ep 1627: ep_len:635 episode reward: total was -31.800000. running mean: -20.066072\n",
      "ep 1627: ep_len:540 episode reward: total was -20.120000. running mean: -20.066612\n",
      "ep 1627: ep_len:3 episode reward: total was 0.000000. running mean: -19.865946\n",
      "ep 1627: ep_len:630 episode reward: total was -25.840000. running mean: -19.925686\n",
      "ep 1627: ep_len:595 episode reward: total was -30.030000. running mean: -20.026729\n",
      "epsilon:0.177778 episode_count: 11396. steps_count: 5036754.000000\n",
      "ep 1628: ep_len:545 episode reward: total was -24.110000. running mean: -20.067562\n",
      "ep 1628: ep_len:505 episode reward: total was -19.140000. running mean: -20.058286\n",
      "ep 1628: ep_len:560 episode reward: total was -38.970000. running mean: -20.247403\n",
      "ep 1628: ep_len:116 episode reward: total was -0.920000. running mean: -20.054129\n",
      "ep 1628: ep_len:3 episode reward: total was 0.000000. running mean: -19.853588\n",
      "ep 1628: ep_len:500 episode reward: total was -41.770000. running mean: -20.072752\n",
      "ep 1628: ep_len:530 episode reward: total was -19.410000. running mean: -20.066125\n",
      "epsilon:0.177641 episode_count: 11403. steps_count: 5039513.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1629: ep_len:255 episode reward: total was -21.370000. running mean: -20.079163\n",
      "ep 1629: ep_len:264 episode reward: total was -15.910000. running mean: -20.037472\n",
      "ep 1629: ep_len:600 episode reward: total was -61.620000. running mean: -20.453297\n",
      "ep 1629: ep_len:520 episode reward: total was -25.540000. running mean: -20.504164\n",
      "ep 1629: ep_len:100 episode reward: total was -13.450000. running mean: -20.433622\n",
      "ep 1629: ep_len:695 episode reward: total was -52.380000. running mean: -20.753086\n",
      "ep 1629: ep_len:284 episode reward: total was -25.880000. running mean: -20.804355\n",
      "epsilon:0.177505 episode_count: 11410. steps_count: 5042231.000000\n",
      "ep 1630: ep_len:535 episode reward: total was -23.630000. running mean: -20.832612\n",
      "ep 1630: ep_len:555 episode reward: total was -24.710000. running mean: -20.871386\n",
      "ep 1630: ep_len:635 episode reward: total was -41.080000. running mean: -21.073472\n",
      "ep 1630: ep_len:359 episode reward: total was -1.650000. running mean: -20.879237\n",
      "ep 1630: ep_len:75 episode reward: total was -0.480000. running mean: -20.675245\n",
      "ep 1630: ep_len:525 episode reward: total was -8.630000. running mean: -20.554792\n",
      "ep 1630: ep_len:500 episode reward: total was -15.920000. running mean: -20.508444\n",
      "epsilon:0.177368 episode_count: 11417. steps_count: 5045415.000000\n",
      "ep 1631: ep_len:500 episode reward: total was -43.580000. running mean: -20.739160\n",
      "ep 1631: ep_len:645 episode reward: total was -11.530000. running mean: -20.647068\n",
      "ep 1631: ep_len:565 episode reward: total was -23.480000. running mean: -20.675398\n",
      "ep 1631: ep_len:525 episode reward: total was -3.200000. running mean: -20.500644\n",
      "ep 1631: ep_len:3 episode reward: total was 0.000000. running mean: -20.295637\n",
      "ep 1631: ep_len:500 episode reward: total was -40.300000. running mean: -20.495681\n",
      "ep 1631: ep_len:620 episode reward: total was -32.070000. running mean: -20.611424\n",
      "epsilon:0.177232 episode_count: 11424. steps_count: 5048773.000000\n",
      "ep 1632: ep_len:510 episode reward: total was -22.060000. running mean: -20.625910\n",
      "ep 1632: ep_len:189 episode reward: total was -7.930000. running mean: -20.498951\n",
      "ep 1632: ep_len:640 episode reward: total was -21.020000. running mean: -20.504161\n",
      "ep 1632: ep_len:500 episode reward: total was -0.160000. running mean: -20.300720\n",
      "ep 1632: ep_len:3 episode reward: total was 0.000000. running mean: -20.097712\n",
      "ep 1632: ep_len:640 episode reward: total was -37.910000. running mean: -20.275835\n",
      "ep 1632: ep_len:304 episode reward: total was -15.310000. running mean: -20.226177\n",
      "epsilon:0.177095 episode_count: 11431. steps_count: 5051559.000000\n",
      "ep 1633: ep_len:500 episode reward: total was -4.280000. running mean: -20.066715\n",
      "ep 1633: ep_len:500 episode reward: total was -20.540000. running mean: -20.071448\n",
      "ep 1633: ep_len:575 episode reward: total was -28.420000. running mean: -20.154934\n",
      "ep 1633: ep_len:510 episode reward: total was -16.130000. running mean: -20.114684\n",
      "ep 1633: ep_len:3 episode reward: total was 0.000000. running mean: -19.913537\n",
      "ep 1633: ep_len:510 episode reward: total was -23.020000. running mean: -19.944602\n",
      "ep 1633: ep_len:530 episode reward: total was -37.680000. running mean: -20.121956\n",
      "epsilon:0.176959 episode_count: 11438. steps_count: 5054687.000000\n",
      "ep 1634: ep_len:650 episode reward: total was -53.500000. running mean: -20.455736\n",
      "ep 1634: ep_len:500 episode reward: total was 1.660000. running mean: -20.234579\n",
      "ep 1634: ep_len:500 episode reward: total was -20.620000. running mean: -20.238433\n",
      "ep 1634: ep_len:555 episode reward: total was -4.630000. running mean: -20.082349\n",
      "ep 1634: ep_len:3 episode reward: total was 0.000000. running mean: -19.881525\n",
      "ep 1634: ep_len:500 episode reward: total was -19.840000. running mean: -19.881110\n",
      "ep 1634: ep_len:605 episode reward: total was -28.340000. running mean: -19.965699\n",
      "epsilon:0.176822 episode_count: 11445. steps_count: 5058000.000000\n",
      "ep 1635: ep_len:500 episode reward: total was -33.330000. running mean: -20.099342\n",
      "ep 1635: ep_len:198 episode reward: total was -8.430000. running mean: -19.982649\n",
      "ep 1635: ep_len:630 episode reward: total was -17.460000. running mean: -19.957422\n",
      "ep 1635: ep_len:560 episode reward: total was -32.050000. running mean: -20.078348\n",
      "ep 1635: ep_len:3 episode reward: total was 0.000000. running mean: -19.877564\n",
      "ep 1635: ep_len:186 episode reward: total was -4.420000. running mean: -19.722989\n",
      "ep 1635: ep_len:510 episode reward: total was -39.230000. running mean: -19.918059\n",
      "epsilon:0.176686 episode_count: 11452. steps_count: 5060587.000000\n",
      "ep 1636: ep_len:620 episode reward: total was -32.870000. running mean: -20.047578\n",
      "ep 1636: ep_len:590 episode reward: total was -1.980000. running mean: -19.866903\n",
      "ep 1636: ep_len:525 episode reward: total was -27.440000. running mean: -19.942634\n",
      "ep 1636: ep_len:413 episode reward: total was -16.180000. running mean: -19.905007\n",
      "ep 1636: ep_len:3 episode reward: total was 0.000000. running mean: -19.705957\n",
      "ep 1636: ep_len:520 episode reward: total was -20.180000. running mean: -19.710698\n",
      "ep 1636: ep_len:510 episode reward: total was -43.220000. running mean: -19.945791\n",
      "epsilon:0.176549 episode_count: 11459. steps_count: 5063768.000000\n",
      "ep 1637: ep_len:222 episode reward: total was -8.890000. running mean: -19.835233\n",
      "ep 1637: ep_len:359 episode reward: total was -19.350000. running mean: -19.830380\n",
      "ep 1637: ep_len:65 episode reward: total was -2.490000. running mean: -19.656977\n",
      "ep 1637: ep_len:500 episode reward: total was -19.110000. running mean: -19.651507\n",
      "ep 1637: ep_len:3 episode reward: total was 0.000000. running mean: -19.454992\n",
      "ep 1637: ep_len:515 episode reward: total was -22.030000. running mean: -19.480742\n",
      "ep 1637: ep_len:515 episode reward: total was -21.020000. running mean: -19.496134\n",
      "epsilon:0.176413 episode_count: 11466. steps_count: 5065947.000000\n",
      "ep 1638: ep_len:510 episode reward: total was -5.110000. running mean: -19.352273\n",
      "ep 1638: ep_len:535 episode reward: total was -3.810000. running mean: -19.196850\n",
      "ep 1638: ep_len:575 episode reward: total was -51.010000. running mean: -19.514982\n",
      "ep 1638: ep_len:56 episode reward: total was 2.570000. running mean: -19.294132\n",
      "ep 1638: ep_len:3 episode reward: total was 0.000000. running mean: -19.101191\n",
      "ep 1638: ep_len:680 episode reward: total was -21.290000. running mean: -19.123079\n",
      "ep 1638: ep_len:580 episode reward: total was -62.100000. running mean: -19.552848\n",
      "epsilon:0.176276 episode_count: 11473. steps_count: 5068886.000000\n",
      "ep 1639: ep_len:238 episode reward: total was 2.610000. running mean: -19.331220\n",
      "ep 1639: ep_len:585 episode reward: total was -18.070000. running mean: -19.318607\n",
      "ep 1639: ep_len:79 episode reward: total was -0.970000. running mean: -19.135121\n",
      "ep 1639: ep_len:500 episode reward: total was -25.600000. running mean: -19.199770\n",
      "ep 1639: ep_len:3 episode reward: total was 0.000000. running mean: -19.007772\n",
      "ep 1639: ep_len:545 episode reward: total was -14.470000. running mean: -18.962395\n",
      "ep 1639: ep_len:535 episode reward: total was -15.580000. running mean: -18.928571\n",
      "epsilon:0.176140 episode_count: 11480. steps_count: 5071371.000000\n",
      "ep 1640: ep_len:218 episode reward: total was -5.410000. running mean: -18.793385\n",
      "ep 1640: ep_len:605 episode reward: total was -18.640000. running mean: -18.791851\n",
      "ep 1640: ep_len:555 episode reward: total was -30.060000. running mean: -18.904533\n",
      "ep 1640: ep_len:140 episode reward: total was -1.410000. running mean: -18.729587\n",
      "ep 1640: ep_len:102 episode reward: total was -12.440000. running mean: -18.666691\n",
      "ep 1640: ep_len:500 episode reward: total was -9.330000. running mean: -18.573324\n",
      "ep 1640: ep_len:500 episode reward: total was -19.960000. running mean: -18.587191\n",
      "epsilon:0.176003 episode_count: 11487. steps_count: 5073991.000000\n",
      "ep 1641: ep_len:635 episode reward: total was -8.490000. running mean: -18.486219\n",
      "ep 1641: ep_len:500 episode reward: total was -6.030000. running mean: -18.361657\n",
      "ep 1641: ep_len:78 episode reward: total was 0.040000. running mean: -18.177641\n",
      "ep 1641: ep_len:500 episode reward: total was -2.470000. running mean: -18.020564\n",
      "ep 1641: ep_len:82 episode reward: total was -7.950000. running mean: -17.919859\n",
      "ep 1641: ep_len:530 episode reward: total was -20.070000. running mean: -17.941360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1641: ep_len:525 episode reward: total was -39.130000. running mean: -18.153246\n",
      "epsilon:0.175867 episode_count: 11494. steps_count: 5076841.000000\n",
      "ep 1642: ep_len:635 episode reward: total was -11.380000. running mean: -18.085514\n",
      "ep 1642: ep_len:540 episode reward: total was -28.930000. running mean: -18.193959\n",
      "ep 1642: ep_len:381 episode reward: total was -29.890000. running mean: -18.310919\n",
      "ep 1642: ep_len:500 episode reward: total was -16.020000. running mean: -18.288010\n",
      "ep 1642: ep_len:90 episode reward: total was -11.940000. running mean: -18.224530\n",
      "ep 1642: ep_len:500 episode reward: total was -17.280000. running mean: -18.215085\n",
      "ep 1642: ep_len:299 episode reward: total was -24.880000. running mean: -18.281734\n",
      "epsilon:0.175730 episode_count: 11501. steps_count: 5079786.000000\n",
      "ep 1643: ep_len:116 episode reward: total was -1.960000. running mean: -18.118516\n",
      "ep 1643: ep_len:505 episode reward: total was -20.500000. running mean: -18.142331\n",
      "ep 1643: ep_len:393 episode reward: total was -1.280000. running mean: -17.973708\n",
      "ep 1643: ep_len:500 episode reward: total was -22.620000. running mean: -18.020171\n",
      "ep 1643: ep_len:84 episode reward: total was 1.550000. running mean: -17.824469\n",
      "ep 1643: ep_len:635 episode reward: total was -22.310000. running mean: -17.869324\n",
      "ep 1643: ep_len:535 episode reward: total was -29.150000. running mean: -17.982131\n",
      "epsilon:0.175594 episode_count: 11508. steps_count: 5082554.000000\n",
      "ep 1644: ep_len:585 episode reward: total was -33.060000. running mean: -18.132910\n",
      "ep 1644: ep_len:299 episode reward: total was -8.860000. running mean: -18.040181\n",
      "ep 1644: ep_len:359 episode reward: total was -22.370000. running mean: -18.083479\n",
      "ep 1644: ep_len:500 episode reward: total was -17.020000. running mean: -18.072844\n",
      "ep 1644: ep_len:3 episode reward: total was 0.000000. running mean: -17.892116\n",
      "ep 1644: ep_len:505 episode reward: total was -29.090000. running mean: -18.004095\n",
      "ep 1644: ep_len:615 episode reward: total was -25.590000. running mean: -18.079954\n",
      "epsilon:0.175457 episode_count: 11515. steps_count: 5085420.000000\n",
      "ep 1645: ep_len:248 episode reward: total was -6.430000. running mean: -17.963454\n",
      "ep 1645: ep_len:555 episode reward: total was -20.050000. running mean: -17.984320\n",
      "ep 1645: ep_len:570 episode reward: total was -30.470000. running mean: -18.109176\n",
      "ep 1645: ep_len:520 episode reward: total was -33.620000. running mean: -18.264285\n",
      "ep 1645: ep_len:3 episode reward: total was 0.000000. running mean: -18.081642\n",
      "ep 1645: ep_len:540 episode reward: total was -23.970000. running mean: -18.140525\n",
      "ep 1645: ep_len:645 episode reward: total was -17.850000. running mean: -18.137620\n",
      "epsilon:0.175321 episode_count: 11522. steps_count: 5088501.000000\n",
      "ep 1646: ep_len:605 episode reward: total was -20.980000. running mean: -18.166044\n",
      "ep 1646: ep_len:560 episode reward: total was -58.190000. running mean: -18.566283\n",
      "ep 1646: ep_len:71 episode reward: total was 1.530000. running mean: -18.365321\n",
      "ep 1646: ep_len:540 episode reward: total was -18.100000. running mean: -18.362667\n",
      "ep 1646: ep_len:52 episode reward: total was 0.500000. running mean: -18.174041\n",
      "ep 1646: ep_len:560 episode reward: total was -16.670000. running mean: -18.159000\n",
      "ep 1646: ep_len:500 episode reward: total was -33.160000. running mean: -18.309010\n",
      "epsilon:0.175184 episode_count: 11529. steps_count: 5091389.000000\n",
      "ep 1647: ep_len:590 episode reward: total was -37.350000. running mean: -18.499420\n",
      "ep 1647: ep_len:510 episode reward: total was -39.810000. running mean: -18.712526\n",
      "ep 1647: ep_len:515 episode reward: total was -29.030000. running mean: -18.815701\n",
      "ep 1647: ep_len:505 episode reward: total was -45.710000. running mean: -19.084644\n",
      "ep 1647: ep_len:3 episode reward: total was 0.000000. running mean: -18.893797\n",
      "ep 1647: ep_len:321 episode reward: total was -12.380000. running mean: -18.828659\n",
      "ep 1647: ep_len:590 episode reward: total was -36.990000. running mean: -19.010273\n",
      "epsilon:0.175048 episode_count: 11536. steps_count: 5094423.000000\n",
      "ep 1648: ep_len:620 episode reward: total was -13.920000. running mean: -18.959370\n",
      "ep 1648: ep_len:610 episode reward: total was -20.140000. running mean: -18.971176\n",
      "ep 1648: ep_len:500 episode reward: total was -24.650000. running mean: -19.027965\n",
      "ep 1648: ep_len:615 episode reward: total was -11.990000. running mean: -18.957585\n",
      "ep 1648: ep_len:3 episode reward: total was 0.000000. running mean: -18.768009\n",
      "ep 1648: ep_len:510 episode reward: total was -29.200000. running mean: -18.872329\n",
      "ep 1648: ep_len:535 episode reward: total was -47.130000. running mean: -19.154906\n",
      "epsilon:0.174911 episode_count: 11543. steps_count: 5097816.000000\n",
      "ep 1649: ep_len:500 episode reward: total was -12.660000. running mean: -19.089957\n",
      "ep 1649: ep_len:625 episode reward: total was -14.520000. running mean: -19.044257\n",
      "ep 1649: ep_len:595 episode reward: total was -38.560000. running mean: -19.239414\n",
      "ep 1649: ep_len:655 episode reward: total was -53.510000. running mean: -19.582120\n",
      "ep 1649: ep_len:3 episode reward: total was 0.000000. running mean: -19.386299\n",
      "ep 1649: ep_len:215 episode reward: total was -31.890000. running mean: -19.511336\n",
      "ep 1649: ep_len:315 episode reward: total was -16.870000. running mean: -19.484923\n",
      "epsilon:0.174775 episode_count: 11550. steps_count: 5100724.000000\n",
      "ep 1650: ep_len:530 episode reward: total was -41.040000. running mean: -19.700474\n",
      "ep 1650: ep_len:500 episode reward: total was -15.570000. running mean: -19.659169\n",
      "ep 1650: ep_len:545 episode reward: total was -20.870000. running mean: -19.671277\n",
      "ep 1650: ep_len:500 episode reward: total was -12.070000. running mean: -19.595264\n",
      "ep 1650: ep_len:3 episode reward: total was 0.000000. running mean: -19.399312\n",
      "ep 1650: ep_len:525 episode reward: total was -20.630000. running mean: -19.411619\n",
      "ep 1650: ep_len:505 episode reward: total was -26.060000. running mean: -19.478102\n",
      "epsilon:0.174638 episode_count: 11557. steps_count: 5103832.000000\n",
      "ep 1651: ep_len:228 episode reward: total was -5.370000. running mean: -19.337021\n",
      "ep 1651: ep_len:535 episode reward: total was -3.330000. running mean: -19.176951\n",
      "ep 1651: ep_len:565 episode reward: total was -24.010000. running mean: -19.225282\n",
      "ep 1651: ep_len:515 episode reward: total was -6.650000. running mean: -19.099529\n",
      "ep 1651: ep_len:89 episode reward: total was 0.550000. running mean: -18.903034\n",
      "ep 1651: ep_len:525 episode reward: total was -22.680000. running mean: -18.940803\n",
      "ep 1651: ep_len:555 episode reward: total was -23.430000. running mean: -18.985695\n",
      "epsilon:0.174502 episode_count: 11564. steps_count: 5106844.000000\n",
      "ep 1652: ep_len:224 episode reward: total was -14.410000. running mean: -18.939938\n",
      "ep 1652: ep_len:520 episode reward: total was -28.650000. running mean: -19.037039\n",
      "ep 1652: ep_len:625 episode reward: total was -16.920000. running mean: -19.015868\n",
      "ep 1652: ep_len:830 episode reward: total was -86.440000. running mean: -19.690110\n",
      "ep 1652: ep_len:3 episode reward: total was 0.000000. running mean: -19.493209\n",
      "ep 1652: ep_len:226 episode reward: total was 2.590000. running mean: -19.272377\n",
      "ep 1652: ep_len:515 episode reward: total was -40.130000. running mean: -19.480953\n",
      "epsilon:0.174365 episode_count: 11571. steps_count: 5109787.000000\n",
      "ep 1653: ep_len:660 episode reward: total was -50.370000. running mean: -19.789843\n",
      "ep 1653: ep_len:625 episode reward: total was -12.120000. running mean: -19.713145\n",
      "ep 1653: ep_len:397 episode reward: total was -16.860000. running mean: -19.684613\n",
      "ep 1653: ep_len:600 episode reward: total was -17.060000. running mean: -19.658367\n",
      "ep 1653: ep_len:3 episode reward: total was 0.000000. running mean: -19.461784\n",
      "ep 1653: ep_len:500 episode reward: total was -8.940000. running mean: -19.356566\n",
      "ep 1653: ep_len:900 episode reward: total was -136.780000. running mean: -20.530800\n",
      "epsilon:0.174229 episode_count: 11578. steps_count: 5113472.000000\n",
      "ep 1654: ep_len:234 episode reward: total was -28.910000. running mean: -20.614592\n",
      "ep 1654: ep_len:540 episode reward: total was -10.050000. running mean: -20.508946\n",
      "ep 1654: ep_len:635 episode reward: total was -28.400000. running mean: -20.587857\n",
      "ep 1654: ep_len:515 episode reward: total was -31.640000. running mean: -20.698378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1654: ep_len:3 episode reward: total was 0.000000. running mean: -20.491394\n",
      "ep 1654: ep_len:540 episode reward: total was -14.940000. running mean: -20.435880\n",
      "ep 1654: ep_len:525 episode reward: total was -24.080000. running mean: -20.472322\n",
      "epsilon:0.174092 episode_count: 11585. steps_count: 5116464.000000\n",
      "ep 1655: ep_len:500 episode reward: total was -14.190000. running mean: -20.409498\n",
      "ep 1655: ep_len:610 episode reward: total was -28.420000. running mean: -20.489603\n",
      "ep 1655: ep_len:545 episode reward: total was -35.090000. running mean: -20.635607\n",
      "ep 1655: ep_len:155 episode reward: total was -9.850000. running mean: -20.527751\n",
      "ep 1655: ep_len:101 episode reward: total was 0.060000. running mean: -20.321874\n",
      "ep 1655: ep_len:550 episode reward: total was -14.140000. running mean: -20.260055\n",
      "ep 1655: ep_len:302 episode reward: total was -11.330000. running mean: -20.170755\n",
      "epsilon:0.173956 episode_count: 11592. steps_count: 5119227.000000\n",
      "ep 1656: ep_len:585 episode reward: total was -34.440000. running mean: -20.313447\n",
      "ep 1656: ep_len:530 episode reward: total was -20.590000. running mean: -20.316213\n",
      "ep 1656: ep_len:590 episode reward: total was -23.530000. running mean: -20.348350\n",
      "ep 1656: ep_len:384 episode reward: total was -11.690000. running mean: -20.261767\n",
      "ep 1656: ep_len:3 episode reward: total was 0.000000. running mean: -20.059149\n",
      "ep 1656: ep_len:515 episode reward: total was -26.300000. running mean: -20.121558\n",
      "ep 1656: ep_len:585 episode reward: total was -39.680000. running mean: -20.317142\n",
      "epsilon:0.173819 episode_count: 11599. steps_count: 5122419.000000\n",
      "ep 1657: ep_len:211 episode reward: total was -12.370000. running mean: -20.237671\n",
      "ep 1657: ep_len:510 episode reward: total was -9.790000. running mean: -20.133194\n",
      "ep 1657: ep_len:500 episode reward: total was -17.750000. running mean: -20.109362\n",
      "ep 1657: ep_len:525 episode reward: total was -27.060000. running mean: -20.178868\n",
      "ep 1657: ep_len:3 episode reward: total was 0.000000. running mean: -19.977080\n",
      "ep 1657: ep_len:565 episode reward: total was -23.520000. running mean: -20.012509\n",
      "ep 1657: ep_len:290 episode reward: total was -13.320000. running mean: -19.945584\n",
      "epsilon:0.173683 episode_count: 11606. steps_count: 5125023.000000\n",
      "ep 1658: ep_len:640 episode reward: total was -20.510000. running mean: -19.951228\n",
      "ep 1658: ep_len:515 episode reward: total was -31.610000. running mean: -20.067816\n",
      "ep 1658: ep_len:600 episode reward: total was -25.650000. running mean: -20.123638\n",
      "ep 1658: ep_len:500 episode reward: total was -4.150000. running mean: -19.963901\n",
      "ep 1658: ep_len:100 episode reward: total was -0.980000. running mean: -19.774062\n",
      "ep 1658: ep_len:530 episode reward: total was -16.970000. running mean: -19.746022\n",
      "ep 1658: ep_len:500 episode reward: total was -23.600000. running mean: -19.784561\n",
      "epsilon:0.173546 episode_count: 11613. steps_count: 5128408.000000\n",
      "ep 1659: ep_len:505 episode reward: total was -11.180000. running mean: -19.698516\n",
      "ep 1659: ep_len:500 episode reward: total was -12.040000. running mean: -19.621931\n",
      "ep 1659: ep_len:535 episode reward: total was -31.700000. running mean: -19.742711\n",
      "ep 1659: ep_len:545 episode reward: total was -21.680000. running mean: -19.762084\n",
      "ep 1659: ep_len:3 episode reward: total was 0.000000. running mean: -19.564463\n",
      "ep 1659: ep_len:500 episode reward: total was -34.700000. running mean: -19.715819\n",
      "ep 1659: ep_len:520 episode reward: total was -20.660000. running mean: -19.725261\n",
      "epsilon:0.173410 episode_count: 11620. steps_count: 5131516.000000\n",
      "ep 1660: ep_len:625 episode reward: total was -34.570000. running mean: -19.873708\n",
      "ep 1660: ep_len:285 episode reward: total was -15.400000. running mean: -19.828971\n",
      "ep 1660: ep_len:515 episode reward: total was -13.460000. running mean: -19.765281\n",
      "ep 1660: ep_len:500 episode reward: total was -17.620000. running mean: -19.743828\n",
      "ep 1660: ep_len:3 episode reward: total was 0.000000. running mean: -19.546390\n",
      "ep 1660: ep_len:500 episode reward: total was -14.310000. running mean: -19.494026\n",
      "ep 1660: ep_len:500 episode reward: total was -28.550000. running mean: -19.584586\n",
      "epsilon:0.173273 episode_count: 11627. steps_count: 5134444.000000\n",
      "ep 1661: ep_len:610 episode reward: total was -38.050000. running mean: -19.769240\n",
      "ep 1661: ep_len:670 episode reward: total was -33.680000. running mean: -19.908348\n",
      "ep 1661: ep_len:770 episode reward: total was -64.830000. running mean: -20.357564\n",
      "ep 1661: ep_len:500 episode reward: total was -27.130000. running mean: -20.425288\n",
      "ep 1661: ep_len:3 episode reward: total was 0.000000. running mean: -20.221036\n",
      "ep 1661: ep_len:560 episode reward: total was -29.650000. running mean: -20.315325\n",
      "ep 1661: ep_len:304 episode reward: total was -18.360000. running mean: -20.295772\n",
      "epsilon:0.173137 episode_count: 11634. steps_count: 5137861.000000\n",
      "ep 1662: ep_len:745 episode reward: total was -32.180000. running mean: -20.414614\n",
      "ep 1662: ep_len:555 episode reward: total was 1.860000. running mean: -20.191868\n",
      "ep 1662: ep_len:545 episode reward: total was -7.570000. running mean: -20.065649\n",
      "ep 1662: ep_len:510 episode reward: total was -16.070000. running mean: -20.025693\n",
      "ep 1662: ep_len:3 episode reward: total was 0.000000. running mean: -19.825436\n",
      "ep 1662: ep_len:500 episode reward: total was -12.950000. running mean: -19.756682\n",
      "ep 1662: ep_len:263 episode reward: total was -14.870000. running mean: -19.707815\n",
      "epsilon:0.173000 episode_count: 11641. steps_count: 5140982.000000\n",
      "ep 1663: ep_len:500 episode reward: total was -18.300000. running mean: -19.693737\n",
      "ep 1663: ep_len:595 episode reward: total was -12.840000. running mean: -19.625199\n",
      "ep 1663: ep_len:418 episode reward: total was -60.830000. running mean: -20.037247\n",
      "ep 1663: ep_len:540 episode reward: total was -12.140000. running mean: -19.958275\n",
      "ep 1663: ep_len:3 episode reward: total was 0.000000. running mean: -19.758692\n",
      "ep 1663: ep_len:500 episode reward: total was -13.990000. running mean: -19.701005\n",
      "ep 1663: ep_len:585 episode reward: total was -37.470000. running mean: -19.878695\n",
      "epsilon:0.172864 episode_count: 11648. steps_count: 5144123.000000\n",
      "ep 1664: ep_len:685 episode reward: total was -50.810000. running mean: -20.188008\n",
      "ep 1664: ep_len:500 episode reward: total was -19.840000. running mean: -20.184528\n",
      "ep 1664: ep_len:825 episode reward: total was -64.840000. running mean: -20.631083\n",
      "ep 1664: ep_len:525 episode reward: total was -17.050000. running mean: -20.595272\n",
      "ep 1664: ep_len:116 episode reward: total was -3.940000. running mean: -20.428719\n",
      "ep 1664: ep_len:605 episode reward: total was -32.400000. running mean: -20.548432\n",
      "ep 1664: ep_len:159 episode reward: total was -10.900000. running mean: -20.451948\n",
      "epsilon:0.172727 episode_count: 11655. steps_count: 5147538.000000\n",
      "ep 1665: ep_len:580 episode reward: total was -22.610000. running mean: -20.473528\n",
      "ep 1665: ep_len:620 episode reward: total was -22.200000. running mean: -20.490793\n",
      "ep 1665: ep_len:362 episode reward: total was -17.770000. running mean: -20.463585\n",
      "ep 1665: ep_len:505 episode reward: total was -18.070000. running mean: -20.439649\n",
      "ep 1665: ep_len:3 episode reward: total was 0.000000. running mean: -20.235253\n",
      "ep 1665: ep_len:550 episode reward: total was -26.440000. running mean: -20.297300\n",
      "ep 1665: ep_len:620 episode reward: total was -39.630000. running mean: -20.490627\n",
      "epsilon:0.172591 episode_count: 11662. steps_count: 5150778.000000\n",
      "ep 1666: ep_len:600 episode reward: total was -27.040000. running mean: -20.556121\n",
      "ep 1666: ep_len:500 episode reward: total was -26.840000. running mean: -20.618960\n",
      "ep 1666: ep_len:635 episode reward: total was -32.440000. running mean: -20.737170\n",
      "ep 1666: ep_len:515 episode reward: total was -18.080000. running mean: -20.710598\n",
      "ep 1666: ep_len:3 episode reward: total was 0.000000. running mean: -20.503492\n",
      "ep 1666: ep_len:560 episode reward: total was -23.120000. running mean: -20.529658\n",
      "ep 1666: ep_len:550 episode reward: total was -19.570000. running mean: -20.520061\n",
      "epsilon:0.172454 episode_count: 11669. steps_count: 5154141.000000\n",
      "ep 1667: ep_len:630 episode reward: total was -30.830000. running mean: -20.623160\n",
      "ep 1667: ep_len:500 episode reward: total was -30.860000. running mean: -20.725529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1667: ep_len:510 episode reward: total was -15.380000. running mean: -20.672073\n",
      "ep 1667: ep_len:610 episode reward: total was -29.070000. running mean: -20.756053\n",
      "ep 1667: ep_len:3 episode reward: total was 0.000000. running mean: -20.548492\n",
      "ep 1667: ep_len:500 episode reward: total was -52.870000. running mean: -20.871707\n",
      "ep 1667: ep_len:500 episode reward: total was -20.150000. running mean: -20.864490\n",
      "epsilon:0.172318 episode_count: 11676. steps_count: 5157394.000000\n",
      "ep 1668: ep_len:585 episode reward: total was -34.950000. running mean: -21.005345\n",
      "ep 1668: ep_len:500 episode reward: total was -25.800000. running mean: -21.053292\n",
      "ep 1668: ep_len:645 episode reward: total was -19.320000. running mean: -21.035959\n",
      "ep 1668: ep_len:505 episode reward: total was -18.020000. running mean: -21.005799\n",
      "ep 1668: ep_len:3 episode reward: total was 0.000000. running mean: -20.795741\n",
      "ep 1668: ep_len:525 episode reward: total was -13.440000. running mean: -20.722184\n",
      "ep 1668: ep_len:600 episode reward: total was -38.660000. running mean: -20.901562\n",
      "epsilon:0.172181 episode_count: 11683. steps_count: 5160757.000000\n",
      "ep 1669: ep_len:500 episode reward: total was -17.880000. running mean: -20.871346\n",
      "ep 1669: ep_len:267 episode reward: total was -24.900000. running mean: -20.911633\n",
      "ep 1669: ep_len:595 episode reward: total was -21.470000. running mean: -20.917217\n",
      "ep 1669: ep_len:500 episode reward: total was -13.120000. running mean: -20.839244\n",
      "ep 1669: ep_len:109 episode reward: total was -0.970000. running mean: -20.640552\n",
      "ep 1669: ep_len:500 episode reward: total was -35.820000. running mean: -20.792347\n",
      "ep 1669: ep_len:520 episode reward: total was -17.600000. running mean: -20.760423\n",
      "epsilon:0.172045 episode_count: 11690. steps_count: 5163748.000000\n",
      "ep 1670: ep_len:580 episode reward: total was -6.610000. running mean: -20.618919\n",
      "ep 1670: ep_len:525 episode reward: total was 2.850000. running mean: -20.384230\n",
      "ep 1670: ep_len:545 episode reward: total was -24.960000. running mean: -20.429987\n",
      "ep 1670: ep_len:56 episode reward: total was -6.960000. running mean: -20.295287\n",
      "ep 1670: ep_len:52 episode reward: total was -7.000000. running mean: -20.162335\n",
      "ep 1670: ep_len:326 episode reward: total was 0.660000. running mean: -19.954111\n",
      "ep 1670: ep_len:590 episode reward: total was -25.000000. running mean: -20.004570\n",
      "epsilon:0.171908 episode_count: 11697. steps_count: 5166422.000000\n",
      "ep 1671: ep_len:555 episode reward: total was -20.680000. running mean: -20.011324\n",
      "ep 1671: ep_len:652 episode reward: total was -67.590000. running mean: -20.487111\n",
      "ep 1671: ep_len:570 episode reward: total was -13.820000. running mean: -20.420440\n",
      "ep 1671: ep_len:505 episode reward: total was -30.130000. running mean: -20.517536\n",
      "ep 1671: ep_len:3 episode reward: total was 0.000000. running mean: -20.312360\n",
      "ep 1671: ep_len:545 episode reward: total was -30.970000. running mean: -20.418937\n",
      "ep 1671: ep_len:555 episode reward: total was -35.590000. running mean: -20.570647\n",
      "epsilon:0.171772 episode_count: 11704. steps_count: 5169807.000000\n",
      "ep 1672: ep_len:565 episode reward: total was -15.960000. running mean: -20.524541\n",
      "ep 1672: ep_len:500 episode reward: total was -16.040000. running mean: -20.479695\n",
      "ep 1672: ep_len:520 episode reward: total was -20.840000. running mean: -20.483299\n",
      "ep 1672: ep_len:500 episode reward: total was -23.680000. running mean: -20.515266\n",
      "ep 1672: ep_len:3 episode reward: total was 0.000000. running mean: -20.310113\n",
      "ep 1672: ep_len:600 episode reward: total was -17.090000. running mean: -20.277912\n",
      "ep 1672: ep_len:302 episode reward: total was -14.380000. running mean: -20.218933\n",
      "epsilon:0.171635 episode_count: 11711. steps_count: 5172797.000000\n",
      "ep 1673: ep_len:232 episode reward: total was 0.610000. running mean: -20.010643\n",
      "ep 1673: ep_len:575 episode reward: total was -36.690000. running mean: -20.177437\n",
      "ep 1673: ep_len:560 episode reward: total was -15.770000. running mean: -20.133363\n",
      "ep 1673: ep_len:149 episode reward: total was -0.890000. running mean: -19.940929\n",
      "ep 1673: ep_len:3 episode reward: total was 0.000000. running mean: -19.741520\n",
      "ep 1673: ep_len:505 episode reward: total was -16.460000. running mean: -19.708704\n",
      "ep 1673: ep_len:193 episode reward: total was -9.360000. running mean: -19.605217\n",
      "epsilon:0.171499 episode_count: 11718. steps_count: 5175014.000000\n",
      "ep 1674: ep_len:242 episode reward: total was -0.860000. running mean: -19.417765\n",
      "ep 1674: ep_len:630 episode reward: total was -7.140000. running mean: -19.294988\n",
      "ep 1674: ep_len:675 episode reward: total was -23.230000. running mean: -19.334338\n",
      "ep 1674: ep_len:749 episode reward: total was -63.530000. running mean: -19.776294\n",
      "ep 1674: ep_len:3 episode reward: total was 0.000000. running mean: -19.578531\n",
      "ep 1674: ep_len:500 episode reward: total was -14.120000. running mean: -19.523946\n",
      "ep 1674: ep_len:510 episode reward: total was -24.900000. running mean: -19.577707\n",
      "epsilon:0.171362 episode_count: 11725. steps_count: 5178323.000000\n",
      "ep 1675: ep_len:500 episode reward: total was -29.780000. running mean: -19.679729\n",
      "ep 1675: ep_len:500 episode reward: total was -7.860000. running mean: -19.561532\n",
      "ep 1675: ep_len:535 episode reward: total was -15.130000. running mean: -19.517217\n",
      "ep 1675: ep_len:56 episode reward: total was -3.460000. running mean: -19.356645\n",
      "ep 1675: ep_len:3 episode reward: total was 0.000000. running mean: -19.163078\n",
      "ep 1675: ep_len:615 episode reward: total was -27.660000. running mean: -19.248047\n",
      "ep 1675: ep_len:515 episode reward: total was -35.250000. running mean: -19.408067\n",
      "epsilon:0.171226 episode_count: 11732. steps_count: 5181047.000000\n",
      "ep 1676: ep_len:246 episode reward: total was -0.360000. running mean: -19.217586\n",
      "ep 1676: ep_len:500 episode reward: total was -3.350000. running mean: -19.058910\n",
      "ep 1676: ep_len:570 episode reward: total was -8.090000. running mean: -18.949221\n",
      "ep 1676: ep_len:500 episode reward: total was -12.560000. running mean: -18.885329\n",
      "ep 1676: ep_len:103 episode reward: total was -7.940000. running mean: -18.775876\n",
      "ep 1676: ep_len:595 episode reward: total was -28.240000. running mean: -18.870517\n",
      "ep 1676: ep_len:570 episode reward: total was -45.170000. running mean: -19.133512\n",
      "epsilon:0.171089 episode_count: 11739. steps_count: 5184131.000000\n",
      "ep 1677: ep_len:535 episode reward: total was -12.440000. running mean: -19.066577\n",
      "ep 1677: ep_len:760 episode reward: total was -58.510000. running mean: -19.461011\n",
      "ep 1677: ep_len:441 episode reward: total was -21.320000. running mean: -19.479601\n",
      "ep 1677: ep_len:635 episode reward: total was -17.930000. running mean: -19.464105\n",
      "ep 1677: ep_len:131 episode reward: total was 4.570000. running mean: -19.223764\n",
      "ep 1677: ep_len:760 episode reward: total was -38.770000. running mean: -19.419226\n",
      "ep 1677: ep_len:307 episode reward: total was -30.340000. running mean: -19.528434\n",
      "epsilon:0.170953 episode_count: 11746. steps_count: 5187700.000000\n",
      "ep 1678: ep_len:500 episode reward: total was -18.400000. running mean: -19.517150\n",
      "ep 1678: ep_len:575 episode reward: total was -33.730000. running mean: -19.659278\n",
      "ep 1678: ep_len:555 episode reward: total was -26.390000. running mean: -19.726585\n",
      "ep 1678: ep_len:520 episode reward: total was -11.160000. running mean: -19.640920\n",
      "ep 1678: ep_len:3 episode reward: total was 0.000000. running mean: -19.444510\n",
      "ep 1678: ep_len:530 episode reward: total was -32.610000. running mean: -19.576165\n",
      "ep 1678: ep_len:550 episode reward: total was -15.360000. running mean: -19.534004\n",
      "epsilon:0.170816 episode_count: 11753. steps_count: 5190933.000000\n",
      "ep 1679: ep_len:505 episode reward: total was -38.780000. running mean: -19.726464\n",
      "ep 1679: ep_len:525 episode reward: total was -37.250000. running mean: -19.901699\n",
      "ep 1679: ep_len:377 episode reward: total was -27.450000. running mean: -19.977182\n",
      "ep 1679: ep_len:525 episode reward: total was -20.110000. running mean: -19.978510\n",
      "ep 1679: ep_len:111 episode reward: total was -17.470000. running mean: -19.953425\n",
      "ep 1679: ep_len:232 episode reward: total was -21.420000. running mean: -19.968091\n",
      "ep 1679: ep_len:615 episode reward: total was -31.610000. running mean: -20.084510\n",
      "epsilon:0.170680 episode_count: 11760. steps_count: 5193823.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1680: ep_len:650 episode reward: total was -43.370000. running mean: -20.317365\n",
      "ep 1680: ep_len:505 episode reward: total was -3.850000. running mean: -20.152691\n",
      "ep 1680: ep_len:605 episode reward: total was -30.040000. running mean: -20.251564\n",
      "ep 1680: ep_len:525 episode reward: total was -10.060000. running mean: -20.149649\n",
      "ep 1680: ep_len:3 episode reward: total was 0.000000. running mean: -19.948152\n",
      "ep 1680: ep_len:555 episode reward: total was -26.080000. running mean: -20.009471\n",
      "ep 1680: ep_len:291 episode reward: total was -15.840000. running mean: -19.967776\n",
      "epsilon:0.170543 episode_count: 11767. steps_count: 5196957.000000\n",
      "ep 1681: ep_len:229 episode reward: total was -0.900000. running mean: -19.777098\n",
      "ep 1681: ep_len:299 episode reward: total was -40.350000. running mean: -19.982827\n",
      "ep 1681: ep_len:79 episode reward: total was -3.970000. running mean: -19.822699\n",
      "ep 1681: ep_len:500 episode reward: total was -10.530000. running mean: -19.729772\n",
      "ep 1681: ep_len:3 episode reward: total was 0.000000. running mean: -19.532474\n",
      "ep 1681: ep_len:635 episode reward: total was -40.140000. running mean: -19.738549\n",
      "ep 1681: ep_len:545 episode reward: total was -28.230000. running mean: -19.823464\n",
      "epsilon:0.170407 episode_count: 11774. steps_count: 5199247.000000\n",
      "ep 1682: ep_len:500 episode reward: total was -18.760000. running mean: -19.812829\n",
      "ep 1682: ep_len:595 episode reward: total was -1.680000. running mean: -19.631501\n",
      "ep 1682: ep_len:625 episode reward: total was -43.100000. running mean: -19.866186\n",
      "ep 1682: ep_len:505 episode reward: total was -24.070000. running mean: -19.908224\n",
      "ep 1682: ep_len:96 episode reward: total was -5.460000. running mean: -19.763742\n",
      "ep 1682: ep_len:545 episode reward: total was -31.760000. running mean: -19.883704\n",
      "ep 1682: ep_len:205 episode reward: total was -13.400000. running mean: -19.818867\n",
      "epsilon:0.170270 episode_count: 11781. steps_count: 5202318.000000\n",
      "ep 1683: ep_len:625 episode reward: total was -32.370000. running mean: -19.944379\n",
      "ep 1683: ep_len:770 episode reward: total was -66.740000. running mean: -20.412335\n",
      "ep 1683: ep_len:655 episode reward: total was -22.920000. running mean: -20.437412\n",
      "ep 1683: ep_len:500 episode reward: total was -45.240000. running mean: -20.685437\n",
      "ep 1683: ep_len:91 episode reward: total was -5.960000. running mean: -20.538183\n",
      "ep 1683: ep_len:235 episode reward: total was -3.880000. running mean: -20.371601\n",
      "ep 1683: ep_len:751 episode reward: total was -69.330000. running mean: -20.861185\n",
      "epsilon:0.170134 episode_count: 11788. steps_count: 5205945.000000\n",
      "ep 1684: ep_len:590 episode reward: total was -30.770000. running mean: -20.960273\n",
      "ep 1684: ep_len:500 episode reward: total was -5.070000. running mean: -20.801371\n",
      "ep 1684: ep_len:560 episode reward: total was -15.970000. running mean: -20.753057\n",
      "ep 1684: ep_len:505 episode reward: total was -18.500000. running mean: -20.730526\n",
      "ep 1684: ep_len:3 episode reward: total was 0.000000. running mean: -20.523221\n",
      "ep 1684: ep_len:610 episode reward: total was -32.400000. running mean: -20.641989\n",
      "ep 1684: ep_len:625 episode reward: total was -63.100000. running mean: -21.066569\n",
      "epsilon:0.169997 episode_count: 11795. steps_count: 5209338.000000\n",
      "ep 1685: ep_len:89 episode reward: total was 3.040000. running mean: -20.825503\n",
      "ep 1685: ep_len:500 episode reward: total was -22.070000. running mean: -20.837948\n",
      "ep 1685: ep_len:640 episode reward: total was -23.400000. running mean: -20.863569\n",
      "ep 1685: ep_len:580 episode reward: total was -50.700000. running mean: -21.161933\n",
      "ep 1685: ep_len:57 episode reward: total was 2.030000. running mean: -20.930014\n",
      "ep 1685: ep_len:695 episode reward: total was -26.290000. running mean: -20.983614\n",
      "ep 1685: ep_len:655 episode reward: total was -47.430000. running mean: -21.248077\n",
      "epsilon:0.169861 episode_count: 11802. steps_count: 5212554.000000\n",
      "ep 1686: ep_len:525 episode reward: total was -17.450000. running mean: -21.210097\n",
      "ep 1686: ep_len:590 episode reward: total was -44.090000. running mean: -21.438896\n",
      "ep 1686: ep_len:550 episode reward: total was -13.100000. running mean: -21.355507\n",
      "ep 1686: ep_len:620 episode reward: total was -6.610000. running mean: -21.208052\n",
      "ep 1686: ep_len:3 episode reward: total was 0.000000. running mean: -20.995971\n",
      "ep 1686: ep_len:325 episode reward: total was -31.380000. running mean: -21.099811\n",
      "ep 1686: ep_len:545 episode reward: total was -29.610000. running mean: -21.184913\n",
      "epsilon:0.169724 episode_count: 11809. steps_count: 5215712.000000\n",
      "ep 1687: ep_len:610 episode reward: total was -22.790000. running mean: -21.200964\n",
      "ep 1687: ep_len:500 episode reward: total was -8.860000. running mean: -21.077555\n",
      "ep 1687: ep_len:416 episode reward: total was -3.300000. running mean: -20.899779\n",
      "ep 1687: ep_len:500 episode reward: total was -9.630000. running mean: -20.787081\n",
      "ep 1687: ep_len:92 episode reward: total was -4.980000. running mean: -20.629010\n",
      "ep 1687: ep_len:590 episode reward: total was -25.670000. running mean: -20.679420\n",
      "ep 1687: ep_len:322 episode reward: total was -17.870000. running mean: -20.651326\n",
      "epsilon:0.169588 episode_count: 11816. steps_count: 5218742.000000\n",
      "ep 1688: ep_len:620 episode reward: total was -39.420000. running mean: -20.839013\n",
      "ep 1688: ep_len:510 episode reward: total was -27.620000. running mean: -20.906823\n",
      "ep 1688: ep_len:510 episode reward: total was -10.150000. running mean: -20.799255\n",
      "ep 1688: ep_len:570 episode reward: total was -13.180000. running mean: -20.723062\n",
      "ep 1688: ep_len:86 episode reward: total was -11.460000. running mean: -20.630431\n",
      "ep 1688: ep_len:525 episode reward: total was -22.670000. running mean: -20.650827\n",
      "ep 1688: ep_len:625 episode reward: total was -37.120000. running mean: -20.815519\n",
      "epsilon:0.169451 episode_count: 11823. steps_count: 5222188.000000\n",
      "ep 1689: ep_len:500 episode reward: total was -4.150000. running mean: -20.648864\n",
      "ep 1689: ep_len:630 episode reward: total was -36.160000. running mean: -20.803975\n",
      "ep 1689: ep_len:570 episode reward: total was -31.850000. running mean: -20.914435\n",
      "ep 1689: ep_len:56 episode reward: total was -2.970000. running mean: -20.734991\n",
      "ep 1689: ep_len:112 episode reward: total was -10.950000. running mean: -20.637141\n",
      "ep 1689: ep_len:555 episode reward: total was -21.100000. running mean: -20.641770\n",
      "ep 1689: ep_len:570 episode reward: total was -27.940000. running mean: -20.714752\n",
      "epsilon:0.169315 episode_count: 11830. steps_count: 5225181.000000\n",
      "ep 1690: ep_len:505 episode reward: total was -29.830000. running mean: -20.805904\n",
      "ep 1690: ep_len:530 episode reward: total was -11.650000. running mean: -20.714345\n",
      "ep 1690: ep_len:555 episode reward: total was -34.590000. running mean: -20.853102\n",
      "ep 1690: ep_len:581 episode reward: total was -45.950000. running mean: -21.104071\n",
      "ep 1690: ep_len:3 episode reward: total was 0.000000. running mean: -20.893030\n",
      "ep 1690: ep_len:505 episode reward: total was -19.080000. running mean: -20.874900\n",
      "ep 1690: ep_len:580 episode reward: total was -19.880000. running mean: -20.864951\n",
      "epsilon:0.169178 episode_count: 11837. steps_count: 5228440.000000\n",
      "ep 1691: ep_len:104 episode reward: total was -12.460000. running mean: -20.780901\n",
      "ep 1691: ep_len:670 episode reward: total was -49.500000. running mean: -21.068092\n",
      "ep 1691: ep_len:815 episode reward: total was -59.240000. running mean: -21.449811\n",
      "ep 1691: ep_len:56 episode reward: total was -4.970000. running mean: -21.285013\n",
      "ep 1691: ep_len:36 episode reward: total was -5.500000. running mean: -21.127163\n",
      "ep 1691: ep_len:500 episode reward: total was -9.140000. running mean: -21.007291\n",
      "ep 1691: ep_len:630 episode reward: total was -39.120000. running mean: -21.188419\n",
      "epsilon:0.169042 episode_count: 11844. steps_count: 5231251.000000\n",
      "ep 1692: ep_len:228 episode reward: total was -10.410000. running mean: -21.080634\n",
      "ep 1692: ep_len:550 episode reward: total was -28.700000. running mean: -21.156828\n",
      "ep 1692: ep_len:500 episode reward: total was -23.040000. running mean: -21.175660\n",
      "ep 1692: ep_len:530 episode reward: total was -32.130000. running mean: -21.285203\n",
      "ep 1692: ep_len:3 episode reward: total was 0.000000. running mean: -21.072351\n",
      "ep 1692: ep_len:284 episode reward: total was -15.410000. running mean: -21.015728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1692: ep_len:630 episode reward: total was -14.880000. running mean: -20.954370\n",
      "epsilon:0.168905 episode_count: 11851. steps_count: 5233976.000000\n",
      "ep 1693: ep_len:640 episode reward: total was -28.430000. running mean: -21.029127\n",
      "ep 1693: ep_len:823 episode reward: total was -86.390000. running mean: -21.682735\n",
      "ep 1693: ep_len:565 episode reward: total was -8.720000. running mean: -21.553108\n",
      "ep 1693: ep_len:500 episode reward: total was -29.130000. running mean: -21.628877\n",
      "ep 1693: ep_len:3 episode reward: total was 0.000000. running mean: -21.412588\n",
      "ep 1693: ep_len:580 episode reward: total was -42.960000. running mean: -21.628062\n",
      "ep 1693: ep_len:505 episode reward: total was -25.020000. running mean: -21.661982\n",
      "epsilon:0.168769 episode_count: 11858. steps_count: 5237592.000000\n",
      "ep 1694: ep_len:500 episode reward: total was -7.840000. running mean: -21.523762\n",
      "ep 1694: ep_len:248 episode reward: total was -12.920000. running mean: -21.437724\n",
      "ep 1694: ep_len:387 episode reward: total was -5.360000. running mean: -21.276947\n",
      "ep 1694: ep_len:515 episode reward: total was -24.140000. running mean: -21.305578\n",
      "ep 1694: ep_len:84 episode reward: total was -3.460000. running mean: -21.127122\n",
      "ep 1694: ep_len:545 episode reward: total was -20.190000. running mean: -21.117751\n",
      "ep 1694: ep_len:655 episode reward: total was -31.820000. running mean: -21.224773\n",
      "epsilon:0.168632 episode_count: 11865. steps_count: 5240526.000000\n",
      "ep 1695: ep_len:640 episode reward: total was -42.640000. running mean: -21.438925\n",
      "ep 1695: ep_len:600 episode reward: total was -24.150000. running mean: -21.466036\n",
      "ep 1695: ep_len:500 episode reward: total was -22.240000. running mean: -21.473776\n",
      "ep 1695: ep_len:500 episode reward: total was -51.250000. running mean: -21.771538\n",
      "ep 1695: ep_len:3 episode reward: total was 0.000000. running mean: -21.553823\n",
      "ep 1695: ep_len:630 episode reward: total was -42.340000. running mean: -21.761684\n",
      "ep 1695: ep_len:500 episode reward: total was -15.170000. running mean: -21.695767\n",
      "epsilon:0.168496 episode_count: 11872. steps_count: 5243899.000000\n",
      "ep 1696: ep_len:575 episode reward: total was -26.090000. running mean: -21.739710\n",
      "ep 1696: ep_len:305 episode reward: total was -21.370000. running mean: -21.736013\n",
      "ep 1696: ep_len:75 episode reward: total was -2.470000. running mean: -21.543353\n",
      "ep 1696: ep_len:154 episode reward: total was -7.950000. running mean: -21.407419\n",
      "ep 1696: ep_len:93 episode reward: total was 1.540000. running mean: -21.177945\n",
      "ep 1696: ep_len:515 episode reward: total was -34.730000. running mean: -21.313465\n",
      "ep 1696: ep_len:500 episode reward: total was -19.310000. running mean: -21.293431\n",
      "epsilon:0.168359 episode_count: 11879. steps_count: 5246116.000000\n",
      "ep 1697: ep_len:510 episode reward: total was -27.530000. running mean: -21.355796\n",
      "ep 1697: ep_len:550 episode reward: total was 6.350000. running mean: -21.078738\n",
      "ep 1697: ep_len:515 episode reward: total was -42.810000. running mean: -21.296051\n",
      "ep 1697: ep_len:595 episode reward: total was -34.010000. running mean: -21.423191\n",
      "ep 1697: ep_len:3 episode reward: total was 0.000000. running mean: -21.208959\n",
      "ep 1697: ep_len:550 episode reward: total was -20.490000. running mean: -21.201769\n",
      "ep 1697: ep_len:525 episode reward: total was -27.500000. running mean: -21.264751\n",
      "epsilon:0.168223 episode_count: 11886. steps_count: 5249364.000000\n",
      "ep 1698: ep_len:670 episode reward: total was -46.870000. running mean: -21.520804\n",
      "ep 1698: ep_len:500 episode reward: total was 0.290000. running mean: -21.302696\n",
      "ep 1698: ep_len:635 episode reward: total was -27.470000. running mean: -21.364369\n",
      "ep 1698: ep_len:161 episode reward: total was -1.430000. running mean: -21.165025\n",
      "ep 1698: ep_len:3 episode reward: total was 0.000000. running mean: -20.953375\n",
      "ep 1698: ep_len:555 episode reward: total was -24.460000. running mean: -20.988441\n",
      "ep 1698: ep_len:610 episode reward: total was -18.420000. running mean: -20.962757\n",
      "epsilon:0.168086 episode_count: 11893. steps_count: 5252498.000000\n",
      "ep 1699: ep_len:540 episode reward: total was -28.360000. running mean: -21.036729\n",
      "ep 1699: ep_len:575 episode reward: total was -69.150000. running mean: -21.517862\n",
      "ep 1699: ep_len:705 episode reward: total was -23.260000. running mean: -21.535283\n",
      "ep 1699: ep_len:605 episode reward: total was -22.530000. running mean: -21.545230\n",
      "ep 1699: ep_len:105 episode reward: total was 1.020000. running mean: -21.319578\n",
      "ep 1699: ep_len:635 episode reward: total was -12.060000. running mean: -21.226982\n",
      "ep 1699: ep_len:615 episode reward: total was -28.990000. running mean: -21.304613\n",
      "epsilon:0.167950 episode_count: 11900. steps_count: 5256278.000000\n",
      "ep 1700: ep_len:560 episode reward: total was -18.570000. running mean: -21.277266\n",
      "ep 1700: ep_len:510 episode reward: total was -20.490000. running mean: -21.269394\n",
      "ep 1700: ep_len:720 episode reward: total was -29.760000. running mean: -21.354300\n",
      "ep 1700: ep_len:111 episode reward: total was -0.410000. running mean: -21.144857\n",
      "ep 1700: ep_len:3 episode reward: total was 0.000000. running mean: -20.933408\n",
      "ep 1700: ep_len:625 episode reward: total was -10.780000. running mean: -20.831874\n",
      "ep 1700: ep_len:560 episode reward: total was -32.520000. running mean: -20.948755\n",
      "epsilon:0.167813 episode_count: 11907. steps_count: 5259367.000000\n",
      "ep 1701: ep_len:545 episode reward: total was -19.780000. running mean: -20.937068\n",
      "ep 1701: ep_len:274 episode reward: total was -20.910000. running mean: -20.936797\n",
      "ep 1701: ep_len:725 episode reward: total was -56.810000. running mean: -21.295529\n",
      "ep 1701: ep_len:520 episode reward: total was -9.200000. running mean: -21.174574\n",
      "ep 1701: ep_len:3 episode reward: total was 0.000000. running mean: -20.962828\n",
      "ep 1701: ep_len:545 episode reward: total was -39.210000. running mean: -21.145300\n",
      "ep 1701: ep_len:560 episode reward: total was -38.940000. running mean: -21.323247\n",
      "epsilon:0.167677 episode_count: 11914. steps_count: 5262539.000000\n",
      "ep 1702: ep_len:525 episode reward: total was -10.230000. running mean: -21.212314\n",
      "ep 1702: ep_len:500 episode reward: total was -4.890000. running mean: -21.049091\n",
      "ep 1702: ep_len:545 episode reward: total was -13.810000. running mean: -20.976700\n",
      "ep 1702: ep_len:560 episode reward: total was -32.710000. running mean: -21.094033\n",
      "ep 1702: ep_len:66 episode reward: total was -2.480000. running mean: -20.907893\n",
      "ep 1702: ep_len:545 episode reward: total was -28.210000. running mean: -20.980914\n",
      "ep 1702: ep_len:505 episode reward: total was -31.030000. running mean: -21.081405\n",
      "epsilon:0.167540 episode_count: 11921. steps_count: 5265785.000000\n",
      "ep 1703: ep_len:500 episode reward: total was -9.690000. running mean: -20.967491\n",
      "ep 1703: ep_len:575 episode reward: total was -14.680000. running mean: -20.904616\n",
      "ep 1703: ep_len:640 episode reward: total was -36.550000. running mean: -21.061070\n",
      "ep 1703: ep_len:505 episode reward: total was -20.530000. running mean: -21.055759\n",
      "ep 1703: ep_len:3 episode reward: total was 0.000000. running mean: -20.845202\n",
      "ep 1703: ep_len:615 episode reward: total was -16.300000. running mean: -20.799750\n",
      "ep 1703: ep_len:520 episode reward: total was -18.040000. running mean: -20.772152\n",
      "epsilon:0.167404 episode_count: 11928. steps_count: 5269143.000000\n",
      "ep 1704: ep_len:130 episode reward: total was -11.480000. running mean: -20.679231\n",
      "ep 1704: ep_len:600 episode reward: total was -18.700000. running mean: -20.659438\n",
      "ep 1704: ep_len:600 episode reward: total was -35.670000. running mean: -20.809544\n",
      "ep 1704: ep_len:56 episode reward: total was -3.980000. running mean: -20.641248\n",
      "ep 1704: ep_len:3 episode reward: total was 0.000000. running mean: -20.434836\n",
      "ep 1704: ep_len:610 episode reward: total was -14.080000. running mean: -20.371288\n",
      "ep 1704: ep_len:505 episode reward: total was -34.160000. running mean: -20.509175\n",
      "epsilon:0.167267 episode_count: 11935. steps_count: 5271647.000000\n",
      "ep 1705: ep_len:505 episode reward: total was -13.510000. running mean: -20.439183\n",
      "ep 1705: ep_len:620 episode reward: total was -10.980000. running mean: -20.344591\n",
      "ep 1705: ep_len:595 episode reward: total was -16.110000. running mean: -20.302245\n",
      "ep 1705: ep_len:535 episode reward: total was -12.430000. running mean: -20.223523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1705: ep_len:95 episode reward: total was 0.050000. running mean: -20.020788\n",
      "ep 1705: ep_len:525 episode reward: total was -47.620000. running mean: -20.296780\n",
      "ep 1705: ep_len:500 episode reward: total was -19.280000. running mean: -20.286612\n",
      "epsilon:0.167131 episode_count: 11942. steps_count: 5275022.000000\n",
      "ep 1706: ep_len:520 episode reward: total was -7.640000. running mean: -20.160146\n",
      "ep 1706: ep_len:500 episode reward: total was -5.480000. running mean: -20.013344\n",
      "ep 1706: ep_len:555 episode reward: total was -10.150000. running mean: -19.914711\n",
      "ep 1706: ep_len:500 episode reward: total was -2.170000. running mean: -19.737264\n",
      "ep 1706: ep_len:88 episode reward: total was 1.040000. running mean: -19.529491\n",
      "ep 1706: ep_len:565 episode reward: total was -13.860000. running mean: -19.472796\n",
      "ep 1706: ep_len:515 episode reward: total was -24.460000. running mean: -19.522668\n",
      "epsilon:0.166994 episode_count: 11949. steps_count: 5278265.000000\n",
      "ep 1707: ep_len:116 episode reward: total was -6.950000. running mean: -19.396942\n",
      "ep 1707: ep_len:500 episode reward: total was -19.990000. running mean: -19.402872\n",
      "ep 1707: ep_len:600 episode reward: total was -40.650000. running mean: -19.615343\n",
      "ep 1707: ep_len:510 episode reward: total was 0.440000. running mean: -19.414790\n",
      "ep 1707: ep_len:99 episode reward: total was -1.470000. running mean: -19.235342\n",
      "ep 1707: ep_len:690 episode reward: total was -52.420000. running mean: -19.567189\n",
      "ep 1707: ep_len:620 episode reward: total was -38.110000. running mean: -19.752617\n",
      "epsilon:0.166858 episode_count: 11956. steps_count: 5281400.000000\n",
      "ep 1708: ep_len:660 episode reward: total was -22.000000. running mean: -19.775091\n",
      "ep 1708: ep_len:560 episode reward: total was -38.740000. running mean: -19.964740\n",
      "ep 1708: ep_len:560 episode reward: total was -10.630000. running mean: -19.871392\n",
      "ep 1708: ep_len:535 episode reward: total was -21.500000. running mean: -19.887678\n",
      "ep 1708: ep_len:93 episode reward: total was 0.560000. running mean: -19.683202\n",
      "ep 1708: ep_len:530 episode reward: total was -53.600000. running mean: -20.022370\n",
      "ep 1708: ep_len:525 episode reward: total was -19.530000. running mean: -20.017446\n",
      "epsilon:0.166721 episode_count: 11963. steps_count: 5284863.000000\n",
      "ep 1709: ep_len:213 episode reward: total was -23.420000. running mean: -20.051471\n",
      "ep 1709: ep_len:520 episode reward: total was -9.700000. running mean: -19.947957\n",
      "ep 1709: ep_len:565 episode reward: total was -21.870000. running mean: -19.967177\n",
      "ep 1709: ep_len:575 episode reward: total was -8.590000. running mean: -19.853405\n",
      "ep 1709: ep_len:3 episode reward: total was 0.000000. running mean: -19.654871\n",
      "ep 1709: ep_len:575 episode reward: total was -17.330000. running mean: -19.631623\n",
      "ep 1709: ep_len:585 episode reward: total was -21.580000. running mean: -19.651106\n",
      "epsilon:0.166585 episode_count: 11970. steps_count: 5287899.000000\n",
      "ep 1710: ep_len:99 episode reward: total was -6.000000. running mean: -19.514595\n",
      "ep 1710: ep_len:510 episode reward: total was -21.720000. running mean: -19.536649\n",
      "ep 1710: ep_len:645 episode reward: total was -38.020000. running mean: -19.721483\n",
      "ep 1710: ep_len:625 episode reward: total was -28.030000. running mean: -19.804568\n",
      "ep 1710: ep_len:3 episode reward: total was 0.000000. running mean: -19.606522\n",
      "ep 1710: ep_len:243 episode reward: total was -4.910000. running mean: -19.459557\n",
      "ep 1710: ep_len:595 episode reward: total was -45.230000. running mean: -19.717262\n",
      "epsilon:0.166448 episode_count: 11977. steps_count: 5290619.000000\n",
      "ep 1711: ep_len:540 episode reward: total was -19.640000. running mean: -19.716489\n",
      "ep 1711: ep_len:730 episode reward: total was -52.450000. running mean: -20.043824\n",
      "ep 1711: ep_len:730 episode reward: total was -37.300000. running mean: -20.216386\n",
      "ep 1711: ep_len:30 episode reward: total was -0.970000. running mean: -20.023922\n",
      "ep 1711: ep_len:3 episode reward: total was 0.000000. running mean: -19.823683\n",
      "ep 1711: ep_len:308 episode reward: total was -2.870000. running mean: -19.654146\n",
      "ep 1711: ep_len:590 episode reward: total was -28.430000. running mean: -19.741904\n",
      "epsilon:0.166312 episode_count: 11984. steps_count: 5293550.000000\n",
      "ep 1712: ep_len:500 episode reward: total was -7.780000. running mean: -19.622285\n",
      "ep 1712: ep_len:515 episode reward: total was -31.070000. running mean: -19.736763\n",
      "ep 1712: ep_len:550 episode reward: total was -5.090000. running mean: -19.590295\n",
      "ep 1712: ep_len:500 episode reward: total was -8.070000. running mean: -19.475092\n",
      "ep 1712: ep_len:3 episode reward: total was 0.000000. running mean: -19.280341\n",
      "ep 1712: ep_len:314 episode reward: total was -4.360000. running mean: -19.131138\n",
      "ep 1712: ep_len:301 episode reward: total was -14.340000. running mean: -19.083226\n",
      "epsilon:0.166175 episode_count: 11991. steps_count: 5296233.000000\n",
      "ep 1713: ep_len:227 episode reward: total was -2.430000. running mean: -18.916694\n",
      "ep 1713: ep_len:500 episode reward: total was -36.190000. running mean: -19.089427\n",
      "ep 1713: ep_len:650 episode reward: total was -20.230000. running mean: -19.100833\n",
      "ep 1713: ep_len:555 episode reward: total was -26.220000. running mean: -19.172024\n",
      "ep 1713: ep_len:50 episode reward: total was 2.000000. running mean: -18.960304\n",
      "ep 1713: ep_len:605 episode reward: total was -28.350000. running mean: -19.054201\n",
      "ep 1713: ep_len:620 episode reward: total was -39.480000. running mean: -19.258459\n",
      "epsilon:0.166039 episode_count: 11998. steps_count: 5299440.000000\n",
      "ep 1714: ep_len:645 episode reward: total was -26.070000. running mean: -19.326575\n",
      "ep 1714: ep_len:535 episode reward: total was -13.890000. running mean: -19.272209\n",
      "ep 1714: ep_len:63 episode reward: total was 1.510000. running mean: -19.064387\n",
      "ep 1714: ep_len:725 episode reward: total was -90.140000. running mean: -19.775143\n",
      "ep 1714: ep_len:3 episode reward: total was 0.000000. running mean: -19.577391\n",
      "ep 1714: ep_len:500 episode reward: total was -16.530000. running mean: -19.546918\n",
      "ep 1714: ep_len:505 episode reward: total was -23.470000. running mean: -19.586148\n",
      "epsilon:0.165902 episode_count: 12005. steps_count: 5302416.000000\n",
      "ep 1715: ep_len:635 episode reward: total was -17.450000. running mean: -19.564787\n",
      "ep 1715: ep_len:635 episode reward: total was -27.130000. running mean: -19.640439\n",
      "ep 1715: ep_len:77 episode reward: total was -0.970000. running mean: -19.453735\n",
      "ep 1715: ep_len:530 episode reward: total was -26.140000. running mean: -19.520597\n",
      "ep 1715: ep_len:3 episode reward: total was 0.000000. running mean: -19.325391\n",
      "ep 1715: ep_len:510 episode reward: total was -12.160000. running mean: -19.253737\n",
      "ep 1715: ep_len:355 episode reward: total was -34.380000. running mean: -19.405000\n",
      "epsilon:0.165766 episode_count: 12012. steps_count: 5305161.000000\n",
      "ep 1716: ep_len:570 episode reward: total was -36.350000. running mean: -19.574450\n",
      "ep 1716: ep_len:530 episode reward: total was 4.330000. running mean: -19.335406\n",
      "ep 1716: ep_len:605 episode reward: total was -13.570000. running mean: -19.277751\n",
      "ep 1716: ep_len:500 episode reward: total was -18.650000. running mean: -19.271474\n",
      "ep 1716: ep_len:3 episode reward: total was 0.000000. running mean: -19.078759\n",
      "ep 1716: ep_len:500 episode reward: total was -41.330000. running mean: -19.301272\n",
      "ep 1716: ep_len:535 episode reward: total was -22.070000. running mean: -19.328959\n",
      "epsilon:0.165629 episode_count: 12019. steps_count: 5308404.000000\n",
      "ep 1717: ep_len:620 episode reward: total was -25.700000. running mean: -19.392669\n",
      "ep 1717: ep_len:520 episode reward: total was -32.180000. running mean: -19.520543\n",
      "ep 1717: ep_len:449 episode reward: total was 0.190000. running mean: -19.323437\n",
      "ep 1717: ep_len:575 episode reward: total was -9.590000. running mean: -19.226103\n",
      "ep 1717: ep_len:3 episode reward: total was 0.000000. running mean: -19.033842\n",
      "ep 1717: ep_len:500 episode reward: total was -45.730000. running mean: -19.300803\n",
      "ep 1717: ep_len:515 episode reward: total was -18.490000. running mean: -19.292695\n",
      "epsilon:0.165493 episode_count: 12026. steps_count: 5311586.000000\n",
      "ep 1718: ep_len:184 episode reward: total was -9.940000. running mean: -19.199168\n",
      "ep 1718: ep_len:555 episode reward: total was -10.420000. running mean: -19.111377\n",
      "ep 1718: ep_len:505 episode reward: total was -14.180000. running mean: -19.062063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1718: ep_len:500 episode reward: total was -11.710000. running mean: -18.988542\n",
      "ep 1718: ep_len:91 episode reward: total was -13.960000. running mean: -18.938257\n",
      "ep 1718: ep_len:545 episode reward: total was -20.970000. running mean: -18.958574\n",
      "ep 1718: ep_len:550 episode reward: total was -21.880000. running mean: -18.987789\n",
      "epsilon:0.165356 episode_count: 12033. steps_count: 5314516.000000\n",
      "ep 1719: ep_len:200 episode reward: total was -18.910000. running mean: -18.987011\n",
      "ep 1719: ep_len:560 episode reward: total was -30.570000. running mean: -19.102841\n",
      "ep 1719: ep_len:610 episode reward: total was -14.530000. running mean: -19.057112\n",
      "ep 1719: ep_len:56 episode reward: total was 0.550000. running mean: -18.861041\n",
      "ep 1719: ep_len:116 episode reward: total was -1.440000. running mean: -18.686831\n",
      "ep 1719: ep_len:525 episode reward: total was -37.390000. running mean: -18.873862\n",
      "ep 1719: ep_len:580 episode reward: total was -23.180000. running mean: -18.916924\n",
      "epsilon:0.165220 episode_count: 12040. steps_count: 5317163.000000\n",
      "ep 1720: ep_len:500 episode reward: total was -9.260000. running mean: -18.820354\n",
      "ep 1720: ep_len:550 episode reward: total was -22.470000. running mean: -18.856851\n",
      "ep 1720: ep_len:585 episode reward: total was -13.510000. running mean: -18.803382\n",
      "ep 1720: ep_len:105 episode reward: total was 0.560000. running mean: -18.609749\n",
      "ep 1720: ep_len:3 episode reward: total was 0.000000. running mean: -18.423651\n",
      "ep 1720: ep_len:545 episode reward: total was -3.940000. running mean: -18.278815\n",
      "ep 1720: ep_len:500 episode reward: total was -29.680000. running mean: -18.392826\n",
      "epsilon:0.165083 episode_count: 12047. steps_count: 5319951.000000\n",
      "ep 1721: ep_len:655 episode reward: total was -13.920000. running mean: -18.348098\n",
      "ep 1721: ep_len:565 episode reward: total was 11.340000. running mean: -18.051217\n",
      "ep 1721: ep_len:550 episode reward: total was -17.590000. running mean: -18.046605\n",
      "ep 1721: ep_len:505 episode reward: total was -81.450000. running mean: -18.680639\n",
      "ep 1721: ep_len:3 episode reward: total was 0.000000. running mean: -18.493833\n",
      "ep 1721: ep_len:500 episode reward: total was -26.060000. running mean: -18.569494\n",
      "ep 1721: ep_len:650 episode reward: total was -50.080000. running mean: -18.884599\n",
      "epsilon:0.164947 episode_count: 12054. steps_count: 5323379.000000\n",
      "ep 1722: ep_len:500 episode reward: total was -19.450000. running mean: -18.890253\n",
      "ep 1722: ep_len:520 episode reward: total was -15.540000. running mean: -18.856751\n",
      "ep 1722: ep_len:510 episode reward: total was -22.510000. running mean: -18.893283\n",
      "ep 1722: ep_len:540 episode reward: total was -5.170000. running mean: -18.756050\n",
      "ep 1722: ep_len:3 episode reward: total was 0.000000. running mean: -18.568490\n",
      "ep 1722: ep_len:715 episode reward: total was -64.950000. running mean: -19.032305\n",
      "ep 1722: ep_len:665 episode reward: total was -72.260000. running mean: -19.564582\n",
      "epsilon:0.164810 episode_count: 12061. steps_count: 5326832.000000\n",
      "ep 1723: ep_len:620 episode reward: total was -37.810000. running mean: -19.747036\n",
      "ep 1723: ep_len:196 episode reward: total was -6.440000. running mean: -19.613966\n",
      "ep 1723: ep_len:500 episode reward: total was -10.970000. running mean: -19.527526\n",
      "ep 1723: ep_len:585 episode reward: total was -24.140000. running mean: -19.573651\n",
      "ep 1723: ep_len:105 episode reward: total was -9.460000. running mean: -19.472514\n",
      "ep 1723: ep_len:515 episode reward: total was -52.330000. running mean: -19.801089\n",
      "ep 1723: ep_len:550 episode reward: total was -16.610000. running mean: -19.769178\n",
      "epsilon:0.164674 episode_count: 12068. steps_count: 5329903.000000\n",
      "ep 1724: ep_len:520 episode reward: total was -40.060000. running mean: -19.972087\n",
      "ep 1724: ep_len:500 episode reward: total was -42.370000. running mean: -20.196066\n",
      "ep 1724: ep_len:565 episode reward: total was -39.450000. running mean: -20.388605\n",
      "ep 1724: ep_len:500 episode reward: total was -15.770000. running mean: -20.342419\n",
      "ep 1724: ep_len:3 episode reward: total was 0.000000. running mean: -20.138995\n",
      "ep 1724: ep_len:625 episode reward: total was -46.670000. running mean: -20.404305\n",
      "ep 1724: ep_len:545 episode reward: total was -19.160000. running mean: -20.391862\n",
      "epsilon:0.164537 episode_count: 12075. steps_count: 5333161.000000\n",
      "ep 1725: ep_len:720 episode reward: total was -49.170000. running mean: -20.679643\n",
      "ep 1725: ep_len:361 episode reward: total was -21.400000. running mean: -20.686847\n",
      "ep 1725: ep_len:540 episode reward: total was -24.060000. running mean: -20.720578\n",
      "ep 1725: ep_len:500 episode reward: total was -19.610000. running mean: -20.709473\n",
      "ep 1725: ep_len:54 episode reward: total was 2.000000. running mean: -20.482378\n",
      "ep 1725: ep_len:665 episode reward: total was -24.790000. running mean: -20.525454\n",
      "ep 1725: ep_len:570 episode reward: total was -14.130000. running mean: -20.461499\n",
      "epsilon:0.164401 episode_count: 12082. steps_count: 5336571.000000\n",
      "ep 1726: ep_len:525 episode reward: total was -49.160000. running mean: -20.748484\n",
      "ep 1726: ep_len:655 episode reward: total was -23.570000. running mean: -20.776700\n",
      "ep 1726: ep_len:745 episode reward: total was -76.980000. running mean: -21.338733\n",
      "ep 1726: ep_len:115 episode reward: total was -7.960000. running mean: -21.204945\n",
      "ep 1726: ep_len:3 episode reward: total was 0.000000. running mean: -20.992896\n",
      "ep 1726: ep_len:520 episode reward: total was -2.450000. running mean: -20.807467\n",
      "ep 1726: ep_len:505 episode reward: total was -25.190000. running mean: -20.851292\n",
      "epsilon:0.164264 episode_count: 12089. steps_count: 5339639.000000\n",
      "ep 1727: ep_len:650 episode reward: total was -10.970000. running mean: -20.752479\n",
      "ep 1727: ep_len:505 episode reward: total was -32.580000. running mean: -20.870755\n",
      "ep 1727: ep_len:705 episode reward: total was -57.400000. running mean: -21.236047\n",
      "ep 1727: ep_len:500 episode reward: total was -11.560000. running mean: -21.139286\n",
      "ep 1727: ep_len:3 episode reward: total was 0.000000. running mean: -20.927894\n",
      "ep 1727: ep_len:515 episode reward: total was -20.400000. running mean: -20.922615\n",
      "ep 1727: ep_len:505 episode reward: total was -25.510000. running mean: -20.968489\n",
      "epsilon:0.164128 episode_count: 12096. steps_count: 5343022.000000\n",
      "ep 1728: ep_len:600 episode reward: total was -22.600000. running mean: -20.984804\n",
      "ep 1728: ep_len:615 episode reward: total was -27.630000. running mean: -21.051256\n",
      "ep 1728: ep_len:540 episode reward: total was -35.460000. running mean: -21.195343\n",
      "ep 1728: ep_len:500 episode reward: total was -12.010000. running mean: -21.103490\n",
      "ep 1728: ep_len:3 episode reward: total was 0.000000. running mean: -20.892455\n",
      "ep 1728: ep_len:520 episode reward: total was -23.960000. running mean: -20.923130\n",
      "ep 1728: ep_len:600 episode reward: total was -30.640000. running mean: -21.020299\n",
      "epsilon:0.163991 episode_count: 12103. steps_count: 5346400.000000\n",
      "ep 1729: ep_len:229 episode reward: total was 3.080000. running mean: -20.779296\n",
      "ep 1729: ep_len:550 episode reward: total was -8.720000. running mean: -20.658703\n",
      "ep 1729: ep_len:595 episode reward: total was -12.760000. running mean: -20.579716\n",
      "ep 1729: ep_len:589 episode reward: total was -39.040000. running mean: -20.764319\n",
      "ep 1729: ep_len:1 episode reward: total was 0.000000. running mean: -20.556676\n",
      "ep 1729: ep_len:640 episode reward: total was -32.630000. running mean: -20.677409\n",
      "ep 1729: ep_len:570 episode reward: total was -30.190000. running mean: -20.772535\n",
      "epsilon:0.163855 episode_count: 12110. steps_count: 5349574.000000\n",
      "ep 1730: ep_len:645 episode reward: total was -24.280000. running mean: -20.807609\n",
      "ep 1730: ep_len:580 episode reward: total was -5.220000. running mean: -20.651733\n",
      "ep 1730: ep_len:590 episode reward: total was -15.510000. running mean: -20.600316\n",
      "ep 1730: ep_len:164 episode reward: total was 0.590000. running mean: -20.388413\n",
      "ep 1730: ep_len:53 episode reward: total was 3.500000. running mean: -20.149529\n",
      "ep 1730: ep_len:565 episode reward: total was -42.150000. running mean: -20.369533\n",
      "ep 1730: ep_len:555 episode reward: total was -23.180000. running mean: -20.397638\n",
      "epsilon:0.163718 episode_count: 12117. steps_count: 5352726.000000\n",
      "ep 1731: ep_len:580 episode reward: total was -30.140000. running mean: -20.495062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1731: ep_len:500 episode reward: total was -30.240000. running mean: -20.592511\n",
      "ep 1731: ep_len:635 episode reward: total was -16.970000. running mean: -20.556286\n",
      "ep 1731: ep_len:500 episode reward: total was -21.000000. running mean: -20.560723\n",
      "ep 1731: ep_len:3 episode reward: total was 0.000000. running mean: -20.355116\n",
      "ep 1731: ep_len:520 episode reward: total was -8.180000. running mean: -20.233365\n",
      "ep 1731: ep_len:520 episode reward: total was -23.930000. running mean: -20.270331\n",
      "epsilon:0.163582 episode_count: 12124. steps_count: 5355984.000000\n",
      "ep 1732: ep_len:550 episode reward: total was -19.940000. running mean: -20.267028\n",
      "ep 1732: ep_len:505 episode reward: total was -2.120000. running mean: -20.085557\n",
      "ep 1732: ep_len:510 episode reward: total was -36.850000. running mean: -20.253202\n",
      "ep 1732: ep_len:515 episode reward: total was -27.140000. running mean: -20.322070\n",
      "ep 1732: ep_len:76 episode reward: total was 1.020000. running mean: -20.108649\n",
      "ep 1732: ep_len:515 episode reward: total was -21.300000. running mean: -20.120563\n",
      "ep 1732: ep_len:565 episode reward: total was -26.670000. running mean: -20.186057\n",
      "epsilon:0.163445 episode_count: 12131. steps_count: 5359220.000000\n",
      "ep 1733: ep_len:585 episode reward: total was -16.560000. running mean: -20.149796\n",
      "ep 1733: ep_len:600 episode reward: total was 0.510000. running mean: -19.943199\n",
      "ep 1733: ep_len:720 episode reward: total was -58.790000. running mean: -20.331667\n",
      "ep 1733: ep_len:535 episode reward: total was -10.650000. running mean: -20.234850\n",
      "ep 1733: ep_len:130 episode reward: total was -13.450000. running mean: -20.167001\n",
      "ep 1733: ep_len:515 episode reward: total was -10.640000. running mean: -20.071731\n",
      "ep 1733: ep_len:610 episode reward: total was -3.230000. running mean: -19.903314\n",
      "epsilon:0.163309 episode_count: 12138. steps_count: 5362915.000000\n",
      "ep 1734: ep_len:665 episode reward: total was -35.810000. running mean: -20.062381\n",
      "ep 1734: ep_len:515 episode reward: total was -37.390000. running mean: -20.235657\n",
      "ep 1734: ep_len:650 episode reward: total was -27.440000. running mean: -20.307701\n",
      "ep 1734: ep_len:545 episode reward: total was -21.200000. running mean: -20.316624\n",
      "ep 1734: ep_len:3 episode reward: total was 0.000000. running mean: -20.113457\n",
      "ep 1734: ep_len:505 episode reward: total was -33.800000. running mean: -20.250323\n",
      "ep 1734: ep_len:590 episode reward: total was -12.100000. running mean: -20.168819\n",
      "epsilon:0.163172 episode_count: 12145. steps_count: 5366388.000000\n",
      "ep 1735: ep_len:500 episode reward: total was -5.650000. running mean: -20.023631\n",
      "ep 1735: ep_len:382 episode reward: total was -52.850000. running mean: -20.351895\n",
      "ep 1735: ep_len:615 episode reward: total was -27.860000. running mean: -20.426976\n",
      "ep 1735: ep_len:560 episode reward: total was -38.540000. running mean: -20.608106\n",
      "ep 1735: ep_len:3 episode reward: total was 0.000000. running mean: -20.402025\n",
      "ep 1735: ep_len:540 episode reward: total was -46.580000. running mean: -20.663805\n",
      "ep 1735: ep_len:185 episode reward: total was -9.920000. running mean: -20.556367\n",
      "epsilon:0.163036 episode_count: 12152. steps_count: 5369173.000000\n",
      "ep 1736: ep_len:610 episode reward: total was -35.920000. running mean: -20.710003\n",
      "ep 1736: ep_len:201 episode reward: total was -6.430000. running mean: -20.567203\n",
      "ep 1736: ep_len:620 episode reward: total was -17.110000. running mean: -20.532631\n",
      "ep 1736: ep_len:540 episode reward: total was -21.050000. running mean: -20.537805\n",
      "ep 1736: ep_len:3 episode reward: total was 0.000000. running mean: -20.332427\n",
      "ep 1736: ep_len:595 episode reward: total was -12.580000. running mean: -20.254903\n",
      "ep 1736: ep_len:580 episode reward: total was -25.180000. running mean: -20.304154\n",
      "epsilon:0.162899 episode_count: 12159. steps_count: 5372322.000000\n",
      "ep 1737: ep_len:645 episode reward: total was -42.560000. running mean: -20.526712\n",
      "ep 1737: ep_len:510 episode reward: total was -33.350000. running mean: -20.654945\n",
      "ep 1737: ep_len:74 episode reward: total was 3.060000. running mean: -20.417795\n",
      "ep 1737: ep_len:500 episode reward: total was -18.090000. running mean: -20.394517\n",
      "ep 1737: ep_len:3 episode reward: total was 0.000000. running mean: -20.190572\n",
      "ep 1737: ep_len:500 episode reward: total was -7.250000. running mean: -20.061167\n",
      "ep 1737: ep_len:500 episode reward: total was -25.270000. running mean: -20.113255\n",
      "epsilon:0.162763 episode_count: 12166. steps_count: 5375054.000000\n",
      "ep 1738: ep_len:605 episode reward: total was -24.570000. running mean: -20.157822\n",
      "ep 1738: ep_len:615 episode reward: total was -48.610000. running mean: -20.442344\n",
      "ep 1738: ep_len:525 episode reward: total was -34.450000. running mean: -20.582421\n",
      "ep 1738: ep_len:56 episode reward: total was -2.450000. running mean: -20.401096\n",
      "ep 1738: ep_len:3 episode reward: total was 0.000000. running mean: -20.197085\n",
      "ep 1738: ep_len:510 episode reward: total was -32.830000. running mean: -20.323415\n",
      "ep 1738: ep_len:555 episode reward: total was -22.630000. running mean: -20.346480\n",
      "epsilon:0.162626 episode_count: 12173. steps_count: 5377923.000000\n",
      "ep 1739: ep_len:640 episode reward: total was -40.420000. running mean: -20.547216\n",
      "ep 1739: ep_len:585 episode reward: total was -86.750000. running mean: -21.209244\n",
      "ep 1739: ep_len:79 episode reward: total was -2.470000. running mean: -21.021851\n",
      "ep 1739: ep_len:500 episode reward: total was -6.510000. running mean: -20.876733\n",
      "ep 1739: ep_len:3 episode reward: total was 0.000000. running mean: -20.667965\n",
      "ep 1739: ep_len:565 episode reward: total was -0.890000. running mean: -20.470186\n",
      "ep 1739: ep_len:585 episode reward: total was -36.680000. running mean: -20.632284\n",
      "epsilon:0.162490 episode_count: 12180. steps_count: 5380880.000000\n",
      "ep 1740: ep_len:500 episode reward: total was -9.270000. running mean: -20.518661\n",
      "ep 1740: ep_len:505 episode reward: total was -51.680000. running mean: -20.830274\n",
      "ep 1740: ep_len:595 episode reward: total was -18.090000. running mean: -20.802872\n",
      "ep 1740: ep_len:510 episode reward: total was -29.110000. running mean: -20.885943\n",
      "ep 1740: ep_len:90 episode reward: total was -7.930000. running mean: -20.756383\n",
      "ep 1740: ep_len:660 episode reward: total was -31.830000. running mean: -20.867120\n",
      "ep 1740: ep_len:285 episode reward: total was -13.870000. running mean: -20.797148\n",
      "epsilon:0.162353 episode_count: 12187. steps_count: 5384025.000000\n",
      "ep 1741: ep_len:530 episode reward: total was -22.930000. running mean: -20.818477\n",
      "ep 1741: ep_len:505 episode reward: total was -21.750000. running mean: -20.827792\n",
      "ep 1741: ep_len:520 episode reward: total was -10.690000. running mean: -20.726414\n",
      "ep 1741: ep_len:590 episode reward: total was -4.970000. running mean: -20.568850\n",
      "ep 1741: ep_len:3 episode reward: total was 0.000000. running mean: -20.363162\n",
      "ep 1741: ep_len:535 episode reward: total was -36.010000. running mean: -20.519630\n",
      "ep 1741: ep_len:500 episode reward: total was -25.330000. running mean: -20.567734\n",
      "epsilon:0.162217 episode_count: 12194. steps_count: 5387208.000000\n",
      "ep 1742: ep_len:219 episode reward: total was -22.950000. running mean: -20.591556\n",
      "ep 1742: ep_len:500 episode reward: total was 2.660000. running mean: -20.359041\n",
      "ep 1742: ep_len:351 episode reward: total was -3.320000. running mean: -20.188650\n",
      "ep 1742: ep_len:515 episode reward: total was -14.030000. running mean: -20.127064\n",
      "ep 1742: ep_len:3 episode reward: total was 0.000000. running mean: -19.925793\n",
      "ep 1742: ep_len:155 episode reward: total was -22.950000. running mean: -19.956035\n",
      "ep 1742: ep_len:725 episode reward: total was -77.560000. running mean: -20.532075\n",
      "epsilon:0.162080 episode_count: 12201. steps_count: 5389676.000000\n",
      "ep 1743: ep_len:126 episode reward: total was -1.940000. running mean: -20.346154\n",
      "ep 1743: ep_len:515 episode reward: total was -7.270000. running mean: -20.215393\n",
      "ep 1743: ep_len:500 episode reward: total was -13.520000. running mean: -20.148439\n",
      "ep 1743: ep_len:515 episode reward: total was -18.540000. running mean: -20.132354\n",
      "ep 1743: ep_len:3 episode reward: total was 0.000000. running mean: -19.931031\n",
      "ep 1743: ep_len:575 episode reward: total was -16.290000. running mean: -19.894620\n",
      "ep 1743: ep_len:515 episode reward: total was -32.020000. running mean: -20.015874\n",
      "epsilon:0.161944 episode_count: 12208. steps_count: 5392425.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1744: ep_len:134 episode reward: total was -5.450000. running mean: -19.870216\n",
      "ep 1744: ep_len:630 episode reward: total was -16.630000. running mean: -19.837813\n",
      "ep 1744: ep_len:610 episode reward: total was -24.100000. running mean: -19.880435\n",
      "ep 1744: ep_len:132 episode reward: total was 4.110000. running mean: -19.640531\n",
      "ep 1744: ep_len:3 episode reward: total was 0.000000. running mean: -19.444126\n",
      "ep 1744: ep_len:715 episode reward: total was -34.440000. running mean: -19.594084\n",
      "ep 1744: ep_len:520 episode reward: total was -21.150000. running mean: -19.609643\n",
      "epsilon:0.161807 episode_count: 12215. steps_count: 5395169.000000\n",
      "ep 1745: ep_len:590 episode reward: total was -45.980000. running mean: -19.873347\n",
      "ep 1745: ep_len:560 episode reward: total was -9.720000. running mean: -19.771814\n",
      "ep 1745: ep_len:404 episode reward: total was -6.320000. running mean: -19.637295\n",
      "ep 1745: ep_len:520 episode reward: total was -8.510000. running mean: -19.526022\n",
      "ep 1745: ep_len:84 episode reward: total was -12.970000. running mean: -19.460462\n",
      "ep 1745: ep_len:625 episode reward: total was -31.310000. running mean: -19.578958\n",
      "ep 1745: ep_len:605 episode reward: total was -35.490000. running mean: -19.738068\n",
      "epsilon:0.161671 episode_count: 12222. steps_count: 5398557.000000\n",
      "ep 1746: ep_len:580 episode reward: total was -9.630000. running mean: -19.636987\n",
      "ep 1746: ep_len:271 episode reward: total was -37.890000. running mean: -19.819517\n",
      "ep 1746: ep_len:505 episode reward: total was -9.670000. running mean: -19.718022\n",
      "ep 1746: ep_len:575 episode reward: total was -11.660000. running mean: -19.637442\n",
      "ep 1746: ep_len:3 episode reward: total was 0.000000. running mean: -19.441068\n",
      "ep 1746: ep_len:515 episode reward: total was -17.760000. running mean: -19.424257\n",
      "ep 1746: ep_len:525 episode reward: total was -21.630000. running mean: -19.446314\n",
      "epsilon:0.161534 episode_count: 12229. steps_count: 5401531.000000\n",
      "ep 1747: ep_len:555 episode reward: total was -21.380000. running mean: -19.465651\n",
      "ep 1747: ep_len:500 episode reward: total was -30.030000. running mean: -19.571295\n",
      "ep 1747: ep_len:640 episode reward: total was -36.870000. running mean: -19.744282\n",
      "ep 1747: ep_len:510 episode reward: total was -23.050000. running mean: -19.777339\n",
      "ep 1747: ep_len:106 episode reward: total was -14.960000. running mean: -19.729166\n",
      "ep 1747: ep_len:307 episode reward: total was -29.820000. running mean: -19.830074\n",
      "ep 1747: ep_len:195 episode reward: total was -4.360000. running mean: -19.675373\n",
      "epsilon:0.161398 episode_count: 12236. steps_count: 5404344.000000\n",
      "ep 1748: ep_len:665 episode reward: total was -30.240000. running mean: -19.781019\n",
      "ep 1748: ep_len:620 episode reward: total was -49.680000. running mean: -20.080009\n",
      "ep 1748: ep_len:545 episode reward: total was -25.900000. running mean: -20.138209\n",
      "ep 1748: ep_len:545 episode reward: total was -4.590000. running mean: -19.982727\n",
      "ep 1748: ep_len:3 episode reward: total was 0.000000. running mean: -19.782900\n",
      "ep 1748: ep_len:575 episode reward: total was -30.450000. running mean: -19.889571\n",
      "ep 1748: ep_len:510 episode reward: total was -27.010000. running mean: -19.960775\n",
      "epsilon:0.161261 episode_count: 12243. steps_count: 5407807.000000\n",
      "ep 1749: ep_len:585 episode reward: total was -22.500000. running mean: -19.986167\n",
      "ep 1749: ep_len:500 episode reward: total was -9.090000. running mean: -19.877206\n",
      "ep 1749: ep_len:377 episode reward: total was -21.900000. running mean: -19.897434\n",
      "ep 1749: ep_len:605 episode reward: total was -3.470000. running mean: -19.733159\n",
      "ep 1749: ep_len:127 episode reward: total was -7.450000. running mean: -19.610328\n",
      "ep 1749: ep_len:670 episode reward: total was -16.760000. running mean: -19.581824\n",
      "ep 1749: ep_len:500 episode reward: total was -15.680000. running mean: -19.542806\n",
      "epsilon:0.161125 episode_count: 12250. steps_count: 5411171.000000\n",
      "ep 1750: ep_len:605 episode reward: total was -33.820000. running mean: -19.685578\n",
      "ep 1750: ep_len:500 episode reward: total was -10.230000. running mean: -19.591022\n",
      "ep 1750: ep_len:500 episode reward: total was -20.700000. running mean: -19.602112\n",
      "ep 1750: ep_len:630 episode reward: total was -67.600000. running mean: -20.082091\n",
      "ep 1750: ep_len:50 episode reward: total was 2.000000. running mean: -19.861270\n",
      "ep 1750: ep_len:500 episode reward: total was -33.230000. running mean: -19.994957\n",
      "ep 1750: ep_len:327 episode reward: total was -22.900000. running mean: -20.024008\n",
      "epsilon:0.160988 episode_count: 12257. steps_count: 5414283.000000\n",
      "ep 1751: ep_len:555 episode reward: total was -8.640000. running mean: -19.910168\n",
      "ep 1751: ep_len:500 episode reward: total was -29.350000. running mean: -20.004566\n",
      "ep 1751: ep_len:74 episode reward: total was -0.950000. running mean: -19.814020\n",
      "ep 1751: ep_len:595 episode reward: total was -8.540000. running mean: -19.701280\n",
      "ep 1751: ep_len:98 episode reward: total was -3.470000. running mean: -19.538967\n",
      "ep 1751: ep_len:500 episode reward: total was -14.580000. running mean: -19.489378\n",
      "ep 1751: ep_len:595 episode reward: total was -33.560000. running mean: -19.630084\n",
      "epsilon:0.160852 episode_count: 12264. steps_count: 5417200.000000\n",
      "ep 1752: ep_len:120 episode reward: total was -18.480000. running mean: -19.618583\n",
      "ep 1752: ep_len:500 episode reward: total was -50.170000. running mean: -19.924097\n",
      "ep 1752: ep_len:500 episode reward: total was -21.460000. running mean: -19.939456\n",
      "ep 1752: ep_len:585 episode reward: total was -10.600000. running mean: -19.846062\n",
      "ep 1752: ep_len:95 episode reward: total was -11.470000. running mean: -19.762301\n",
      "ep 1752: ep_len:655 episode reward: total was -29.340000. running mean: -19.858078\n",
      "ep 1752: ep_len:500 episode reward: total was -39.790000. running mean: -20.057397\n",
      "epsilon:0.160715 episode_count: 12271. steps_count: 5420155.000000\n",
      "ep 1753: ep_len:610 episode reward: total was -23.920000. running mean: -20.096023\n",
      "ep 1753: ep_len:565 episode reward: total was -2.190000. running mean: -19.916963\n",
      "ep 1753: ep_len:535 episode reward: total was -21.190000. running mean: -19.929693\n",
      "ep 1753: ep_len:500 episode reward: total was -24.570000. running mean: -19.976097\n",
      "ep 1753: ep_len:81 episode reward: total was 4.550000. running mean: -19.730836\n",
      "ep 1753: ep_len:565 episode reward: total was -47.170000. running mean: -20.005227\n",
      "ep 1753: ep_len:500 episode reward: total was -19.030000. running mean: -19.995475\n",
      "epsilon:0.160579 episode_count: 12278. steps_count: 5423511.000000\n",
      "ep 1754: ep_len:605 episode reward: total was -17.480000. running mean: -19.970320\n",
      "ep 1754: ep_len:575 episode reward: total was -17.090000. running mean: -19.941517\n",
      "ep 1754: ep_len:575 episode reward: total was -11.030000. running mean: -19.852402\n",
      "ep 1754: ep_len:505 episode reward: total was -25.050000. running mean: -19.904378\n",
      "ep 1754: ep_len:3 episode reward: total was 0.000000. running mean: -19.705334\n",
      "ep 1754: ep_len:695 episode reward: total was -17.780000. running mean: -19.686081\n",
      "ep 1754: ep_len:575 episode reward: total was -35.720000. running mean: -19.846420\n",
      "epsilon:0.160442 episode_count: 12285. steps_count: 5427044.000000\n",
      "ep 1755: ep_len:233 episode reward: total was -3.420000. running mean: -19.682156\n",
      "ep 1755: ep_len:292 episode reward: total was -21.390000. running mean: -19.699234\n",
      "ep 1755: ep_len:665 episode reward: total was -27.990000. running mean: -19.782142\n",
      "ep 1755: ep_len:550 episode reward: total was -13.140000. running mean: -19.715720\n",
      "ep 1755: ep_len:3 episode reward: total was 0.000000. running mean: -19.518563\n",
      "ep 1755: ep_len:585 episode reward: total was -42.750000. running mean: -19.750878\n",
      "ep 1755: ep_len:630 episode reward: total was -52.610000. running mean: -20.079469\n",
      "epsilon:0.160306 episode_count: 12292. steps_count: 5430002.000000\n",
      "ep 1756: ep_len:500 episode reward: total was 0.270000. running mean: -19.875974\n",
      "ep 1756: ep_len:675 episode reward: total was -50.400000. running mean: -20.181214\n",
      "ep 1756: ep_len:525 episode reward: total was -40.000000. running mean: -20.379402\n",
      "ep 1756: ep_len:565 episode reward: total was -56.320000. running mean: -20.738808\n",
      "ep 1756: ep_len:3 episode reward: total was 0.000000. running mean: -20.531420\n",
      "ep 1756: ep_len:247 episode reward: total was -0.890000. running mean: -20.335006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1756: ep_len:515 episode reward: total was -26.010000. running mean: -20.391756\n",
      "epsilon:0.160169 episode_count: 12299. steps_count: 5433032.000000\n",
      "ep 1757: ep_len:249 episode reward: total was -2.420000. running mean: -20.212038\n",
      "ep 1757: ep_len:600 episode reward: total was -25.990000. running mean: -20.269818\n",
      "ep 1757: ep_len:695 episode reward: total was -25.710000. running mean: -20.324220\n",
      "ep 1757: ep_len:515 episode reward: total was -16.200000. running mean: -20.282978\n",
      "ep 1757: ep_len:52 episode reward: total was 2.000000. running mean: -20.060148\n",
      "ep 1757: ep_len:525 episode reward: total was -16.680000. running mean: -20.026346\n",
      "ep 1757: ep_len:500 episode reward: total was -34.640000. running mean: -20.172483\n",
      "epsilon:0.160033 episode_count: 12306. steps_count: 5436168.000000\n",
      "ep 1758: ep_len:685 episode reward: total was -36.870000. running mean: -20.339458\n",
      "ep 1758: ep_len:550 episode reward: total was -35.130000. running mean: -20.487363\n",
      "ep 1758: ep_len:575 episode reward: total was -29.930000. running mean: -20.581790\n",
      "ep 1758: ep_len:560 episode reward: total was -5.080000. running mean: -20.426772\n",
      "ep 1758: ep_len:75 episode reward: total was -10.980000. running mean: -20.332304\n",
      "ep 1758: ep_len:500 episode reward: total was -28.540000. running mean: -20.414381\n",
      "ep 1758: ep_len:595 episode reward: total was -16.110000. running mean: -20.371337\n",
      "epsilon:0.159896 episode_count: 12313. steps_count: 5439708.000000\n",
      "ep 1759: ep_len:675 episode reward: total was -35.880000. running mean: -20.526424\n",
      "ep 1759: ep_len:500 episode reward: total was -7.820000. running mean: -20.399360\n",
      "ep 1759: ep_len:555 episode reward: total was -16.620000. running mean: -20.361566\n",
      "ep 1759: ep_len:505 episode reward: total was -27.660000. running mean: -20.434550\n",
      "ep 1759: ep_len:81 episode reward: total was -0.500000. running mean: -20.235205\n",
      "ep 1759: ep_len:525 episode reward: total was -14.490000. running mean: -20.177753\n",
      "ep 1759: ep_len:181 episode reward: total was -8.920000. running mean: -20.065175\n",
      "epsilon:0.159760 episode_count: 12320. steps_count: 5442730.000000\n",
      "ep 1760: ep_len:660 episode reward: total was -62.690000. running mean: -20.491424\n",
      "ep 1760: ep_len:500 episode reward: total was -19.550000. running mean: -20.482009\n",
      "ep 1760: ep_len:505 episode reward: total was -21.530000. running mean: -20.492489\n",
      "ep 1760: ep_len:630 episode reward: total was -30.100000. running mean: -20.588564\n",
      "ep 1760: ep_len:100 episode reward: total was 2.050000. running mean: -20.362179\n",
      "ep 1760: ep_len:500 episode reward: total was -14.790000. running mean: -20.306457\n",
      "ep 1760: ep_len:515 episode reward: total was -29.990000. running mean: -20.403292\n",
      "epsilon:0.159623 episode_count: 12327. steps_count: 5446140.000000\n",
      "ep 1761: ep_len:214 episode reward: total was -8.950000. running mean: -20.288759\n",
      "ep 1761: ep_len:565 episode reward: total was -17.690000. running mean: -20.262772\n",
      "ep 1761: ep_len:358 episode reward: total was -8.850000. running mean: -20.148644\n",
      "ep 1761: ep_len:500 episode reward: total was -24.700000. running mean: -20.194158\n",
      "ep 1761: ep_len:3 episode reward: total was 0.000000. running mean: -19.992216\n",
      "ep 1761: ep_len:640 episode reward: total was -27.290000. running mean: -20.065194\n",
      "ep 1761: ep_len:580 episode reward: total was -22.880000. running mean: -20.093342\n",
      "epsilon:0.159487 episode_count: 12334. steps_count: 5449000.000000\n",
      "ep 1762: ep_len:805 episode reward: total was -99.750000. running mean: -20.889909\n",
      "ep 1762: ep_len:500 episode reward: total was -23.070000. running mean: -20.911710\n",
      "ep 1762: ep_len:640 episode reward: total was -39.020000. running mean: -21.092792\n",
      "ep 1762: ep_len:386 episode reward: total was -35.200000. running mean: -21.233864\n",
      "ep 1762: ep_len:3 episode reward: total was 0.000000. running mean: -21.021526\n",
      "ep 1762: ep_len:530 episode reward: total was -13.080000. running mean: -20.942111\n",
      "ep 1762: ep_len:565 episode reward: total was -18.160000. running mean: -20.914289\n",
      "epsilon:0.159350 episode_count: 12341. steps_count: 5452429.000000\n",
      "ep 1763: ep_len:510 episode reward: total was -30.090000. running mean: -21.006047\n",
      "ep 1763: ep_len:560 episode reward: total was -13.900000. running mean: -20.934986\n",
      "ep 1763: ep_len:675 episode reward: total was -54.980000. running mean: -21.275436\n",
      "ep 1763: ep_len:560 episode reward: total was -7.590000. running mean: -21.138582\n",
      "ep 1763: ep_len:49 episode reward: total was 1.500000. running mean: -20.912196\n",
      "ep 1763: ep_len:620 episode reward: total was -44.570000. running mean: -21.148774\n",
      "ep 1763: ep_len:535 episode reward: total was -40.060000. running mean: -21.337886\n",
      "epsilon:0.159214 episode_count: 12348. steps_count: 5455938.000000\n",
      "ep 1764: ep_len:545 episode reward: total was -4.950000. running mean: -21.174008\n",
      "ep 1764: ep_len:500 episode reward: total was -30.840000. running mean: -21.270667\n",
      "ep 1764: ep_len:705 episode reward: total was -82.570000. running mean: -21.883661\n",
      "ep 1764: ep_len:540 episode reward: total was -13.020000. running mean: -21.795024\n",
      "ep 1764: ep_len:3 episode reward: total was 0.000000. running mean: -21.577074\n",
      "ep 1764: ep_len:565 episode reward: total was -10.770000. running mean: -21.469003\n",
      "ep 1764: ep_len:188 episode reward: total was -11.450000. running mean: -21.368813\n",
      "epsilon:0.159077 episode_count: 12355. steps_count: 5458984.000000\n",
      "ep 1765: ep_len:540 episode reward: total was -35.130000. running mean: -21.506425\n",
      "ep 1765: ep_len:505 episode reward: total was -21.910000. running mean: -21.510461\n",
      "ep 1765: ep_len:570 episode reward: total was -14.130000. running mean: -21.436656\n",
      "ep 1765: ep_len:500 episode reward: total was -21.110000. running mean: -21.433390\n",
      "ep 1765: ep_len:3 episode reward: total was 0.000000. running mean: -21.219056\n",
      "ep 1765: ep_len:585 episode reward: total was -27.150000. running mean: -21.278365\n",
      "ep 1765: ep_len:500 episode reward: total was -5.580000. running mean: -21.121381\n",
      "epsilon:0.158941 episode_count: 12362. steps_count: 5462187.000000\n",
      "ep 1766: ep_len:530 episode reward: total was -21.770000. running mean: -21.127868\n",
      "ep 1766: ep_len:510 episode reward: total was -18.930000. running mean: -21.105889\n",
      "ep 1766: ep_len:565 episode reward: total was -32.840000. running mean: -21.223230\n",
      "ep 1766: ep_len:500 episode reward: total was -19.560000. running mean: -21.206598\n",
      "ep 1766: ep_len:3 episode reward: total was 0.000000. running mean: -20.994532\n",
      "ep 1766: ep_len:535 episode reward: total was -21.990000. running mean: -21.004487\n",
      "ep 1766: ep_len:505 episode reward: total was -38.010000. running mean: -21.174542\n",
      "epsilon:0.158804 episode_count: 12369. steps_count: 5465335.000000\n",
      "ep 1767: ep_len:248 episode reward: total was -2.350000. running mean: -20.986296\n",
      "ep 1767: ep_len:304 episode reward: total was -16.350000. running mean: -20.939933\n",
      "ep 1767: ep_len:515 episode reward: total was -26.910000. running mean: -20.999634\n",
      "ep 1767: ep_len:500 episode reward: total was -1.690000. running mean: -20.806538\n",
      "ep 1767: ep_len:89 episode reward: total was 2.050000. running mean: -20.577972\n",
      "ep 1767: ep_len:500 episode reward: total was -55.830000. running mean: -20.930492\n",
      "ep 1767: ep_len:610 episode reward: total was -22.610000. running mean: -20.947288\n",
      "epsilon:0.158668 episode_count: 12376. steps_count: 5468101.000000\n",
      "ep 1768: ep_len:530 episode reward: total was -3.660000. running mean: -20.774415\n",
      "ep 1768: ep_len:505 episode reward: total was -24.260000. running mean: -20.809271\n",
      "ep 1768: ep_len:500 episode reward: total was -18.000000. running mean: -20.781178\n",
      "ep 1768: ep_len:378 episode reward: total was -14.260000. running mean: -20.715966\n",
      "ep 1768: ep_len:95 episode reward: total was -4.450000. running mean: -20.553306\n",
      "ep 1768: ep_len:535 episode reward: total was 3.110000. running mean: -20.316673\n",
      "ep 1768: ep_len:530 episode reward: total was -36.020000. running mean: -20.473707\n",
      "epsilon:0.158531 episode_count: 12383. steps_count: 5471174.000000\n",
      "ep 1769: ep_len:590 episode reward: total was -24.610000. running mean: -20.515070\n",
      "ep 1769: ep_len:500 episode reward: total was 1.180000. running mean: -20.298119\n",
      "ep 1769: ep_len:830 episode reward: total was -68.840000. running mean: -20.783538\n",
      "ep 1769: ep_len:500 episode reward: total was -27.140000. running mean: -20.847102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1769: ep_len:38 episode reward: total was -5.500000. running mean: -20.693631\n",
      "ep 1769: ep_len:560 episode reward: total was -13.260000. running mean: -20.619295\n",
      "ep 1769: ep_len:309 episode reward: total was -8.320000. running mean: -20.496302\n",
      "epsilon:0.158395 episode_count: 12390. steps_count: 5474501.000000\n",
      "ep 1770: ep_len:213 episode reward: total was -1.910000. running mean: -20.310439\n",
      "ep 1770: ep_len:545 episode reward: total was -7.210000. running mean: -20.179435\n",
      "ep 1770: ep_len:550 episode reward: total was -29.840000. running mean: -20.276040\n",
      "ep 1770: ep_len:500 episode reward: total was -23.580000. running mean: -20.309080\n",
      "ep 1770: ep_len:84 episode reward: total was -7.950000. running mean: -20.185489\n",
      "ep 1770: ep_len:525 episode reward: total was -24.540000. running mean: -20.229034\n",
      "ep 1770: ep_len:505 episode reward: total was -49.210000. running mean: -20.518844\n",
      "epsilon:0.158258 episode_count: 12397. steps_count: 5477423.000000\n",
      "ep 1771: ep_len:204 episode reward: total was -1.410000. running mean: -20.327755\n",
      "ep 1771: ep_len:605 episode reward: total was -24.500000. running mean: -20.369478\n",
      "ep 1771: ep_len:540 episode reward: total was -40.000000. running mean: -20.565783\n",
      "ep 1771: ep_len:505 episode reward: total was -23.580000. running mean: -20.595925\n",
      "ep 1771: ep_len:76 episode reward: total was 5.000000. running mean: -20.339966\n",
      "ep 1771: ep_len:650 episode reward: total was -17.260000. running mean: -20.309166\n",
      "ep 1771: ep_len:600 episode reward: total was -15.070000. running mean: -20.256775\n",
      "epsilon:0.158122 episode_count: 12404. steps_count: 5480603.000000\n",
      "ep 1772: ep_len:565 episode reward: total was -20.140000. running mean: -20.255607\n",
      "ep 1772: ep_len:610 episode reward: total was -9.040000. running mean: -20.143451\n",
      "ep 1772: ep_len:469 episode reward: total was -24.290000. running mean: -20.184916\n",
      "ep 1772: ep_len:545 episode reward: total was -16.970000. running mean: -20.152767\n",
      "ep 1772: ep_len:118 episode reward: total was -7.950000. running mean: -20.030739\n",
      "ep 1772: ep_len:645 episode reward: total was -34.060000. running mean: -20.171032\n",
      "ep 1772: ep_len:505 episode reward: total was -29.840000. running mean: -20.267722\n",
      "epsilon:0.157985 episode_count: 12411. steps_count: 5484060.000000\n",
      "ep 1773: ep_len:515 episode reward: total was -44.590000. running mean: -20.510945\n",
      "ep 1773: ep_len:500 episode reward: total was -9.490000. running mean: -20.400735\n",
      "ep 1773: ep_len:535 episode reward: total was -15.900000. running mean: -20.355728\n",
      "ep 1773: ep_len:505 episode reward: total was -28.060000. running mean: -20.432770\n",
      "ep 1773: ep_len:3 episode reward: total was 0.000000. running mean: -20.228443\n",
      "ep 1773: ep_len:585 episode reward: total was -20.310000. running mean: -20.229258\n",
      "ep 1773: ep_len:179 episode reward: total was -19.470000. running mean: -20.221666\n",
      "epsilon:0.157849 episode_count: 12418. steps_count: 5486882.000000\n",
      "ep 1774: ep_len:595 episode reward: total was -28.390000. running mean: -20.303349\n",
      "ep 1774: ep_len:184 episode reward: total was -19.980000. running mean: -20.300116\n",
      "ep 1774: ep_len:555 episode reward: total was -8.620000. running mean: -20.183314\n",
      "ep 1774: ep_len:500 episode reward: total was -0.910000. running mean: -19.990581\n",
      "ep 1774: ep_len:92 episode reward: total was -14.970000. running mean: -19.940375\n",
      "ep 1774: ep_len:545 episode reward: total was -11.390000. running mean: -19.854872\n",
      "ep 1774: ep_len:590 episode reward: total was -19.950000. running mean: -19.855823\n",
      "epsilon:0.157712 episode_count: 12425. steps_count: 5489943.000000\n",
      "ep 1775: ep_len:128 episode reward: total was -11.920000. running mean: -19.776465\n",
      "ep 1775: ep_len:196 episode reward: total was -12.440000. running mean: -19.703100\n",
      "ep 1775: ep_len:640 episode reward: total was -41.540000. running mean: -19.921469\n",
      "ep 1775: ep_len:535 episode reward: total was 7.440000. running mean: -19.647854\n",
      "ep 1775: ep_len:117 episode reward: total was -10.460000. running mean: -19.555976\n",
      "ep 1775: ep_len:565 episode reward: total was -17.780000. running mean: -19.538216\n",
      "ep 1775: ep_len:211 episode reward: total was -11.400000. running mean: -19.456834\n",
      "epsilon:0.157576 episode_count: 12432. steps_count: 5492335.000000\n",
      "ep 1776: ep_len:500 episode reward: total was -19.630000. running mean: -19.458566\n",
      "ep 1776: ep_len:500 episode reward: total was -8.360000. running mean: -19.347580\n",
      "ep 1776: ep_len:500 episode reward: total was -11.260000. running mean: -19.266704\n",
      "ep 1776: ep_len:520 episode reward: total was -29.170000. running mean: -19.365737\n",
      "ep 1776: ep_len:3 episode reward: total was 0.000000. running mean: -19.172080\n",
      "ep 1776: ep_len:550 episode reward: total was -25.310000. running mean: -19.233459\n",
      "ep 1776: ep_len:505 episode reward: total was -19.650000. running mean: -19.237624\n",
      "epsilon:0.157439 episode_count: 12439. steps_count: 5495413.000000\n",
      "ep 1777: ep_len:715 episode reward: total was -47.400000. running mean: -19.519248\n",
      "ep 1777: ep_len:344 episode reward: total was -23.340000. running mean: -19.557456\n",
      "ep 1777: ep_len:575 episode reward: total was -18.170000. running mean: -19.543581\n",
      "ep 1777: ep_len:595 episode reward: total was -7.500000. running mean: -19.423145\n",
      "ep 1777: ep_len:93 episode reward: total was 2.520000. running mean: -19.203714\n",
      "ep 1777: ep_len:500 episode reward: total was -5.050000. running mean: -19.062177\n",
      "ep 1777: ep_len:580 episode reward: total was -34.160000. running mean: -19.213155\n",
      "epsilon:0.157303 episode_count: 12446. steps_count: 5498815.000000\n",
      "ep 1778: ep_len:625 episode reward: total was -19.530000. running mean: -19.216323\n",
      "ep 1778: ep_len:500 episode reward: total was -24.860000. running mean: -19.272760\n",
      "ep 1778: ep_len:560 episode reward: total was -8.110000. running mean: -19.161133\n",
      "ep 1778: ep_len:535 episode reward: total was -17.030000. running mean: -19.139821\n",
      "ep 1778: ep_len:3 episode reward: total was 0.000000. running mean: -18.948423\n",
      "ep 1778: ep_len:500 episode reward: total was -17.720000. running mean: -18.936139\n",
      "ep 1778: ep_len:590 episode reward: total was -19.950000. running mean: -18.946277\n",
      "epsilon:0.157166 episode_count: 12453. steps_count: 5502128.000000\n",
      "ep 1779: ep_len:635 episode reward: total was -36.850000. running mean: -19.125315\n",
      "ep 1779: ep_len:500 episode reward: total was -2.370000. running mean: -18.957761\n",
      "ep 1779: ep_len:500 episode reward: total was -23.100000. running mean: -18.999184\n",
      "ep 1779: ep_len:525 episode reward: total was -2.540000. running mean: -18.834592\n",
      "ep 1779: ep_len:130 episode reward: total was 2.060000. running mean: -18.625646\n",
      "ep 1779: ep_len:645 episode reward: total was -12.340000. running mean: -18.562790\n",
      "ep 1779: ep_len:530 episode reward: total was -24.970000. running mean: -18.626862\n",
      "epsilon:0.157030 episode_count: 12460. steps_count: 5505593.000000\n",
      "ep 1780: ep_len:500 episode reward: total was -26.730000. running mean: -18.707893\n",
      "ep 1780: ep_len:505 episode reward: total was -34.720000. running mean: -18.868014\n",
      "ep 1780: ep_len:560 episode reward: total was -20.790000. running mean: -18.887234\n",
      "ep 1780: ep_len:163 episode reward: total was -4.410000. running mean: -18.742462\n",
      "ep 1780: ep_len:3 episode reward: total was 0.000000. running mean: -18.555037\n",
      "ep 1780: ep_len:505 episode reward: total was -30.820000. running mean: -18.677687\n",
      "ep 1780: ep_len:500 episode reward: total was -16.930000. running mean: -18.660210\n",
      "epsilon:0.156893 episode_count: 12467. steps_count: 5508329.000000\n",
      "ep 1781: ep_len:204 episode reward: total was -1.890000. running mean: -18.492508\n",
      "ep 1781: ep_len:350 episode reward: total was -31.970000. running mean: -18.627283\n",
      "ep 1781: ep_len:48 episode reward: total was 1.540000. running mean: -18.425610\n",
      "ep 1781: ep_len:515 episode reward: total was -19.580000. running mean: -18.437154\n",
      "ep 1781: ep_len:3 episode reward: total was 0.000000. running mean: -18.252782\n",
      "ep 1781: ep_len:505 episode reward: total was -13.680000. running mean: -18.207054\n",
      "ep 1781: ep_len:500 episode reward: total was -21.180000. running mean: -18.236784\n",
      "epsilon:0.156757 episode_count: 12474. steps_count: 5510454.000000\n",
      "ep 1782: ep_len:610 episode reward: total was -28.050000. running mean: -18.334916\n",
      "ep 1782: ep_len:358 episode reward: total was -25.370000. running mean: -18.405267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1782: ep_len:650 episode reward: total was -15.760000. running mean: -18.378814\n",
      "ep 1782: ep_len:500 episode reward: total was -20.610000. running mean: -18.401126\n",
      "ep 1782: ep_len:102 episode reward: total was 1.040000. running mean: -18.206715\n",
      "ep 1782: ep_len:535 episode reward: total was -26.000000. running mean: -18.284648\n",
      "ep 1782: ep_len:515 episode reward: total was -43.160000. running mean: -18.533401\n",
      "epsilon:0.156620 episode_count: 12481. steps_count: 5513724.000000\n",
      "ep 1783: ep_len:207 episode reward: total was -0.380000. running mean: -18.351867\n",
      "ep 1783: ep_len:505 episode reward: total was 0.180000. running mean: -18.166548\n",
      "ep 1783: ep_len:580 episode reward: total was -12.580000. running mean: -18.110683\n",
      "ep 1783: ep_len:162 episode reward: total was -1.370000. running mean: -17.943276\n",
      "ep 1783: ep_len:3 episode reward: total was 0.000000. running mean: -17.763843\n",
      "ep 1783: ep_len:505 episode reward: total was -22.250000. running mean: -17.808705\n",
      "ep 1783: ep_len:640 episode reward: total was -25.910000. running mean: -17.889718\n",
      "epsilon:0.156484 episode_count: 12488. steps_count: 5516326.000000\n",
      "ep 1784: ep_len:535 episode reward: total was -37.510000. running mean: -18.085921\n",
      "ep 1784: ep_len:575 episode reward: total was -10.740000. running mean: -18.012462\n",
      "ep 1784: ep_len:650 episode reward: total was -32.840000. running mean: -18.160737\n",
      "ep 1784: ep_len:610 episode reward: total was -33.900000. running mean: -18.318130\n",
      "ep 1784: ep_len:3 episode reward: total was 0.000000. running mean: -18.134948\n",
      "ep 1784: ep_len:525 episode reward: total was -0.520000. running mean: -17.958799\n",
      "ep 1784: ep_len:605 episode reward: total was -20.160000. running mean: -17.980811\n",
      "epsilon:0.156347 episode_count: 12495. steps_count: 5519829.000000\n",
      "ep 1785: ep_len:500 episode reward: total was -41.140000. running mean: -18.212403\n",
      "ep 1785: ep_len:535 episode reward: total was -0.130000. running mean: -18.031579\n",
      "ep 1785: ep_len:565 episode reward: total was -19.860000. running mean: -18.049863\n",
      "ep 1785: ep_len:680 episode reward: total was -36.100000. running mean: -18.230364\n",
      "ep 1785: ep_len:3 episode reward: total was 0.000000. running mean: -18.048061\n",
      "ep 1785: ep_len:515 episode reward: total was -23.970000. running mean: -18.107280\n",
      "ep 1785: ep_len:510 episode reward: total was -12.400000. running mean: -18.050207\n",
      "epsilon:0.156211 episode_count: 12502. steps_count: 5523137.000000\n",
      "ep 1786: ep_len:635 episode reward: total was -32.350000. running mean: -18.193205\n",
      "ep 1786: ep_len:625 episode reward: total was -42.230000. running mean: -18.433573\n",
      "ep 1786: ep_len:660 episode reward: total was -37.900000. running mean: -18.628237\n",
      "ep 1786: ep_len:505 episode reward: total was -27.600000. running mean: -18.717955\n",
      "ep 1786: ep_len:3 episode reward: total was 0.000000. running mean: -18.530775\n",
      "ep 1786: ep_len:500 episode reward: total was -21.030000. running mean: -18.555768\n",
      "ep 1786: ep_len:500 episode reward: total was -21.780000. running mean: -18.588010\n",
      "epsilon:0.156074 episode_count: 12509. steps_count: 5526565.000000\n",
      "ep 1787: ep_len:216 episode reward: total was 1.620000. running mean: -18.385930\n",
      "ep 1787: ep_len:580 episode reward: total was -13.940000. running mean: -18.341471\n",
      "ep 1787: ep_len:500 episode reward: total was -27.090000. running mean: -18.428956\n",
      "ep 1787: ep_len:545 episode reward: total was -27.140000. running mean: -18.516066\n",
      "ep 1787: ep_len:3 episode reward: total was 0.000000. running mean: -18.330906\n",
      "ep 1787: ep_len:615 episode reward: total was -15.140000. running mean: -18.298997\n",
      "ep 1787: ep_len:515 episode reward: total was -16.930000. running mean: -18.285307\n",
      "epsilon:0.155938 episode_count: 12516. steps_count: 5529539.000000\n",
      "ep 1788: ep_len:605 episode reward: total was -6.940000. running mean: -18.171854\n",
      "ep 1788: ep_len:630 episode reward: total was -44.070000. running mean: -18.430835\n",
      "ep 1788: ep_len:500 episode reward: total was -25.610000. running mean: -18.502627\n",
      "ep 1788: ep_len:585 episode reward: total was -9.160000. running mean: -18.409200\n",
      "ep 1788: ep_len:104 episode reward: total was 3.030000. running mean: -18.194808\n",
      "ep 1788: ep_len:302 episode reward: total was -6.880000. running mean: -18.081660\n",
      "ep 1788: ep_len:550 episode reward: total was -20.380000. running mean: -18.104644\n",
      "epsilon:0.155801 episode_count: 12523. steps_count: 5532815.000000\n",
      "ep 1789: ep_len:630 episode reward: total was -37.440000. running mean: -18.297997\n",
      "ep 1789: ep_len:382 episode reward: total was -36.330000. running mean: -18.478317\n",
      "ep 1789: ep_len:500 episode reward: total was -6.150000. running mean: -18.355034\n",
      "ep 1789: ep_len:520 episode reward: total was -6.040000. running mean: -18.231884\n",
      "ep 1789: ep_len:3 episode reward: total was 0.000000. running mean: -18.049565\n",
      "ep 1789: ep_len:590 episode reward: total was -15.610000. running mean: -18.025169\n",
      "ep 1789: ep_len:500 episode reward: total was -25.170000. running mean: -18.096618\n",
      "epsilon:0.155665 episode_count: 12530. steps_count: 5535940.000000\n",
      "ep 1790: ep_len:570 episode reward: total was -7.660000. running mean: -17.992251\n",
      "ep 1790: ep_len:535 episode reward: total was -20.520000. running mean: -18.017529\n",
      "ep 1790: ep_len:660 episode reward: total was -24.370000. running mean: -18.081054\n",
      "ep 1790: ep_len:535 episode reward: total was -17.120000. running mean: -18.071443\n",
      "ep 1790: ep_len:3 episode reward: total was 0.000000. running mean: -17.890729\n",
      "ep 1790: ep_len:535 episode reward: total was -56.380000. running mean: -18.275621\n",
      "ep 1790: ep_len:560 episode reward: total was -26.140000. running mean: -18.354265\n",
      "epsilon:0.155528 episode_count: 12537. steps_count: 5539338.000000\n",
      "ep 1791: ep_len:256 episode reward: total was 2.130000. running mean: -18.149423\n",
      "ep 1791: ep_len:630 episode reward: total was -36.100000. running mean: -18.328928\n",
      "ep 1791: ep_len:560 episode reward: total was -39.480000. running mean: -18.540439\n",
      "ep 1791: ep_len:132 episode reward: total was -1.900000. running mean: -18.374035\n",
      "ep 1791: ep_len:3 episode reward: total was 0.000000. running mean: -18.190294\n",
      "ep 1791: ep_len:650 episode reward: total was -32.060000. running mean: -18.328991\n",
      "ep 1791: ep_len:199 episode reward: total was -3.820000. running mean: -18.183901\n",
      "epsilon:0.155392 episode_count: 12544. steps_count: 5541768.000000\n",
      "ep 1792: ep_len:555 episode reward: total was -22.530000. running mean: -18.227362\n",
      "ep 1792: ep_len:500 episode reward: total was -22.920000. running mean: -18.274289\n",
      "ep 1792: ep_len:715 episode reward: total was -51.790000. running mean: -18.609446\n",
      "ep 1792: ep_len:530 episode reward: total was -14.140000. running mean: -18.564751\n",
      "ep 1792: ep_len:3 episode reward: total was 0.000000. running mean: -18.379104\n",
      "ep 1792: ep_len:175 episode reward: total was -4.890000. running mean: -18.244213\n",
      "ep 1792: ep_len:322 episode reward: total was -5.310000. running mean: -18.114871\n",
      "epsilon:0.155255 episode_count: 12551. steps_count: 5544568.000000\n",
      "ep 1793: ep_len:550 episode reward: total was -26.830000. running mean: -18.202022\n",
      "ep 1793: ep_len:520 episode reward: total was -3.330000. running mean: -18.053302\n",
      "ep 1793: ep_len:615 episode reward: total was -21.070000. running mean: -18.083469\n",
      "ep 1793: ep_len:590 episode reward: total was -44.570000. running mean: -18.348334\n",
      "ep 1793: ep_len:3 episode reward: total was 0.000000. running mean: -18.164851\n",
      "ep 1793: ep_len:675 episode reward: total was -15.220000. running mean: -18.135402\n",
      "ep 1793: ep_len:190 episode reward: total was -6.390000. running mean: -18.017948\n",
      "epsilon:0.155119 episode_count: 12558. steps_count: 5547711.000000\n",
      "ep 1794: ep_len:215 episode reward: total was -9.890000. running mean: -17.936669\n",
      "ep 1794: ep_len:555 episode reward: total was -14.490000. running mean: -17.902202\n",
      "ep 1794: ep_len:580 episode reward: total was -21.790000. running mean: -17.941080\n",
      "ep 1794: ep_len:500 episode reward: total was -37.090000. running mean: -18.132569\n",
      "ep 1794: ep_len:118 episode reward: total was -2.450000. running mean: -17.975744\n",
      "ep 1794: ep_len:209 episode reward: total was -1.860000. running mean: -17.814586\n",
      "ep 1794: ep_len:605 episode reward: total was -35.230000. running mean: -17.988740\n",
      "epsilon:0.154982 episode_count: 12565. steps_count: 5550493.000000\n",
      "ep 1795: ep_len:610 episode reward: total was -12.940000. running mean: -17.938253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1795: ep_len:500 episode reward: total was -1.410000. running mean: -17.772970\n",
      "ep 1795: ep_len:595 episode reward: total was -26.160000. running mean: -17.856841\n",
      "ep 1795: ep_len:510 episode reward: total was -17.480000. running mean: -17.853072\n",
      "ep 1795: ep_len:3 episode reward: total was 0.000000. running mean: -17.674541\n",
      "ep 1795: ep_len:505 episode reward: total was -9.130000. running mean: -17.589096\n",
      "ep 1795: ep_len:585 episode reward: total was -29.100000. running mean: -17.704205\n",
      "epsilon:0.154846 episode_count: 12572. steps_count: 5553801.000000\n",
      "ep 1796: ep_len:555 episode reward: total was -9.680000. running mean: -17.623963\n",
      "ep 1796: ep_len:530 episode reward: total was -55.080000. running mean: -17.998523\n",
      "ep 1796: ep_len:645 episode reward: total was -20.300000. running mean: -18.021538\n",
      "ep 1796: ep_len:520 episode reward: total was -5.080000. running mean: -17.892123\n",
      "ep 1796: ep_len:60 episode reward: total was -5.960000. running mean: -17.772802\n",
      "ep 1796: ep_len:560 episode reward: total was -6.710000. running mean: -17.662174\n",
      "ep 1796: ep_len:530 episode reward: total was -20.120000. running mean: -17.686752\n",
      "epsilon:0.154709 episode_count: 12579. steps_count: 5557201.000000\n",
      "ep 1797: ep_len:520 episode reward: total was -25.560000. running mean: -17.765484\n",
      "ep 1797: ep_len:505 episode reward: total was -18.750000. running mean: -17.775329\n",
      "ep 1797: ep_len:349 episode reward: total was -5.290000. running mean: -17.650476\n",
      "ep 1797: ep_len:610 episode reward: total was -15.970000. running mean: -17.633671\n",
      "ep 1797: ep_len:108 episode reward: total was -0.440000. running mean: -17.461735\n",
      "ep 1797: ep_len:825 episode reward: total was -59.820000. running mean: -17.885317\n",
      "ep 1797: ep_len:525 episode reward: total was -54.560000. running mean: -18.252064\n",
      "epsilon:0.154573 episode_count: 12586. steps_count: 5560643.000000\n",
      "ep 1798: ep_len:500 episode reward: total was -15.690000. running mean: -18.226444\n",
      "ep 1798: ep_len:261 episode reward: total was -34.880000. running mean: -18.392979\n",
      "ep 1798: ep_len:381 episode reward: total was -0.810000. running mean: -18.217149\n",
      "ep 1798: ep_len:520 episode reward: total was -27.530000. running mean: -18.310278\n",
      "ep 1798: ep_len:41 episode reward: total was -2.000000. running mean: -18.147175\n",
      "ep 1798: ep_len:530 episode reward: total was -3.450000. running mean: -18.000203\n",
      "ep 1798: ep_len:520 episode reward: total was -33.180000. running mean: -18.152001\n",
      "epsilon:0.154436 episode_count: 12593. steps_count: 5563396.000000\n",
      "ep 1799: ep_len:510 episode reward: total was -27.610000. running mean: -18.246581\n",
      "ep 1799: ep_len:510 episode reward: total was -12.310000. running mean: -18.187215\n",
      "ep 1799: ep_len:505 episode reward: total was -10.980000. running mean: -18.115143\n",
      "ep 1799: ep_len:500 episode reward: total was -15.160000. running mean: -18.085592\n",
      "ep 1799: ep_len:131 episode reward: total was 5.570000. running mean: -17.849036\n",
      "ep 1799: ep_len:510 episode reward: total was -35.320000. running mean: -18.023746\n",
      "ep 1799: ep_len:630 episode reward: total was -30.610000. running mean: -18.149608\n",
      "epsilon:0.154300 episode_count: 12600. steps_count: 5566692.000000\n",
      "ep 1800: ep_len:252 episode reward: total was -14.860000. running mean: -18.116712\n",
      "ep 1800: ep_len:665 episode reward: total was -67.610000. running mean: -18.611645\n",
      "ep 1800: ep_len:645 episode reward: total was -18.250000. running mean: -18.608028\n",
      "ep 1800: ep_len:550 episode reward: total was -8.160000. running mean: -18.503548\n",
      "ep 1800: ep_len:110 episode reward: total was 1.550000. running mean: -18.303013\n",
      "ep 1800: ep_len:585 episode reward: total was -17.030000. running mean: -18.290283\n",
      "ep 1800: ep_len:610 episode reward: total was -35.950000. running mean: -18.466880\n",
      "epsilon:0.154163 episode_count: 12607. steps_count: 5570109.000000\n",
      "ep 1801: ep_len:570 episode reward: total was -13.550000. running mean: -18.417711\n",
      "ep 1801: ep_len:500 episode reward: total was -5.480000. running mean: -18.288334\n",
      "ep 1801: ep_len:357 episode reward: total was -15.830000. running mean: -18.263751\n",
      "ep 1801: ep_len:540 episode reward: total was -17.390000. running mean: -18.255013\n",
      "ep 1801: ep_len:3 episode reward: total was 0.000000. running mean: -18.072463\n",
      "ep 1801: ep_len:500 episode reward: total was -32.280000. running mean: -18.214538\n",
      "ep 1801: ep_len:590 episode reward: total was -16.070000. running mean: -18.193093\n",
      "epsilon:0.154027 episode_count: 12614. steps_count: 5573169.000000\n",
      "ep 1802: ep_len:510 episode reward: total was -18.490000. running mean: -18.196062\n",
      "ep 1802: ep_len:550 episode reward: total was -10.510000. running mean: -18.119201\n",
      "ep 1802: ep_len:670 episode reward: total was -7.700000. running mean: -18.015009\n",
      "ep 1802: ep_len:540 episode reward: total was -52.130000. running mean: -18.356159\n",
      "ep 1802: ep_len:109 episode reward: total was -1.460000. running mean: -18.187198\n",
      "ep 1802: ep_len:615 episode reward: total was -17.590000. running mean: -18.181226\n",
      "ep 1802: ep_len:520 episode reward: total was -19.370000. running mean: -18.193113\n",
      "epsilon:0.153890 episode_count: 12621. steps_count: 5576683.000000\n",
      "ep 1803: ep_len:197 episode reward: total was 4.130000. running mean: -17.969882\n",
      "ep 1803: ep_len:505 episode reward: total was -8.010000. running mean: -17.870283\n",
      "ep 1803: ep_len:520 episode reward: total was -31.980000. running mean: -18.011381\n",
      "ep 1803: ep_len:152 episode reward: total was 0.110000. running mean: -17.830167\n",
      "ep 1803: ep_len:55 episode reward: total was -9.990000. running mean: -17.751765\n",
      "ep 1803: ep_len:645 episode reward: total was -83.640000. running mean: -18.410647\n",
      "ep 1803: ep_len:585 episode reward: total was -16.520000. running mean: -18.391741\n",
      "epsilon:0.153754 episode_count: 12628. steps_count: 5579342.000000\n",
      "ep 1804: ep_len:500 episode reward: total was -21.790000. running mean: -18.425724\n",
      "ep 1804: ep_len:620 episode reward: total was -68.140000. running mean: -18.922866\n",
      "ep 1804: ep_len:620 episode reward: total was -15.790000. running mean: -18.891538\n",
      "ep 1804: ep_len:590 episode reward: total was -7.990000. running mean: -18.782522\n",
      "ep 1804: ep_len:3 episode reward: total was 0.000000. running mean: -18.594697\n",
      "ep 1804: ep_len:565 episode reward: total was -16.280000. running mean: -18.571550\n",
      "ep 1804: ep_len:645 episode reward: total was -41.420000. running mean: -18.800035\n",
      "epsilon:0.153617 episode_count: 12635. steps_count: 5582885.000000\n",
      "ep 1805: ep_len:675 episode reward: total was -25.750000. running mean: -18.869534\n",
      "ep 1805: ep_len:580 episode reward: total was -12.410000. running mean: -18.804939\n",
      "ep 1805: ep_len:665 episode reward: total was -37.830000. running mean: -18.995190\n",
      "ep 1805: ep_len:530 episode reward: total was -4.910000. running mean: -18.854338\n",
      "ep 1805: ep_len:3 episode reward: total was 0.000000. running mean: -18.665794\n",
      "ep 1805: ep_len:500 episode reward: total was -22.750000. running mean: -18.706636\n",
      "ep 1805: ep_len:600 episode reward: total was -15.940000. running mean: -18.678970\n",
      "epsilon:0.153481 episode_count: 12642. steps_count: 5586438.000000\n",
      "ep 1806: ep_len:500 episode reward: total was -33.550000. running mean: -18.827680\n",
      "ep 1806: ep_len:505 episode reward: total was 4.710000. running mean: -18.592303\n",
      "ep 1806: ep_len:515 episode reward: total was -31.700000. running mean: -18.723380\n",
      "ep 1806: ep_len:500 episode reward: total was -20.680000. running mean: -18.742947\n",
      "ep 1806: ep_len:49 episode reward: total was 3.000000. running mean: -18.525517\n",
      "ep 1806: ep_len:710 episode reward: total was -18.670000. running mean: -18.526962\n",
      "ep 1806: ep_len:500 episode reward: total was -8.230000. running mean: -18.423992\n",
      "epsilon:0.153344 episode_count: 12649. steps_count: 5589717.000000\n",
      "ep 1807: ep_len:500 episode reward: total was -45.600000. running mean: -18.695752\n",
      "ep 1807: ep_len:500 episode reward: total was -11.080000. running mean: -18.619595\n",
      "ep 1807: ep_len:625 episode reward: total was -16.260000. running mean: -18.595999\n",
      "ep 1807: ep_len:545 episode reward: total was -26.040000. running mean: -18.670439\n",
      "ep 1807: ep_len:3 episode reward: total was 0.000000. running mean: -18.483735\n",
      "ep 1807: ep_len:600 episode reward: total was -47.170000. running mean: -18.770597\n",
      "ep 1807: ep_len:535 episode reward: total was -26.920000. running mean: -18.852091\n",
      "epsilon:0.153208 episode_count: 12656. steps_count: 5593025.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1808: ep_len:650 episode reward: total was -34.830000. running mean: -19.011870\n",
      "ep 1808: ep_len:600 episode reward: total was -20.990000. running mean: -19.031652\n",
      "ep 1808: ep_len:530 episode reward: total was -29.750000. running mean: -19.138835\n",
      "ep 1808: ep_len:540 episode reward: total was -21.280000. running mean: -19.160247\n",
      "ep 1808: ep_len:3 episode reward: total was 0.000000. running mean: -18.968644\n",
      "ep 1808: ep_len:565 episode reward: total was -6.860000. running mean: -18.847558\n",
      "ep 1808: ep_len:620 episode reward: total was -14.320000. running mean: -18.802282\n",
      "epsilon:0.153071 episode_count: 12663. steps_count: 5596533.000000\n",
      "ep 1809: ep_len:540 episode reward: total was -25.360000. running mean: -18.867859\n",
      "ep 1809: ep_len:505 episode reward: total was -41.130000. running mean: -19.090481\n",
      "ep 1809: ep_len:418 episode reward: total was -29.290000. running mean: -19.192476\n",
      "ep 1809: ep_len:129 episode reward: total was 1.580000. running mean: -18.984751\n",
      "ep 1809: ep_len:3 episode reward: total was 0.000000. running mean: -18.794904\n",
      "ep 1809: ep_len:530 episode reward: total was -52.080000. running mean: -19.127755\n",
      "ep 1809: ep_len:530 episode reward: total was -27.160000. running mean: -19.208077\n",
      "epsilon:0.152935 episode_count: 12670. steps_count: 5599188.000000\n",
      "ep 1810: ep_len:550 episode reward: total was -21.120000. running mean: -19.227196\n",
      "ep 1810: ep_len:505 episode reward: total was -9.620000. running mean: -19.131124\n",
      "ep 1810: ep_len:525 episode reward: total was -21.180000. running mean: -19.151613\n",
      "ep 1810: ep_len:500 episode reward: total was 2.490000. running mean: -18.935197\n",
      "ep 1810: ep_len:3 episode reward: total was 0.000000. running mean: -18.745845\n",
      "ep 1810: ep_len:525 episode reward: total was -1.980000. running mean: -18.578187\n",
      "ep 1810: ep_len:336 episode reward: total was -45.400000. running mean: -18.846405\n",
      "epsilon:0.152798 episode_count: 12677. steps_count: 5602132.000000\n",
      "ep 1811: ep_len:260 episode reward: total was 1.100000. running mean: -18.646941\n",
      "ep 1811: ep_len:565 episode reward: total was -27.820000. running mean: -18.738671\n",
      "ep 1811: ep_len:565 episode reward: total was -7.250000. running mean: -18.623785\n",
      "ep 1811: ep_len:530 episode reward: total was -3.650000. running mean: -18.474047\n",
      "ep 1811: ep_len:3 episode reward: total was 0.000000. running mean: -18.289306\n",
      "ep 1811: ep_len:685 episode reward: total was -39.290000. running mean: -18.499313\n",
      "ep 1811: ep_len:615 episode reward: total was -11.760000. running mean: -18.431920\n",
      "epsilon:0.152662 episode_count: 12684. steps_count: 5605355.000000\n",
      "ep 1812: ep_len:595 episode reward: total was -17.660000. running mean: -18.424201\n",
      "ep 1812: ep_len:590 episode reward: total was -18.390000. running mean: -18.423859\n",
      "ep 1812: ep_len:605 episode reward: total was -32.590000. running mean: -18.565520\n",
      "ep 1812: ep_len:113 episode reward: total was 3.580000. running mean: -18.344065\n",
      "ep 1812: ep_len:3 episode reward: total was 0.000000. running mean: -18.160624\n",
      "ep 1812: ep_len:640 episode reward: total was -8.250000. running mean: -18.061518\n",
      "ep 1812: ep_len:520 episode reward: total was -20.450000. running mean: -18.085403\n",
      "epsilon:0.152525 episode_count: 12691. steps_count: 5608421.000000\n",
      "ep 1813: ep_len:680 episode reward: total was -38.810000. running mean: -18.292649\n",
      "ep 1813: ep_len:505 episode reward: total was -36.100000. running mean: -18.470723\n",
      "ep 1813: ep_len:540 episode reward: total was -33.900000. running mean: -18.625015\n",
      "ep 1813: ep_len:670 episode reward: total was -42.990000. running mean: -18.868665\n",
      "ep 1813: ep_len:3 episode reward: total was 0.000000. running mean: -18.679978\n",
      "ep 1813: ep_len:570 episode reward: total was -14.110000. running mean: -18.634279\n",
      "ep 1813: ep_len:605 episode reward: total was -21.940000. running mean: -18.667336\n",
      "epsilon:0.152389 episode_count: 12698. steps_count: 5611994.000000\n",
      "ep 1814: ep_len:187 episode reward: total was -12.900000. running mean: -18.609663\n",
      "ep 1814: ep_len:303 episode reward: total was -19.860000. running mean: -18.622166\n",
      "ep 1814: ep_len:545 episode reward: total was -36.430000. running mean: -18.800244\n",
      "ep 1814: ep_len:520 episode reward: total was -10.600000. running mean: -18.718242\n",
      "ep 1814: ep_len:3 episode reward: total was 0.000000. running mean: -18.531059\n",
      "ep 1814: ep_len:505 episode reward: total was -5.090000. running mean: -18.396649\n",
      "ep 1814: ep_len:203 episode reward: total was -8.880000. running mean: -18.301482\n",
      "epsilon:0.152252 episode_count: 12705. steps_count: 5614260.000000\n",
      "ep 1815: ep_len:213 episode reward: total was -1.880000. running mean: -18.137268\n",
      "ep 1815: ep_len:560 episode reward: total was -7.030000. running mean: -18.026195\n",
      "ep 1815: ep_len:565 episode reward: total was -11.660000. running mean: -17.962533\n",
      "ep 1815: ep_len:56 episode reward: total was 1.560000. running mean: -17.767308\n",
      "ep 1815: ep_len:104 episode reward: total was -10.450000. running mean: -17.694134\n",
      "ep 1815: ep_len:500 episode reward: total was -70.810000. running mean: -18.225293\n",
      "ep 1815: ep_len:580 episode reward: total was -51.290000. running mean: -18.555940\n",
      "epsilon:0.152116 episode_count: 12712. steps_count: 5616838.000000\n",
      "ep 1816: ep_len:510 episode reward: total was -27.960000. running mean: -18.649981\n",
      "ep 1816: ep_len:368 episode reward: total was -14.860000. running mean: -18.612081\n",
      "ep 1816: ep_len:79 episode reward: total was -1.460000. running mean: -18.440560\n",
      "ep 1816: ep_len:610 episode reward: total was -22.950000. running mean: -18.485655\n",
      "ep 1816: ep_len:3 episode reward: total was 0.000000. running mean: -18.300798\n",
      "ep 1816: ep_len:580 episode reward: total was -38.930000. running mean: -18.507090\n",
      "ep 1816: ep_len:555 episode reward: total was -29.100000. running mean: -18.613019\n",
      "epsilon:0.151979 episode_count: 12719. steps_count: 5619543.000000\n",
      "ep 1817: ep_len:203 episode reward: total was -4.930000. running mean: -18.476189\n",
      "ep 1817: ep_len:271 episode reward: total was -13.890000. running mean: -18.430327\n",
      "ep 1817: ep_len:620 episode reward: total was -28.530000. running mean: -18.531324\n",
      "ep 1817: ep_len:535 episode reward: total was -39.070000. running mean: -18.736711\n",
      "ep 1817: ep_len:108 episode reward: total was 0.040000. running mean: -18.548943\n",
      "ep 1817: ep_len:630 episode reward: total was -21.010000. running mean: -18.573554\n",
      "ep 1817: ep_len:350 episode reward: total was -30.380000. running mean: -18.691618\n",
      "epsilon:0.151843 episode_count: 12726. steps_count: 5622260.000000\n",
      "ep 1818: ep_len:116 episode reward: total was -0.430000. running mean: -18.509002\n",
      "ep 1818: ep_len:500 episode reward: total was -15.350000. running mean: -18.477412\n",
      "ep 1818: ep_len:750 episode reward: total was -60.930000. running mean: -18.901938\n",
      "ep 1818: ep_len:575 episode reward: total was -58.190000. running mean: -19.294819\n",
      "ep 1818: ep_len:3 episode reward: total was 0.000000. running mean: -19.101871\n",
      "ep 1818: ep_len:525 episode reward: total was -30.950000. running mean: -19.220352\n",
      "ep 1818: ep_len:550 episode reward: total was -24.690000. running mean: -19.275048\n",
      "epsilon:0.151706 episode_count: 12733. steps_count: 5625279.000000\n",
      "ep 1819: ep_len:208 episode reward: total was -0.940000. running mean: -19.091698\n",
      "ep 1819: ep_len:565 episode reward: total was -26.950000. running mean: -19.170281\n",
      "ep 1819: ep_len:500 episode reward: total was -10.610000. running mean: -19.084678\n",
      "ep 1819: ep_len:505 episode reward: total was -33.610000. running mean: -19.229931\n",
      "ep 1819: ep_len:3 episode reward: total was 0.000000. running mean: -19.037632\n",
      "ep 1819: ep_len:595 episode reward: total was -25.740000. running mean: -19.104656\n",
      "ep 1819: ep_len:630 episode reward: total was -9.740000. running mean: -19.011009\n",
      "epsilon:0.151570 episode_count: 12740. steps_count: 5628285.000000\n",
      "ep 1820: ep_len:500 episode reward: total was -47.890000. running mean: -19.299799\n",
      "ep 1820: ep_len:655 episode reward: total was -6.530000. running mean: -19.172101\n",
      "ep 1820: ep_len:660 episode reward: total was -16.750000. running mean: -19.147880\n",
      "ep 1820: ep_len:41 episode reward: total was -1.960000. running mean: -18.976001\n",
      "ep 1820: ep_len:66 episode reward: total was 0.540000. running mean: -18.780841\n",
      "ep 1820: ep_len:500 episode reward: total was -23.170000. running mean: -18.824733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1820: ep_len:525 episode reward: total was -27.410000. running mean: -18.910585\n",
      "epsilon:0.151433 episode_count: 12747. steps_count: 5631232.000000\n",
      "ep 1821: ep_len:590 episode reward: total was -23.040000. running mean: -18.951880\n",
      "ep 1821: ep_len:540 episode reward: total was -15.620000. running mean: -18.918561\n",
      "ep 1821: ep_len:386 episode reward: total was -5.330000. running mean: -18.782675\n",
      "ep 1821: ep_len:505 episode reward: total was -40.200000. running mean: -18.996848\n",
      "ep 1821: ep_len:102 episode reward: total was 5.540000. running mean: -18.751480\n",
      "ep 1821: ep_len:500 episode reward: total was -64.230000. running mean: -19.206265\n",
      "ep 1821: ep_len:540 episode reward: total was -33.470000. running mean: -19.348903\n",
      "epsilon:0.151297 episode_count: 12754. steps_count: 5634395.000000\n",
      "ep 1822: ep_len:515 episode reward: total was -7.120000. running mean: -19.226614\n",
      "ep 1822: ep_len:540 episode reward: total was -13.070000. running mean: -19.165047\n",
      "ep 1822: ep_len:61 episode reward: total was -4.980000. running mean: -19.023197\n",
      "ep 1822: ep_len:580 episode reward: total was -8.590000. running mean: -18.918865\n",
      "ep 1822: ep_len:84 episode reward: total was -8.460000. running mean: -18.814276\n",
      "ep 1822: ep_len:580 episode reward: total was -88.840000. running mean: -19.514534\n",
      "ep 1822: ep_len:510 episode reward: total was -27.930000. running mean: -19.598688\n",
      "epsilon:0.151160 episode_count: 12761. steps_count: 5637265.000000\n",
      "ep 1823: ep_len:500 episode reward: total was -26.520000. running mean: -19.667901\n",
      "ep 1823: ep_len:600 episode reward: total was -46.080000. running mean: -19.932022\n",
      "ep 1823: ep_len:585 episode reward: total was -46.460000. running mean: -20.197302\n",
      "ep 1823: ep_len:505 episode reward: total was -32.210000. running mean: -20.317429\n",
      "ep 1823: ep_len:3 episode reward: total was 0.000000. running mean: -20.114255\n",
      "ep 1823: ep_len:615 episode reward: total was -7.580000. running mean: -19.988912\n",
      "ep 1823: ep_len:600 episode reward: total was -55.260000. running mean: -20.341623\n",
      "epsilon:0.151024 episode_count: 12768. steps_count: 5640673.000000\n",
      "ep 1824: ep_len:555 episode reward: total was -20.250000. running mean: -20.340707\n",
      "ep 1824: ep_len:560 episode reward: total was -28.100000. running mean: -20.418300\n",
      "ep 1824: ep_len:650 episode reward: total was -32.610000. running mean: -20.540217\n",
      "ep 1824: ep_len:167 episode reward: total was -3.930000. running mean: -20.374115\n",
      "ep 1824: ep_len:3 episode reward: total was 0.000000. running mean: -20.170373\n",
      "ep 1824: ep_len:520 episode reward: total was -41.300000. running mean: -20.381670\n",
      "ep 1824: ep_len:295 episode reward: total was -19.850000. running mean: -20.376353\n",
      "epsilon:0.150887 episode_count: 12775. steps_count: 5643423.000000\n",
      "ep 1825: ep_len:241 episode reward: total was -6.400000. running mean: -20.236590\n",
      "ep 1825: ep_len:565 episode reward: total was -54.650000. running mean: -20.580724\n",
      "ep 1825: ep_len:500 episode reward: total was -26.790000. running mean: -20.642816\n",
      "ep 1825: ep_len:520 episode reward: total was -20.610000. running mean: -20.642488\n",
      "ep 1825: ep_len:118 episode reward: total was 5.550000. running mean: -20.380563\n",
      "ep 1825: ep_len:520 episode reward: total was -8.690000. running mean: -20.263658\n",
      "ep 1825: ep_len:315 episode reward: total was -19.410000. running mean: -20.255121\n",
      "epsilon:0.150751 episode_count: 12782. steps_count: 5646202.000000\n",
      "ep 1826: ep_len:100 episode reward: total was -0.980000. running mean: -20.062370\n",
      "ep 1826: ep_len:510 episode reward: total was -26.030000. running mean: -20.122046\n",
      "ep 1826: ep_len:575 episode reward: total was -11.640000. running mean: -20.037226\n",
      "ep 1826: ep_len:555 episode reward: total was -6.540000. running mean: -19.902253\n",
      "ep 1826: ep_len:97 episode reward: total was 1.030000. running mean: -19.692931\n",
      "ep 1826: ep_len:635 episode reward: total was -13.640000. running mean: -19.632402\n",
      "ep 1826: ep_len:540 episode reward: total was -33.250000. running mean: -19.768578\n",
      "epsilon:0.150614 episode_count: 12789. steps_count: 5649214.000000\n",
      "ep 1827: ep_len:241 episode reward: total was -1.900000. running mean: -19.589892\n",
      "ep 1827: ep_len:500 episode reward: total was -19.230000. running mean: -19.586293\n",
      "ep 1827: ep_len:545 episode reward: total was -31.470000. running mean: -19.705130\n",
      "ep 1827: ep_len:120 episode reward: total was 1.110000. running mean: -19.496979\n",
      "ep 1827: ep_len:3 episode reward: total was 0.000000. running mean: -19.302009\n",
      "ep 1827: ep_len:525 episode reward: total was -43.740000. running mean: -19.546389\n",
      "ep 1827: ep_len:540 episode reward: total was -41.140000. running mean: -19.762325\n",
      "epsilon:0.150478 episode_count: 12796. steps_count: 5651688.000000\n",
      "ep 1828: ep_len:645 episode reward: total was -17.730000. running mean: -19.742002\n",
      "ep 1828: ep_len:565 episode reward: total was -4.490000. running mean: -19.589482\n",
      "ep 1828: ep_len:515 episode reward: total was -28.440000. running mean: -19.677987\n",
      "ep 1828: ep_len:520 episode reward: total was -30.710000. running mean: -19.788307\n",
      "ep 1828: ep_len:3 episode reward: total was 0.000000. running mean: -19.590424\n",
      "ep 1828: ep_len:635 episode reward: total was -35.610000. running mean: -19.750620\n",
      "ep 1828: ep_len:545 episode reward: total was -32.150000. running mean: -19.874613\n",
      "epsilon:0.150341 episode_count: 12803. steps_count: 5655116.000000\n",
      "ep 1829: ep_len:500 episode reward: total was -40.620000. running mean: -20.082067\n",
      "ep 1829: ep_len:343 episode reward: total was -16.900000. running mean: -20.050247\n",
      "ep 1829: ep_len:580 episode reward: total was -6.620000. running mean: -19.915944\n",
      "ep 1829: ep_len:540 episode reward: total was -23.020000. running mean: -19.946985\n",
      "ep 1829: ep_len:49 episode reward: total was 3.000000. running mean: -19.717515\n",
      "ep 1829: ep_len:252 episode reward: total was 0.100000. running mean: -19.519340\n",
      "ep 1829: ep_len:510 episode reward: total was -30.470000. running mean: -19.628846\n",
      "epsilon:0.150205 episode_count: 12810. steps_count: 5657890.000000\n",
      "ep 1830: ep_len:580 episode reward: total was -56.760000. running mean: -20.000158\n",
      "ep 1830: ep_len:354 episode reward: total was -49.850000. running mean: -20.298656\n",
      "ep 1830: ep_len:680 episode reward: total was -22.220000. running mean: -20.317870\n",
      "ep 1830: ep_len:500 episode reward: total was -17.130000. running mean: -20.285991\n",
      "ep 1830: ep_len:3 episode reward: total was 0.000000. running mean: -20.083131\n",
      "ep 1830: ep_len:500 episode reward: total was -19.040000. running mean: -20.072700\n",
      "ep 1830: ep_len:600 episode reward: total was -30.020000. running mean: -20.172173\n",
      "epsilon:0.150068 episode_count: 12817. steps_count: 5661107.000000\n",
      "ep 1831: ep_len:640 episode reward: total was -36.870000. running mean: -20.339151\n",
      "ep 1831: ep_len:580 episode reward: total was -9.550000. running mean: -20.231260\n",
      "ep 1831: ep_len:615 episode reward: total was -15.020000. running mean: -20.179147\n",
      "ep 1831: ep_len:540 episode reward: total was -14.210000. running mean: -20.119456\n",
      "ep 1831: ep_len:3 episode reward: total was 0.000000. running mean: -19.918261\n",
      "ep 1831: ep_len:500 episode reward: total was -24.620000. running mean: -19.965278\n",
      "ep 1831: ep_len:530 episode reward: total was -43.580000. running mean: -20.201426\n",
      "epsilon:0.149932 episode_count: 12824. steps_count: 5664515.000000\n",
      "ep 1832: ep_len:95 episode reward: total was -0.440000. running mean: -20.003811\n",
      "ep 1832: ep_len:545 episode reward: total was 0.370000. running mean: -19.800073\n",
      "ep 1832: ep_len:550 episode reward: total was -14.840000. running mean: -19.750472\n",
      "ep 1832: ep_len:510 episode reward: total was -30.220000. running mean: -19.855168\n",
      "ep 1832: ep_len:3 episode reward: total was 0.000000. running mean: -19.656616\n",
      "ep 1832: ep_len:500 episode reward: total was -16.550000. running mean: -19.625550\n",
      "ep 1832: ep_len:635 episode reward: total was -21.630000. running mean: -19.645594\n",
      "epsilon:0.149795 episode_count: 12831. steps_count: 5667353.000000\n",
      "ep 1833: ep_len:545 episode reward: total was -21.220000. running mean: -19.661338\n",
      "ep 1833: ep_len:630 episode reward: total was -15.140000. running mean: -19.616125\n",
      "ep 1833: ep_len:615 episode reward: total was -36.290000. running mean: -19.782864\n",
      "ep 1833: ep_len:505 episode reward: total was -15.530000. running mean: -19.740335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1833: ep_len:112 episode reward: total was -12.450000. running mean: -19.667432\n",
      "ep 1833: ep_len:580 episode reward: total was -46.240000. running mean: -19.933158\n",
      "ep 1833: ep_len:630 episode reward: total was -24.310000. running mean: -19.976926\n",
      "epsilon:0.149659 episode_count: 12838. steps_count: 5670970.000000\n",
      "ep 1834: ep_len:500 episode reward: total was -36.590000. running mean: -20.143057\n",
      "ep 1834: ep_len:500 episode reward: total was -43.610000. running mean: -20.377726\n",
      "ep 1834: ep_len:530 episode reward: total was -17.160000. running mean: -20.345549\n",
      "ep 1834: ep_len:530 episode reward: total was -43.750000. running mean: -20.579593\n",
      "ep 1834: ep_len:93 episode reward: total was -13.960000. running mean: -20.513397\n",
      "ep 1834: ep_len:510 episode reward: total was -39.670000. running mean: -20.704963\n",
      "ep 1834: ep_len:545 episode reward: total was -20.610000. running mean: -20.704014\n",
      "epsilon:0.149522 episode_count: 12845. steps_count: 5674178.000000\n",
      "ep 1835: ep_len:600 episode reward: total was -16.720000. running mean: -20.664174\n",
      "ep 1835: ep_len:505 episode reward: total was -30.050000. running mean: -20.758032\n",
      "ep 1835: ep_len:500 episode reward: total was -32.170000. running mean: -20.872152\n",
      "ep 1835: ep_len:157 episode reward: total was 4.590000. running mean: -20.617530\n",
      "ep 1835: ep_len:103 episode reward: total was -9.440000. running mean: -20.505755\n",
      "ep 1835: ep_len:505 episode reward: total was -25.640000. running mean: -20.557097\n",
      "ep 1835: ep_len:545 episode reward: total was -18.860000. running mean: -20.540126\n",
      "epsilon:0.149386 episode_count: 12852. steps_count: 5677093.000000\n",
      "ep 1836: ep_len:500 episode reward: total was -14.810000. running mean: -20.482825\n",
      "ep 1836: ep_len:615 episode reward: total was -19.380000. running mean: -20.471797\n",
      "ep 1836: ep_len:590 episode reward: total was -25.210000. running mean: -20.519179\n",
      "ep 1836: ep_len:590 episode reward: total was -5.980000. running mean: -20.373787\n",
      "ep 1836: ep_len:103 episode reward: total was 3.030000. running mean: -20.139749\n",
      "ep 1836: ep_len:520 episode reward: total was -11.520000. running mean: -20.053552\n",
      "ep 1836: ep_len:500 episode reward: total was -24.650000. running mean: -20.099516\n",
      "epsilon:0.149249 episode_count: 12859. steps_count: 5680511.000000\n",
      "ep 1837: ep_len:560 episode reward: total was -21.530000. running mean: -20.113821\n",
      "ep 1837: ep_len:500 episode reward: total was -24.000000. running mean: -20.152683\n",
      "ep 1837: ep_len:560 episode reward: total was -64.160000. running mean: -20.592756\n",
      "ep 1837: ep_len:122 episode reward: total was -4.950000. running mean: -20.436328\n",
      "ep 1837: ep_len:89 episode reward: total was -0.940000. running mean: -20.241365\n",
      "ep 1837: ep_len:500 episode reward: total was -46.780000. running mean: -20.506751\n",
      "ep 1837: ep_len:500 episode reward: total was -52.290000. running mean: -20.824584\n",
      "epsilon:0.149113 episode_count: 12866. steps_count: 5683342.000000\n",
      "ep 1838: ep_len:635 episode reward: total was -10.960000. running mean: -20.725938\n",
      "ep 1838: ep_len:520 episode reward: total was -6.300000. running mean: -20.581679\n",
      "ep 1838: ep_len:605 episode reward: total was -20.980000. running mean: -20.585662\n",
      "ep 1838: ep_len:525 episode reward: total was -3.710000. running mean: -20.416905\n",
      "ep 1838: ep_len:95 episode reward: total was 4.030000. running mean: -20.172436\n",
      "ep 1838: ep_len:149 episode reward: total was 1.040000. running mean: -19.960312\n",
      "ep 1838: ep_len:625 episode reward: total was -7.820000. running mean: -19.838909\n",
      "epsilon:0.148976 episode_count: 12873. steps_count: 5686496.000000\n",
      "ep 1839: ep_len:545 episode reward: total was -20.110000. running mean: -19.841620\n",
      "ep 1839: ep_len:620 episode reward: total was -40.930000. running mean: -20.052504\n",
      "ep 1839: ep_len:605 episode reward: total was -11.920000. running mean: -19.971178\n",
      "ep 1839: ep_len:161 episode reward: total was 1.110000. running mean: -19.760367\n",
      "ep 1839: ep_len:3 episode reward: total was 0.000000. running mean: -19.562763\n",
      "ep 1839: ep_len:540 episode reward: total was -40.150000. running mean: -19.768635\n",
      "ep 1839: ep_len:211 episode reward: total was -9.900000. running mean: -19.669949\n",
      "epsilon:0.148840 episode_count: 12880. steps_count: 5689181.000000\n",
      "ep 1840: ep_len:505 episode reward: total was -12.700000. running mean: -19.600250\n",
      "ep 1840: ep_len:620 episode reward: total was -15.680000. running mean: -19.561047\n",
      "ep 1840: ep_len:680 episode reward: total was -32.870000. running mean: -19.694137\n",
      "ep 1840: ep_len:525 episode reward: total was -2.140000. running mean: -19.518595\n",
      "ep 1840: ep_len:86 episode reward: total was 0.550000. running mean: -19.317909\n",
      "ep 1840: ep_len:500 episode reward: total was -17.820000. running mean: -19.302930\n",
      "ep 1840: ep_len:620 episode reward: total was -15.840000. running mean: -19.268301\n",
      "epsilon:0.148703 episode_count: 12887. steps_count: 5692717.000000\n",
      "ep 1841: ep_len:216 episode reward: total was 2.140000. running mean: -19.054218\n",
      "ep 1841: ep_len:610 episode reward: total was -28.530000. running mean: -19.148976\n",
      "ep 1841: ep_len:710 episode reward: total was -38.320000. running mean: -19.340686\n",
      "ep 1841: ep_len:132 episode reward: total was -0.940000. running mean: -19.156679\n",
      "ep 1841: ep_len:76 episode reward: total was 4.020000. running mean: -18.924912\n",
      "ep 1841: ep_len:500 episode reward: total was -11.180000. running mean: -18.847463\n",
      "ep 1841: ep_len:530 episode reward: total was -31.130000. running mean: -18.970289\n",
      "epsilon:0.148567 episode_count: 12894. steps_count: 5695491.000000\n",
      "ep 1842: ep_len:500 episode reward: total was -2.760000. running mean: -18.808186\n",
      "ep 1842: ep_len:650 episode reward: total was -3.940000. running mean: -18.659504\n",
      "ep 1842: ep_len:640 episode reward: total was -23.290000. running mean: -18.705809\n",
      "ep 1842: ep_len:570 episode reward: total was -20.480000. running mean: -18.723551\n",
      "ep 1842: ep_len:3 episode reward: total was 0.000000. running mean: -18.536315\n",
      "ep 1842: ep_len:650 episode reward: total was -39.080000. running mean: -18.741752\n",
      "ep 1842: ep_len:500 episode reward: total was -40.120000. running mean: -18.955534\n",
      "epsilon:0.148430 episode_count: 12901. steps_count: 5699004.000000\n",
      "ep 1843: ep_len:122 episode reward: total was -2.440000. running mean: -18.790379\n",
      "ep 1843: ep_len:620 episode reward: total was -11.810000. running mean: -18.720575\n",
      "ep 1843: ep_len:670 episode reward: total was -24.170000. running mean: -18.775070\n",
      "ep 1843: ep_len:379 episode reward: total was -31.270000. running mean: -18.900019\n",
      "ep 1843: ep_len:73 episode reward: total was 2.510000. running mean: -18.685919\n",
      "ep 1843: ep_len:575 episode reward: total was -17.360000. running mean: -18.672660\n",
      "ep 1843: ep_len:500 episode reward: total was -24.220000. running mean: -18.728133\n",
      "epsilon:0.148294 episode_count: 12908. steps_count: 5701943.000000\n",
      "ep 1844: ep_len:535 episode reward: total was -14.520000. running mean: -18.686052\n",
      "ep 1844: ep_len:655 episode reward: total was -4.500000. running mean: -18.544191\n",
      "ep 1844: ep_len:439 episode reward: total was -8.280000. running mean: -18.441549\n",
      "ep 1844: ep_len:525 episode reward: total was -2.620000. running mean: -18.283334\n",
      "ep 1844: ep_len:3 episode reward: total was 0.000000. running mean: -18.100500\n",
      "ep 1844: ep_len:590 episode reward: total was -12.620000. running mean: -18.045695\n",
      "ep 1844: ep_len:510 episode reward: total was -14.860000. running mean: -18.013838\n",
      "epsilon:0.148157 episode_count: 12915. steps_count: 5705200.000000\n",
      "ep 1845: ep_len:1270 episode reward: total was -154.710000. running mean: -19.380800\n",
      "ep 1845: ep_len:515 episode reward: total was -15.790000. running mean: -19.344892\n",
      "ep 1845: ep_len:680 episode reward: total was -23.780000. running mean: -19.389243\n",
      "ep 1845: ep_len:510 episode reward: total was -31.190000. running mean: -19.507251\n",
      "ep 1845: ep_len:3 episode reward: total was 0.000000. running mean: -19.312178\n",
      "ep 1845: ep_len:665 episode reward: total was -67.610000. running mean: -19.795156\n",
      "ep 1845: ep_len:615 episode reward: total was -28.500000. running mean: -19.882205\n",
      "epsilon:0.148021 episode_count: 12922. steps_count: 5709458.000000\n",
      "ep 1846: ep_len:625 episode reward: total was -25.080000. running mean: -19.934183\n",
      "ep 1846: ep_len:570 episode reward: total was 4.580000. running mean: -19.689041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1846: ep_len:600 episode reward: total was -11.070000. running mean: -19.602851\n",
      "ep 1846: ep_len:620 episode reward: total was -17.060000. running mean: -19.577422\n",
      "ep 1846: ep_len:56 episode reward: total was 5.500000. running mean: -19.326648\n",
      "ep 1846: ep_len:243 episode reward: total was -2.890000. running mean: -19.162281\n",
      "ep 1846: ep_len:565 episode reward: total was -20.610000. running mean: -19.176758\n",
      "epsilon:0.147884 episode_count: 12929. steps_count: 5712737.000000\n",
      "ep 1847: ep_len:775 episode reward: total was -44.760000. running mean: -19.432591\n",
      "ep 1847: ep_len:500 episode reward: total was -51.780000. running mean: -19.756065\n",
      "ep 1847: ep_len:570 episode reward: total was -29.970000. running mean: -19.858204\n",
      "ep 1847: ep_len:510 episode reward: total was -37.680000. running mean: -20.036422\n",
      "ep 1847: ep_len:3 episode reward: total was 0.000000. running mean: -19.836058\n",
      "ep 1847: ep_len:515 episode reward: total was -9.170000. running mean: -19.729397\n",
      "ep 1847: ep_len:540 episode reward: total was -44.600000. running mean: -19.978104\n",
      "epsilon:0.147748 episode_count: 12936. steps_count: 5716150.000000\n",
      "ep 1848: ep_len:535 episode reward: total was -8.000000. running mean: -19.858322\n",
      "ep 1848: ep_len:595 episode reward: total was -17.760000. running mean: -19.837339\n",
      "ep 1848: ep_len:825 episode reward: total was -55.260000. running mean: -20.191566\n",
      "ep 1848: ep_len:500 episode reward: total was -14.100000. running mean: -20.130650\n",
      "ep 1848: ep_len:81 episode reward: total was -2.980000. running mean: -19.959144\n",
      "ep 1848: ep_len:505 episode reward: total was -21.710000. running mean: -19.976652\n",
      "ep 1848: ep_len:515 episode reward: total was -23.120000. running mean: -20.008086\n",
      "epsilon:0.147611 episode_count: 12943. steps_count: 5719706.000000\n",
      "ep 1849: ep_len:134 episode reward: total was -6.460000. running mean: -19.872605\n",
      "ep 1849: ep_len:565 episode reward: total was -45.070000. running mean: -20.124579\n",
      "ep 1849: ep_len:570 episode reward: total was -31.100000. running mean: -20.234333\n",
      "ep 1849: ep_len:500 episode reward: total was -1.690000. running mean: -20.048890\n",
      "ep 1849: ep_len:3 episode reward: total was 0.000000. running mean: -19.848401\n",
      "ep 1849: ep_len:237 episode reward: total was -3.380000. running mean: -19.683717\n",
      "ep 1849: ep_len:510 episode reward: total was -43.740000. running mean: -19.924280\n",
      "epsilon:0.147475 episode_count: 12950. steps_count: 5722225.000000\n",
      "ep 1850: ep_len:500 episode reward: total was -38.550000. running mean: -20.110537\n",
      "ep 1850: ep_len:500 episode reward: total was -9.880000. running mean: -20.008231\n",
      "ep 1850: ep_len:540 episode reward: total was -25.170000. running mean: -20.059849\n",
      "ep 1850: ep_len:47 episode reward: total was -2.990000. running mean: -19.889151\n",
      "ep 1850: ep_len:3 episode reward: total was 0.000000. running mean: -19.690259\n",
      "ep 1850: ep_len:625 episode reward: total was -24.100000. running mean: -19.734357\n",
      "ep 1850: ep_len:605 episode reward: total was -17.130000. running mean: -19.708313\n",
      "epsilon:0.147338 episode_count: 12957. steps_count: 5725045.000000\n",
      "ep 1851: ep_len:540 episode reward: total was -26.940000. running mean: -19.780630\n",
      "ep 1851: ep_len:560 episode reward: total was -16.100000. running mean: -19.743824\n",
      "ep 1851: ep_len:500 episode reward: total was -22.090000. running mean: -19.767285\n",
      "ep 1851: ep_len:383 episode reward: total was -27.170000. running mean: -19.841312\n",
      "ep 1851: ep_len:96 episode reward: total was 1.040000. running mean: -19.632499\n",
      "ep 1851: ep_len:308 episode reward: total was -8.870000. running mean: -19.524874\n",
      "ep 1851: ep_len:500 episode reward: total was -46.730000. running mean: -19.796926\n",
      "epsilon:0.147202 episode_count: 12964. steps_count: 5727932.000000\n",
      "ep 1852: ep_len:550 episode reward: total was -18.670000. running mean: -19.785656\n",
      "ep 1852: ep_len:295 episode reward: total was -19.880000. running mean: -19.786600\n",
      "ep 1852: ep_len:635 episode reward: total was -33.020000. running mean: -19.918934\n",
      "ep 1852: ep_len:575 episode reward: total was -9.010000. running mean: -19.809844\n",
      "ep 1852: ep_len:100 episode reward: total was -9.440000. running mean: -19.706146\n",
      "ep 1852: ep_len:600 episode reward: total was -23.400000. running mean: -19.743085\n",
      "ep 1852: ep_len:535 episode reward: total was -37.060000. running mean: -19.916254\n",
      "epsilon:0.147065 episode_count: 12971. steps_count: 5731222.000000\n",
      "ep 1853: ep_len:235 episode reward: total was -3.390000. running mean: -19.750991\n",
      "ep 1853: ep_len:349 episode reward: total was -20.870000. running mean: -19.762181\n",
      "ep 1853: ep_len:570 episode reward: total was -26.440000. running mean: -19.828959\n",
      "ep 1853: ep_len:398 episode reward: total was -17.720000. running mean: -19.807870\n",
      "ep 1853: ep_len:3 episode reward: total was 0.000000. running mean: -19.609791\n",
      "ep 1853: ep_len:530 episode reward: total was -23.890000. running mean: -19.652593\n",
      "ep 1853: ep_len:283 episode reward: total was -16.900000. running mean: -19.625067\n",
      "epsilon:0.146929 episode_count: 12978. steps_count: 5733590.000000\n",
      "ep 1854: ep_len:585 episode reward: total was -17.450000. running mean: -19.603317\n",
      "ep 1854: ep_len:197 episode reward: total was -5.890000. running mean: -19.466183\n",
      "ep 1854: ep_len:670 episode reward: total was -61.220000. running mean: -19.883722\n",
      "ep 1854: ep_len:540 episode reward: total was 0.890000. running mean: -19.675984\n",
      "ep 1854: ep_len:3 episode reward: total was 0.000000. running mean: -19.479225\n",
      "ep 1854: ep_len:570 episode reward: total was -5.150000. running mean: -19.335932\n",
      "ep 1854: ep_len:690 episode reward: total was -56.950000. running mean: -19.712073\n",
      "epsilon:0.146792 episode_count: 12985. steps_count: 5736845.000000\n",
      "ep 1855: ep_len:226 episode reward: total was -1.390000. running mean: -19.528852\n",
      "ep 1855: ep_len:500 episode reward: total was -35.730000. running mean: -19.690864\n",
      "ep 1855: ep_len:500 episode reward: total was -7.060000. running mean: -19.564555\n",
      "ep 1855: ep_len:595 episode reward: total was -11.240000. running mean: -19.481310\n",
      "ep 1855: ep_len:3 episode reward: total was 0.000000. running mean: -19.286496\n",
      "ep 1855: ep_len:510 episode reward: total was -41.830000. running mean: -19.511932\n",
      "ep 1855: ep_len:170 episode reward: total was -8.880000. running mean: -19.405612\n",
      "epsilon:0.146656 episode_count: 12992. steps_count: 5739349.000000\n",
      "ep 1856: ep_len:249 episode reward: total was -20.830000. running mean: -19.419856\n",
      "ep 1856: ep_len:550 episode reward: total was -43.590000. running mean: -19.661558\n",
      "ep 1856: ep_len:555 episode reward: total was -8.650000. running mean: -19.551442\n",
      "ep 1856: ep_len:500 episode reward: total was -25.210000. running mean: -19.608028\n",
      "ep 1856: ep_len:3 episode reward: total was 0.000000. running mean: -19.411947\n",
      "ep 1856: ep_len:312 episode reward: total was -6.410000. running mean: -19.281928\n",
      "ep 1856: ep_len:500 episode reward: total was -18.640000. running mean: -19.275509\n",
      "epsilon:0.146519 episode_count: 12999. steps_count: 5742018.000000\n",
      "ep 1857: ep_len:620 episode reward: total was -26.320000. running mean: -19.345953\n",
      "ep 1857: ep_len:500 episode reward: total was -40.620000. running mean: -19.558694\n",
      "ep 1857: ep_len:560 episode reward: total was -44.160000. running mean: -19.804707\n",
      "ep 1857: ep_len:635 episode reward: total was -38.070000. running mean: -19.987360\n",
      "ep 1857: ep_len:3 episode reward: total was 0.000000. running mean: -19.787486\n",
      "ep 1857: ep_len:178 episode reward: total was 3.090000. running mean: -19.558711\n",
      "ep 1857: ep_len:314 episode reward: total was -20.330000. running mean: -19.566424\n",
      "epsilon:0.146383 episode_count: 13006. steps_count: 5744828.000000\n",
      "ep 1858: ep_len:555 episode reward: total was -33.660000. running mean: -19.707360\n",
      "ep 1858: ep_len:188 episode reward: total was -4.900000. running mean: -19.559286\n",
      "ep 1858: ep_len:500 episode reward: total was -37.790000. running mean: -19.741594\n",
      "ep 1858: ep_len:550 episode reward: total was -5.590000. running mean: -19.600078\n",
      "ep 1858: ep_len:3 episode reward: total was 0.000000. running mean: -19.404077\n",
      "ep 1858: ep_len:321 episode reward: total was -6.350000. running mean: -19.273536\n",
      "ep 1858: ep_len:600 episode reward: total was -25.930000. running mean: -19.340101\n",
      "epsilon:0.146246 episode_count: 13013. steps_count: 5747545.000000\n",
      "ep 1859: ep_len:595 episode reward: total was -2.080000. running mean: -19.167500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1859: ep_len:500 episode reward: total was -9.480000. running mean: -19.070625\n",
      "ep 1859: ep_len:635 episode reward: total was -32.610000. running mean: -19.206018\n",
      "ep 1859: ep_len:725 episode reward: total was -84.300000. running mean: -19.856958\n",
      "ep 1859: ep_len:3 episode reward: total was 0.000000. running mean: -19.658389\n",
      "ep 1859: ep_len:173 episode reward: total was -3.410000. running mean: -19.495905\n",
      "ep 1859: ep_len:301 episode reward: total was -11.330000. running mean: -19.414246\n",
      "epsilon:0.146110 episode_count: 13020. steps_count: 5750477.000000\n",
      "ep 1860: ep_len:645 episode reward: total was -38.850000. running mean: -19.608603\n",
      "ep 1860: ep_len:500 episode reward: total was -10.910000. running mean: -19.521617\n",
      "ep 1860: ep_len:520 episode reward: total was -12.680000. running mean: -19.453201\n",
      "ep 1860: ep_len:500 episode reward: total was -19.690000. running mean: -19.455569\n",
      "ep 1860: ep_len:3 episode reward: total was 0.000000. running mean: -19.261013\n",
      "ep 1860: ep_len:555 episode reward: total was -20.310000. running mean: -19.271503\n",
      "ep 1860: ep_len:500 episode reward: total was -18.270000. running mean: -19.261488\n",
      "epsilon:0.145973 episode_count: 13027. steps_count: 5753700.000000\n",
      "ep 1861: ep_len:500 episode reward: total was -28.830000. running mean: -19.357173\n",
      "ep 1861: ep_len:510 episode reward: total was -33.650000. running mean: -19.500102\n",
      "ep 1861: ep_len:510 episode reward: total was -10.650000. running mean: -19.411601\n",
      "ep 1861: ep_len:520 episode reward: total was -10.570000. running mean: -19.323185\n",
      "ep 1861: ep_len:92 episode reward: total was -0.480000. running mean: -19.134753\n",
      "ep 1861: ep_len:630 episode reward: total was -14.460000. running mean: -19.088005\n",
      "ep 1861: ep_len:515 episode reward: total was -21.910000. running mean: -19.116225\n",
      "epsilon:0.145837 episode_count: 13034. steps_count: 5756977.000000\n",
      "ep 1862: ep_len:1115 episode reward: total was -105.550000. running mean: -19.980563\n",
      "ep 1862: ep_len:580 episode reward: total was -17.660000. running mean: -19.957357\n",
      "ep 1862: ep_len:575 episode reward: total was -7.750000. running mean: -19.835284\n",
      "ep 1862: ep_len:520 episode reward: total was -26.090000. running mean: -19.897831\n",
      "ep 1862: ep_len:3 episode reward: total was 0.000000. running mean: -19.698853\n",
      "ep 1862: ep_len:500 episode reward: total was -17.240000. running mean: -19.674264\n",
      "ep 1862: ep_len:605 episode reward: total was -25.080000. running mean: -19.728321\n",
      "epsilon:0.145700 episode_count: 13041. steps_count: 5760875.000000\n",
      "ep 1863: ep_len:695 episode reward: total was -45.390000. running mean: -19.984938\n",
      "ep 1863: ep_len:585 episode reward: total was -29.230000. running mean: -20.077389\n",
      "ep 1863: ep_len:625 episode reward: total was -23.470000. running mean: -20.111315\n",
      "ep 1863: ep_len:132 episode reward: total was -0.420000. running mean: -19.914402\n",
      "ep 1863: ep_len:3 episode reward: total was 0.000000. running mean: -19.715258\n",
      "ep 1863: ep_len:560 episode reward: total was -10.550000. running mean: -19.623605\n",
      "ep 1863: ep_len:550 episode reward: total was -35.120000. running mean: -19.778569\n",
      "epsilon:0.145564 episode_count: 13048. steps_count: 5764025.000000\n",
      "ep 1864: ep_len:535 episode reward: total was -35.430000. running mean: -19.935083\n",
      "ep 1864: ep_len:500 episode reward: total was -5.810000. running mean: -19.793833\n",
      "ep 1864: ep_len:590 episode reward: total was -14.660000. running mean: -19.742494\n",
      "ep 1864: ep_len:500 episode reward: total was -7.640000. running mean: -19.621469\n",
      "ep 1864: ep_len:3 episode reward: total was 0.000000. running mean: -19.425255\n",
      "ep 1864: ep_len:650 episode reward: total was -19.220000. running mean: -19.423202\n",
      "ep 1864: ep_len:172 episode reward: total was -5.880000. running mean: -19.287770\n",
      "epsilon:0.145427 episode_count: 13055. steps_count: 5766975.000000\n",
      "ep 1865: ep_len:134 episode reward: total was -2.450000. running mean: -19.119392\n",
      "ep 1865: ep_len:660 episode reward: total was -42.290000. running mean: -19.351098\n",
      "ep 1865: ep_len:685 episode reward: total was -39.810000. running mean: -19.555687\n",
      "ep 1865: ep_len:520 episode reward: total was -30.110000. running mean: -19.661231\n",
      "ep 1865: ep_len:3 episode reward: total was 0.000000. running mean: -19.464618\n",
      "ep 1865: ep_len:165 episode reward: total was -0.910000. running mean: -19.279072\n",
      "ep 1865: ep_len:525 episode reward: total was -41.150000. running mean: -19.497781\n",
      "epsilon:0.145291 episode_count: 13062. steps_count: 5769667.000000\n",
      "ep 1866: ep_len:635 episode reward: total was -16.460000. running mean: -19.467404\n",
      "ep 1866: ep_len:595 episode reward: total was -13.350000. running mean: -19.406230\n",
      "ep 1866: ep_len:580 episode reward: total was -21.070000. running mean: -19.422867\n",
      "ep 1866: ep_len:429 episode reward: total was -9.680000. running mean: -19.325439\n",
      "ep 1866: ep_len:3 episode reward: total was 0.000000. running mean: -19.132184\n",
      "ep 1866: ep_len:640 episode reward: total was -40.150000. running mean: -19.342362\n",
      "ep 1866: ep_len:535 episode reward: total was -27.910000. running mean: -19.428039\n",
      "epsilon:0.145154 episode_count: 13069. steps_count: 5773084.000000\n",
      "ep 1867: ep_len:605 episode reward: total was -23.080000. running mean: -19.464558\n",
      "ep 1867: ep_len:645 episode reward: total was -39.900000. running mean: -19.668913\n",
      "ep 1867: ep_len:392 episode reward: total was -4.820000. running mean: -19.520424\n",
      "ep 1867: ep_len:570 episode reward: total was -45.480000. running mean: -19.780019\n",
      "ep 1867: ep_len:3 episode reward: total was 0.000000. running mean: -19.582219\n",
      "ep 1867: ep_len:500 episode reward: total was -40.580000. running mean: -19.792197\n",
      "ep 1867: ep_len:500 episode reward: total was -16.250000. running mean: -19.756775\n",
      "epsilon:0.145018 episode_count: 13076. steps_count: 5776299.000000\n",
      "ep 1868: ep_len:107 episode reward: total was -4.940000. running mean: -19.608607\n",
      "ep 1868: ep_len:625 episode reward: total was -0.710000. running mean: -19.419621\n",
      "ep 1868: ep_len:670 episode reward: total was -28.730000. running mean: -19.512725\n",
      "ep 1868: ep_len:56 episode reward: total was 1.560000. running mean: -19.301998\n",
      "ep 1868: ep_len:3 episode reward: total was 0.000000. running mean: -19.108978\n",
      "ep 1868: ep_len:545 episode reward: total was 4.140000. running mean: -18.876488\n",
      "ep 1868: ep_len:570 episode reward: total was -22.010000. running mean: -18.907823\n",
      "epsilon:0.144881 episode_count: 13083. steps_count: 5778875.000000\n",
      "ep 1869: ep_len:625 episode reward: total was -6.930000. running mean: -18.788045\n",
      "ep 1869: ep_len:500 episode reward: total was -9.350000. running mean: -18.693664\n",
      "ep 1869: ep_len:665 episode reward: total was -25.190000. running mean: -18.758628\n",
      "ep 1869: ep_len:560 episode reward: total was -27.050000. running mean: -18.841542\n",
      "ep 1869: ep_len:3 episode reward: total was 0.000000. running mean: -18.653126\n",
      "ep 1869: ep_len:249 episode reward: total was 0.120000. running mean: -18.465395\n",
      "ep 1869: ep_len:605 episode reward: total was -23.930000. running mean: -18.520041\n",
      "epsilon:0.144745 episode_count: 13090. steps_count: 5782082.000000\n",
      "ep 1870: ep_len:500 episode reward: total was -10.250000. running mean: -18.437340\n",
      "ep 1870: ep_len:645 episode reward: total was 1.420000. running mean: -18.238767\n",
      "ep 1870: ep_len:500 episode reward: total was -10.120000. running mean: -18.157579\n",
      "ep 1870: ep_len:500 episode reward: total was -29.660000. running mean: -18.272604\n",
      "ep 1870: ep_len:3 episode reward: total was 0.000000. running mean: -18.089878\n",
      "ep 1870: ep_len:505 episode reward: total was -42.110000. running mean: -18.330079\n",
      "ep 1870: ep_len:540 episode reward: total was -13.910000. running mean: -18.285878\n",
      "epsilon:0.144608 episode_count: 13097. steps_count: 5785275.000000\n",
      "ep 1871: ep_len:625 episode reward: total was -19.460000. running mean: -18.297619\n",
      "ep 1871: ep_len:500 episode reward: total was -21.630000. running mean: -18.330943\n",
      "ep 1871: ep_len:590 episode reward: total was -15.140000. running mean: -18.299034\n",
      "ep 1871: ep_len:505 episode reward: total was -13.570000. running mean: -18.251743\n",
      "ep 1871: ep_len:128 episode reward: total was -16.430000. running mean: -18.233526\n",
      "ep 1871: ep_len:555 episode reward: total was -28.900000. running mean: -18.340191\n",
      "ep 1871: ep_len:510 episode reward: total was -16.910000. running mean: -18.325889\n",
      "epsilon:0.144472 episode_count: 13104. steps_count: 5788688.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1872: ep_len:186 episode reward: total was 5.610000. running mean: -18.086530\n",
      "ep 1872: ep_len:555 episode reward: total was -29.140000. running mean: -18.197064\n",
      "ep 1872: ep_len:555 episode reward: total was -19.840000. running mean: -18.213494\n",
      "ep 1872: ep_len:615 episode reward: total was -24.050000. running mean: -18.271859\n",
      "ep 1872: ep_len:3 episode reward: total was 0.000000. running mean: -18.089140\n",
      "ep 1872: ep_len:336 episode reward: total was -7.370000. running mean: -17.981949\n",
      "ep 1872: ep_len:510 episode reward: total was -23.480000. running mean: -18.036929\n",
      "epsilon:0.144335 episode_count: 13111. steps_count: 5791448.000000\n",
      "ep 1873: ep_len:560 episode reward: total was -21.770000. running mean: -18.074260\n",
      "ep 1873: ep_len:555 episode reward: total was -90.350000. running mean: -18.797018\n",
      "ep 1873: ep_len:585 episode reward: total was -10.760000. running mean: -18.716647\n",
      "ep 1873: ep_len:570 episode reward: total was -15.740000. running mean: -18.686881\n",
      "ep 1873: ep_len:3 episode reward: total was 0.000000. running mean: -18.500012\n",
      "ep 1873: ep_len:575 episode reward: total was -10.290000. running mean: -18.417912\n",
      "ep 1873: ep_len:515 episode reward: total was -47.780000. running mean: -18.711533\n",
      "epsilon:0.144199 episode_count: 13118. steps_count: 5794811.000000\n",
      "ep 1874: ep_len:545 episode reward: total was -18.600000. running mean: -18.710418\n",
      "ep 1874: ep_len:500 episode reward: total was -3.360000. running mean: -18.556913\n",
      "ep 1874: ep_len:545 episode reward: total was -27.360000. running mean: -18.644944\n",
      "ep 1874: ep_len:123 episode reward: total was 2.090000. running mean: -18.437595\n",
      "ep 1874: ep_len:3 episode reward: total was 0.000000. running mean: -18.253219\n",
      "ep 1874: ep_len:530 episode reward: total was -18.140000. running mean: -18.252087\n",
      "ep 1874: ep_len:505 episode reward: total was -29.010000. running mean: -18.359666\n",
      "epsilon:0.144062 episode_count: 13125. steps_count: 5797562.000000\n",
      "ep 1875: ep_len:500 episode reward: total was -17.290000. running mean: -18.348969\n",
      "ep 1875: ep_len:515 episode reward: total was -27.000000. running mean: -18.435479\n",
      "ep 1875: ep_len:715 episode reward: total was -20.250000. running mean: -18.453625\n",
      "ep 1875: ep_len:500 episode reward: total was -22.090000. running mean: -18.489988\n",
      "ep 1875: ep_len:110 episode reward: total was -12.940000. running mean: -18.434488\n",
      "ep 1875: ep_len:515 episode reward: total was -14.150000. running mean: -18.391644\n",
      "ep 1875: ep_len:510 episode reward: total was -37.570000. running mean: -18.583427\n",
      "epsilon:0.143926 episode_count: 13132. steps_count: 5800927.000000\n",
      "ep 1876: ep_len:191 episode reward: total was 0.100000. running mean: -18.396593\n",
      "ep 1876: ep_len:585 episode reward: total was -13.150000. running mean: -18.344127\n",
      "ep 1876: ep_len:695 episode reward: total was -38.310000. running mean: -18.543786\n",
      "ep 1876: ep_len:500 episode reward: total was -22.550000. running mean: -18.583848\n",
      "ep 1876: ep_len:3 episode reward: total was 0.000000. running mean: -18.398009\n",
      "ep 1876: ep_len:500 episode reward: total was -21.800000. running mean: -18.432029\n",
      "ep 1876: ep_len:500 episode reward: total was -30.530000. running mean: -18.553009\n",
      "epsilon:0.143789 episode_count: 13139. steps_count: 5803901.000000\n",
      "ep 1877: ep_len:109 episode reward: total was -1.460000. running mean: -18.382079\n",
      "ep 1877: ep_len:500 episode reward: total was 2.150000. running mean: -18.176758\n",
      "ep 1877: ep_len:655 episode reward: total was -13.920000. running mean: -18.134191\n",
      "ep 1877: ep_len:550 episode reward: total was -32.710000. running mean: -18.279949\n",
      "ep 1877: ep_len:3 episode reward: total was 0.000000. running mean: -18.097149\n",
      "ep 1877: ep_len:685 episode reward: total was -45.870000. running mean: -18.374878\n",
      "ep 1877: ep_len:280 episode reward: total was -40.880000. running mean: -18.599929\n",
      "epsilon:0.143653 episode_count: 13146. steps_count: 5806683.000000\n",
      "ep 1878: ep_len:500 episode reward: total was -21.930000. running mean: -18.633230\n",
      "ep 1878: ep_len:525 episode reward: total was -1.170000. running mean: -18.458597\n",
      "ep 1878: ep_len:645 episode reward: total was -9.710000. running mean: -18.371111\n",
      "ep 1878: ep_len:500 episode reward: total was -16.560000. running mean: -18.353000\n",
      "ep 1878: ep_len:91 episode reward: total was -8.940000. running mean: -18.258870\n",
      "ep 1878: ep_len:530 episode reward: total was -18.450000. running mean: -18.260781\n",
      "ep 1878: ep_len:500 episode reward: total was -27.150000. running mean: -18.349674\n",
      "epsilon:0.143516 episode_count: 13153. steps_count: 5809974.000000\n",
      "ep 1879: ep_len:194 episode reward: total was -5.430000. running mean: -18.220477\n",
      "ep 1879: ep_len:530 episode reward: total was -8.800000. running mean: -18.126272\n",
      "ep 1879: ep_len:590 episode reward: total was -25.600000. running mean: -18.201009\n",
      "ep 1879: ep_len:500 episode reward: total was -18.570000. running mean: -18.204699\n",
      "ep 1879: ep_len:3 episode reward: total was 0.000000. running mean: -18.022652\n",
      "ep 1879: ep_len:510 episode reward: total was -12.000000. running mean: -17.962426\n",
      "ep 1879: ep_len:575 episode reward: total was -28.100000. running mean: -18.063802\n",
      "epsilon:0.143380 episode_count: 13160. steps_count: 5812876.000000\n",
      "ep 1880: ep_len:615 episode reward: total was -22.240000. running mean: -18.105564\n",
      "ep 1880: ep_len:660 episode reward: total was -38.880000. running mean: -18.313308\n",
      "ep 1880: ep_len:555 episode reward: total was -31.930000. running mean: -18.449475\n",
      "ep 1880: ep_len:520 episode reward: total was -19.670000. running mean: -18.461680\n",
      "ep 1880: ep_len:3 episode reward: total was 0.000000. running mean: -18.277063\n",
      "ep 1880: ep_len:570 episode reward: total was -23.860000. running mean: -18.332893\n",
      "ep 1880: ep_len:500 episode reward: total was -49.750000. running mean: -18.647064\n",
      "epsilon:0.143243 episode_count: 13167. steps_count: 5816299.000000\n",
      "ep 1881: ep_len:500 episode reward: total was -27.500000. running mean: -18.735593\n",
      "ep 1881: ep_len:595 episode reward: total was -30.620000. running mean: -18.854437\n",
      "ep 1881: ep_len:620 episode reward: total was -6.180000. running mean: -18.727693\n",
      "ep 1881: ep_len:560 episode reward: total was -34.020000. running mean: -18.880616\n",
      "ep 1881: ep_len:88 episode reward: total was 3.550000. running mean: -18.656310\n",
      "ep 1881: ep_len:635 episode reward: total was -16.690000. running mean: -18.636647\n",
      "ep 1881: ep_len:500 episode reward: total was -12.510000. running mean: -18.575380\n",
      "epsilon:0.143107 episode_count: 13174. steps_count: 5819797.000000\n",
      "ep 1882: ep_len:555 episode reward: total was -5.970000. running mean: -18.449326\n",
      "ep 1882: ep_len:500 episode reward: total was -21.610000. running mean: -18.480933\n",
      "ep 1882: ep_len:605 episode reward: total was -1.010000. running mean: -18.306224\n",
      "ep 1882: ep_len:500 episode reward: total was -24.630000. running mean: -18.369462\n",
      "ep 1882: ep_len:48 episode reward: total was 4.500000. running mean: -18.140767\n",
      "ep 1882: ep_len:770 episode reward: total was -68.470000. running mean: -18.644059\n",
      "ep 1882: ep_len:525 episode reward: total was -21.140000. running mean: -18.669019\n",
      "epsilon:0.142970 episode_count: 13181. steps_count: 5823300.000000\n",
      "ep 1883: ep_len:500 episode reward: total was -63.880000. running mean: -19.121128\n",
      "ep 1883: ep_len:510 episode reward: total was -20.050000. running mean: -19.130417\n",
      "ep 1883: ep_len:585 episode reward: total was -14.970000. running mean: -19.088813\n",
      "ep 1883: ep_len:630 episode reward: total was -31.980000. running mean: -19.217725\n",
      "ep 1883: ep_len:109 episode reward: total was 7.050000. running mean: -18.955048\n",
      "ep 1883: ep_len:645 episode reward: total was -57.000000. running mean: -19.335497\n",
      "ep 1883: ep_len:600 episode reward: total was -26.060000. running mean: -19.402742\n",
      "epsilon:0.142834 episode_count: 13188. steps_count: 5826879.000000\n",
      "ep 1884: ep_len:197 episode reward: total was -0.410000. running mean: -19.212815\n",
      "ep 1884: ep_len:585 episode reward: total was -18.460000. running mean: -19.205287\n",
      "ep 1884: ep_len:815 episode reward: total was -64.670000. running mean: -19.659934\n",
      "ep 1884: ep_len:500 episode reward: total was -26.130000. running mean: -19.724634\n",
      "ep 1884: ep_len:113 episode reward: total was -12.940000. running mean: -19.656788\n",
      "ep 1884: ep_len:177 episode reward: total was -17.410000. running mean: -19.634320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1884: ep_len:325 episode reward: total was -44.880000. running mean: -19.886777\n",
      "epsilon:0.142697 episode_count: 13195. steps_count: 5829591.000000\n",
      "ep 1885: ep_len:215 episode reward: total was -5.390000. running mean: -19.741809\n",
      "ep 1885: ep_len:610 episode reward: total was -22.940000. running mean: -19.773791\n",
      "ep 1885: ep_len:555 episode reward: total was -24.840000. running mean: -19.824453\n",
      "ep 1885: ep_len:500 episode reward: total was -15.170000. running mean: -19.777909\n",
      "ep 1885: ep_len:75 episode reward: total was 1.540000. running mean: -19.564730\n",
      "ep 1885: ep_len:500 episode reward: total was -14.250000. running mean: -19.511582\n",
      "ep 1885: ep_len:333 episode reward: total was -36.400000. running mean: -19.680466\n",
      "epsilon:0.142561 episode_count: 13202. steps_count: 5832379.000000\n",
      "ep 1886: ep_len:515 episode reward: total was -26.060000. running mean: -19.744262\n",
      "ep 1886: ep_len:545 episode reward: total was -10.000000. running mean: -19.646819\n",
      "ep 1886: ep_len:570 episode reward: total was -34.390000. running mean: -19.794251\n",
      "ep 1886: ep_len:500 episode reward: total was -4.340000. running mean: -19.639708\n",
      "ep 1886: ep_len:97 episode reward: total was -11.960000. running mean: -19.562911\n",
      "ep 1886: ep_len:178 episode reward: total was -8.900000. running mean: -19.456282\n",
      "ep 1886: ep_len:590 episode reward: total was -20.510000. running mean: -19.466819\n",
      "epsilon:0.142424 episode_count: 13209. steps_count: 5835374.000000\n",
      "ep 1887: ep_len:655 episode reward: total was -15.420000. running mean: -19.426351\n",
      "ep 1887: ep_len:179 episode reward: total was -8.930000. running mean: -19.321388\n",
      "ep 1887: ep_len:500 episode reward: total was -14.030000. running mean: -19.268474\n",
      "ep 1887: ep_len:157 episode reward: total was 6.090000. running mean: -19.014889\n",
      "ep 1887: ep_len:3 episode reward: total was 0.000000. running mean: -18.824740\n",
      "ep 1887: ep_len:630 episode reward: total was -36.140000. running mean: -18.997893\n",
      "ep 1887: ep_len:510 episode reward: total was -26.030000. running mean: -19.068214\n",
      "epsilon:0.142288 episode_count: 13216. steps_count: 5838008.000000\n",
      "ep 1888: ep_len:760 episode reward: total was -50.800000. running mean: -19.385532\n",
      "ep 1888: ep_len:595 episode reward: total was -41.740000. running mean: -19.609076\n",
      "ep 1888: ep_len:520 episode reward: total was -10.830000. running mean: -19.521286\n",
      "ep 1888: ep_len:560 episode reward: total was -57.580000. running mean: -19.901873\n",
      "ep 1888: ep_len:101 episode reward: total was -8.480000. running mean: -19.787654\n",
      "ep 1888: ep_len:580 episode reward: total was -12.080000. running mean: -19.710578\n",
      "ep 1888: ep_len:500 episode reward: total was -12.300000. running mean: -19.636472\n",
      "epsilon:0.142151 episode_count: 13223. steps_count: 5841624.000000\n",
      "ep 1889: ep_len:510 episode reward: total was 0.340000. running mean: -19.436707\n",
      "ep 1889: ep_len:500 episode reward: total was -20.700000. running mean: -19.449340\n",
      "ep 1889: ep_len:575 episode reward: total was -26.620000. running mean: -19.521047\n",
      "ep 1889: ep_len:115 episode reward: total was 0.110000. running mean: -19.324736\n",
      "ep 1889: ep_len:3 episode reward: total was 0.000000. running mean: -19.131489\n",
      "ep 1889: ep_len:505 episode reward: total was -27.240000. running mean: -19.212574\n",
      "ep 1889: ep_len:288 episode reward: total was -18.430000. running mean: -19.204748\n",
      "epsilon:0.142015 episode_count: 13230. steps_count: 5844120.000000\n",
      "ep 1890: ep_len:560 episode reward: total was -14.150000. running mean: -19.154201\n",
      "ep 1890: ep_len:515 episode reward: total was -33.000000. running mean: -19.292659\n",
      "ep 1890: ep_len:580 episode reward: total was -19.390000. running mean: -19.293632\n",
      "ep 1890: ep_len:500 episode reward: total was -16.040000. running mean: -19.261096\n",
      "ep 1890: ep_len:72 episode reward: total was -10.480000. running mean: -19.173285\n",
      "ep 1890: ep_len:590 episode reward: total was -22.630000. running mean: -19.207852\n",
      "ep 1890: ep_len:530 episode reward: total was -37.530000. running mean: -19.391073\n",
      "epsilon:0.141878 episode_count: 13237. steps_count: 5847467.000000\n",
      "ep 1891: ep_len:134 episode reward: total was -10.900000. running mean: -19.306163\n",
      "ep 1891: ep_len:565 episode reward: total was -35.500000. running mean: -19.468101\n",
      "ep 1891: ep_len:665 episode reward: total was -19.740000. running mean: -19.470820\n",
      "ep 1891: ep_len:565 episode reward: total was -15.130000. running mean: -19.427412\n",
      "ep 1891: ep_len:3 episode reward: total was 0.000000. running mean: -19.233138\n",
      "ep 1891: ep_len:665 episode reward: total was -6.210000. running mean: -19.102906\n",
      "ep 1891: ep_len:565 episode reward: total was -26.190000. running mean: -19.173777\n",
      "epsilon:0.141742 episode_count: 13244. steps_count: 5850629.000000\n",
      "ep 1892: ep_len:500 episode reward: total was -11.880000. running mean: -19.100840\n",
      "ep 1892: ep_len:271 episode reward: total was -34.860000. running mean: -19.258431\n",
      "ep 1892: ep_len:79 episode reward: total was -4.950000. running mean: -19.115347\n",
      "ep 1892: ep_len:500 episode reward: total was -22.670000. running mean: -19.150893\n",
      "ep 1892: ep_len:92 episode reward: total was -10.440000. running mean: -19.063784\n",
      "ep 1892: ep_len:560 episode reward: total was -20.560000. running mean: -19.078747\n",
      "ep 1892: ep_len:310 episode reward: total was -12.310000. running mean: -19.011059\n",
      "epsilon:0.141605 episode_count: 13251. steps_count: 5852941.000000\n",
      "ep 1893: ep_len:250 episode reward: total was -24.860000. running mean: -19.069549\n",
      "ep 1893: ep_len:500 episode reward: total was -14.910000. running mean: -19.027953\n",
      "ep 1893: ep_len:384 episode reward: total was -31.830000. running mean: -19.155973\n",
      "ep 1893: ep_len:500 episode reward: total was -20.670000. running mean: -19.171114\n",
      "ep 1893: ep_len:3 episode reward: total was 0.000000. running mean: -18.979403\n",
      "ep 1893: ep_len:655 episode reward: total was -3.170000. running mean: -18.821309\n",
      "ep 1893: ep_len:595 episode reward: total was -15.050000. running mean: -18.783596\n",
      "epsilon:0.141469 episode_count: 13258. steps_count: 5855828.000000\n",
      "ep 1894: ep_len:95 episode reward: total was 1.070000. running mean: -18.585060\n",
      "ep 1894: ep_len:505 episode reward: total was -9.940000. running mean: -18.498609\n",
      "ep 1894: ep_len:655 episode reward: total was -18.430000. running mean: -18.497923\n",
      "ep 1894: ep_len:329 episode reward: total was -4.770000. running mean: -18.360644\n",
      "ep 1894: ep_len:3 episode reward: total was 0.000000. running mean: -18.177037\n",
      "ep 1894: ep_len:550 episode reward: total was -10.940000. running mean: -18.104667\n",
      "ep 1894: ep_len:341 episode reward: total was -10.840000. running mean: -18.032020\n",
      "epsilon:0.141332 episode_count: 13265. steps_count: 5858306.000000\n",
      "ep 1895: ep_len:600 episode reward: total was -20.690000. running mean: -18.058600\n",
      "ep 1895: ep_len:530 episode reward: total was -10.500000. running mean: -17.983014\n",
      "ep 1895: ep_len:615 episode reward: total was -14.750000. running mean: -17.950684\n",
      "ep 1895: ep_len:545 episode reward: total was -14.000000. running mean: -17.911177\n",
      "ep 1895: ep_len:3 episode reward: total was 0.000000. running mean: -17.732065\n",
      "ep 1895: ep_len:605 episode reward: total was -2.030000. running mean: -17.575045\n",
      "ep 1895: ep_len:690 episode reward: total was -28.740000. running mean: -17.686694\n",
      "epsilon:0.141196 episode_count: 13272. steps_count: 5861894.000000\n",
      "ep 1896: ep_len:515 episode reward: total was -24.910000. running mean: -17.758927\n",
      "ep 1896: ep_len:665 episode reward: total was -12.010000. running mean: -17.701438\n",
      "ep 1896: ep_len:595 episode reward: total was -15.730000. running mean: -17.681724\n",
      "ep 1896: ep_len:520 episode reward: total was -35.210000. running mean: -17.857006\n",
      "ep 1896: ep_len:3 episode reward: total was 0.000000. running mean: -17.678436\n",
      "ep 1896: ep_len:525 episode reward: total was -23.080000. running mean: -17.732452\n",
      "ep 1896: ep_len:334 episode reward: total was -12.790000. running mean: -17.683027\n",
      "epsilon:0.141059 episode_count: 13279. steps_count: 5865051.000000\n",
      "ep 1897: ep_len:231 episode reward: total was -17.360000. running mean: -17.679797\n",
      "ep 1897: ep_len:555 episode reward: total was 3.800000. running mean: -17.464999\n",
      "ep 1897: ep_len:695 episode reward: total was -44.790000. running mean: -17.738249\n",
      "ep 1897: ep_len:780 episode reward: total was -50.540000. running mean: -18.066267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1897: ep_len:3 episode reward: total was 0.000000. running mean: -17.885604\n",
      "ep 1897: ep_len:500 episode reward: total was -0.520000. running mean: -17.711948\n",
      "ep 1897: ep_len:635 episode reward: total was -59.750000. running mean: -18.132328\n",
      "epsilon:0.140923 episode_count: 13286. steps_count: 5868450.000000\n",
      "ep 1898: ep_len:550 episode reward: total was -7.130000. running mean: -18.022305\n",
      "ep 1898: ep_len:675 episode reward: total was -44.910000. running mean: -18.291182\n",
      "ep 1898: ep_len:421 episode reward: total was -5.250000. running mean: -18.160770\n",
      "ep 1898: ep_len:515 episode reward: total was -30.110000. running mean: -18.280263\n",
      "ep 1898: ep_len:3 episode reward: total was 0.000000. running mean: -18.097460\n",
      "ep 1898: ep_len:530 episode reward: total was -51.820000. running mean: -18.434685\n",
      "ep 1898: ep_len:580 episode reward: total was -30.530000. running mean: -18.555638\n",
      "epsilon:0.140786 episode_count: 13293. steps_count: 5871724.000000\n",
      "ep 1899: ep_len:545 episode reward: total was -41.020000. running mean: -18.780282\n",
      "ep 1899: ep_len:590 episode reward: total was -36.060000. running mean: -18.953079\n",
      "ep 1899: ep_len:625 episode reward: total was -19.540000. running mean: -18.958948\n",
      "ep 1899: ep_len:168 episode reward: total was -12.410000. running mean: -18.893459\n",
      "ep 1899: ep_len:3 episode reward: total was 0.000000. running mean: -18.704524\n",
      "ep 1899: ep_len:540 episode reward: total was -39.640000. running mean: -18.913879\n",
      "ep 1899: ep_len:555 episode reward: total was -32.130000. running mean: -19.046040\n",
      "epsilon:0.140650 episode_count: 13300. steps_count: 5874750.000000\n",
      "ep 1900: ep_len:530 episode reward: total was 4.350000. running mean: -18.812080\n",
      "ep 1900: ep_len:505 episode reward: total was 0.170000. running mean: -18.622259\n",
      "ep 1900: ep_len:575 episode reward: total was -33.650000. running mean: -18.772537\n",
      "ep 1900: ep_len:118 episode reward: total was 1.590000. running mean: -18.568911\n",
      "ep 1900: ep_len:3 episode reward: total was 0.000000. running mean: -18.383222\n",
      "ep 1900: ep_len:605 episode reward: total was -11.790000. running mean: -18.317290\n",
      "ep 1900: ep_len:292 episode reward: total was -15.280000. running mean: -18.286917\n",
      "epsilon:0.140513 episode_count: 13307. steps_count: 5877378.000000\n",
      "ep 1901: ep_len:129 episode reward: total was -9.920000. running mean: -18.203248\n",
      "ep 1901: ep_len:500 episode reward: total was -13.280000. running mean: -18.154015\n",
      "ep 1901: ep_len:500 episode reward: total was -8.040000. running mean: -18.052875\n",
      "ep 1901: ep_len:570 episode reward: total was -11.960000. running mean: -17.991946\n",
      "ep 1901: ep_len:3 episode reward: total was 0.000000. running mean: -17.812027\n",
      "ep 1901: ep_len:620 episode reward: total was -24.100000. running mean: -17.874907\n",
      "ep 1901: ep_len:545 episode reward: total was -38.610000. running mean: -18.082258\n",
      "epsilon:0.140377 episode_count: 13314. steps_count: 5880245.000000\n",
      "ep 1902: ep_len:615 episode reward: total was -9.590000. running mean: -17.997335\n",
      "ep 1902: ep_len:630 episode reward: total was -7.060000. running mean: -17.887962\n",
      "ep 1902: ep_len:600 episode reward: total was -9.540000. running mean: -17.804482\n",
      "ep 1902: ep_len:500 episode reward: total was -39.720000. running mean: -18.023637\n",
      "ep 1902: ep_len:113 episode reward: total was -7.970000. running mean: -17.923101\n",
      "ep 1902: ep_len:660 episode reward: total was -54.490000. running mean: -18.288770\n",
      "ep 1902: ep_len:500 episode reward: total was -26.160000. running mean: -18.367482\n",
      "epsilon:0.140240 episode_count: 13321. steps_count: 5883863.000000\n",
      "ep 1903: ep_len:500 episode reward: total was -30.490000. running mean: -18.488707\n",
      "ep 1903: ep_len:560 episode reward: total was -29.180000. running mean: -18.595620\n",
      "ep 1903: ep_len:620 episode reward: total was -21.470000. running mean: -18.624364\n",
      "ep 1903: ep_len:132 episode reward: total was 6.100000. running mean: -18.377120\n",
      "ep 1903: ep_len:71 episode reward: total was -8.950000. running mean: -18.282849\n",
      "ep 1903: ep_len:650 episode reward: total was -17.240000. running mean: -18.272421\n",
      "ep 1903: ep_len:555 episode reward: total was -28.990000. running mean: -18.379597\n",
      "epsilon:0.140104 episode_count: 13328. steps_count: 5886951.000000\n",
      "ep 1904: ep_len:625 episode reward: total was -15.230000. running mean: -18.348101\n",
      "ep 1904: ep_len:515 episode reward: total was -0.310000. running mean: -18.167720\n",
      "ep 1904: ep_len:560 episode reward: total was -9.130000. running mean: -18.077342\n",
      "ep 1904: ep_len:394 episode reward: total was -8.670000. running mean: -17.983269\n",
      "ep 1904: ep_len:3 episode reward: total was 0.000000. running mean: -17.803436\n",
      "ep 1904: ep_len:500 episode reward: total was -26.560000. running mean: -17.891002\n",
      "ep 1904: ep_len:620 episode reward: total was -25.540000. running mean: -17.967492\n",
      "epsilon:0.139967 episode_count: 13335. steps_count: 5890168.000000\n",
      "ep 1905: ep_len:505 episode reward: total was -12.990000. running mean: -17.917717\n",
      "ep 1905: ep_len:500 episode reward: total was -18.630000. running mean: -17.924840\n",
      "ep 1905: ep_len:60 episode reward: total was -2.960000. running mean: -17.775191\n",
      "ep 1905: ep_len:520 episode reward: total was -0.110000. running mean: -17.598539\n",
      "ep 1905: ep_len:120 episode reward: total was 2.580000. running mean: -17.396754\n",
      "ep 1905: ep_len:595 episode reward: total was -13.840000. running mean: -17.361187\n",
      "ep 1905: ep_len:530 episode reward: total was -28.930000. running mean: -17.476875\n",
      "epsilon:0.139831 episode_count: 13342. steps_count: 5892998.000000\n",
      "ep 1906: ep_len:635 episode reward: total was -25.400000. running mean: -17.556106\n",
      "ep 1906: ep_len:285 episode reward: total was -13.320000. running mean: -17.513745\n",
      "ep 1906: ep_len:500 episode reward: total was -7.900000. running mean: -17.417607\n",
      "ep 1906: ep_len:515 episode reward: total was -17.030000. running mean: -17.413731\n",
      "ep 1906: ep_len:3 episode reward: total was 0.000000. running mean: -17.239594\n",
      "ep 1906: ep_len:550 episode reward: total was -12.820000. running mean: -17.195398\n",
      "ep 1906: ep_len:510 episode reward: total was -15.430000. running mean: -17.177744\n",
      "epsilon:0.139694 episode_count: 13349. steps_count: 5895996.000000\n",
      "ep 1907: ep_len:570 episode reward: total was -1.080000. running mean: -17.016767\n",
      "ep 1907: ep_len:200 episode reward: total was -8.370000. running mean: -16.930299\n",
      "ep 1907: ep_len:500 episode reward: total was -16.460000. running mean: -16.925596\n",
      "ep 1907: ep_len:570 episode reward: total was -7.630000. running mean: -16.832640\n",
      "ep 1907: ep_len:114 episode reward: total was -1.440000. running mean: -16.678714\n",
      "ep 1907: ep_len:500 episode reward: total was -26.550000. running mean: -16.777427\n",
      "ep 1907: ep_len:316 episode reward: total was -11.330000. running mean: -16.722952\n",
      "epsilon:0.139558 episode_count: 13356. steps_count: 5898766.000000\n",
      "ep 1908: ep_len:258 episode reward: total was 0.630000. running mean: -16.549423\n",
      "ep 1908: ep_len:500 episode reward: total was -0.350000. running mean: -16.387429\n",
      "ep 1908: ep_len:605 episode reward: total was -14.380000. running mean: -16.367354\n",
      "ep 1908: ep_len:500 episode reward: total was 3.360000. running mean: -16.170081\n",
      "ep 1908: ep_len:3 episode reward: total was 0.000000. running mean: -16.008380\n",
      "ep 1908: ep_len:625 episode reward: total was -11.450000. running mean: -15.962796\n",
      "ep 1908: ep_len:189 episode reward: total was -2.850000. running mean: -15.831668\n",
      "epsilon:0.139421 episode_count: 13363. steps_count: 5901446.000000\n",
      "ep 1909: ep_len:635 episode reward: total was -13.880000. running mean: -15.812151\n",
      "ep 1909: ep_len:500 episode reward: total was -5.510000. running mean: -15.709130\n",
      "ep 1909: ep_len:418 episode reward: total was -4.830000. running mean: -15.600339\n",
      "ep 1909: ep_len:565 episode reward: total was -14.990000. running mean: -15.594235\n",
      "ep 1909: ep_len:3 episode reward: total was 0.000000. running mean: -15.438293\n",
      "ep 1909: ep_len:505 episode reward: total was -38.620000. running mean: -15.670110\n",
      "ep 1909: ep_len:535 episode reward: total was -11.440000. running mean: -15.627809\n",
      "epsilon:0.139285 episode_count: 13370. steps_count: 5904607.000000\n",
      "ep 1910: ep_len:850 episode reward: total was -105.170000. running mean: -16.523231\n",
      "ep 1910: ep_len:625 episode reward: total was -23.580000. running mean: -16.593798\n",
      "ep 1910: ep_len:580 episode reward: total was -21.290000. running mean: -16.640760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1910: ep_len:500 episode reward: total was -10.290000. running mean: -16.577253\n",
      "ep 1910: ep_len:3 episode reward: total was 0.000000. running mean: -16.411480\n",
      "ep 1910: ep_len:525 episode reward: total was -23.920000. running mean: -16.486566\n",
      "ep 1910: ep_len:520 episode reward: total was -18.420000. running mean: -16.505900\n",
      "epsilon:0.139148 episode_count: 13377. steps_count: 5908210.000000\n",
      "ep 1911: ep_len:545 episode reward: total was -13.430000. running mean: -16.475141\n",
      "ep 1911: ep_len:515 episode reward: total was -12.730000. running mean: -16.437689\n",
      "ep 1911: ep_len:555 episode reward: total was -37.910000. running mean: -16.652413\n",
      "ep 1911: ep_len:520 episode reward: total was -13.120000. running mean: -16.617088\n",
      "ep 1911: ep_len:3 episode reward: total was 0.000000. running mean: -16.450918\n",
      "ep 1911: ep_len:520 episode reward: total was -31.110000. running mean: -16.597508\n",
      "ep 1911: ep_len:590 episode reward: total was -17.930000. running mean: -16.610833\n",
      "epsilon:0.139012 episode_count: 13384. steps_count: 5911458.000000\n",
      "ep 1912: ep_len:610 episode reward: total was -10.950000. running mean: -16.554225\n",
      "ep 1912: ep_len:500 episode reward: total was -23.590000. running mean: -16.624583\n",
      "ep 1912: ep_len:500 episode reward: total was -23.160000. running mean: -16.689937\n",
      "ep 1912: ep_len:112 episode reward: total was -1.420000. running mean: -16.537238\n",
      "ep 1912: ep_len:3 episode reward: total was 0.000000. running mean: -16.371865\n",
      "ep 1912: ep_len:500 episode reward: total was -42.870000. running mean: -16.636847\n",
      "ep 1912: ep_len:590 episode reward: total was -38.530000. running mean: -16.855778\n",
      "epsilon:0.138875 episode_count: 13391. steps_count: 5914273.000000\n",
      "ep 1913: ep_len:530 episode reward: total was -29.500000. running mean: -16.982220\n",
      "ep 1913: ep_len:575 episode reward: total was -21.170000. running mean: -17.024098\n",
      "ep 1913: ep_len:525 episode reward: total was -34.230000. running mean: -17.196157\n",
      "ep 1913: ep_len:515 episode reward: total was -10.680000. running mean: -17.130996\n",
      "ep 1913: ep_len:3 episode reward: total was 0.000000. running mean: -16.959686\n",
      "ep 1913: ep_len:620 episode reward: total was -41.210000. running mean: -17.202189\n",
      "ep 1913: ep_len:610 episode reward: total was -18.620000. running mean: -17.216367\n",
      "epsilon:0.138739 episode_count: 13398. steps_count: 5917651.000000\n",
      "ep 1914: ep_len:215 episode reward: total was -9.430000. running mean: -17.138503\n",
      "ep 1914: ep_len:500 episode reward: total was 0.170000. running mean: -16.965418\n",
      "ep 1914: ep_len:840 episode reward: total was -58.290000. running mean: -17.378664\n",
      "ep 1914: ep_len:510 episode reward: total was -21.630000. running mean: -17.421177\n",
      "ep 1914: ep_len:56 episode reward: total was -8.000000. running mean: -17.326966\n",
      "ep 1914: ep_len:530 episode reward: total was -20.490000. running mean: -17.358596\n",
      "ep 1914: ep_len:610 episode reward: total was -37.700000. running mean: -17.562010\n",
      "epsilon:0.138602 episode_count: 13405. steps_count: 5920912.000000\n",
      "ep 1915: ep_len:256 episode reward: total was -5.890000. running mean: -17.445290\n",
      "ep 1915: ep_len:195 episode reward: total was -7.880000. running mean: -17.349637\n",
      "ep 1915: ep_len:605 episode reward: total was -8.660000. running mean: -17.262741\n",
      "ep 1915: ep_len:670 episode reward: total was -25.500000. running mean: -17.345113\n",
      "ep 1915: ep_len:3 episode reward: total was 0.000000. running mean: -17.171662\n",
      "ep 1915: ep_len:645 episode reward: total was -17.300000. running mean: -17.172945\n",
      "ep 1915: ep_len:500 episode reward: total was -19.250000. running mean: -17.193716\n",
      "epsilon:0.138466 episode_count: 13412. steps_count: 5923786.000000\n",
      "ep 1916: ep_len:580 episode reward: total was -26.740000. running mean: -17.289179\n",
      "ep 1916: ep_len:550 episode reward: total was -6.070000. running mean: -17.176987\n",
      "ep 1916: ep_len:500 episode reward: total was -10.550000. running mean: -17.110717\n",
      "ep 1916: ep_len:545 episode reward: total was -0.180000. running mean: -16.941410\n",
      "ep 1916: ep_len:103 episode reward: total was 3.550000. running mean: -16.736496\n",
      "ep 1916: ep_len:600 episode reward: total was -23.530000. running mean: -16.804431\n",
      "ep 1916: ep_len:344 episode reward: total was -7.750000. running mean: -16.713887\n",
      "epsilon:0.138329 episode_count: 13419. steps_count: 5927008.000000\n",
      "ep 1917: ep_len:640 episode reward: total was -12.980000. running mean: -16.676548\n",
      "ep 1917: ep_len:535 episode reward: total was -36.750000. running mean: -16.877282\n",
      "ep 1917: ep_len:585 episode reward: total was -9.580000. running mean: -16.804309\n",
      "ep 1917: ep_len:399 episode reward: total was -28.190000. running mean: -16.918166\n",
      "ep 1917: ep_len:87 episode reward: total was 0.550000. running mean: -16.743485\n",
      "ep 1917: ep_len:500 episode reward: total was -5.620000. running mean: -16.632250\n",
      "ep 1917: ep_len:500 episode reward: total was -13.770000. running mean: -16.603627\n",
      "epsilon:0.138193 episode_count: 13426. steps_count: 5930254.000000\n",
      "ep 1918: ep_len:630 episode reward: total was -66.560000. running mean: -17.103191\n",
      "ep 1918: ep_len:500 episode reward: total was -7.090000. running mean: -17.003059\n",
      "ep 1918: ep_len:381 episode reward: total was -2.370000. running mean: -16.856729\n",
      "ep 1918: ep_len:500 episode reward: total was -69.340000. running mean: -17.381561\n",
      "ep 1918: ep_len:3 episode reward: total was 0.000000. running mean: -17.207746\n",
      "ep 1918: ep_len:655 episode reward: total was -23.830000. running mean: -17.273968\n",
      "ep 1918: ep_len:680 episode reward: total was -42.080000. running mean: -17.522028\n",
      "epsilon:0.138056 episode_count: 13433. steps_count: 5933603.000000\n",
      "ep 1919: ep_len:229 episode reward: total was -26.350000. running mean: -17.610308\n",
      "ep 1919: ep_len:655 episode reward: total was -24.320000. running mean: -17.677405\n",
      "ep 1919: ep_len:565 episode reward: total was -23.350000. running mean: -17.734131\n",
      "ep 1919: ep_len:550 episode reward: total was -5.570000. running mean: -17.612490\n",
      "ep 1919: ep_len:94 episode reward: total was 2.550000. running mean: -17.410865\n",
      "ep 1919: ep_len:680 episode reward: total was -48.850000. running mean: -17.725256\n",
      "ep 1919: ep_len:500 episode reward: total was -39.140000. running mean: -17.939404\n",
      "epsilon:0.137920 episode_count: 13440. steps_count: 5936876.000000\n",
      "ep 1920: ep_len:217 episode reward: total was -5.910000. running mean: -17.819110\n",
      "ep 1920: ep_len:500 episode reward: total was -23.610000. running mean: -17.877019\n",
      "ep 1920: ep_len:500 episode reward: total was -22.950000. running mean: -17.927748\n",
      "ep 1920: ep_len:560 episode reward: total was -6.030000. running mean: -17.808771\n",
      "ep 1920: ep_len:3 episode reward: total was 0.000000. running mean: -17.630683\n",
      "ep 1920: ep_len:605 episode reward: total was -15.560000. running mean: -17.609976\n",
      "ep 1920: ep_len:535 episode reward: total was -11.620000. running mean: -17.550077\n",
      "epsilon:0.137783 episode_count: 13447. steps_count: 5939796.000000\n",
      "ep 1921: ep_len:630 episode reward: total was -13.970000. running mean: -17.514276\n",
      "ep 1921: ep_len:500 episode reward: total was -8.860000. running mean: -17.427733\n",
      "ep 1921: ep_len:570 episode reward: total was -14.680000. running mean: -17.400256\n",
      "ep 1921: ep_len:540 episode reward: total was -10.620000. running mean: -17.332453\n",
      "ep 1921: ep_len:3 episode reward: total was 0.000000. running mean: -17.159129\n",
      "ep 1921: ep_len:302 episode reward: total was -13.870000. running mean: -17.126237\n",
      "ep 1921: ep_len:630 episode reward: total was -51.460000. running mean: -17.469575\n",
      "epsilon:0.137647 episode_count: 13454. steps_count: 5942971.000000\n",
      "ep 1922: ep_len:585 episode reward: total was -3.020000. running mean: -17.325079\n",
      "ep 1922: ep_len:555 episode reward: total was 17.910000. running mean: -16.972728\n",
      "ep 1922: ep_len:650 episode reward: total was -20.960000. running mean: -17.012601\n",
      "ep 1922: ep_len:500 episode reward: total was -2.070000. running mean: -16.863175\n",
      "ep 1922: ep_len:3 episode reward: total was 0.000000. running mean: -16.694543\n",
      "ep 1922: ep_len:680 episode reward: total was -33.010000. running mean: -16.857698\n",
      "ep 1922: ep_len:580 episode reward: total was -8.860000. running mean: -16.777721\n",
      "epsilon:0.137510 episode_count: 13461. steps_count: 5946524.000000\n",
      "ep 1923: ep_len:540 episode reward: total was -11.020000. running mean: -16.720144\n",
      "ep 1923: ep_len:545 episode reward: total was -15.420000. running mean: -16.707142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1923: ep_len:77 episode reward: total was 0.040000. running mean: -16.539671\n",
      "ep 1923: ep_len:520 episode reward: total was -15.500000. running mean: -16.529274\n",
      "ep 1923: ep_len:100 episode reward: total was -3.450000. running mean: -16.398481\n",
      "ep 1923: ep_len:500 episode reward: total was -20.160000. running mean: -16.436097\n",
      "ep 1923: ep_len:605 episode reward: total was -36.560000. running mean: -16.637336\n",
      "epsilon:0.137374 episode_count: 13468. steps_count: 5949411.000000\n",
      "ep 1924: ep_len:540 episode reward: total was -3.640000. running mean: -16.507362\n",
      "ep 1924: ep_len:530 episode reward: total was -26.540000. running mean: -16.607689\n",
      "ep 1924: ep_len:437 episode reward: total was -30.800000. running mean: -16.749612\n",
      "ep 1924: ep_len:379 episode reward: total was -13.190000. running mean: -16.714016\n",
      "ep 1924: ep_len:94 episode reward: total was -3.450000. running mean: -16.581376\n",
      "ep 1924: ep_len:560 episode reward: total was -26.820000. running mean: -16.683762\n",
      "ep 1924: ep_len:750 episode reward: total was -50.060000. running mean: -17.017524\n",
      "epsilon:0.137237 episode_count: 13475. steps_count: 5952701.000000\n",
      "ep 1925: ep_len:232 episode reward: total was 0.120000. running mean: -16.846149\n",
      "ep 1925: ep_len:615 episode reward: total was -33.590000. running mean: -17.013587\n",
      "ep 1925: ep_len:384 episode reward: total was -22.320000. running mean: -17.066652\n",
      "ep 1925: ep_len:685 episode reward: total was -52.540000. running mean: -17.421385\n",
      "ep 1925: ep_len:3 episode reward: total was 0.000000. running mean: -17.247171\n",
      "ep 1925: ep_len:170 episode reward: total was -18.920000. running mean: -17.263899\n",
      "ep 1925: ep_len:790 episode reward: total was -65.370000. running mean: -17.744960\n",
      "epsilon:0.137101 episode_count: 13482. steps_count: 5955580.000000\n",
      "ep 1926: ep_len:500 episode reward: total was -5.790000. running mean: -17.625411\n",
      "ep 1926: ep_len:555 episode reward: total was -2.210000. running mean: -17.471257\n",
      "ep 1926: ep_len:610 episode reward: total was -19.630000. running mean: -17.492844\n",
      "ep 1926: ep_len:500 episode reward: total was -16.080000. running mean: -17.478716\n",
      "ep 1926: ep_len:3 episode reward: total was 0.000000. running mean: -17.303929\n",
      "ep 1926: ep_len:525 episode reward: total was -18.120000. running mean: -17.312089\n",
      "ep 1926: ep_len:560 episode reward: total was -30.240000. running mean: -17.441368\n",
      "epsilon:0.136964 episode_count: 13489. steps_count: 5958833.000000\n",
      "ep 1927: ep_len:630 episode reward: total was -16.930000. running mean: -17.436255\n",
      "ep 1927: ep_len:515 episode reward: total was -19.010000. running mean: -17.451992\n",
      "ep 1927: ep_len:695 episode reward: total was -51.970000. running mean: -17.797172\n",
      "ep 1927: ep_len:590 episode reward: total was -6.040000. running mean: -17.679601\n",
      "ep 1927: ep_len:3 episode reward: total was 0.000000. running mean: -17.502805\n",
      "ep 1927: ep_len:500 episode reward: total was -12.260000. running mean: -17.450376\n",
      "ep 1927: ep_len:530 episode reward: total was -31.470000. running mean: -17.590573\n",
      "epsilon:0.136828 episode_count: 13496. steps_count: 5962296.000000\n",
      "ep 1928: ep_len:185 episode reward: total was -2.910000. running mean: -17.443767\n",
      "ep 1928: ep_len:356 episode reward: total was -39.790000. running mean: -17.667229\n",
      "ep 1928: ep_len:590 episode reward: total was -13.520000. running mean: -17.625757\n",
      "ep 1928: ep_len:386 episode reward: total was -10.170000. running mean: -17.551199\n",
      "ep 1928: ep_len:132 episode reward: total was 5.050000. running mean: -17.325187\n",
      "ep 1928: ep_len:179 episode reward: total was -6.460000. running mean: -17.216536\n",
      "ep 1928: ep_len:520 episode reward: total was -31.460000. running mean: -17.358970\n",
      "epsilon:0.136691 episode_count: 13503. steps_count: 5964644.000000\n",
      "ep 1929: ep_len:196 episode reward: total was -3.440000. running mean: -17.219781\n",
      "ep 1929: ep_len:500 episode reward: total was -23.160000. running mean: -17.279183\n",
      "ep 1929: ep_len:520 episode reward: total was -30.420000. running mean: -17.410591\n",
      "ep 1929: ep_len:500 episode reward: total was -16.260000. running mean: -17.399085\n",
      "ep 1929: ep_len:106 episode reward: total was -2.960000. running mean: -17.254694\n",
      "ep 1929: ep_len:500 episode reward: total was -6.310000. running mean: -17.145247\n",
      "ep 1929: ep_len:610 episode reward: total was -19.880000. running mean: -17.172595\n",
      "epsilon:0.136555 episode_count: 13510. steps_count: 5967576.000000\n",
      "ep 1930: ep_len:500 episode reward: total was -11.390000. running mean: -17.114769\n",
      "ep 1930: ep_len:680 episode reward: total was -30.740000. running mean: -17.251021\n",
      "ep 1930: ep_len:500 episode reward: total was -7.990000. running mean: -17.158411\n",
      "ep 1930: ep_len:510 episode reward: total was -16.070000. running mean: -17.147527\n",
      "ep 1930: ep_len:3 episode reward: total was 0.000000. running mean: -16.976051\n",
      "ep 1930: ep_len:500 episode reward: total was -20.280000. running mean: -17.009091\n",
      "ep 1930: ep_len:635 episode reward: total was -24.390000. running mean: -17.082900\n",
      "epsilon:0.136418 episode_count: 13517. steps_count: 5970904.000000\n",
      "ep 1931: ep_len:635 episode reward: total was -54.510000. running mean: -17.457171\n",
      "ep 1931: ep_len:500 episode reward: total was -41.600000. running mean: -17.698599\n",
      "ep 1931: ep_len:595 episode reward: total was -12.470000. running mean: -17.646313\n",
      "ep 1931: ep_len:500 episode reward: total was -12.490000. running mean: -17.594750\n",
      "ep 1931: ep_len:87 episode reward: total was 3.550000. running mean: -17.383303\n",
      "ep 1931: ep_len:545 episode reward: total was -26.460000. running mean: -17.474070\n",
      "ep 1931: ep_len:570 episode reward: total was -18.920000. running mean: -17.488529\n",
      "epsilon:0.136282 episode_count: 13524. steps_count: 5974336.000000\n",
      "ep 1932: ep_len:570 episode reward: total was -10.490000. running mean: -17.418544\n",
      "ep 1932: ep_len:274 episode reward: total was -27.890000. running mean: -17.523258\n",
      "ep 1932: ep_len:66 episode reward: total was -3.960000. running mean: -17.387626\n",
      "ep 1932: ep_len:101 episode reward: total was 0.090000. running mean: -17.212849\n",
      "ep 1932: ep_len:3 episode reward: total was 0.000000. running mean: -17.040721\n",
      "ep 1932: ep_len:500 episode reward: total was -12.780000. running mean: -16.998114\n",
      "ep 1932: ep_len:520 episode reward: total was -36.570000. running mean: -17.193833\n",
      "epsilon:0.136145 episode_count: 13531. steps_count: 5976370.000000\n",
      "ep 1933: ep_len:500 episode reward: total was -19.190000. running mean: -17.213794\n",
      "ep 1933: ep_len:319 episode reward: total was -25.350000. running mean: -17.295156\n",
      "ep 1933: ep_len:620 episode reward: total was -17.070000. running mean: -17.292905\n",
      "ep 1933: ep_len:505 episode reward: total was -19.570000. running mean: -17.315676\n",
      "ep 1933: ep_len:91 episode reward: total was 7.020000. running mean: -17.072319\n",
      "ep 1933: ep_len:500 episode reward: total was -20.300000. running mean: -17.104596\n",
      "ep 1933: ep_len:600 episode reward: total was -20.140000. running mean: -17.134950\n",
      "epsilon:0.136009 episode_count: 13538. steps_count: 5979505.000000\n",
      "ep 1934: ep_len:740 episode reward: total was -45.810000. running mean: -17.421700\n",
      "ep 1934: ep_len:545 episode reward: total was -34.100000. running mean: -17.588483\n",
      "ep 1934: ep_len:605 episode reward: total was -40.210000. running mean: -17.814698\n",
      "ep 1934: ep_len:36 episode reward: total was 0.040000. running mean: -17.636151\n",
      "ep 1934: ep_len:3 episode reward: total was 0.000000. running mean: -17.459790\n",
      "ep 1934: ep_len:595 episode reward: total was -5.550000. running mean: -17.340692\n",
      "ep 1934: ep_len:500 episode reward: total was -37.600000. running mean: -17.543285\n",
      "epsilon:0.135872 episode_count: 13545. steps_count: 5982529.000000\n",
      "ep 1935: ep_len:560 episode reward: total was -20.690000. running mean: -17.574752\n",
      "ep 1935: ep_len:520 episode reward: total was -24.970000. running mean: -17.648705\n",
      "ep 1935: ep_len:700 episode reward: total was -69.940000. running mean: -18.171618\n",
      "ep 1935: ep_len:505 episode reward: total was -30.040000. running mean: -18.290302\n",
      "ep 1935: ep_len:3 episode reward: total was 0.000000. running mean: -18.107399\n",
      "ep 1935: ep_len:680 episode reward: total was -58.900000. running mean: -18.515325\n",
      "ep 1935: ep_len:353 episode reward: total was -36.300000. running mean: -18.693171\n",
      "epsilon:0.135736 episode_count: 13552. steps_count: 5985850.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1936: ep_len:635 episode reward: total was -32.920000. running mean: -18.835440\n",
      "ep 1936: ep_len:930 episode reward: total was -96.640000. running mean: -19.613485\n",
      "ep 1936: ep_len:380 episode reward: total was -19.890000. running mean: -19.616250\n",
      "ep 1936: ep_len:500 episode reward: total was -12.050000. running mean: -19.540588\n",
      "ep 1936: ep_len:3 episode reward: total was 0.000000. running mean: -19.345182\n",
      "ep 1936: ep_len:580 episode reward: total was -9.620000. running mean: -19.247930\n",
      "ep 1936: ep_len:615 episode reward: total was -19.870000. running mean: -19.254151\n",
      "epsilon:0.135599 episode_count: 13559. steps_count: 5989493.000000\n",
      "ep 1937: ep_len:500 episode reward: total was -40.070000. running mean: -19.462309\n",
      "ep 1937: ep_len:530 episode reward: total was -35.020000. running mean: -19.617886\n",
      "ep 1937: ep_len:650 episode reward: total was -47.120000. running mean: -19.892907\n",
      "ep 1937: ep_len:575 episode reward: total was -1.480000. running mean: -19.708778\n",
      "ep 1937: ep_len:3 episode reward: total was 0.000000. running mean: -19.511691\n",
      "ep 1937: ep_len:625 episode reward: total was -20.020000. running mean: -19.516774\n",
      "ep 1937: ep_len:540 episode reward: total was -17.100000. running mean: -19.492606\n",
      "epsilon:0.135463 episode_count: 13566. steps_count: 5992916.000000\n",
      "ep 1938: ep_len:570 episode reward: total was -1.640000. running mean: -19.314080\n",
      "ep 1938: ep_len:590 episode reward: total was 7.320000. running mean: -19.047739\n",
      "ep 1938: ep_len:530 episode reward: total was -47.200000. running mean: -19.329262\n",
      "ep 1938: ep_len:510 episode reward: total was 0.980000. running mean: -19.126169\n",
      "ep 1938: ep_len:3 episode reward: total was 0.000000. running mean: -18.934907\n",
      "ep 1938: ep_len:500 episode reward: total was -28.330000. running mean: -19.028858\n",
      "ep 1938: ep_len:725 episode reward: total was -55.520000. running mean: -19.393770\n",
      "epsilon:0.135326 episode_count: 13573. steps_count: 5996344.000000\n",
      "ep 1939: ep_len:262 episode reward: total was -23.830000. running mean: -19.438132\n",
      "ep 1939: ep_len:615 episode reward: total was -60.010000. running mean: -19.843851\n",
      "ep 1939: ep_len:690 episode reward: total was -25.260000. running mean: -19.898012\n",
      "ep 1939: ep_len:590 episode reward: total was -5.970000. running mean: -19.758732\n",
      "ep 1939: ep_len:111 episode reward: total was -10.460000. running mean: -19.665745\n",
      "ep 1939: ep_len:700 episode reward: total was -46.900000. running mean: -19.938087\n",
      "ep 1939: ep_len:565 episode reward: total was -20.470000. running mean: -19.943406\n",
      "epsilon:0.135190 episode_count: 13580. steps_count: 5999877.000000\n",
      "ep 1940: ep_len:605 episode reward: total was -20.110000. running mean: -19.945072\n",
      "ep 1940: ep_len:500 episode reward: total was -13.060000. running mean: -19.876222\n",
      "ep 1940: ep_len:620 episode reward: total was -29.610000. running mean: -19.973559\n",
      "ep 1940: ep_len:56 episode reward: total was -1.940000. running mean: -19.793224\n",
      "ep 1940: ep_len:3 episode reward: total was 0.000000. running mean: -19.595292\n",
      "ep 1940: ep_len:505 episode reward: total was -32.120000. running mean: -19.720539\n",
      "ep 1940: ep_len:630 episode reward: total was -20.550000. running mean: -19.728833\n",
      "epsilon:0.135053 episode_count: 13587. steps_count: 6002796.000000\n",
      "ep 1941: ep_len:600 episode reward: total was -18.490000. running mean: -19.716445\n",
      "ep 1941: ep_len:590 episode reward: total was -19.930000. running mean: -19.718580\n",
      "ep 1941: ep_len:64 episode reward: total was 1.540000. running mean: -19.505995\n",
      "ep 1941: ep_len:378 episode reward: total was -4.190000. running mean: -19.352835\n",
      "ep 1941: ep_len:3 episode reward: total was 0.000000. running mean: -19.159306\n",
      "ep 1941: ep_len:580 episode reward: total was -12.030000. running mean: -19.088013\n",
      "ep 1941: ep_len:319 episode reward: total was -14.320000. running mean: -19.040333\n",
      "epsilon:0.134917 episode_count: 13594. steps_count: 6005330.000000\n",
      "ep 1942: ep_len:615 episode reward: total was -3.070000. running mean: -18.880630\n",
      "ep 1942: ep_len:500 episode reward: total was -4.640000. running mean: -18.738224\n",
      "ep 1942: ep_len:575 episode reward: total was -42.480000. running mean: -18.975641\n",
      "ep 1942: ep_len:520 episode reward: total was -27.610000. running mean: -19.061985\n",
      "ep 1942: ep_len:3 episode reward: total was 0.000000. running mean: -18.871365\n",
      "ep 1942: ep_len:605 episode reward: total was -22.120000. running mean: -18.903851\n",
      "ep 1942: ep_len:321 episode reward: total was -17.370000. running mean: -18.888513\n",
      "epsilon:0.134780 episode_count: 13601. steps_count: 6008469.000000\n",
      "ep 1943: ep_len:221 episode reward: total was -4.370000. running mean: -18.743328\n",
      "ep 1943: ep_len:535 episode reward: total was -1.780000. running mean: -18.573694\n",
      "ep 1943: ep_len:560 episode reward: total was -24.150000. running mean: -18.629458\n",
      "ep 1943: ep_len:351 episode reward: total was -3.180000. running mean: -18.474963\n",
      "ep 1943: ep_len:134 episode reward: total was -14.950000. running mean: -18.439713\n",
      "ep 1943: ep_len:630 episode reward: total was -10.420000. running mean: -18.359516\n",
      "ep 1943: ep_len:500 episode reward: total was -25.440000. running mean: -18.430321\n",
      "epsilon:0.134644 episode_count: 13608. steps_count: 6011400.000000\n",
      "ep 1944: ep_len:600 episode reward: total was -10.500000. running mean: -18.351018\n",
      "ep 1944: ep_len:500 episode reward: total was 13.150000. running mean: -18.036008\n",
      "ep 1944: ep_len:605 episode reward: total was -5.600000. running mean: -17.911648\n",
      "ep 1944: ep_len:565 episode reward: total was -27.570000. running mean: -18.008231\n",
      "ep 1944: ep_len:3 episode reward: total was 0.000000. running mean: -17.828149\n",
      "ep 1944: ep_len:785 episode reward: total was -80.460000. running mean: -18.454467\n",
      "ep 1944: ep_len:585 episode reward: total was -20.450000. running mean: -18.474423\n",
      "epsilon:0.134507 episode_count: 13615. steps_count: 6015043.000000\n",
      "ep 1945: ep_len:250 episode reward: total was 2.130000. running mean: -18.268378\n",
      "ep 1945: ep_len:605 episode reward: total was -17.080000. running mean: -18.256495\n",
      "ep 1945: ep_len:665 episode reward: total was -39.850000. running mean: -18.472430\n",
      "ep 1945: ep_len:515 episode reward: total was -26.560000. running mean: -18.553305\n",
      "ep 1945: ep_len:51 episode reward: total was 5.000000. running mean: -18.317772\n",
      "ep 1945: ep_len:610 episode reward: total was -31.610000. running mean: -18.450695\n",
      "ep 1945: ep_len:515 episode reward: total was -19.440000. running mean: -18.460588\n",
      "epsilon:0.134371 episode_count: 13622. steps_count: 6018254.000000\n",
      "ep 1946: ep_len:575 episode reward: total was -13.490000. running mean: -18.410882\n",
      "ep 1946: ep_len:530 episode reward: total was -16.000000. running mean: -18.386773\n",
      "ep 1946: ep_len:500 episode reward: total was -28.710000. running mean: -18.490005\n",
      "ep 1946: ep_len:555 episode reward: total was -27.540000. running mean: -18.580505\n",
      "ep 1946: ep_len:3 episode reward: total was 0.000000. running mean: -18.394700\n",
      "ep 1946: ep_len:214 episode reward: total was 4.610000. running mean: -18.164653\n",
      "ep 1946: ep_len:595 episode reward: total was -31.510000. running mean: -18.298107\n",
      "epsilon:0.134234 episode_count: 13629. steps_count: 6021226.000000\n",
      "ep 1947: ep_len:255 episode reward: total was -5.850000. running mean: -18.173626\n",
      "ep 1947: ep_len:500 episode reward: total was -5.530000. running mean: -18.047189\n",
      "ep 1947: ep_len:500 episode reward: total was -11.640000. running mean: -17.983117\n",
      "ep 1947: ep_len:515 episode reward: total was -9.500000. running mean: -17.898286\n",
      "ep 1947: ep_len:89 episode reward: total was -3.450000. running mean: -17.753803\n",
      "ep 1947: ep_len:570 episode reward: total was -14.600000. running mean: -17.722265\n",
      "ep 1947: ep_len:560 episode reward: total was -45.190000. running mean: -17.996943\n",
      "epsilon:0.134098 episode_count: 13636. steps_count: 6024215.000000\n",
      "ep 1948: ep_len:610 episode reward: total was -27.760000. running mean: -18.094573\n",
      "ep 1948: ep_len:580 episode reward: total was -18.990000. running mean: -18.103527\n",
      "ep 1948: ep_len:555 episode reward: total was -44.620000. running mean: -18.368692\n",
      "ep 1948: ep_len:585 episode reward: total was -41.530000. running mean: -18.600305\n",
      "ep 1948: ep_len:3 episode reward: total was 0.000000. running mean: -18.414302\n",
      "ep 1948: ep_len:610 episode reward: total was -18.590000. running mean: -18.416059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1948: ep_len:500 episode reward: total was -27.660000. running mean: -18.508499\n",
      "epsilon:0.133961 episode_count: 13643. steps_count: 6027658.000000\n",
      "ep 1949: ep_len:209 episode reward: total was 3.110000. running mean: -18.292314\n",
      "ep 1949: ep_len:580 episode reward: total was -29.650000. running mean: -18.405891\n",
      "ep 1949: ep_len:500 episode reward: total was -10.000000. running mean: -18.321832\n",
      "ep 1949: ep_len:510 episode reward: total was -17.080000. running mean: -18.309413\n",
      "ep 1949: ep_len:97 episode reward: total was 1.550000. running mean: -18.110819\n",
      "ep 1949: ep_len:980 episode reward: total was -65.630000. running mean: -18.586011\n",
      "ep 1949: ep_len:515 episode reward: total was -29.910000. running mean: -18.699251\n",
      "epsilon:0.133825 episode_count: 13650. steps_count: 6031049.000000\n",
      "ep 1950: ep_len:600 episode reward: total was -5.450000. running mean: -18.566758\n",
      "ep 1950: ep_len:197 episode reward: total was 0.660000. running mean: -18.374491\n",
      "ep 1950: ep_len:630 episode reward: total was -15.070000. running mean: -18.341446\n",
      "ep 1950: ep_len:510 episode reward: total was -28.190000. running mean: -18.439931\n",
      "ep 1950: ep_len:3 episode reward: total was 0.000000. running mean: -18.255532\n",
      "ep 1950: ep_len:655 episode reward: total was -65.520000. running mean: -18.728177\n",
      "ep 1950: ep_len:310 episode reward: total was -14.320000. running mean: -18.684095\n",
      "epsilon:0.133688 episode_count: 13657. steps_count: 6033954.000000\n",
      "ep 1951: ep_len:500 episode reward: total was -25.850000. running mean: -18.755754\n",
      "ep 1951: ep_len:500 episode reward: total was -10.230000. running mean: -18.670496\n",
      "ep 1951: ep_len:540 episode reward: total was -22.520000. running mean: -18.708992\n",
      "ep 1951: ep_len:500 episode reward: total was -12.010000. running mean: -18.642002\n",
      "ep 1951: ep_len:3 episode reward: total was 0.000000. running mean: -18.455582\n",
      "ep 1951: ep_len:535 episode reward: total was -13.220000. running mean: -18.403226\n",
      "ep 1951: ep_len:585 episode reward: total was -17.550000. running mean: -18.394694\n",
      "epsilon:0.133552 episode_count: 13664. steps_count: 6037117.000000\n",
      "ep 1952: ep_len:500 episode reward: total was -14.120000. running mean: -18.351947\n",
      "ep 1952: ep_len:500 episode reward: total was -2.300000. running mean: -18.191427\n",
      "ep 1952: ep_len:595 episode reward: total was -21.610000. running mean: -18.225613\n",
      "ep 1952: ep_len:154 episode reward: total was -0.420000. running mean: -18.047557\n",
      "ep 1952: ep_len:3 episode reward: total was 0.000000. running mean: -17.867081\n",
      "ep 1952: ep_len:580 episode reward: total was -18.100000. running mean: -17.869410\n",
      "ep 1952: ep_len:500 episode reward: total was -33.660000. running mean: -18.027316\n",
      "epsilon:0.133415 episode_count: 13671. steps_count: 6039949.000000\n",
      "ep 1953: ep_len:132 episode reward: total was -3.430000. running mean: -17.881343\n",
      "ep 1953: ep_len:660 episode reward: total was 9.110000. running mean: -17.611430\n",
      "ep 1953: ep_len:474 episode reward: total was -7.230000. running mean: -17.507615\n",
      "ep 1953: ep_len:520 episode reward: total was -9.120000. running mean: -17.423739\n",
      "ep 1953: ep_len:91 episode reward: total was 5.030000. running mean: -17.199202\n",
      "ep 1953: ep_len:510 episode reward: total was -31.740000. running mean: -17.344610\n",
      "ep 1953: ep_len:530 episode reward: total was -23.010000. running mean: -17.401264\n",
      "epsilon:0.133279 episode_count: 13678. steps_count: 6042866.000000\n",
      "ep 1954: ep_len:520 episode reward: total was -8.580000. running mean: -17.313051\n",
      "ep 1954: ep_len:329 episode reward: total was -18.890000. running mean: -17.328821\n",
      "ep 1954: ep_len:645 episode reward: total was -18.900000. running mean: -17.344532\n",
      "ep 1954: ep_len:560 episode reward: total was -7.600000. running mean: -17.247087\n",
      "ep 1954: ep_len:129 episode reward: total was -11.440000. running mean: -17.189016\n",
      "ep 1954: ep_len:600 episode reward: total was -19.110000. running mean: -17.208226\n",
      "ep 1954: ep_len:500 episode reward: total was -26.280000. running mean: -17.298944\n",
      "epsilon:0.133142 episode_count: 13685. steps_count: 6046149.000000\n",
      "ep 1955: ep_len:590 episode reward: total was -8.970000. running mean: -17.215654\n",
      "ep 1955: ep_len:510 episode reward: total was -46.170000. running mean: -17.505198\n",
      "ep 1955: ep_len:625 episode reward: total was -25.070000. running mean: -17.580846\n",
      "ep 1955: ep_len:500 episode reward: total was -28.620000. running mean: -17.691237\n",
      "ep 1955: ep_len:3 episode reward: total was 0.000000. running mean: -17.514325\n",
      "ep 1955: ep_len:292 episode reward: total was -12.870000. running mean: -17.467882\n",
      "ep 1955: ep_len:252 episode reward: total was -9.330000. running mean: -17.386503\n",
      "epsilon:0.133006 episode_count: 13692. steps_count: 6048921.000000\n",
      "ep 1956: ep_len:500 episode reward: total was -11.160000. running mean: -17.324238\n",
      "ep 1956: ep_len:505 episode reward: total was -6.300000. running mean: -17.213995\n",
      "ep 1956: ep_len:645 episode reward: total was -47.430000. running mean: -17.516156\n",
      "ep 1956: ep_len:515 episode reward: total was -14.170000. running mean: -17.482694\n",
      "ep 1956: ep_len:81 episode reward: total was -11.960000. running mean: -17.427467\n",
      "ep 1956: ep_len:630 episode reward: total was -23.580000. running mean: -17.488992\n",
      "ep 1956: ep_len:505 episode reward: total was -33.010000. running mean: -17.644202\n",
      "epsilon:0.132869 episode_count: 13699. steps_count: 6052302.000000\n",
      "ep 1957: ep_len:555 episode reward: total was -9.660000. running mean: -17.564360\n",
      "ep 1957: ep_len:545 episode reward: total was -19.970000. running mean: -17.588417\n",
      "ep 1957: ep_len:595 episode reward: total was -17.130000. running mean: -17.583833\n",
      "ep 1957: ep_len:595 episode reward: total was -6.000000. running mean: -17.467994\n",
      "ep 1957: ep_len:87 episode reward: total was 1.530000. running mean: -17.278014\n",
      "ep 1957: ep_len:570 episode reward: total was -25.820000. running mean: -17.363434\n",
      "ep 1957: ep_len:505 episode reward: total was -17.740000. running mean: -17.367200\n",
      "epsilon:0.132733 episode_count: 13706. steps_count: 6055754.000000\n",
      "ep 1958: ep_len:500 episode reward: total was -1.690000. running mean: -17.210428\n",
      "ep 1958: ep_len:595 episode reward: total was -13.010000. running mean: -17.168424\n",
      "ep 1958: ep_len:510 episode reward: total was -13.390000. running mean: -17.130639\n",
      "ep 1958: ep_len:505 episode reward: total was -15.590000. running mean: -17.115233\n",
      "ep 1958: ep_len:3 episode reward: total was 0.000000. running mean: -16.944081\n",
      "ep 1958: ep_len:575 episode reward: total was -20.820000. running mean: -16.982840\n",
      "ep 1958: ep_len:535 episode reward: total was -31.980000. running mean: -17.132811\n",
      "epsilon:0.132596 episode_count: 13713. steps_count: 6058977.000000\n",
      "ep 1959: ep_len:615 episode reward: total was -27.990000. running mean: -17.241383\n",
      "ep 1959: ep_len:605 episode reward: total was -32.670000. running mean: -17.395669\n",
      "ep 1959: ep_len:390 episode reward: total was -9.300000. running mean: -17.314713\n",
      "ep 1959: ep_len:605 episode reward: total was -7.480000. running mean: -17.216366\n",
      "ep 1959: ep_len:92 episode reward: total was 4.050000. running mean: -17.003702\n",
      "ep 1959: ep_len:675 episode reward: total was -47.420000. running mean: -17.307865\n",
      "ep 1959: ep_len:560 episode reward: total was -8.600000. running mean: -17.220786\n",
      "epsilon:0.132460 episode_count: 13720. steps_count: 6062519.000000\n",
      "ep 1960: ep_len:560 episode reward: total was 1.930000. running mean: -17.029278\n",
      "ep 1960: ep_len:525 episode reward: total was -29.460000. running mean: -17.153586\n",
      "ep 1960: ep_len:500 episode reward: total was -4.570000. running mean: -17.027750\n",
      "ep 1960: ep_len:505 episode reward: total was 4.410000. running mean: -16.813372\n",
      "ep 1960: ep_len:3 episode reward: total was 0.000000. running mean: -16.645239\n",
      "ep 1960: ep_len:525 episode reward: total was -26.570000. running mean: -16.744486\n",
      "ep 1960: ep_len:500 episode reward: total was -16.460000. running mean: -16.741641\n",
      "epsilon:0.132323 episode_count: 13727. steps_count: 6065637.000000\n",
      "ep 1961: ep_len:500 episode reward: total was 7.710000. running mean: -16.497125\n",
      "ep 1961: ep_len:327 episode reward: total was -26.290000. running mean: -16.595054\n",
      "ep 1961: ep_len:505 episode reward: total was -7.570000. running mean: -16.504803\n",
      "ep 1961: ep_len:53 episode reward: total was 0.050000. running mean: -16.339255\n",
      "ep 1961: ep_len:56 episode reward: total was 5.500000. running mean: -16.120863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1961: ep_len:605 episode reward: total was -21.340000. running mean: -16.173054\n",
      "ep 1961: ep_len:555 episode reward: total was -24.380000. running mean: -16.255123\n",
      "epsilon:0.132187 episode_count: 13734. steps_count: 6068238.000000\n",
      "ep 1962: ep_len:243 episode reward: total was -4.330000. running mean: -16.135872\n",
      "ep 1962: ep_len:500 episode reward: total was -2.870000. running mean: -16.003213\n",
      "ep 1962: ep_len:670 episode reward: total was -27.460000. running mean: -16.117781\n",
      "ep 1962: ep_len:685 episode reward: total was -113.830000. running mean: -17.094904\n",
      "ep 1962: ep_len:23 episode reward: total was -2.500000. running mean: -16.948954\n",
      "ep 1962: ep_len:515 episode reward: total was -20.300000. running mean: -16.982465\n",
      "ep 1962: ep_len:194 episode reward: total was -8.360000. running mean: -16.896240\n",
      "epsilon:0.132050 episode_count: 13741. steps_count: 6071068.000000\n",
      "ep 1963: ep_len:680 episode reward: total was -20.290000. running mean: -16.930178\n",
      "ep 1963: ep_len:530 episode reward: total was -40.700000. running mean: -17.167876\n",
      "ep 1963: ep_len:625 episode reward: total was -29.510000. running mean: -17.291297\n",
      "ep 1963: ep_len:530 episode reward: total was -29.100000. running mean: -17.409384\n",
      "ep 1963: ep_len:96 episode reward: total was 6.050000. running mean: -17.174791\n",
      "ep 1963: ep_len:179 episode reward: total was -0.920000. running mean: -17.012243\n",
      "ep 1963: ep_len:585 episode reward: total was -22.930000. running mean: -17.071420\n",
      "epsilon:0.131914 episode_count: 13748. steps_count: 6074293.000000\n",
      "ep 1964: ep_len:605 episode reward: total was -88.240000. running mean: -17.783106\n",
      "ep 1964: ep_len:560 episode reward: total was 1.020000. running mean: -17.595075\n",
      "ep 1964: ep_len:580 episode reward: total was -7.030000. running mean: -17.489424\n",
      "ep 1964: ep_len:360 episode reward: total was -0.750000. running mean: -17.322030\n",
      "ep 1964: ep_len:71 episode reward: total was -3.460000. running mean: -17.183410\n",
      "ep 1964: ep_len:645 episode reward: total was -38.860000. running mean: -17.400176\n",
      "ep 1964: ep_len:525 episode reward: total was -39.660000. running mean: -17.622774\n",
      "epsilon:0.131777 episode_count: 13755. steps_count: 6077639.000000\n",
      "ep 1965: ep_len:228 episode reward: total was -2.400000. running mean: -17.470546\n",
      "ep 1965: ep_len:600 episode reward: total was -49.960000. running mean: -17.795441\n",
      "ep 1965: ep_len:580 episode reward: total was -72.100000. running mean: -18.338486\n",
      "ep 1965: ep_len:510 episode reward: total was -4.580000. running mean: -18.200901\n",
      "ep 1965: ep_len:109 episode reward: total was 5.030000. running mean: -17.968592\n",
      "ep 1965: ep_len:168 episode reward: total was -2.440000. running mean: -17.813306\n",
      "ep 1965: ep_len:500 episode reward: total was -21.270000. running mean: -17.847873\n",
      "epsilon:0.131641 episode_count: 13762. steps_count: 6080334.000000\n",
      "ep 1966: ep_len:500 episode reward: total was -1.200000. running mean: -17.681395\n",
      "ep 1966: ep_len:510 episode reward: total was -14.060000. running mean: -17.645181\n",
      "ep 1966: ep_len:670 episode reward: total was -30.260000. running mean: -17.771329\n",
      "ep 1966: ep_len:396 episode reward: total was -5.700000. running mean: -17.650616\n",
      "ep 1966: ep_len:3 episode reward: total was 0.000000. running mean: -17.474109\n",
      "ep 1966: ep_len:500 episode reward: total was -31.810000. running mean: -17.617468\n",
      "ep 1966: ep_len:500 episode reward: total was -9.070000. running mean: -17.531994\n",
      "epsilon:0.131504 episode_count: 13769. steps_count: 6083413.000000\n",
      "ep 1967: ep_len:900 episode reward: total was -129.300000. running mean: -18.649674\n",
      "ep 1967: ep_len:555 episode reward: total was -25.070000. running mean: -18.713877\n",
      "ep 1967: ep_len:79 episode reward: total was -0.970000. running mean: -18.536438\n",
      "ep 1967: ep_len:500 episode reward: total was -3.160000. running mean: -18.382674\n",
      "ep 1967: ep_len:3 episode reward: total was 0.000000. running mean: -18.198847\n",
      "ep 1967: ep_len:505 episode reward: total was -10.090000. running mean: -18.117759\n",
      "ep 1967: ep_len:585 episode reward: total was -27.070000. running mean: -18.207281\n",
      "epsilon:0.131368 episode_count: 13776. steps_count: 6086540.000000\n",
      "ep 1968: ep_len:505 episode reward: total was -17.690000. running mean: -18.202108\n",
      "ep 1968: ep_len:500 episode reward: total was -4.500000. running mean: -18.065087\n",
      "ep 1968: ep_len:810 episode reward: total was -41.790000. running mean: -18.302336\n",
      "ep 1968: ep_len:570 episode reward: total was -26.440000. running mean: -18.383713\n",
      "ep 1968: ep_len:3 episode reward: total was 0.000000. running mean: -18.199876\n",
      "ep 1968: ep_len:324 episode reward: total was -3.850000. running mean: -18.056377\n",
      "ep 1968: ep_len:520 episode reward: total was -25.130000. running mean: -18.127113\n",
      "epsilon:0.131231 episode_count: 13783. steps_count: 6089772.000000\n",
      "ep 1969: ep_len:242 episode reward: total was -6.370000. running mean: -18.009542\n",
      "ep 1969: ep_len:500 episode reward: total was -12.430000. running mean: -17.953747\n",
      "ep 1969: ep_len:590 episode reward: total was -21.820000. running mean: -17.992409\n",
      "ep 1969: ep_len:500 episode reward: total was -7.960000. running mean: -17.892085\n",
      "ep 1969: ep_len:3 episode reward: total was 0.000000. running mean: -17.713164\n",
      "ep 1969: ep_len:610 episode reward: total was -31.080000. running mean: -17.846833\n",
      "ep 1969: ep_len:500 episode reward: total was -32.510000. running mean: -17.993464\n",
      "epsilon:0.131095 episode_count: 13790. steps_count: 6092717.000000\n",
      "ep 1970: ep_len:500 episode reward: total was -9.310000. running mean: -17.906630\n",
      "ep 1970: ep_len:575 episode reward: total was -50.590000. running mean: -18.233463\n",
      "ep 1970: ep_len:645 episode reward: total was -17.790000. running mean: -18.229029\n",
      "ep 1970: ep_len:540 episode reward: total was -33.670000. running mean: -18.383438\n",
      "ep 1970: ep_len:3 episode reward: total was 0.000000. running mean: -18.199604\n",
      "ep 1970: ep_len:520 episode reward: total was -19.830000. running mean: -18.215908\n",
      "ep 1970: ep_len:500 episode reward: total was -34.200000. running mean: -18.375749\n",
      "epsilon:0.130958 episode_count: 13797. steps_count: 6096000.000000\n",
      "ep 1971: ep_len:755 episode reward: total was -42.320000. running mean: -18.615191\n",
      "ep 1971: ep_len:515 episode reward: total was 7.820000. running mean: -18.350840\n",
      "ep 1971: ep_len:550 episode reward: total was -16.800000. running mean: -18.335331\n",
      "ep 1971: ep_len:595 episode reward: total was 3.420000. running mean: -18.117778\n",
      "ep 1971: ep_len:3 episode reward: total was 0.000000. running mean: -17.936600\n",
      "ep 1971: ep_len:535 episode reward: total was -14.050000. running mean: -17.897734\n",
      "ep 1971: ep_len:560 episode reward: total was -21.800000. running mean: -17.936757\n",
      "epsilon:0.130822 episode_count: 13804. steps_count: 6099513.000000\n",
      "ep 1972: ep_len:650 episode reward: total was -38.510000. running mean: -18.142489\n",
      "ep 1972: ep_len:500 episode reward: total was 2.650000. running mean: -17.934564\n",
      "ep 1972: ep_len:415 episode reward: total was -6.780000. running mean: -17.823019\n",
      "ep 1972: ep_len:500 episode reward: total was -30.150000. running mean: -17.946288\n",
      "ep 1972: ep_len:3 episode reward: total was 0.000000. running mean: -17.766826\n",
      "ep 1972: ep_len:244 episode reward: total was -3.380000. running mean: -17.622957\n",
      "ep 1972: ep_len:545 episode reward: total was -19.970000. running mean: -17.646428\n",
      "epsilon:0.130685 episode_count: 13811. steps_count: 6102370.000000\n",
      "ep 1973: ep_len:500 episode reward: total was -22.320000. running mean: -17.693163\n",
      "ep 1973: ep_len:600 episode reward: total was -22.140000. running mean: -17.737632\n",
      "ep 1973: ep_len:610 episode reward: total was -17.960000. running mean: -17.739855\n",
      "ep 1973: ep_len:870 episode reward: total was -59.980000. running mean: -18.162257\n",
      "ep 1973: ep_len:3 episode reward: total was 0.000000. running mean: -17.980634\n",
      "ep 1973: ep_len:610 episode reward: total was -2.020000. running mean: -17.821028\n",
      "ep 1973: ep_len:540 episode reward: total was -18.020000. running mean: -17.823018\n",
      "epsilon:0.130549 episode_count: 13818. steps_count: 6106103.000000\n",
      "ep 1974: ep_len:229 episode reward: total was -2.430000. running mean: -17.669088\n",
      "ep 1974: ep_len:540 episode reward: total was 8.210000. running mean: -17.410297\n",
      "ep 1974: ep_len:540 episode reward: total was -17.430000. running mean: -17.410494\n",
      "ep 1974: ep_len:605 episode reward: total was -6.010000. running mean: -17.296489\n",
      "ep 1974: ep_len:3 episode reward: total was 0.000000. running mean: -17.123524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1974: ep_len:530 episode reward: total was -28.320000. running mean: -17.235489\n",
      "ep 1974: ep_len:263 episode reward: total was -0.850000. running mean: -17.071634\n",
      "epsilon:0.130412 episode_count: 13825. steps_count: 6108813.000000\n",
      "ep 1975: ep_len:550 episode reward: total was -8.100000. running mean: -16.981917\n",
      "ep 1975: ep_len:515 episode reward: total was -11.290000. running mean: -16.924998\n",
      "ep 1975: ep_len:595 episode reward: total was -17.460000. running mean: -16.930348\n",
      "ep 1975: ep_len:52 episode reward: total was -1.480000. running mean: -16.775845\n",
      "ep 1975: ep_len:88 episode reward: total was 5.050000. running mean: -16.557586\n",
      "ep 1975: ep_len:545 episode reward: total was -53.510000. running mean: -16.927110\n",
      "ep 1975: ep_len:675 episode reward: total was -46.630000. running mean: -17.224139\n",
      "epsilon:0.130276 episode_count: 13832. steps_count: 6111833.000000\n",
      "ep 1976: ep_len:655 episode reward: total was -27.230000. running mean: -17.324198\n",
      "ep 1976: ep_len:510 episode reward: total was -17.760000. running mean: -17.328556\n",
      "ep 1976: ep_len:580 episode reward: total was -27.850000. running mean: -17.433770\n",
      "ep 1976: ep_len:545 episode reward: total was -19.180000. running mean: -17.451233\n",
      "ep 1976: ep_len:65 episode reward: total was 2.040000. running mean: -17.256320\n",
      "ep 1976: ep_len:520 episode reward: total was -4.410000. running mean: -17.127857\n",
      "ep 1976: ep_len:520 episode reward: total was -23.900000. running mean: -17.195579\n",
      "epsilon:0.130139 episode_count: 13839. steps_count: 6115228.000000\n",
      "ep 1977: ep_len:665 episode reward: total was -59.910000. running mean: -17.622723\n",
      "ep 1977: ep_len:545 episode reward: total was -38.600000. running mean: -17.832496\n",
      "ep 1977: ep_len:680 episode reward: total was -40.270000. running mean: -18.056871\n",
      "ep 1977: ep_len:505 episode reward: total was 5.670000. running mean: -17.819602\n",
      "ep 1977: ep_len:3 episode reward: total was 0.000000. running mean: -17.641406\n",
      "ep 1977: ep_len:630 episode reward: total was -17.360000. running mean: -17.638592\n",
      "ep 1977: ep_len:545 episode reward: total was -20.150000. running mean: -17.663706\n",
      "epsilon:0.130003 episode_count: 13846. steps_count: 6118801.000000\n",
      "ep 1978: ep_len:735 episode reward: total was -52.650000. running mean: -18.013569\n",
      "ep 1978: ep_len:580 episode reward: total was 5.060000. running mean: -17.782833\n",
      "ep 1978: ep_len:555 episode reward: total was -17.910000. running mean: -17.784105\n",
      "ep 1978: ep_len:535 episode reward: total was -0.090000. running mean: -17.607164\n",
      "ep 1978: ep_len:111 episode reward: total was 7.060000. running mean: -17.360492\n",
      "ep 1978: ep_len:520 episode reward: total was -19.800000. running mean: -17.384887\n",
      "ep 1978: ep_len:520 episode reward: total was -6.760000. running mean: -17.278638\n",
      "epsilon:0.129866 episode_count: 13853. steps_count: 6122357.000000\n",
      "ep 1979: ep_len:500 episode reward: total was -0.110000. running mean: -17.106952\n",
      "ep 1979: ep_len:500 episode reward: total was -50.410000. running mean: -17.439982\n",
      "ep 1979: ep_len:560 episode reward: total was -14.350000. running mean: -17.409083\n",
      "ep 1979: ep_len:610 episode reward: total was -2.900000. running mean: -17.263992\n",
      "ep 1979: ep_len:49 episode reward: total was 1.500000. running mean: -17.076352\n",
      "ep 1979: ep_len:520 episode reward: total was -5.580000. running mean: -16.961388\n",
      "ep 1979: ep_len:595 episode reward: total was -19.880000. running mean: -16.990575\n",
      "epsilon:0.129730 episode_count: 13860. steps_count: 6125691.000000\n",
      "ep 1980: ep_len:134 episode reward: total was -15.970000. running mean: -16.980369\n",
      "ep 1980: ep_len:625 episode reward: total was -39.750000. running mean: -17.208065\n",
      "ep 1980: ep_len:420 episode reward: total was -10.810000. running mean: -17.144084\n",
      "ep 1980: ep_len:520 episode reward: total was -38.270000. running mean: -17.355344\n",
      "ep 1980: ep_len:3 episode reward: total was 0.000000. running mean: -17.181790\n",
      "ep 1980: ep_len:540 episode reward: total was -28.660000. running mean: -17.296572\n",
      "ep 1980: ep_len:570 episode reward: total was -21.400000. running mean: -17.337607\n",
      "epsilon:0.129593 episode_count: 13867. steps_count: 6128503.000000\n",
      "ep 1981: ep_len:600 episode reward: total was -29.030000. running mean: -17.454530\n",
      "ep 1981: ep_len:610 episode reward: total was -3.020000. running mean: -17.310185\n",
      "ep 1981: ep_len:545 episode reward: total was -1.600000. running mean: -17.153083\n",
      "ep 1981: ep_len:555 episode reward: total was -47.180000. running mean: -17.453352\n",
      "ep 1981: ep_len:3 episode reward: total was 0.000000. running mean: -17.278819\n",
      "ep 1981: ep_len:705 episode reward: total was -41.390000. running mean: -17.519931\n",
      "ep 1981: ep_len:560 episode reward: total was -14.930000. running mean: -17.494031\n",
      "epsilon:0.129457 episode_count: 13874. steps_count: 6132081.000000\n",
      "ep 1982: ep_len:630 episode reward: total was -19.980000. running mean: -17.518891\n",
      "ep 1982: ep_len:199 episode reward: total was -9.380000. running mean: -17.437502\n",
      "ep 1982: ep_len:424 episode reward: total was -3.780000. running mean: -17.300927\n",
      "ep 1982: ep_len:500 episode reward: total was -11.610000. running mean: -17.244018\n",
      "ep 1982: ep_len:3 episode reward: total was 0.000000. running mean: -17.071578\n",
      "ep 1982: ep_len:500 episode reward: total was -13.290000. running mean: -17.033762\n",
      "ep 1982: ep_len:500 episode reward: total was -35.750000. running mean: -17.220924\n",
      "epsilon:0.129320 episode_count: 13881. steps_count: 6134837.000000\n",
      "ep 1983: ep_len:515 episode reward: total was -23.990000. running mean: -17.288615\n",
      "ep 1983: ep_len:620 episode reward: total was -49.980000. running mean: -17.615529\n",
      "ep 1983: ep_len:590 episode reward: total was -28.900000. running mean: -17.728374\n",
      "ep 1983: ep_len:565 episode reward: total was 2.540000. running mean: -17.525690\n",
      "ep 1983: ep_len:3 episode reward: total was 0.000000. running mean: -17.350433\n",
      "ep 1983: ep_len:695 episode reward: total was -44.760000. running mean: -17.624529\n",
      "ep 1983: ep_len:500 episode reward: total was -12.650000. running mean: -17.574783\n",
      "epsilon:0.129184 episode_count: 13888. steps_count: 6138325.000000\n",
      "ep 1984: ep_len:580 episode reward: total was -3.410000. running mean: -17.433136\n",
      "ep 1984: ep_len:199 episode reward: total was -10.420000. running mean: -17.363004\n",
      "ep 1984: ep_len:500 episode reward: total was -6.690000. running mean: -17.256274\n",
      "ep 1984: ep_len:520 episode reward: total was -27.100000. running mean: -17.354711\n",
      "ep 1984: ep_len:56 episode reward: total was -11.000000. running mean: -17.291164\n",
      "ep 1984: ep_len:570 episode reward: total was -0.060000. running mean: -17.118853\n",
      "ep 1984: ep_len:500 episode reward: total was -12.560000. running mean: -17.073264\n",
      "epsilon:0.129047 episode_count: 13895. steps_count: 6141250.000000\n",
      "ep 1985: ep_len:500 episode reward: total was -4.750000. running mean: -16.950032\n",
      "ep 1985: ep_len:990 episode reward: total was -106.950000. running mean: -17.850031\n",
      "ep 1985: ep_len:525 episode reward: total was -9.130000. running mean: -17.762831\n",
      "ep 1985: ep_len:620 episode reward: total was -54.150000. running mean: -18.126703\n",
      "ep 1985: ep_len:3 episode reward: total was 0.000000. running mean: -17.945436\n",
      "ep 1985: ep_len:625 episode reward: total was -5.110000. running mean: -17.817081\n",
      "ep 1985: ep_len:500 episode reward: total was -14.230000. running mean: -17.781210\n",
      "epsilon:0.128911 episode_count: 13902. steps_count: 6145013.000000\n",
      "ep 1986: ep_len:500 episode reward: total was 7.820000. running mean: -17.525198\n",
      "ep 1986: ep_len:565 episode reward: total was -23.540000. running mean: -17.585346\n",
      "ep 1986: ep_len:380 episode reward: total was -40.390000. running mean: -17.813393\n",
      "ep 1986: ep_len:670 episode reward: total was -48.690000. running mean: -18.122159\n",
      "ep 1986: ep_len:98 episode reward: total was 3.540000. running mean: -17.905537\n",
      "ep 1986: ep_len:500 episode reward: total was -38.310000. running mean: -18.109582\n",
      "ep 1986: ep_len:535 episode reward: total was -21.100000. running mean: -18.139486\n",
      "epsilon:0.128774 episode_count: 13909. steps_count: 6148261.000000\n",
      "ep 1987: ep_len:216 episode reward: total was -27.380000. running mean: -18.231891\n",
      "ep 1987: ep_len:605 episode reward: total was -16.080000. running mean: -18.210372\n",
      "ep 1987: ep_len:595 episode reward: total was -22.490000. running mean: -18.253169\n",
      "ep 1987: ep_len:565 episode reward: total was -28.950000. running mean: -18.360137\n",
      "ep 1987: ep_len:3 episode reward: total was 0.000000. running mean: -18.176536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1987: ep_len:600 episode reward: total was -9.110000. running mean: -18.085870\n",
      "ep 1987: ep_len:289 episode reward: total was -7.770000. running mean: -17.982712\n",
      "epsilon:0.128638 episode_count: 13916. steps_count: 6151134.000000\n",
      "ep 1988: ep_len:131 episode reward: total was -2.970000. running mean: -17.832584\n",
      "ep 1988: ep_len:500 episode reward: total was -19.230000. running mean: -17.846559\n",
      "ep 1988: ep_len:580 episode reward: total was -16.300000. running mean: -17.831093\n",
      "ep 1988: ep_len:118 episode reward: total was -5.940000. running mean: -17.712182\n",
      "ep 1988: ep_len:85 episode reward: total was -11.950000. running mean: -17.654560\n",
      "ep 1988: ep_len:610 episode reward: total was -4.560000. running mean: -17.523615\n",
      "ep 1988: ep_len:505 episode reward: total was -33.560000. running mean: -17.683978\n",
      "epsilon:0.128501 episode_count: 13923. steps_count: 6153663.000000\n",
      "ep 1989: ep_len:560 episode reward: total was -3.130000. running mean: -17.538439\n",
      "ep 1989: ep_len:565 episode reward: total was -12.710000. running mean: -17.490154\n",
      "ep 1989: ep_len:505 episode reward: total was -4.090000. running mean: -17.356153\n",
      "ep 1989: ep_len:550 episode reward: total was -27.670000. running mean: -17.459291\n",
      "ep 1989: ep_len:65 episode reward: total was -4.980000. running mean: -17.334498\n",
      "ep 1989: ep_len:545 episode reward: total was -11.300000. running mean: -17.274153\n",
      "ep 1989: ep_len:545 episode reward: total was -20.950000. running mean: -17.310912\n",
      "epsilon:0.128365 episode_count: 13930. steps_count: 6156998.000000\n",
      "ep 1990: ep_len:123 episode reward: total was -0.940000. running mean: -17.147203\n",
      "ep 1990: ep_len:279 episode reward: total was -14.410000. running mean: -17.119831\n",
      "ep 1990: ep_len:79 episode reward: total was 0.040000. running mean: -16.948232\n",
      "ep 1990: ep_len:500 episode reward: total was -23.620000. running mean: -17.014950\n",
      "ep 1990: ep_len:3 episode reward: total was 0.000000. running mean: -16.844801\n",
      "ep 1990: ep_len:540 episode reward: total was -34.550000. running mean: -17.021853\n",
      "ep 1990: ep_len:515 episode reward: total was -35.150000. running mean: -17.203134\n",
      "epsilon:0.128228 episode_count: 13937. steps_count: 6159037.000000\n",
      "ep 1991: ep_len:590 episode reward: total was -9.910000. running mean: -17.130203\n",
      "ep 1991: ep_len:500 episode reward: total was -19.990000. running mean: -17.158801\n",
      "ep 1991: ep_len:352 episode reward: total was -2.310000. running mean: -17.010313\n",
      "ep 1991: ep_len:500 episode reward: total was -15.570000. running mean: -16.995909\n",
      "ep 1991: ep_len:3 episode reward: total was 0.000000. running mean: -16.825950\n",
      "ep 1991: ep_len:595 episode reward: total was -12.320000. running mean: -16.780891\n",
      "ep 1991: ep_len:500 episode reward: total was -24.040000. running mean: -16.853482\n",
      "epsilon:0.128092 episode_count: 13944. steps_count: 6162077.000000\n",
      "ep 1992: ep_len:500 episode reward: total was -7.480000. running mean: -16.759747\n",
      "ep 1992: ep_len:545 episode reward: total was 0.790000. running mean: -16.584250\n",
      "ep 1992: ep_len:665 episode reward: total was -41.970000. running mean: -16.838107\n",
      "ep 1992: ep_len:505 episode reward: total was -12.190000. running mean: -16.791626\n",
      "ep 1992: ep_len:103 episode reward: total was -10.450000. running mean: -16.728210\n",
      "ep 1992: ep_len:505 episode reward: total was -18.560000. running mean: -16.746528\n",
      "ep 1992: ep_len:333 episode reward: total was -12.330000. running mean: -16.702362\n",
      "epsilon:0.127955 episode_count: 13951. steps_count: 6165233.000000\n",
      "ep 1993: ep_len:565 episode reward: total was -1.600000. running mean: -16.551339\n",
      "ep 1993: ep_len:500 episode reward: total was -5.890000. running mean: -16.444725\n",
      "ep 1993: ep_len:500 episode reward: total was -14.030000. running mean: -16.420578\n",
      "ep 1993: ep_len:500 episode reward: total was -20.560000. running mean: -16.461972\n",
      "ep 1993: ep_len:114 episode reward: total was -10.450000. running mean: -16.401853\n",
      "ep 1993: ep_len:550 episode reward: total was -44.960000. running mean: -16.687434\n",
      "ep 1993: ep_len:515 episode reward: total was -66.280000. running mean: -17.183360\n",
      "epsilon:0.127819 episode_count: 13958. steps_count: 6168477.000000\n",
      "ep 1994: ep_len:500 episode reward: total was -58.340000. running mean: -17.594926\n",
      "ep 1994: ep_len:555 episode reward: total was -12.920000. running mean: -17.548177\n",
      "ep 1994: ep_len:600 episode reward: total was -20.790000. running mean: -17.580595\n",
      "ep 1994: ep_len:510 episode reward: total was -11.570000. running mean: -17.520489\n",
      "ep 1994: ep_len:3 episode reward: total was 0.000000. running mean: -17.345284\n",
      "ep 1994: ep_len:675 episode reward: total was -46.770000. running mean: -17.639532\n",
      "ep 1994: ep_len:600 episode reward: total was -30.000000. running mean: -17.763136\n",
      "epsilon:0.127682 episode_count: 13965. steps_count: 6171920.000000\n",
      "ep 1995: ep_len:525 episode reward: total was -36.000000. running mean: -17.945505\n",
      "ep 1995: ep_len:505 episode reward: total was -28.830000. running mean: -18.054350\n",
      "ep 1995: ep_len:575 episode reward: total was -19.160000. running mean: -18.065406\n",
      "ep 1995: ep_len:500 episode reward: total was -25.150000. running mean: -18.136252\n",
      "ep 1995: ep_len:3 episode reward: total was 0.000000. running mean: -17.954890\n",
      "ep 1995: ep_len:500 episode reward: total was -38.580000. running mean: -18.161141\n",
      "ep 1995: ep_len:595 episode reward: total was -22.550000. running mean: -18.205029\n",
      "epsilon:0.127546 episode_count: 13972. steps_count: 6175123.000000\n",
      "ep 1996: ep_len:580 episode reward: total was -0.570000. running mean: -18.028679\n",
      "ep 1996: ep_len:500 episode reward: total was -22.700000. running mean: -18.075392\n",
      "ep 1996: ep_len:575 episode reward: total was -15.100000. running mean: -18.045638\n",
      "ep 1996: ep_len:500 episode reward: total was -13.530000. running mean: -18.000482\n",
      "ep 1996: ep_len:102 episode reward: total was -7.440000. running mean: -17.894877\n",
      "ep 1996: ep_len:630 episode reward: total was -8.560000. running mean: -17.801528\n",
      "ep 1996: ep_len:540 episode reward: total was -19.420000. running mean: -17.817713\n",
      "epsilon:0.127409 episode_count: 13979. steps_count: 6178550.000000\n",
      "ep 1997: ep_len:575 episode reward: total was -3.610000. running mean: -17.675636\n",
      "ep 1997: ep_len:500 episode reward: total was 1.640000. running mean: -17.482480\n",
      "ep 1997: ep_len:600 episode reward: total was -14.770000. running mean: -17.455355\n",
      "ep 1997: ep_len:530 episode reward: total was -4.620000. running mean: -17.327001\n",
      "ep 1997: ep_len:3 episode reward: total was 0.000000. running mean: -17.153731\n",
      "ep 1997: ep_len:500 episode reward: total was -8.000000. running mean: -17.062194\n",
      "ep 1997: ep_len:555 episode reward: total was -40.980000. running mean: -17.301372\n",
      "epsilon:0.127273 episode_count: 13986. steps_count: 6181813.000000\n",
      "ep 1998: ep_len:605 episode reward: total was -31.020000. running mean: -17.438558\n",
      "ep 1998: ep_len:555 episode reward: total was 12.820000. running mean: -17.135973\n",
      "ep 1998: ep_len:379 episode reward: total was -25.870000. running mean: -17.223313\n",
      "ep 1998: ep_len:505 episode reward: total was -15.180000. running mean: -17.202880\n",
      "ep 1998: ep_len:3 episode reward: total was 0.000000. running mean: -17.030851\n",
      "ep 1998: ep_len:515 episode reward: total was -7.640000. running mean: -16.936943\n",
      "ep 1998: ep_len:600 episode reward: total was -17.250000. running mean: -16.940073\n",
      "epsilon:0.127136 episode_count: 13993. steps_count: 6184975.000000\n",
      "ep 1999: ep_len:500 episode reward: total was -9.800000. running mean: -16.868672\n",
      "ep 1999: ep_len:535 episode reward: total was -9.990000. running mean: -16.799886\n",
      "ep 1999: ep_len:51 episode reward: total was 0.540000. running mean: -16.626487\n",
      "ep 1999: ep_len:600 episode reward: total was -12.560000. running mean: -16.585822\n",
      "ep 1999: ep_len:3 episode reward: total was 0.000000. running mean: -16.419964\n",
      "ep 1999: ep_len:535 episode reward: total was -20.490000. running mean: -16.460664\n",
      "ep 1999: ep_len:500 episode reward: total was -21.020000. running mean: -16.506257\n",
      "epsilon:0.127000 episode_count: 14000. steps_count: 6187699.000000\n",
      "ep 2000: ep_len:655 episode reward: total was -22.730000. running mean: -16.568495\n",
      "ep 2000: ep_len:196 episode reward: total was -5.890000. running mean: -16.461710\n",
      "ep 2000: ep_len:570 episode reward: total was -8.030000. running mean: -16.377393\n",
      "ep 2000: ep_len:500 episode reward: total was -2.490000. running mean: -16.238519\n",
      "ep 2000: ep_len:3 episode reward: total was 0.000000. running mean: -16.076134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2000: ep_len:540 episode reward: total was -22.960000. running mean: -16.144972\n",
      "ep 2000: ep_len:580 episode reward: total was -33.120000. running mean: -16.314723\n",
      "epsilon:0.126863 episode_count: 14007. steps_count: 6190743.000000\n",
      "ep 2001: ep_len:625 episode reward: total was -24.290000. running mean: -16.394475\n",
      "ep 2001: ep_len:620 episode reward: total was -16.180000. running mean: -16.392331\n",
      "ep 2001: ep_len:505 episode reward: total was -12.980000. running mean: -16.358207\n",
      "ep 2001: ep_len:500 episode reward: total was -25.610000. running mean: -16.450725\n",
      "ep 2001: ep_len:3 episode reward: total was 0.000000. running mean: -16.286218\n",
      "ep 2001: ep_len:585 episode reward: total was -2.460000. running mean: -16.147956\n",
      "ep 2001: ep_len:535 episode reward: total was -10.800000. running mean: -16.094476\n",
      "epsilon:0.126727 episode_count: 14014. steps_count: 6194116.000000\n",
      "ep 2002: ep_len:196 episode reward: total was 4.580000. running mean: -15.887732\n",
      "ep 2002: ep_len:184 episode reward: total was -8.920000. running mean: -15.818054\n",
      "ep 2002: ep_len:600 episode reward: total was -5.090000. running mean: -15.710774\n",
      "ep 2002: ep_len:540 episode reward: total was -27.610000. running mean: -15.829766\n",
      "ep 2002: ep_len:3 episode reward: total was 0.000000. running mean: -15.671468\n",
      "ep 2002: ep_len:500 episode reward: total was -14.750000. running mean: -15.662254\n",
      "ep 2002: ep_len:545 episode reward: total was -16.570000. running mean: -15.671331\n",
      "epsilon:0.126590 episode_count: 14021. steps_count: 6196684.000000\n",
      "ep 2003: ep_len:570 episode reward: total was -25.300000. running mean: -15.767618\n",
      "ep 2003: ep_len:287 episode reward: total was -10.840000. running mean: -15.718342\n",
      "ep 2003: ep_len:565 episode reward: total was -9.470000. running mean: -15.655858\n",
      "ep 2003: ep_len:540 episode reward: total was -30.110000. running mean: -15.800400\n",
      "ep 2003: ep_len:100 episode reward: total was -0.480000. running mean: -15.647196\n",
      "ep 2003: ep_len:500 episode reward: total was -1.710000. running mean: -15.507824\n",
      "ep 2003: ep_len:545 episode reward: total was -23.940000. running mean: -15.592145\n",
      "epsilon:0.126454 episode_count: 14028. steps_count: 6199791.000000\n",
      "ep 2004: ep_len:575 episode reward: total was -20.240000. running mean: -15.638624\n",
      "ep 2004: ep_len:505 episode reward: total was -8.360000. running mean: -15.565838\n",
      "ep 2004: ep_len:635 episode reward: total was -44.060000. running mean: -15.850779\n",
      "ep 2004: ep_len:555 episode reward: total was -16.650000. running mean: -15.858772\n",
      "ep 2004: ep_len:127 episode reward: total was -14.440000. running mean: -15.844584\n",
      "ep 2004: ep_len:500 episode reward: total was -5.740000. running mean: -15.743538\n",
      "ep 2004: ep_len:194 episode reward: total was -9.420000. running mean: -15.680303\n",
      "epsilon:0.126317 episode_count: 14035. steps_count: 6202882.000000\n",
      "ep 2005: ep_len:213 episode reward: total was 0.110000. running mean: -15.522400\n",
      "ep 2005: ep_len:630 episode reward: total was -10.040000. running mean: -15.467576\n",
      "ep 2005: ep_len:595 episode reward: total was -24.760000. running mean: -15.560500\n",
      "ep 2005: ep_len:402 episode reward: total was -34.720000. running mean: -15.752095\n",
      "ep 2005: ep_len:49 episode reward: total was 4.500000. running mean: -15.549574\n",
      "ep 2005: ep_len:500 episode reward: total was -1.120000. running mean: -15.405278\n",
      "ep 2005: ep_len:515 episode reward: total was -23.450000. running mean: -15.485725\n",
      "epsilon:0.126181 episode_count: 14042. steps_count: 6205786.000000\n",
      "ep 2006: ep_len:555 episode reward: total was -38.920000. running mean: -15.720068\n",
      "ep 2006: ep_len:600 episode reward: total was 6.560000. running mean: -15.497267\n",
      "ep 2006: ep_len:700 episode reward: total was -12.140000. running mean: -15.463695\n",
      "ep 2006: ep_len:575 episode reward: total was -57.180000. running mean: -15.880858\n",
      "ep 2006: ep_len:3 episode reward: total was 0.000000. running mean: -15.722049\n",
      "ep 2006: ep_len:705 episode reward: total was -8.610000. running mean: -15.650929\n",
      "ep 2006: ep_len:575 episode reward: total was -37.630000. running mean: -15.870719\n",
      "epsilon:0.126044 episode_count: 14049. steps_count: 6209499.000000\n",
      "ep 2007: ep_len:610 episode reward: total was -33.150000. running mean: -16.043512\n",
      "ep 2007: ep_len:565 episode reward: total was -22.160000. running mean: -16.104677\n",
      "ep 2007: ep_len:580 episode reward: total was -11.460000. running mean: -16.058230\n",
      "ep 2007: ep_len:131 episode reward: total was 4.110000. running mean: -15.856548\n",
      "ep 2007: ep_len:3 episode reward: total was 0.000000. running mean: -15.697983\n",
      "ep 2007: ep_len:246 episode reward: total was 4.650000. running mean: -15.494503\n",
      "ep 2007: ep_len:625 episode reward: total was -18.870000. running mean: -15.528258\n",
      "epsilon:0.125908 episode_count: 14056. steps_count: 6212259.000000\n",
      "ep 2008: ep_len:595 episode reward: total was -0.050000. running mean: -15.373475\n",
      "ep 2008: ep_len:201 episode reward: total was -5.910000. running mean: -15.278840\n",
      "ep 2008: ep_len:500 episode reward: total was -1.030000. running mean: -15.136352\n",
      "ep 2008: ep_len:525 episode reward: total was -5.150000. running mean: -15.036488\n",
      "ep 2008: ep_len:109 episode reward: total was 3.530000. running mean: -14.850824\n",
      "ep 2008: ep_len:555 episode reward: total was -26.540000. running mean: -14.967715\n",
      "ep 2008: ep_len:570 episode reward: total was -31.880000. running mean: -15.136838\n",
      "epsilon:0.125771 episode_count: 14063. steps_count: 6215314.000000\n",
      "ep 2009: ep_len:555 episode reward: total was -21.290000. running mean: -15.198370\n",
      "ep 2009: ep_len:500 episode reward: total was -7.320000. running mean: -15.119586\n",
      "ep 2009: ep_len:545 episode reward: total was -20.330000. running mean: -15.171690\n",
      "ep 2009: ep_len:585 episode reward: total was -6.010000. running mean: -15.080073\n",
      "ep 2009: ep_len:3 episode reward: total was 0.000000. running mean: -14.929273\n",
      "ep 2009: ep_len:645 episode reward: total was -8.240000. running mean: -14.862380\n",
      "ep 2009: ep_len:550 episode reward: total was -39.160000. running mean: -15.105356\n",
      "epsilon:0.125635 episode_count: 14070. steps_count: 6218697.000000\n",
      "ep 2010: ep_len:241 episode reward: total was -21.330000. running mean: -15.167603\n",
      "ep 2010: ep_len:525 episode reward: total was 7.350000. running mean: -14.942427\n",
      "ep 2010: ep_len:500 episode reward: total was -7.920000. running mean: -14.872202\n",
      "ep 2010: ep_len:500 episode reward: total was -20.560000. running mean: -14.929080\n",
      "ep 2010: ep_len:3 episode reward: total was 0.000000. running mean: -14.779789\n",
      "ep 2010: ep_len:535 episode reward: total was -28.220000. running mean: -14.914192\n",
      "ep 2010: ep_len:540 episode reward: total was -20.460000. running mean: -14.969650\n",
      "epsilon:0.125498 episode_count: 14077. steps_count: 6221541.000000\n",
      "ep 2011: ep_len:640 episode reward: total was -19.010000. running mean: -15.010053\n",
      "ep 2011: ep_len:590 episode reward: total was -15.010000. running mean: -15.010053\n",
      "ep 2011: ep_len:575 episode reward: total was -21.390000. running mean: -15.073852\n",
      "ep 2011: ep_len:515 episode reward: total was -46.220000. running mean: -15.385314\n",
      "ep 2011: ep_len:32 episode reward: total was 0.500000. running mean: -15.226460\n",
      "ep 2011: ep_len:555 episode reward: total was -14.620000. running mean: -15.220396\n",
      "ep 2011: ep_len:570 episode reward: total was -24.050000. running mean: -15.308692\n",
      "epsilon:0.125362 episode_count: 14084. steps_count: 6225018.000000\n",
      "ep 2012: ep_len:540 episode reward: total was -36.180000. running mean: -15.517405\n",
      "ep 2012: ep_len:500 episode reward: total was -7.230000. running mean: -15.434531\n",
      "ep 2012: ep_len:585 episode reward: total was -14.220000. running mean: -15.422386\n",
      "ep 2012: ep_len:590 episode reward: total was -66.180000. running mean: -15.929962\n",
      "ep 2012: ep_len:3 episode reward: total was 0.000000. running mean: -15.770662\n",
      "ep 2012: ep_len:555 episode reward: total was -9.290000. running mean: -15.705855\n",
      "ep 2012: ep_len:302 episode reward: total was -16.380000. running mean: -15.712597\n",
      "epsilon:0.125225 episode_count: 14091. steps_count: 6228093.000000\n",
      "ep 2013: ep_len:645 episode reward: total was -29.270000. running mean: -15.848171\n",
      "ep 2013: ep_len:625 episode reward: total was -38.920000. running mean: -16.078889\n",
      "ep 2013: ep_len:500 episode reward: total was -42.800000. running mean: -16.346100\n",
      "ep 2013: ep_len:620 episode reward: total was -53.460000. running mean: -16.717239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2013: ep_len:113 episode reward: total was 5.530000. running mean: -16.494767\n",
      "ep 2013: ep_len:675 episode reward: total was -32.880000. running mean: -16.658619\n",
      "ep 2013: ep_len:530 episode reward: total was -33.490000. running mean: -16.826933\n",
      "epsilon:0.125089 episode_count: 14098. steps_count: 6231801.000000\n",
      "ep 2014: ep_len:221 episode reward: total was -8.920000. running mean: -16.747864\n",
      "ep 2014: ep_len:510 episode reward: total was -8.400000. running mean: -16.664385\n",
      "ep 2014: ep_len:389 episode reward: total was 0.730000. running mean: -16.490441\n",
      "ep 2014: ep_len:610 episode reward: total was -9.980000. running mean: -16.425337\n",
      "ep 2014: ep_len:52 episode reward: total was 2.000000. running mean: -16.241083\n",
      "ep 2014: ep_len:610 episode reward: total was -15.080000. running mean: -16.229473\n",
      "ep 2014: ep_len:500 episode reward: total was -10.000000. running mean: -16.167178\n",
      "epsilon:0.124952 episode_count: 14105. steps_count: 6234693.000000\n",
      "ep 2015: ep_len:565 episode reward: total was -12.480000. running mean: -16.130306\n",
      "ep 2015: ep_len:600 episode reward: total was 4.550000. running mean: -15.923503\n",
      "ep 2015: ep_len:575 episode reward: total was -3.740000. running mean: -15.801668\n",
      "ep 2015: ep_len:500 episode reward: total was -11.180000. running mean: -15.755451\n",
      "ep 2015: ep_len:52 episode reward: total was 3.500000. running mean: -15.562897\n",
      "ep 2015: ep_len:255 episode reward: total was -2.360000. running mean: -15.430868\n",
      "ep 2015: ep_len:580 episode reward: total was -12.540000. running mean: -15.401959\n",
      "epsilon:0.124816 episode_count: 14112. steps_count: 6237820.000000\n",
      "ep 2016: ep_len:635 episode reward: total was -18.980000. running mean: -15.437740\n",
      "ep 2016: ep_len:305 episode reward: total was -39.840000. running mean: -15.681762\n",
      "ep 2016: ep_len:695 episode reward: total was -12.700000. running mean: -15.651945\n",
      "ep 2016: ep_len:500 episode reward: total was -29.680000. running mean: -15.792225\n",
      "ep 2016: ep_len:86 episode reward: total was 7.010000. running mean: -15.564203\n",
      "ep 2016: ep_len:550 episode reward: total was -6.540000. running mean: -15.473961\n",
      "ep 2016: ep_len:515 episode reward: total was -34.660000. running mean: -15.665821\n",
      "epsilon:0.124679 episode_count: 14119. steps_count: 6241106.000000\n",
      "ep 2017: ep_len:525 episode reward: total was -20.510000. running mean: -15.714263\n",
      "ep 2017: ep_len:201 episode reward: total was -3.890000. running mean: -15.596020\n",
      "ep 2017: ep_len:64 episode reward: total was -6.480000. running mean: -15.504860\n",
      "ep 2017: ep_len:605 episode reward: total was -12.990000. running mean: -15.479712\n",
      "ep 2017: ep_len:90 episode reward: total was 1.020000. running mean: -15.314714\n",
      "ep 2017: ep_len:500 episode reward: total was 5.400000. running mean: -15.107567\n",
      "ep 2017: ep_len:505 episode reward: total was -24.070000. running mean: -15.197192\n",
      "epsilon:0.124543 episode_count: 14126. steps_count: 6243596.000000\n",
      "ep 2018: ep_len:545 episode reward: total was -15.470000. running mean: -15.199920\n",
      "ep 2018: ep_len:500 episode reward: total was -18.050000. running mean: -15.228421\n",
      "ep 2018: ep_len:500 episode reward: total was -24.340000. running mean: -15.319536\n",
      "ep 2018: ep_len:530 episode reward: total was -17.560000. running mean: -15.341941\n",
      "ep 2018: ep_len:89 episode reward: total was -12.960000. running mean: -15.318122\n",
      "ep 2018: ep_len:230 episode reward: total was -1.840000. running mean: -15.183340\n",
      "ep 2018: ep_len:300 episode reward: total was -12.830000. running mean: -15.159807\n",
      "epsilon:0.124406 episode_count: 14133. steps_count: 6246290.000000\n",
      "ep 2019: ep_len:225 episode reward: total was -2.450000. running mean: -15.032709\n",
      "ep 2019: ep_len:575 episode reward: total was -6.480000. running mean: -14.947182\n",
      "ep 2019: ep_len:560 episode reward: total was -41.950000. running mean: -15.217210\n",
      "ep 2019: ep_len:500 episode reward: total was 0.370000. running mean: -15.061338\n",
      "ep 2019: ep_len:110 episode reward: total was -1.450000. running mean: -14.925225\n",
      "ep 2019: ep_len:630 episode reward: total was -8.440000. running mean: -14.860372\n",
      "ep 2019: ep_len:525 episode reward: total was -17.920000. running mean: -14.890969\n",
      "epsilon:0.124270 episode_count: 14140. steps_count: 6249415.000000\n",
      "ep 2020: ep_len:595 episode reward: total was -19.050000. running mean: -14.932559\n",
      "ep 2020: ep_len:500 episode reward: total was -7.500000. running mean: -14.858233\n",
      "ep 2020: ep_len:560 episode reward: total was -27.620000. running mean: -14.985851\n",
      "ep 2020: ep_len:404 episode reward: total was -11.210000. running mean: -14.948092\n",
      "ep 2020: ep_len:78 episode reward: total was 3.530000. running mean: -14.763311\n",
      "ep 2020: ep_len:500 episode reward: total was -25.300000. running mean: -14.868678\n",
      "ep 2020: ep_len:580 episode reward: total was -32.900000. running mean: -15.048992\n",
      "epsilon:0.124133 episode_count: 14147. steps_count: 6252632.000000\n",
      "ep 2021: ep_len:665 episode reward: total was -25.280000. running mean: -15.151302\n",
      "ep 2021: ep_len:157 episode reward: total was -6.920000. running mean: -15.068989\n",
      "ep 2021: ep_len:401 episode reward: total was -7.810000. running mean: -14.996399\n",
      "ep 2021: ep_len:35 episode reward: total was 0.040000. running mean: -14.846035\n",
      "ep 2021: ep_len:114 episode reward: total was 2.530000. running mean: -14.672274\n",
      "ep 2021: ep_len:500 episode reward: total was -10.630000. running mean: -14.631852\n",
      "ep 2021: ep_len:555 episode reward: total was -17.620000. running mean: -14.661733\n",
      "epsilon:0.123997 episode_count: 14154. steps_count: 6255059.000000\n",
      "ep 2022: ep_len:570 episode reward: total was -8.130000. running mean: -14.596416\n",
      "ep 2022: ep_len:515 episode reward: total was -19.150000. running mean: -14.641952\n",
      "ep 2022: ep_len:505 episode reward: total was -41.790000. running mean: -14.913432\n",
      "ep 2022: ep_len:530 episode reward: total was 1.890000. running mean: -14.745398\n",
      "ep 2022: ep_len:3 episode reward: total was 0.000000. running mean: -14.597944\n",
      "ep 2022: ep_len:530 episode reward: total was -25.900000. running mean: -14.710964\n",
      "ep 2022: ep_len:625 episode reward: total was -15.840000. running mean: -14.722255\n",
      "epsilon:0.123860 episode_count: 14161. steps_count: 6258337.000000\n",
      "ep 2023: ep_len:530 episode reward: total was -12.500000. running mean: -14.700032\n",
      "ep 2023: ep_len:640 episode reward: total was -44.960000. running mean: -15.002632\n",
      "ep 2023: ep_len:387 episode reward: total was -10.830000. running mean: -14.960906\n",
      "ep 2023: ep_len:125 episode reward: total was -1.390000. running mean: -14.825197\n",
      "ep 2023: ep_len:3 episode reward: total was 0.000000. running mean: -14.676945\n",
      "ep 2023: ep_len:555 episode reward: total was -28.850000. running mean: -14.818675\n",
      "ep 2023: ep_len:565 episode reward: total was -10.820000. running mean: -14.778688\n",
      "epsilon:0.123724 episode_count: 14168. steps_count: 6261142.000000\n",
      "ep 2024: ep_len:134 episode reward: total was -0.430000. running mean: -14.635202\n",
      "ep 2024: ep_len:620 episode reward: total was 5.850000. running mean: -14.430349\n",
      "ep 2024: ep_len:520 episode reward: total was -30.270000. running mean: -14.588746\n",
      "ep 2024: ep_len:560 episode reward: total was -29.420000. running mean: -14.737059\n",
      "ep 2024: ep_len:3 episode reward: total was 0.000000. running mean: -14.589688\n",
      "ep 2024: ep_len:500 episode reward: total was -21.530000. running mean: -14.659091\n",
      "ep 2024: ep_len:595 episode reward: total was -19.900000. running mean: -14.711500\n",
      "epsilon:0.123587 episode_count: 14175. steps_count: 6264074.000000\n",
      "ep 2025: ep_len:555 episode reward: total was -36.220000. running mean: -14.926585\n",
      "ep 2025: ep_len:590 episode reward: total was 5.360000. running mean: -14.723719\n",
      "ep 2025: ep_len:360 episode reward: total was -17.820000. running mean: -14.754682\n",
      "ep 2025: ep_len:545 episode reward: total was -1.070000. running mean: -14.617835\n",
      "ep 2025: ep_len:3 episode reward: total was 0.000000. running mean: -14.471657\n",
      "ep 2025: ep_len:500 episode reward: total was -3.630000. running mean: -14.363240\n",
      "ep 2025: ep_len:600 episode reward: total was -40.530000. running mean: -14.624908\n",
      "epsilon:0.123451 episode_count: 14182. steps_count: 6267227.000000\n",
      "ep 2026: ep_len:595 episode reward: total was -23.370000. running mean: -14.712359\n",
      "ep 2026: ep_len:575 episode reward: total was -15.940000. running mean: -14.724635\n",
      "ep 2026: ep_len:640 episode reward: total was -17.770000. running mean: -14.755089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2026: ep_len:605 episode reward: total was -18.040000. running mean: -14.787938\n",
      "ep 2026: ep_len:3 episode reward: total was 0.000000. running mean: -14.640059\n",
      "ep 2026: ep_len:635 episode reward: total was -10.450000. running mean: -14.598158\n",
      "ep 2026: ep_len:525 episode reward: total was -6.900000. running mean: -14.521177\n",
      "epsilon:0.123314 episode_count: 14189. steps_count: 6270805.000000\n",
      "ep 2027: ep_len:600 episode reward: total was 6.440000. running mean: -14.311565\n",
      "ep 2027: ep_len:500 episode reward: total was -3.000000. running mean: -14.198449\n",
      "ep 2027: ep_len:665 episode reward: total was -30.850000. running mean: -14.364965\n",
      "ep 2027: ep_len:500 episode reward: total was -30.210000. running mean: -14.523415\n",
      "ep 2027: ep_len:3 episode reward: total was 0.000000. running mean: -14.378181\n",
      "ep 2027: ep_len:505 episode reward: total was -38.860000. running mean: -14.622999\n",
      "ep 2027: ep_len:590 episode reward: total was -35.560000. running mean: -14.832369\n",
      "epsilon:0.123178 episode_count: 14196. steps_count: 6274168.000000\n",
      "ep 2028: ep_len:540 episode reward: total was -16.960000. running mean: -14.853645\n",
      "ep 2028: ep_len:610 episode reward: total was -15.010000. running mean: -14.855209\n",
      "ep 2028: ep_len:515 episode reward: total was -19.550000. running mean: -14.902157\n",
      "ep 2028: ep_len:550 episode reward: total was -2.650000. running mean: -14.779635\n",
      "ep 2028: ep_len:3 episode reward: total was 0.000000. running mean: -14.631839\n",
      "ep 2028: ep_len:500 episode reward: total was -21.830000. running mean: -14.703820\n",
      "ep 2028: ep_len:590 episode reward: total was -22.020000. running mean: -14.776982\n",
      "epsilon:0.123041 episode_count: 14203. steps_count: 6277476.000000\n",
      "ep 2029: ep_len:755 episode reward: total was -47.260000. running mean: -15.101812\n",
      "ep 2029: ep_len:650 episode reward: total was -48.980000. running mean: -15.440594\n",
      "ep 2029: ep_len:580 episode reward: total was -24.120000. running mean: -15.527388\n",
      "ep 2029: ep_len:525 episode reward: total was -8.630000. running mean: -15.458414\n",
      "ep 2029: ep_len:3 episode reward: total was 0.000000. running mean: -15.303830\n",
      "ep 2029: ep_len:670 episode reward: total was -51.420000. running mean: -15.664992\n",
      "ep 2029: ep_len:585 episode reward: total was -19.130000. running mean: -15.699642\n",
      "epsilon:0.122905 episode_count: 14210. steps_count: 6281244.000000\n",
      "ep 2030: ep_len:525 episode reward: total was -7.180000. running mean: -15.614446\n",
      "ep 2030: ep_len:600 episode reward: total was -35.430000. running mean: -15.812601\n",
      "ep 2030: ep_len:560 episode reward: total was -14.580000. running mean: -15.800275\n",
      "ep 2030: ep_len:550 episode reward: total was -16.940000. running mean: -15.811672\n",
      "ep 2030: ep_len:3 episode reward: total was 0.000000. running mean: -15.653556\n",
      "ep 2030: ep_len:311 episode reward: total was -15.410000. running mean: -15.651120\n",
      "ep 2030: ep_len:585 episode reward: total was -19.490000. running mean: -15.689509\n",
      "epsilon:0.122768 episode_count: 14217. steps_count: 6284378.000000\n",
      "ep 2031: ep_len:670 episode reward: total was -32.280000. running mean: -15.855414\n",
      "ep 2031: ep_len:535 episode reward: total was -27.140000. running mean: -15.968260\n",
      "ep 2031: ep_len:79 episode reward: total was -0.970000. running mean: -15.818277\n",
      "ep 2031: ep_len:520 episode reward: total was -11.130000. running mean: -15.771394\n",
      "ep 2031: ep_len:3 episode reward: total was 0.000000. running mean: -15.613680\n",
      "ep 2031: ep_len:252 episode reward: total was -2.380000. running mean: -15.481344\n",
      "ep 2031: ep_len:299 episode reward: total was -14.850000. running mean: -15.475030\n",
      "epsilon:0.122632 episode_count: 14224. steps_count: 6286736.000000\n",
      "ep 2032: ep_len:675 episode reward: total was -40.320000. running mean: -15.723480\n",
      "ep 2032: ep_len:505 episode reward: total was -8.350000. running mean: -15.649745\n",
      "ep 2032: ep_len:505 episode reward: total was -22.510000. running mean: -15.718348\n",
      "ep 2032: ep_len:500 episode reward: total was -19.560000. running mean: -15.756764\n",
      "ep 2032: ep_len:3 episode reward: total was 0.000000. running mean: -15.599197\n",
      "ep 2032: ep_len:178 episode reward: total was 0.610000. running mean: -15.437105\n",
      "ep 2032: ep_len:610 episode reward: total was -14.920000. running mean: -15.431934\n",
      "epsilon:0.122495 episode_count: 14231. steps_count: 6289712.000000\n",
      "ep 2033: ep_len:590 episode reward: total was -4.600000. running mean: -15.323614\n",
      "ep 2033: ep_len:500 episode reward: total was 2.160000. running mean: -15.148778\n",
      "ep 2033: ep_len:895 episode reward: total was -59.650000. running mean: -15.593790\n",
      "ep 2033: ep_len:500 episode reward: total was -26.690000. running mean: -15.704752\n",
      "ep 2033: ep_len:3 episode reward: total was 0.000000. running mean: -15.547705\n",
      "ep 2033: ep_len:500 episode reward: total was -55.800000. running mean: -15.950228\n",
      "ep 2033: ep_len:505 episode reward: total was -11.490000. running mean: -15.905626\n",
      "epsilon:0.122359 episode_count: 14238. steps_count: 6293205.000000\n",
      "ep 2034: ep_len:131 episode reward: total was -12.920000. running mean: -15.875769\n",
      "ep 2034: ep_len:550 episode reward: total was -3.330000. running mean: -15.750312\n",
      "ep 2034: ep_len:620 episode reward: total was -22.230000. running mean: -15.815108\n",
      "ep 2034: ep_len:520 episode reward: total was -28.190000. running mean: -15.938857\n",
      "ep 2034: ep_len:3 episode reward: total was 0.000000. running mean: -15.779469\n",
      "ep 2034: ep_len:600 episode reward: total was -13.240000. running mean: -15.754074\n",
      "ep 2034: ep_len:505 episode reward: total was -12.850000. running mean: -15.725033\n",
      "epsilon:0.122222 episode_count: 14245. steps_count: 6296134.000000\n",
      "ep 2035: ep_len:845 episode reward: total was -60.720000. running mean: -16.174983\n",
      "ep 2035: ep_len:515 episode reward: total was -12.080000. running mean: -16.134033\n",
      "ep 2035: ep_len:436 episode reward: total was -24.770000. running mean: -16.220393\n",
      "ep 2035: ep_len:170 episode reward: total was -7.380000. running mean: -16.131989\n",
      "ep 2035: ep_len:3 episode reward: total was 0.000000. running mean: -15.970669\n",
      "ep 2035: ep_len:545 episode reward: total was -39.490000. running mean: -16.205862\n",
      "ep 2035: ep_len:515 episode reward: total was -35.570000. running mean: -16.399504\n",
      "epsilon:0.122086 episode_count: 14252. steps_count: 6299163.000000\n",
      "ep 2036: ep_len:540 episode reward: total was -36.520000. running mean: -16.600709\n",
      "ep 2036: ep_len:745 episode reward: total was -51.990000. running mean: -16.954602\n",
      "ep 2036: ep_len:565 episode reward: total was -14.340000. running mean: -16.928456\n",
      "ep 2036: ep_len:585 episode reward: total was -58.050000. running mean: -17.339671\n",
      "ep 2036: ep_len:3 episode reward: total was 0.000000. running mean: -17.166274\n",
      "ep 2036: ep_len:505 episode reward: total was -9.070000. running mean: -17.085312\n",
      "ep 2036: ep_len:585 episode reward: total was -41.510000. running mean: -17.329558\n",
      "epsilon:0.121949 episode_count: 14259. steps_count: 6302691.000000\n",
      "ep 2037: ep_len:685 episode reward: total was -13.610000. running mean: -17.292363\n",
      "ep 2037: ep_len:505 episode reward: total was -2.290000. running mean: -17.142339\n",
      "ep 2037: ep_len:565 episode reward: total was -18.290000. running mean: -17.153816\n",
      "ep 2037: ep_len:156 episode reward: total was -0.420000. running mean: -16.986478\n",
      "ep 2037: ep_len:3 episode reward: total was 0.000000. running mean: -16.816613\n",
      "ep 2037: ep_len:535 episode reward: total was -0.350000. running mean: -16.651947\n",
      "ep 2037: ep_len:575 episode reward: total was -15.420000. running mean: -16.639627\n",
      "epsilon:0.121813 episode_count: 14266. steps_count: 6305715.000000\n",
      "ep 2038: ep_len:500 episode reward: total was -13.230000. running mean: -16.605531\n",
      "ep 2038: ep_len:570 episode reward: total was -87.350000. running mean: -17.312976\n",
      "ep 2038: ep_len:620 episode reward: total was -4.070000. running mean: -17.180546\n",
      "ep 2038: ep_len:127 episode reward: total was -0.440000. running mean: -17.013141\n",
      "ep 2038: ep_len:3 episode reward: total was 0.000000. running mean: -16.843009\n",
      "ep 2038: ep_len:600 episode reward: total was -19.110000. running mean: -16.865679\n",
      "ep 2038: ep_len:505 episode reward: total was -5.810000. running mean: -16.755122\n",
      "epsilon:0.121676 episode_count: 14273. steps_count: 6308640.000000\n",
      "ep 2039: ep_len:560 episode reward: total was -17.470000. running mean: -16.762271\n",
      "ep 2039: ep_len:565 episode reward: total was -13.920000. running mean: -16.733848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2039: ep_len:545 episode reward: total was -30.410000. running mean: -16.870610\n",
      "ep 2039: ep_len:529 episode reward: total was -28.490000. running mean: -16.986804\n",
      "ep 2039: ep_len:94 episode reward: total was 0.530000. running mean: -16.811636\n",
      "ep 2039: ep_len:570 episode reward: total was -12.320000. running mean: -16.766719\n",
      "ep 2039: ep_len:505 episode reward: total was -25.510000. running mean: -16.854152\n",
      "epsilon:0.121540 episode_count: 14280. steps_count: 6312008.000000\n",
      "ep 2040: ep_len:640 episode reward: total was -15.260000. running mean: -16.838211\n",
      "ep 2040: ep_len:595 episode reward: total was -28.970000. running mean: -16.959529\n",
      "ep 2040: ep_len:535 episode reward: total was -12.580000. running mean: -16.915733\n",
      "ep 2040: ep_len:505 episode reward: total was -9.480000. running mean: -16.841376\n",
      "ep 2040: ep_len:2 episode reward: total was 0.000000. running mean: -16.672962\n",
      "ep 2040: ep_len:540 episode reward: total was -43.660000. running mean: -16.942833\n",
      "ep 2040: ep_len:500 episode reward: total was -28.300000. running mean: -17.056404\n",
      "epsilon:0.121403 episode_count: 14287. steps_count: 6315325.000000\n",
      "ep 2041: ep_len:121 episode reward: total was -6.440000. running mean: -16.950240\n",
      "ep 2041: ep_len:500 episode reward: total was -33.350000. running mean: -17.114238\n",
      "ep 2041: ep_len:760 episode reward: total was -51.820000. running mean: -17.461295\n",
      "ep 2041: ep_len:401 episode reward: total was -33.680000. running mean: -17.623482\n",
      "ep 2041: ep_len:3 episode reward: total was 0.000000. running mean: -17.447248\n",
      "ep 2041: ep_len:555 episode reward: total was -13.620000. running mean: -17.408975\n",
      "ep 2041: ep_len:278 episode reward: total was -10.360000. running mean: -17.338485\n",
      "epsilon:0.121267 episode_count: 14294. steps_count: 6317943.000000\n",
      "ep 2042: ep_len:500 episode reward: total was -39.290000. running mean: -17.558001\n",
      "ep 2042: ep_len:615 episode reward: total was -34.670000. running mean: -17.729121\n",
      "ep 2042: ep_len:565 episode reward: total was -16.850000. running mean: -17.720329\n",
      "ep 2042: ep_len:529 episode reward: total was -40.550000. running mean: -17.948626\n",
      "ep 2042: ep_len:80 episode reward: total was -1.940000. running mean: -17.788540\n",
      "ep 2042: ep_len:510 episode reward: total was -16.590000. running mean: -17.776554\n",
      "ep 2042: ep_len:625 episode reward: total was -9.290000. running mean: -17.691689\n",
      "epsilon:0.121130 episode_count: 14301. steps_count: 6321367.000000\n",
      "ep 2043: ep_len:500 episode reward: total was -19.930000. running mean: -17.714072\n",
      "ep 2043: ep_len:580 episode reward: total was -16.110000. running mean: -17.698031\n",
      "ep 2043: ep_len:840 episode reward: total was -54.730000. running mean: -18.068351\n",
      "ep 2043: ep_len:545 episode reward: total was 4.430000. running mean: -17.843367\n",
      "ep 2043: ep_len:96 episode reward: total was 3.540000. running mean: -17.629534\n",
      "ep 2043: ep_len:610 episode reward: total was -0.000000. running mean: -17.453238\n",
      "ep 2043: ep_len:197 episode reward: total was -13.400000. running mean: -17.412706\n",
      "epsilon:0.120994 episode_count: 14308. steps_count: 6324735.000000\n",
      "ep 2044: ep_len:500 episode reward: total was -8.690000. running mean: -17.325479\n",
      "ep 2044: ep_len:500 episode reward: total was 14.160000. running mean: -17.010624\n",
      "ep 2044: ep_len:540 episode reward: total was -17.310000. running mean: -17.013618\n",
      "ep 2044: ep_len:579 episode reward: total was -26.970000. running mean: -17.113182\n",
      "ep 2044: ep_len:3 episode reward: total was 0.000000. running mean: -16.942050\n",
      "ep 2044: ep_len:505 episode reward: total was -8.780000. running mean: -16.860429\n",
      "ep 2044: ep_len:610 episode reward: total was -26.540000. running mean: -16.957225\n",
      "epsilon:0.120857 episode_count: 14315. steps_count: 6327972.000000\n",
      "ep 2045: ep_len:570 episode reward: total was -8.630000. running mean: -16.873953\n",
      "ep 2045: ep_len:184 episode reward: total was -15.930000. running mean: -16.864513\n",
      "ep 2045: ep_len:640 episode reward: total was -18.720000. running mean: -16.883068\n",
      "ep 2045: ep_len:401 episode reward: total was -35.700000. running mean: -17.071238\n",
      "ep 2045: ep_len:117 episode reward: total was 3.560000. running mean: -16.864925\n",
      "ep 2045: ep_len:695 episode reward: total was -0.640000. running mean: -16.702676\n",
      "ep 2045: ep_len:520 episode reward: total was -23.080000. running mean: -16.766449\n",
      "epsilon:0.120721 episode_count: 14322. steps_count: 6331099.000000\n",
      "ep 2046: ep_len:555 episode reward: total was -19.270000. running mean: -16.791485\n",
      "ep 2046: ep_len:570 episode reward: total was -9.950000. running mean: -16.723070\n",
      "ep 2046: ep_len:515 episode reward: total was -6.450000. running mean: -16.620339\n",
      "ep 2046: ep_len:56 episode reward: total was 0.060000. running mean: -16.453536\n",
      "ep 2046: ep_len:3 episode reward: total was 0.000000. running mean: -16.289000\n",
      "ep 2046: ep_len:550 episode reward: total was -16.800000. running mean: -16.294110\n",
      "ep 2046: ep_len:620 episode reward: total was -38.580000. running mean: -16.516969\n",
      "epsilon:0.120584 episode_count: 14329. steps_count: 6333968.000000\n",
      "ep 2047: ep_len:525 episode reward: total was -24.870000. running mean: -16.600500\n",
      "ep 2047: ep_len:500 episode reward: total was -13.510000. running mean: -16.569595\n",
      "ep 2047: ep_len:505 episode reward: total was -26.000000. running mean: -16.663899\n",
      "ep 2047: ep_len:565 episode reward: total was -5.510000. running mean: -16.552360\n",
      "ep 2047: ep_len:96 episode reward: total was -0.960000. running mean: -16.396436\n",
      "ep 2047: ep_len:500 episode reward: total was -17.790000. running mean: -16.410372\n",
      "ep 2047: ep_len:194 episode reward: total was -4.830000. running mean: -16.294568\n",
      "epsilon:0.120448 episode_count: 14336. steps_count: 6336853.000000\n",
      "ep 2048: ep_len:575 episode reward: total was -17.590000. running mean: -16.307522\n",
      "ep 2048: ep_len:650 episode reward: total was -47.470000. running mean: -16.619147\n",
      "ep 2048: ep_len:349 episode reward: total was -10.870000. running mean: -16.561656\n",
      "ep 2048: ep_len:139 episode reward: total was -0.410000. running mean: -16.400139\n",
      "ep 2048: ep_len:3 episode reward: total was 0.000000. running mean: -16.236138\n",
      "ep 2048: ep_len:251 episode reward: total was 2.120000. running mean: -16.052576\n",
      "ep 2048: ep_len:530 episode reward: total was -35.420000. running mean: -16.246250\n",
      "epsilon:0.120311 episode_count: 14343. steps_count: 6339350.000000\n",
      "ep 2049: ep_len:605 episode reward: total was -27.160000. running mean: -16.355388\n",
      "ep 2049: ep_len:575 episode reward: total was 4.360000. running mean: -16.148234\n",
      "ep 2049: ep_len:394 episode reward: total was -14.310000. running mean: -16.129852\n",
      "ep 2049: ep_len:540 episode reward: total was -5.040000. running mean: -16.018953\n",
      "ep 2049: ep_len:76 episode reward: total was -6.450000. running mean: -15.923264\n",
      "ep 2049: ep_len:500 episode reward: total was -8.990000. running mean: -15.853931\n",
      "ep 2049: ep_len:580 episode reward: total was -17.100000. running mean: -15.866392\n",
      "epsilon:0.120175 episode_count: 14350. steps_count: 6342620.000000\n",
      "ep 2050: ep_len:500 episode reward: total was -5.270000. running mean: -15.760428\n",
      "ep 2050: ep_len:500 episode reward: total was -12.720000. running mean: -15.730024\n",
      "ep 2050: ep_len:550 episode reward: total was -26.930000. running mean: -15.842023\n",
      "ep 2050: ep_len:530 episode reward: total was -7.140000. running mean: -15.755003\n",
      "ep 2050: ep_len:3 episode reward: total was 0.000000. running mean: -15.597453\n",
      "ep 2050: ep_len:500 episode reward: total was -7.260000. running mean: -15.514079\n",
      "ep 2050: ep_len:580 episode reward: total was -8.430000. running mean: -15.443238\n",
      "epsilon:0.120038 episode_count: 14357. steps_count: 6345783.000000\n",
      "ep 2051: ep_len:208 episode reward: total was -1.390000. running mean: -15.302705\n",
      "ep 2051: ep_len:540 episode reward: total was -25.660000. running mean: -15.406278\n",
      "ep 2051: ep_len:505 episode reward: total was -15.470000. running mean: -15.406916\n",
      "ep 2051: ep_len:377 episode reward: total was -10.280000. running mean: -15.355646\n",
      "ep 2051: ep_len:94 episode reward: total was 4.540000. running mean: -15.156690\n",
      "ep 2051: ep_len:515 episode reward: total was -16.080000. running mean: -15.165923\n",
      "ep 2051: ep_len:590 episode reward: total was -19.950000. running mean: -15.213764\n",
      "epsilon:0.119902 episode_count: 14364. steps_count: 6348612.000000\n",
      "ep 2052: ep_len:219 episode reward: total was -8.410000. running mean: -15.145726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2052: ep_len:278 episode reward: total was -14.390000. running mean: -15.138169\n",
      "ep 2052: ep_len:555 episode reward: total was -26.860000. running mean: -15.255387\n",
      "ep 2052: ep_len:500 episode reward: total was -18.050000. running mean: -15.283333\n",
      "ep 2052: ep_len:3 episode reward: total was 0.000000. running mean: -15.130500\n",
      "ep 2052: ep_len:610 episode reward: total was -26.170000. running mean: -15.240895\n",
      "ep 2052: ep_len:321 episode reward: total was -14.810000. running mean: -15.236586\n",
      "epsilon:0.119765 episode_count: 14371. steps_count: 6351098.000000\n",
      "ep 2053: ep_len:510 episode reward: total was -15.520000. running mean: -15.239420\n",
      "ep 2053: ep_len:500 episode reward: total was -31.230000. running mean: -15.399326\n",
      "ep 2053: ep_len:365 episode reward: total was -13.840000. running mean: -15.383733\n",
      "ep 2053: ep_len:520 episode reward: total was -1.120000. running mean: -15.241095\n",
      "ep 2053: ep_len:3 episode reward: total was 0.000000. running mean: -15.088684\n",
      "ep 2053: ep_len:600 episode reward: total was -11.080000. running mean: -15.048598\n",
      "ep 2053: ep_len:555 episode reward: total was -13.530000. running mean: -15.033412\n",
      "epsilon:0.119629 episode_count: 14378. steps_count: 6354151.000000\n",
      "ep 2054: ep_len:650 episode reward: total was -35.780000. running mean: -15.240878\n",
      "ep 2054: ep_len:525 episode reward: total was 6.830000. running mean: -15.020169\n",
      "ep 2054: ep_len:500 episode reward: total was -14.680000. running mean: -15.016767\n",
      "ep 2054: ep_len:505 episode reward: total was -22.640000. running mean: -15.092999\n",
      "ep 2054: ep_len:3 episode reward: total was 0.000000. running mean: -14.942069\n",
      "ep 2054: ep_len:530 episode reward: total was -16.630000. running mean: -14.958949\n",
      "ep 2054: ep_len:540 episode reward: total was -16.030000. running mean: -14.969659\n",
      "epsilon:0.119492 episode_count: 14385. steps_count: 6357404.000000\n",
      "ep 2055: ep_len:650 episode reward: total was -15.670000. running mean: -14.976663\n",
      "ep 2055: ep_len:361 episode reward: total was -8.810000. running mean: -14.914996\n",
      "ep 2055: ep_len:500 episode reward: total was -32.650000. running mean: -15.092346\n",
      "ep 2055: ep_len:500 episode reward: total was -10.230000. running mean: -15.043723\n",
      "ep 2055: ep_len:3 episode reward: total was 0.000000. running mean: -14.893285\n",
      "ep 2055: ep_len:500 episode reward: total was -44.080000. running mean: -15.185152\n",
      "ep 2055: ep_len:585 episode reward: total was -18.580000. running mean: -15.219101\n",
      "epsilon:0.119356 episode_count: 14392. steps_count: 6360503.000000\n",
      "ep 2056: ep_len:570 episode reward: total was -20.630000. running mean: -15.273210\n",
      "ep 2056: ep_len:630 episode reward: total was -23.640000. running mean: -15.356878\n",
      "ep 2056: ep_len:505 episode reward: total was -2.950000. running mean: -15.232809\n",
      "ep 2056: ep_len:515 episode reward: total was -37.580000. running mean: -15.456281\n",
      "ep 2056: ep_len:3 episode reward: total was 0.000000. running mean: -15.301718\n",
      "ep 2056: ep_len:510 episode reward: total was -7.160000. running mean: -15.220301\n",
      "ep 2056: ep_len:505 episode reward: total was -20.810000. running mean: -15.276198\n",
      "epsilon:0.119219 episode_count: 14399. steps_count: 6363741.000000\n",
      "ep 2057: ep_len:595 episode reward: total was -4.590000. running mean: -15.169336\n",
      "ep 2057: ep_len:170 episode reward: total was -12.440000. running mean: -15.142043\n",
      "ep 2057: ep_len:500 episode reward: total was -18.200000. running mean: -15.172622\n",
      "ep 2057: ep_len:570 episode reward: total was -21.710000. running mean: -15.237996\n",
      "ep 2057: ep_len:68 episode reward: total was -9.480000. running mean: -15.180416\n",
      "ep 2057: ep_len:575 episode reward: total was 3.650000. running mean: -14.992112\n",
      "ep 2057: ep_len:525 episode reward: total was -13.820000. running mean: -14.980391\n",
      "epsilon:0.119083 episode_count: 14406. steps_count: 6366744.000000\n",
      "ep 2058: ep_len:580 episode reward: total was 1.880000. running mean: -14.811787\n",
      "ep 2058: ep_len:580 episode reward: total was -29.980000. running mean: -14.963469\n",
      "ep 2058: ep_len:535 episode reward: total was -14.450000. running mean: -14.958334\n",
      "ep 2058: ep_len:500 episode reward: total was -9.110000. running mean: -14.899851\n",
      "ep 2058: ep_len:3 episode reward: total was 0.000000. running mean: -14.750852\n",
      "ep 2058: ep_len:550 episode reward: total was -8.490000. running mean: -14.688244\n",
      "ep 2058: ep_len:200 episode reward: total was -7.870000. running mean: -14.620061\n",
      "epsilon:0.118946 episode_count: 14413. steps_count: 6369692.000000\n",
      "ep 2059: ep_len:570 episode reward: total was -4.410000. running mean: -14.517961\n",
      "ep 2059: ep_len:550 episode reward: total was -10.920000. running mean: -14.481981\n",
      "ep 2059: ep_len:650 episode reward: total was -37.140000. running mean: -14.708561\n",
      "ep 2059: ep_len:500 episode reward: total was -12.020000. running mean: -14.681676\n",
      "ep 2059: ep_len:87 episode reward: total was 4.040000. running mean: -14.494459\n",
      "ep 2059: ep_len:660 episode reward: total was -8.210000. running mean: -14.431614\n",
      "ep 2059: ep_len:520 episode reward: total was -36.690000. running mean: -14.654198\n",
      "epsilon:0.118810 episode_count: 14420. steps_count: 6373229.000000\n",
      "ep 2060: ep_len:510 episode reward: total was -23.430000. running mean: -14.741956\n",
      "ep 2060: ep_len:500 episode reward: total was -39.340000. running mean: -14.987937\n",
      "ep 2060: ep_len:550 episode reward: total was -6.050000. running mean: -14.898557\n",
      "ep 2060: ep_len:500 episode reward: total was 1.370000. running mean: -14.735872\n",
      "ep 2060: ep_len:3 episode reward: total was 0.000000. running mean: -14.588513\n",
      "ep 2060: ep_len:635 episode reward: total was -28.340000. running mean: -14.726028\n",
      "ep 2060: ep_len:625 episode reward: total was -5.280000. running mean: -14.631568\n",
      "epsilon:0.118673 episode_count: 14427. steps_count: 6376552.000000\n",
      "ep 2061: ep_len:500 episode reward: total was -22.450000. running mean: -14.709752\n",
      "ep 2061: ep_len:500 episode reward: total was -16.100000. running mean: -14.723655\n",
      "ep 2061: ep_len:560 episode reward: total was -18.340000. running mean: -14.759818\n",
      "ep 2061: ep_len:535 episode reward: total was 4.440000. running mean: -14.567820\n",
      "ep 2061: ep_len:3 episode reward: total was 0.000000. running mean: -14.422142\n",
      "ep 2061: ep_len:595 episode reward: total was -26.870000. running mean: -14.546620\n",
      "ep 2061: ep_len:500 episode reward: total was -20.260000. running mean: -14.603754\n",
      "epsilon:0.118537 episode_count: 14434. steps_count: 6379745.000000\n",
      "ep 2062: ep_len:233 episode reward: total was -17.340000. running mean: -14.631116\n",
      "ep 2062: ep_len:600 episode reward: total was -13.530000. running mean: -14.620105\n",
      "ep 2062: ep_len:605 episode reward: total was -5.590000. running mean: -14.529804\n",
      "ep 2062: ep_len:56 episode reward: total was 1.070000. running mean: -14.373806\n",
      "ep 2062: ep_len:3 episode reward: total was 0.000000. running mean: -14.230068\n",
      "ep 2062: ep_len:615 episode reward: total was -44.650000. running mean: -14.534267\n",
      "ep 2062: ep_len:510 episode reward: total was -36.760000. running mean: -14.756525\n",
      "epsilon:0.118400 episode_count: 14441. steps_count: 6382367.000000\n",
      "ep 2063: ep_len:660 episode reward: total was -7.920000. running mean: -14.688160\n",
      "ep 2063: ep_len:500 episode reward: total was -28.500000. running mean: -14.826278\n",
      "ep 2063: ep_len:815 episode reward: total was -55.770000. running mean: -15.235715\n",
      "ep 2063: ep_len:565 episode reward: total was 4.960000. running mean: -15.033758\n",
      "ep 2063: ep_len:3 episode reward: total was 0.000000. running mean: -14.883420\n",
      "ep 2063: ep_len:500 episode reward: total was -23.100000. running mean: -14.965586\n",
      "ep 2063: ep_len:500 episode reward: total was -15.820000. running mean: -14.974130\n",
      "epsilon:0.118264 episode_count: 14448. steps_count: 6385910.000000\n",
      "ep 2064: ep_len:235 episode reward: total was 2.640000. running mean: -14.797989\n",
      "ep 2064: ep_len:580 episode reward: total was -36.670000. running mean: -15.016709\n",
      "ep 2064: ep_len:545 episode reward: total was -13.810000. running mean: -15.004642\n",
      "ep 2064: ep_len:500 episode reward: total was -1.950000. running mean: -14.874096\n",
      "ep 2064: ep_len:3 episode reward: total was 0.000000. running mean: -14.725355\n",
      "ep 2064: ep_len:255 episode reward: total was 3.140000. running mean: -14.546701\n",
      "ep 2064: ep_len:505 episode reward: total was -16.920000. running mean: -14.570434\n",
      "epsilon:0.118127 episode_count: 14455. steps_count: 6388533.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2065: ep_len:525 episode reward: total was -1.680000. running mean: -14.441530\n",
      "ep 2065: ep_len:775 episode reward: total was -50.920000. running mean: -14.806314\n",
      "ep 2065: ep_len:500 episode reward: total was -6.790000. running mean: -14.726151\n",
      "ep 2065: ep_len:525 episode reward: total was -20.510000. running mean: -14.783990\n",
      "ep 2065: ep_len:129 episode reward: total was 3.050000. running mean: -14.605650\n",
      "ep 2065: ep_len:615 episode reward: total was -18.070000. running mean: -14.640293\n",
      "ep 2065: ep_len:620 episode reward: total was -16.910000. running mean: -14.662990\n",
      "epsilon:0.117991 episode_count: 14462. steps_count: 6392222.000000\n",
      "ep 2066: ep_len:560 episode reward: total was -24.170000. running mean: -14.758061\n",
      "ep 2066: ep_len:520 episode reward: total was -11.950000. running mean: -14.729980\n",
      "ep 2066: ep_len:560 episode reward: total was -8.730000. running mean: -14.669980\n",
      "ep 2066: ep_len:51 episode reward: total was -0.960000. running mean: -14.532880\n",
      "ep 2066: ep_len:90 episode reward: total was -6.470000. running mean: -14.452252\n",
      "ep 2066: ep_len:640 episode reward: total was -22.100000. running mean: -14.528729\n",
      "ep 2066: ep_len:540 episode reward: total was -27.960000. running mean: -14.663042\n",
      "epsilon:0.117854 episode_count: 14469. steps_count: 6395183.000000\n",
      "ep 2067: ep_len:207 episode reward: total was -0.420000. running mean: -14.520611\n",
      "ep 2067: ep_len:500 episode reward: total was -12.800000. running mean: -14.503405\n",
      "ep 2067: ep_len:650 episode reward: total was -24.900000. running mean: -14.607371\n",
      "ep 2067: ep_len:500 episode reward: total was -20.090000. running mean: -14.662197\n",
      "ep 2067: ep_len:3 episode reward: total was 0.000000. running mean: -14.515575\n",
      "ep 2067: ep_len:540 episode reward: total was -12.980000. running mean: -14.500220\n",
      "ep 2067: ep_len:635 episode reward: total was -45.400000. running mean: -14.809218\n",
      "epsilon:0.117718 episode_count: 14476. steps_count: 6398218.000000\n",
      "ep 2068: ep_len:595 episode reward: total was -8.650000. running mean: -14.747625\n",
      "ep 2068: ep_len:500 episode reward: total was -10.710000. running mean: -14.707249\n",
      "ep 2068: ep_len:675 episode reward: total was -35.300000. running mean: -14.913177\n",
      "ep 2068: ep_len:500 episode reward: total was -7.050000. running mean: -14.834545\n",
      "ep 2068: ep_len:3 episode reward: total was 0.000000. running mean: -14.686199\n",
      "ep 2068: ep_len:675 episode reward: total was -56.460000. running mean: -15.103937\n",
      "ep 2068: ep_len:334 episode reward: total was -17.870000. running mean: -15.131598\n",
      "epsilon:0.117581 episode_count: 14483. steps_count: 6401500.000000\n",
      "ep 2069: ep_len:590 episode reward: total was -36.370000. running mean: -15.343982\n",
      "ep 2069: ep_len:525 episode reward: total was -17.690000. running mean: -15.367442\n",
      "ep 2069: ep_len:500 episode reward: total was -39.320000. running mean: -15.606968\n",
      "ep 2069: ep_len:540 episode reward: total was -2.640000. running mean: -15.477298\n",
      "ep 2069: ep_len:90 episode reward: total was -0.970000. running mean: -15.332225\n",
      "ep 2069: ep_len:505 episode reward: total was -7.050000. running mean: -15.249403\n",
      "ep 2069: ep_len:268 episode reward: total was -9.340000. running mean: -15.190309\n",
      "epsilon:0.117445 episode_count: 14490. steps_count: 6404518.000000\n",
      "ep 2070: ep_len:253 episode reward: total was -6.880000. running mean: -15.107206\n",
      "ep 2070: ep_len:505 episode reward: total was -32.410000. running mean: -15.280234\n",
      "ep 2070: ep_len:560 episode reward: total was -5.990000. running mean: -15.187331\n",
      "ep 2070: ep_len:555 episode reward: total was -7.630000. running mean: -15.111758\n",
      "ep 2070: ep_len:96 episode reward: total was 7.550000. running mean: -14.885140\n",
      "ep 2070: ep_len:585 episode reward: total was -34.660000. running mean: -15.082889\n",
      "ep 2070: ep_len:291 episode reward: total was -15.840000. running mean: -15.090460\n",
      "epsilon:0.117308 episode_count: 14497. steps_count: 6407363.000000\n",
      "ep 2071: ep_len:500 episode reward: total was -50.360000. running mean: -15.443156\n",
      "ep 2071: ep_len:500 episode reward: total was -24.060000. running mean: -15.529324\n",
      "ep 2071: ep_len:645 episode reward: total was -35.040000. running mean: -15.724431\n",
      "ep 2071: ep_len:530 episode reward: total was -22.030000. running mean: -15.787486\n",
      "ep 2071: ep_len:3 episode reward: total was 0.000000. running mean: -15.629612\n",
      "ep 2071: ep_len:520 episode reward: total was -34.330000. running mean: -15.816616\n",
      "ep 2071: ep_len:535 episode reward: total was -21.030000. running mean: -15.868749\n",
      "epsilon:0.117172 episode_count: 14504. steps_count: 6410596.000000\n",
      "ep 2072: ep_len:500 episode reward: total was -26.180000. running mean: -15.971862\n",
      "ep 2072: ep_len:358 episode reward: total was -16.320000. running mean: -15.975343\n",
      "ep 2072: ep_len:685 episode reward: total was -11.250000. running mean: -15.928090\n",
      "ep 2072: ep_len:505 episode reward: total was -14.150000. running mean: -15.910309\n",
      "ep 2072: ep_len:88 episode reward: total was -12.960000. running mean: -15.880806\n",
      "ep 2072: ep_len:645 episode reward: total was -9.770000. running mean: -15.819698\n",
      "ep 2072: ep_len:585 episode reward: total was -38.030000. running mean: -16.041801\n",
      "epsilon:0.117035 episode_count: 14511. steps_count: 6413962.000000\n",
      "ep 2073: ep_len:246 episode reward: total was -9.390000. running mean: -15.975283\n",
      "ep 2073: ep_len:515 episode reward: total was -40.080000. running mean: -16.216330\n",
      "ep 2073: ep_len:535 episode reward: total was -7.610000. running mean: -16.130267\n",
      "ep 2073: ep_len:520 episode reward: total was -13.040000. running mean: -16.099364\n",
      "ep 2073: ep_len:49 episode reward: total was 1.500000. running mean: -15.923370\n",
      "ep 2073: ep_len:575 episode reward: total was -6.540000. running mean: -15.829537\n",
      "ep 2073: ep_len:505 episode reward: total was -23.500000. running mean: -15.906241\n",
      "epsilon:0.116899 episode_count: 14518. steps_count: 6416907.000000\n",
      "ep 2074: ep_len:555 episode reward: total was -6.590000. running mean: -15.813079\n",
      "ep 2074: ep_len:500 episode reward: total was 2.190000. running mean: -15.633048\n",
      "ep 2074: ep_len:550 episode reward: total was -13.770000. running mean: -15.614418\n",
      "ep 2074: ep_len:600 episode reward: total was -16.480000. running mean: -15.623073\n",
      "ep 2074: ep_len:3 episode reward: total was 0.000000. running mean: -15.466843\n",
      "ep 2074: ep_len:540 episode reward: total was -35.420000. running mean: -15.666374\n",
      "ep 2074: ep_len:505 episode reward: total was -26.000000. running mean: -15.769711\n",
      "epsilon:0.116762 episode_count: 14525. steps_count: 6420160.000000\n",
      "ep 2075: ep_len:134 episode reward: total was -14.940000. running mean: -15.761413\n",
      "ep 2075: ep_len:595 episode reward: total was -5.830000. running mean: -15.662099\n",
      "ep 2075: ep_len:595 episode reward: total was -19.960000. running mean: -15.705078\n",
      "ep 2075: ep_len:615 episode reward: total was -2.880000. running mean: -15.576828\n",
      "ep 2075: ep_len:3 episode reward: total was 0.000000. running mean: -15.421059\n",
      "ep 2075: ep_len:565 episode reward: total was -15.820000. running mean: -15.425049\n",
      "ep 2075: ep_len:575 episode reward: total was -12.590000. running mean: -15.396698\n",
      "epsilon:0.116626 episode_count: 14532. steps_count: 6423242.000000\n",
      "ep 2076: ep_len:595 episode reward: total was -7.650000. running mean: -15.319231\n",
      "ep 2076: ep_len:585 episode reward: total was -67.640000. running mean: -15.842439\n",
      "ep 2076: ep_len:565 episode reward: total was -38.660000. running mean: -16.070614\n",
      "ep 2076: ep_len:56 episode reward: total was 1.560000. running mean: -15.894308\n",
      "ep 2076: ep_len:3 episode reward: total was 0.000000. running mean: -15.735365\n",
      "ep 2076: ep_len:590 episode reward: total was -10.090000. running mean: -15.678912\n",
      "ep 2076: ep_len:590 episode reward: total was -28.600000. running mean: -15.808122\n",
      "epsilon:0.116489 episode_count: 14539. steps_count: 6426226.000000\n",
      "ep 2077: ep_len:550 episode reward: total was -4.970000. running mean: -15.699741\n",
      "ep 2077: ep_len:595 episode reward: total was -6.920000. running mean: -15.611944\n",
      "ep 2077: ep_len:580 episode reward: total was -11.590000. running mean: -15.571724\n",
      "ep 2077: ep_len:500 episode reward: total was -3.990000. running mean: -15.455907\n",
      "ep 2077: ep_len:79 episode reward: total was -9.460000. running mean: -15.395948\n",
      "ep 2077: ep_len:520 episode reward: total was -8.600000. running mean: -15.327989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2077: ep_len:510 episode reward: total was -13.530000. running mean: -15.310009\n",
      "epsilon:0.116353 episode_count: 14546. steps_count: 6429560.000000\n",
      "ep 2078: ep_len:560 episode reward: total was -23.730000. running mean: -15.394209\n",
      "ep 2078: ep_len:500 episode reward: total was -4.350000. running mean: -15.283767\n",
      "ep 2078: ep_len:565 episode reward: total was -17.780000. running mean: -15.308729\n",
      "ep 2078: ep_len:530 episode reward: total was -24.580000. running mean: -15.401442\n",
      "ep 2078: ep_len:89 episode reward: total was -13.970000. running mean: -15.387127\n",
      "ep 2078: ep_len:655 episode reward: total was -7.270000. running mean: -15.305956\n",
      "ep 2078: ep_len:550 episode reward: total was -15.630000. running mean: -15.309196\n",
      "epsilon:0.116216 episode_count: 14553. steps_count: 6433009.000000\n",
      "ep 2079: ep_len:570 episode reward: total was -20.770000. running mean: -15.363804\n",
      "ep 2079: ep_len:620 episode reward: total was -8.540000. running mean: -15.295566\n",
      "ep 2079: ep_len:366 episode reward: total was -7.850000. running mean: -15.221111\n",
      "ep 2079: ep_len:500 episode reward: total was -17.750000. running mean: -15.246400\n",
      "ep 2079: ep_len:51 episode reward: total was -10.000000. running mean: -15.193936\n",
      "ep 2079: ep_len:645 episode reward: total was -38.090000. running mean: -15.422896\n",
      "ep 2079: ep_len:625 episode reward: total was -16.050000. running mean: -15.429167\n",
      "epsilon:0.116080 episode_count: 14560. steps_count: 6436386.000000\n",
      "ep 2080: ep_len:600 episode reward: total was -4.080000. running mean: -15.315676\n",
      "ep 2080: ep_len:580 episode reward: total was -25.650000. running mean: -15.419019\n",
      "ep 2080: ep_len:500 episode reward: total was -12.020000. running mean: -15.385029\n",
      "ep 2080: ep_len:510 episode reward: total was -18.980000. running mean: -15.420978\n",
      "ep 2080: ep_len:3 episode reward: total was 0.000000. running mean: -15.266769\n",
      "ep 2080: ep_len:219 episode reward: total was -2.880000. running mean: -15.142901\n",
      "ep 2080: ep_len:565 episode reward: total was -15.110000. running mean: -15.142572\n",
      "epsilon:0.115943 episode_count: 14567. steps_count: 6439363.000000\n",
      "ep 2081: ep_len:635 episode reward: total was -21.980000. running mean: -15.210946\n",
      "ep 2081: ep_len:560 episode reward: total was -43.090000. running mean: -15.489737\n",
      "ep 2081: ep_len:645 episode reward: total was -32.910000. running mean: -15.663939\n",
      "ep 2081: ep_len:500 episode reward: total was -24.040000. running mean: -15.747700\n",
      "ep 2081: ep_len:106 episode reward: total was -13.960000. running mean: -15.729823\n",
      "ep 2081: ep_len:525 episode reward: total was -6.420000. running mean: -15.636725\n",
      "ep 2081: ep_len:560 episode reward: total was -19.550000. running mean: -15.675857\n",
      "epsilon:0.115807 episode_count: 14574. steps_count: 6442894.000000\n",
      "ep 2082: ep_len:500 episode reward: total was -27.500000. running mean: -15.794099\n",
      "ep 2082: ep_len:500 episode reward: total was 0.630000. running mean: -15.629858\n",
      "ep 2082: ep_len:665 episode reward: total was -8.770000. running mean: -15.561259\n",
      "ep 2082: ep_len:595 episode reward: total was -11.500000. running mean: -15.520647\n",
      "ep 2082: ep_len:48 episode reward: total was 1.500000. running mean: -15.350440\n",
      "ep 2082: ep_len:530 episode reward: total was -29.410000. running mean: -15.491036\n",
      "ep 2082: ep_len:277 episode reward: total was -11.910000. running mean: -15.455225\n",
      "epsilon:0.115670 episode_count: 14581. steps_count: 6446009.000000\n",
      "ep 2083: ep_len:121 episode reward: total was -14.960000. running mean: -15.450273\n",
      "ep 2083: ep_len:540 episode reward: total was 0.990000. running mean: -15.285871\n",
      "ep 2083: ep_len:645 episode reward: total was -8.730000. running mean: -15.220312\n",
      "ep 2083: ep_len:515 episode reward: total was -2.560000. running mean: -15.093709\n",
      "ep 2083: ep_len:3 episode reward: total was 0.000000. running mean: -14.942772\n",
      "ep 2083: ep_len:550 episode reward: total was -31.370000. running mean: -15.107044\n",
      "ep 2083: ep_len:600 episode reward: total was -56.460000. running mean: -15.520573\n",
      "epsilon:0.115534 episode_count: 14588. steps_count: 6448983.000000\n",
      "ep 2084: ep_len:560 episode reward: total was -10.030000. running mean: -15.465668\n",
      "ep 2084: ep_len:555 episode reward: total was -28.040000. running mean: -15.591411\n",
      "ep 2084: ep_len:655 episode reward: total was -32.750000. running mean: -15.762997\n",
      "ep 2084: ep_len:112 episode reward: total was -5.400000. running mean: -15.659367\n",
      "ep 2084: ep_len:3 episode reward: total was 0.000000. running mean: -15.502773\n",
      "ep 2084: ep_len:500 episode reward: total was -7.270000. running mean: -15.420446\n",
      "ep 2084: ep_len:620 episode reward: total was -29.590000. running mean: -15.562141\n",
      "epsilon:0.115397 episode_count: 14595. steps_count: 6451988.000000\n",
      "ep 2085: ep_len:770 episode reward: total was -48.300000. running mean: -15.889520\n",
      "ep 2085: ep_len:525 episode reward: total was -17.630000. running mean: -15.906924\n",
      "ep 2085: ep_len:570 episode reward: total was -6.950000. running mean: -15.817355\n",
      "ep 2085: ep_len:500 episode reward: total was -22.560000. running mean: -15.884782\n",
      "ep 2085: ep_len:3 episode reward: total was 0.000000. running mean: -15.725934\n",
      "ep 2085: ep_len:560 episode reward: total was -26.070000. running mean: -15.829375\n",
      "ep 2085: ep_len:555 episode reward: total was -40.130000. running mean: -16.072381\n",
      "epsilon:0.115261 episode_count: 14602. steps_count: 6455471.000000\n",
      "ep 2086: ep_len:575 episode reward: total was -6.450000. running mean: -15.976157\n",
      "ep 2086: ep_len:500 episode reward: total was -1.470000. running mean: -15.831095\n",
      "ep 2086: ep_len:570 episode reward: total was -16.940000. running mean: -15.842184\n",
      "ep 2086: ep_len:615 episode reward: total was -18.620000. running mean: -15.869963\n",
      "ep 2086: ep_len:3 episode reward: total was 0.000000. running mean: -15.711263\n",
      "ep 2086: ep_len:595 episode reward: total was -13.140000. running mean: -15.685550\n",
      "ep 2086: ep_len:560 episode reward: total was -26.150000. running mean: -15.790195\n",
      "epsilon:0.115124 episode_count: 14609. steps_count: 6458889.000000\n",
      "ep 2087: ep_len:500 episode reward: total was -14.730000. running mean: -15.779593\n",
      "ep 2087: ep_len:585 episode reward: total was -13.130000. running mean: -15.753097\n",
      "ep 2087: ep_len:79 episode reward: total was -2.960000. running mean: -15.625166\n",
      "ep 2087: ep_len:600 episode reward: total was 5.530000. running mean: -15.413614\n",
      "ep 2087: ep_len:96 episode reward: total was -3.470000. running mean: -15.294178\n",
      "ep 2087: ep_len:675 episode reward: total was -10.140000. running mean: -15.242636\n",
      "ep 2087: ep_len:187 episode reward: total was -12.430000. running mean: -15.214510\n",
      "epsilon:0.114988 episode_count: 14616. steps_count: 6461611.000000\n",
      "ep 2088: ep_len:550 episode reward: total was -5.610000. running mean: -15.118465\n",
      "ep 2088: ep_len:500 episode reward: total was -8.850000. running mean: -15.055780\n",
      "ep 2088: ep_len:620 episode reward: total was -34.900000. running mean: -15.254222\n",
      "ep 2088: ep_len:565 episode reward: total was -18.060000. running mean: -15.282280\n",
      "ep 2088: ep_len:104 episode reward: total was 3.550000. running mean: -15.093957\n",
      "ep 2088: ep_len:186 episode reward: total was 3.110000. running mean: -14.911918\n",
      "ep 2088: ep_len:575 episode reward: total was -18.860000. running mean: -14.951399\n",
      "epsilon:0.114851 episode_count: 14623. steps_count: 6464711.000000\n",
      "ep 2089: ep_len:219 episode reward: total was -2.910000. running mean: -14.830985\n",
      "ep 2089: ep_len:585 episode reward: total was -21.850000. running mean: -14.901175\n",
      "ep 2089: ep_len:585 episode reward: total was -15.160000. running mean: -14.903763\n",
      "ep 2089: ep_len:535 episode reward: total was -22.990000. running mean: -14.984626\n",
      "ep 2089: ep_len:97 episode reward: total was -11.470000. running mean: -14.949479\n",
      "ep 2089: ep_len:257 episode reward: total was 2.160000. running mean: -14.778384\n",
      "ep 2089: ep_len:500 episode reward: total was -13.000000. running mean: -14.760601\n",
      "epsilon:0.114715 episode_count: 14630. steps_count: 6467489.000000\n",
      "ep 2090: ep_len:595 episode reward: total was -5.130000. running mean: -14.664295\n",
      "ep 2090: ep_len:505 episode reward: total was -16.810000. running mean: -14.685752\n",
      "ep 2090: ep_len:600 episode reward: total was -31.960000. running mean: -14.858494\n",
      "ep 2090: ep_len:595 episode reward: total was -20.020000. running mean: -14.910109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2090: ep_len:3 episode reward: total was 0.000000. running mean: -14.761008\n",
      "ep 2090: ep_len:500 episode reward: total was -25.330000. running mean: -14.866698\n",
      "ep 2090: ep_len:500 episode reward: total was -21.040000. running mean: -14.928431\n",
      "epsilon:0.114578 episode_count: 14637. steps_count: 6470787.000000\n",
      "ep 2091: ep_len:500 episode reward: total was -21.470000. running mean: -14.993847\n",
      "ep 2091: ep_len:500 episode reward: total was -17.800000. running mean: -15.021908\n",
      "ep 2091: ep_len:620 episode reward: total was -15.040000. running mean: -15.022089\n",
      "ep 2091: ep_len:545 episode reward: total was -3.600000. running mean: -14.907868\n",
      "ep 2091: ep_len:111 episode reward: total was 4.550000. running mean: -14.713290\n",
      "ep 2091: ep_len:234 episode reward: total was 4.100000. running mean: -14.525157\n",
      "ep 2091: ep_len:530 episode reward: total was -35.480000. running mean: -14.734705\n",
      "epsilon:0.114442 episode_count: 14644. steps_count: 6473827.000000\n",
      "ep 2092: ep_len:505 episode reward: total was -21.090000. running mean: -14.798258\n",
      "ep 2092: ep_len:555 episode reward: total was -29.620000. running mean: -14.946476\n",
      "ep 2092: ep_len:431 episode reward: total was -4.800000. running mean: -14.845011\n",
      "ep 2092: ep_len:500 episode reward: total was -23.650000. running mean: -14.933061\n",
      "ep 2092: ep_len:3 episode reward: total was 0.000000. running mean: -14.783730\n",
      "ep 2092: ep_len:705 episode reward: total was -51.820000. running mean: -15.154093\n",
      "ep 2092: ep_len:620 episode reward: total was -31.000000. running mean: -15.312552\n",
      "epsilon:0.114305 episode_count: 14651. steps_count: 6477146.000000\n",
      "ep 2093: ep_len:680 episode reward: total was -20.200000. running mean: -15.361426\n",
      "ep 2093: ep_len:500 episode reward: total was -5.330000. running mean: -15.261112\n",
      "ep 2093: ep_len:635 episode reward: total was -16.440000. running mean: -15.272901\n",
      "ep 2093: ep_len:510 episode reward: total was -20.600000. running mean: -15.326172\n",
      "ep 2093: ep_len:3 episode reward: total was 0.000000. running mean: -15.172910\n",
      "ep 2093: ep_len:770 episode reward: total was -40.250000. running mean: -15.423681\n",
      "ep 2093: ep_len:208 episode reward: total was -8.870000. running mean: -15.358144\n",
      "epsilon:0.114169 episode_count: 14658. steps_count: 6480452.000000\n",
      "ep 2094: ep_len:229 episode reward: total was -5.400000. running mean: -15.258563\n",
      "ep 2094: ep_len:350 episode reward: total was -39.370000. running mean: -15.499677\n",
      "ep 2094: ep_len:520 episode reward: total was 3.350000. running mean: -15.311180\n",
      "ep 2094: ep_len:126 episode reward: total was -3.410000. running mean: -15.192169\n",
      "ep 2094: ep_len:109 episode reward: total was -11.970000. running mean: -15.159947\n",
      "ep 2094: ep_len:535 episode reward: total was -11.430000. running mean: -15.122647\n",
      "ep 2094: ep_len:600 episode reward: total was -26.960000. running mean: -15.241021\n",
      "epsilon:0.114032 episode_count: 14665. steps_count: 6482921.000000\n",
      "ep 2095: ep_len:640 episode reward: total was -19.040000. running mean: -15.279011\n",
      "ep 2095: ep_len:500 episode reward: total was 1.640000. running mean: -15.109821\n",
      "ep 2095: ep_len:680 episode reward: total was -9.640000. running mean: -15.055122\n",
      "ep 2095: ep_len:146 episode reward: total was -0.900000. running mean: -14.913571\n",
      "ep 2095: ep_len:108 episode reward: total was 7.050000. running mean: -14.693936\n",
      "ep 2095: ep_len:695 episode reward: total was -13.800000. running mean: -14.684996\n",
      "ep 2095: ep_len:261 episode reward: total was -11.410000. running mean: -14.652246\n",
      "epsilon:0.113896 episode_count: 14672. steps_count: 6485951.000000\n",
      "ep 2096: ep_len:570 episode reward: total was -2.130000. running mean: -14.527024\n",
      "ep 2096: ep_len:520 episode reward: total was -39.600000. running mean: -14.777754\n",
      "ep 2096: ep_len:500 episode reward: total was -6.790000. running mean: -14.697876\n",
      "ep 2096: ep_len:500 episode reward: total was -20.130000. running mean: -14.752197\n",
      "ep 2096: ep_len:105 episode reward: total was 2.030000. running mean: -14.584375\n",
      "ep 2096: ep_len:560 episode reward: total was -11.240000. running mean: -14.550931\n",
      "ep 2096: ep_len:520 episode reward: total was -5.900000. running mean: -14.464422\n",
      "epsilon:0.113759 episode_count: 14679. steps_count: 6489226.000000\n",
      "ep 2097: ep_len:550 episode reward: total was -20.320000. running mean: -14.522978\n",
      "ep 2097: ep_len:500 episode reward: total was -7.900000. running mean: -14.456748\n",
      "ep 2097: ep_len:605 episode reward: total was -4.100000. running mean: -14.353181\n",
      "ep 2097: ep_len:610 episode reward: total was -18.090000. running mean: -14.390549\n",
      "ep 2097: ep_len:3 episode reward: total was 0.000000. running mean: -14.246643\n",
      "ep 2097: ep_len:515 episode reward: total was -16.980000. running mean: -14.273977\n",
      "ep 2097: ep_len:525 episode reward: total was -21.660000. running mean: -14.347837\n",
      "epsilon:0.113623 episode_count: 14686. steps_count: 6492534.000000\n",
      "ep 2098: ep_len:595 episode reward: total was -14.490000. running mean: -14.349259\n",
      "ep 2098: ep_len:520 episode reward: total was -21.230000. running mean: -14.418066\n",
      "ep 2098: ep_len:585 episode reward: total was -22.270000. running mean: -14.496586\n",
      "ep 2098: ep_len:540 episode reward: total was -19.210000. running mean: -14.543720\n",
      "ep 2098: ep_len:3 episode reward: total was 0.000000. running mean: -14.398283\n",
      "ep 2098: ep_len:645 episode reward: total was -25.610000. running mean: -14.510400\n",
      "ep 2098: ep_len:203 episode reward: total was -14.420000. running mean: -14.509496\n",
      "epsilon:0.113486 episode_count: 14693. steps_count: 6495625.000000\n",
      "ep 2099: ep_len:560 episode reward: total was -3.620000. running mean: -14.400601\n",
      "ep 2099: ep_len:650 episode reward: total was -9.540000. running mean: -14.351995\n",
      "ep 2099: ep_len:590 episode reward: total was -37.840000. running mean: -14.586875\n",
      "ep 2099: ep_len:545 episode reward: total was -42.680000. running mean: -14.867806\n",
      "ep 2099: ep_len:3 episode reward: total was 0.000000. running mean: -14.719128\n",
      "ep 2099: ep_len:605 episode reward: total was -11.260000. running mean: -14.684537\n",
      "ep 2099: ep_len:500 episode reward: total was -19.280000. running mean: -14.730491\n",
      "epsilon:0.113350 episode_count: 14700. steps_count: 6499078.000000\n",
      "ep 2100: ep_len:500 episode reward: total was 2.780000. running mean: -14.555386\n",
      "ep 2100: ep_len:500 episode reward: total was 5.650000. running mean: -14.353333\n",
      "ep 2100: ep_len:580 episode reward: total was -4.550000. running mean: -14.255299\n",
      "ep 2100: ep_len:520 episode reward: total was -16.700000. running mean: -14.279746\n",
      "ep 2100: ep_len:2 episode reward: total was 0.000000. running mean: -14.136949\n",
      "ep 2100: ep_len:535 episode reward: total was -19.770000. running mean: -14.193279\n",
      "ep 2100: ep_len:610 episode reward: total was -19.850000. running mean: -14.249846\n",
      "epsilon:0.113213 episode_count: 14707. steps_count: 6502325.000000\n",
      "ep 2101: ep_len:251 episode reward: total was -9.900000. running mean: -14.206348\n",
      "ep 2101: ep_len:505 episode reward: total was 2.840000. running mean: -14.035885\n",
      "ep 2101: ep_len:399 episode reward: total was -12.320000. running mean: -14.018726\n",
      "ep 2101: ep_len:570 episode reward: total was 3.860000. running mean: -13.839938\n",
      "ep 2101: ep_len:3 episode reward: total was 0.000000. running mean: -13.701539\n",
      "ep 2101: ep_len:585 episode reward: total was -38.140000. running mean: -13.945924\n",
      "ep 2101: ep_len:290 episode reward: total was -8.300000. running mean: -13.889464\n",
      "epsilon:0.113077 episode_count: 14714. steps_count: 6504928.000000\n",
      "ep 2102: ep_len:540 episode reward: total was -10.960000. running mean: -13.860170\n",
      "ep 2102: ep_len:500 episode reward: total was -31.280000. running mean: -14.034368\n",
      "ep 2102: ep_len:540 episode reward: total was -12.160000. running mean: -14.015624\n",
      "ep 2102: ep_len:519 episode reward: total was -34.480000. running mean: -14.220268\n",
      "ep 2102: ep_len:3 episode reward: total was 0.000000. running mean: -14.078065\n",
      "ep 2102: ep_len:510 episode reward: total was -13.520000. running mean: -14.072485\n",
      "ep 2102: ep_len:575 episode reward: total was -54.220000. running mean: -14.473960\n",
      "epsilon:0.112940 episode_count: 14721. steps_count: 6508115.000000\n",
      "ep 2103: ep_len:550 episode reward: total was -23.000000. running mean: -14.559220\n",
      "ep 2103: ep_len:500 episode reward: total was -27.190000. running mean: -14.685528\n",
      "ep 2103: ep_len:368 episode reward: total was -11.360000. running mean: -14.652273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2103: ep_len:565 episode reward: total was -8.160000. running mean: -14.587350\n",
      "ep 2103: ep_len:3 episode reward: total was 0.000000. running mean: -14.441477\n",
      "ep 2103: ep_len:655 episode reward: total was -58.480000. running mean: -14.881862\n",
      "ep 2103: ep_len:570 episode reward: total was -21.020000. running mean: -14.943243\n",
      "epsilon:0.112804 episode_count: 14728. steps_count: 6511326.000000\n",
      "ep 2104: ep_len:530 episode reward: total was -6.420000. running mean: -14.858011\n",
      "ep 2104: ep_len:864 episode reward: total was -68.360000. running mean: -15.393031\n",
      "ep 2104: ep_len:555 episode reward: total was -21.180000. running mean: -15.450900\n",
      "ep 2104: ep_len:595 episode reward: total was -2.940000. running mean: -15.325791\n",
      "ep 2104: ep_len:88 episode reward: total was 3.550000. running mean: -15.137034\n",
      "ep 2104: ep_len:500 episode reward: total was -6.430000. running mean: -15.049963\n",
      "ep 2104: ep_len:191 episode reward: total was -7.400000. running mean: -14.973464\n",
      "epsilon:0.112667 episode_count: 14735. steps_count: 6514649.000000\n",
      "ep 2105: ep_len:550 episode reward: total was -11.450000. running mean: -14.938229\n",
      "ep 2105: ep_len:500 episode reward: total was -1.010000. running mean: -14.798947\n",
      "ep 2105: ep_len:650 episode reward: total was -30.950000. running mean: -14.960457\n",
      "ep 2105: ep_len:500 episode reward: total was -1.600000. running mean: -14.826853\n",
      "ep 2105: ep_len:79 episode reward: total was 3.010000. running mean: -14.648484\n",
      "ep 2105: ep_len:650 episode reward: total was -20.920000. running mean: -14.711199\n",
      "ep 2105: ep_len:570 episode reward: total was -57.320000. running mean: -15.137287\n",
      "epsilon:0.112531 episode_count: 14742. steps_count: 6518148.000000\n",
      "ep 2106: ep_len:640 episode reward: total was -0.860000. running mean: -14.994514\n",
      "ep 2106: ep_len:500 episode reward: total was -4.180000. running mean: -14.886369\n",
      "ep 2106: ep_len:560 episode reward: total was -27.920000. running mean: -15.016706\n",
      "ep 2106: ep_len:427 episode reward: total was -35.650000. running mean: -15.223038\n",
      "ep 2106: ep_len:3 episode reward: total was 0.000000. running mean: -15.070808\n",
      "ep 2106: ep_len:530 episode reward: total was -23.100000. running mean: -15.151100\n",
      "ep 2106: ep_len:510 episode reward: total was -20.650000. running mean: -15.206089\n",
      "epsilon:0.112394 episode_count: 14749. steps_count: 6521318.000000\n",
      "ep 2107: ep_len:675 episode reward: total was -35.300000. running mean: -15.407028\n",
      "ep 2107: ep_len:685 episode reward: total was -53.470000. running mean: -15.787658\n",
      "ep 2107: ep_len:500 episode reward: total was -11.990000. running mean: -15.749681\n",
      "ep 2107: ep_len:500 episode reward: total was -22.580000. running mean: -15.817984\n",
      "ep 2107: ep_len:3 episode reward: total was 0.000000. running mean: -15.659805\n",
      "ep 2107: ep_len:530 episode reward: total was -12.050000. running mean: -15.623707\n",
      "ep 2107: ep_len:530 episode reward: total was -24.130000. running mean: -15.708769\n",
      "epsilon:0.112258 episode_count: 14756. steps_count: 6524741.000000\n",
      "ep 2108: ep_len:248 episode reward: total was -29.840000. running mean: -15.850082\n",
      "ep 2108: ep_len:530 episode reward: total was 1.640000. running mean: -15.675181\n",
      "ep 2108: ep_len:710 episode reward: total was -59.710000. running mean: -16.115529\n",
      "ep 2108: ep_len:535 episode reward: total was -12.920000. running mean: -16.083574\n",
      "ep 2108: ep_len:3 episode reward: total was 0.000000. running mean: -15.922738\n",
      "ep 2108: ep_len:157 episode reward: total was -1.940000. running mean: -15.782911\n",
      "ep 2108: ep_len:281 episode reward: total was -4.300000. running mean: -15.668082\n",
      "epsilon:0.112121 episode_count: 14763. steps_count: 6527205.000000\n",
      "ep 2109: ep_len:207 episode reward: total was -0.390000. running mean: -15.515301\n",
      "ep 2109: ep_len:570 episode reward: total was 6.820000. running mean: -15.291948\n",
      "ep 2109: ep_len:690 episode reward: total was -15.740000. running mean: -15.296428\n",
      "ep 2109: ep_len:530 episode reward: total was -7.670000. running mean: -15.220164\n",
      "ep 2109: ep_len:3 episode reward: total was 0.000000. running mean: -15.067962\n",
      "ep 2109: ep_len:298 episode reward: total was -3.870000. running mean: -14.955983\n",
      "ep 2109: ep_len:585 episode reward: total was -30.020000. running mean: -15.106623\n",
      "epsilon:0.111985 episode_count: 14770. steps_count: 6530088.000000\n",
      "ep 2110: ep_len:655 episode reward: total was -27.310000. running mean: -15.228657\n",
      "ep 2110: ep_len:550 episode reward: total was -23.680000. running mean: -15.313170\n",
      "ep 2110: ep_len:505 episode reward: total was -12.130000. running mean: -15.281338\n",
      "ep 2110: ep_len:520 episode reward: total was -34.140000. running mean: -15.469925\n",
      "ep 2110: ep_len:3 episode reward: total was 0.000000. running mean: -15.315226\n",
      "ep 2110: ep_len:570 episode reward: total was -9.720000. running mean: -15.259274\n",
      "ep 2110: ep_len:610 episode reward: total was -20.430000. running mean: -15.310981\n",
      "epsilon:0.111848 episode_count: 14777. steps_count: 6533501.000000\n",
      "ep 2111: ep_len:640 episode reward: total was -24.910000. running mean: -15.406971\n",
      "ep 2111: ep_len:625 episode reward: total was 7.570000. running mean: -15.177201\n",
      "ep 2111: ep_len:620 episode reward: total was -41.380000. running mean: -15.439229\n",
      "ep 2111: ep_len:510 episode reward: total was -14.050000. running mean: -15.425337\n",
      "ep 2111: ep_len:54 episode reward: total was 2.000000. running mean: -15.251084\n",
      "ep 2111: ep_len:590 episode reward: total was -13.230000. running mean: -15.230873\n",
      "ep 2111: ep_len:545 episode reward: total was -21.920000. running mean: -15.297764\n",
      "epsilon:0.111712 episode_count: 14784. steps_count: 6537085.000000\n",
      "ep 2112: ep_len:213 episode reward: total was -13.900000. running mean: -15.283786\n",
      "ep 2112: ep_len:585 episode reward: total was -19.180000. running mean: -15.322749\n",
      "ep 2112: ep_len:580 episode reward: total was -10.580000. running mean: -15.275321\n",
      "ep 2112: ep_len:530 episode reward: total was 5.440000. running mean: -15.068168\n",
      "ep 2112: ep_len:77 episode reward: total was 3.040000. running mean: -14.887086\n",
      "ep 2112: ep_len:545 episode reward: total was -17.930000. running mean: -14.917515\n",
      "ep 2112: ep_len:500 episode reward: total was -17.780000. running mean: -14.946140\n",
      "epsilon:0.111575 episode_count: 14791. steps_count: 6540115.000000\n",
      "ep 2113: ep_len:665 episode reward: total was -24.730000. running mean: -15.043979\n",
      "ep 2113: ep_len:510 episode reward: total was -18.500000. running mean: -15.078539\n",
      "ep 2113: ep_len:630 episode reward: total was -29.860000. running mean: -15.226354\n",
      "ep 2113: ep_len:550 episode reward: total was -19.500000. running mean: -15.269090\n",
      "ep 2113: ep_len:3 episode reward: total was 0.000000. running mean: -15.116399\n",
      "ep 2113: ep_len:177 episode reward: total was 1.620000. running mean: -14.949035\n",
      "ep 2113: ep_len:560 episode reward: total was -21.960000. running mean: -15.019145\n",
      "epsilon:0.111439 episode_count: 14798. steps_count: 6543210.000000\n",
      "ep 2114: ep_len:570 episode reward: total was -15.640000. running mean: -15.025353\n",
      "ep 2114: ep_len:625 episode reward: total was -4.100000. running mean: -14.916100\n",
      "ep 2114: ep_len:570 episode reward: total was -21.290000. running mean: -14.979839\n",
      "ep 2114: ep_len:625 episode reward: total was -7.480000. running mean: -14.904840\n",
      "ep 2114: ep_len:3 episode reward: total was 0.000000. running mean: -14.755792\n",
      "ep 2114: ep_len:600 episode reward: total was -37.160000. running mean: -14.979834\n",
      "ep 2114: ep_len:535 episode reward: total was -26.930000. running mean: -15.099336\n",
      "epsilon:0.111302 episode_count: 14805. steps_count: 6546738.000000\n",
      "ep 2115: ep_len:550 episode reward: total was -11.950000. running mean: -15.067842\n",
      "ep 2115: ep_len:500 episode reward: total was -15.270000. running mean: -15.069864\n",
      "ep 2115: ep_len:426 episode reward: total was -13.820000. running mean: -15.057365\n",
      "ep 2115: ep_len:580 episode reward: total was -53.700000. running mean: -15.443792\n",
      "ep 2115: ep_len:3 episode reward: total was 0.000000. running mean: -15.289354\n",
      "ep 2115: ep_len:565 episode reward: total was -9.090000. running mean: -15.227360\n",
      "ep 2115: ep_len:330 episode reward: total was -11.830000. running mean: -15.193387\n",
      "epsilon:0.111166 episode_count: 14812. steps_count: 6549692.000000\n",
      "ep 2116: ep_len:640 episode reward: total was -27.290000. running mean: -15.314353\n",
      "ep 2116: ep_len:505 episode reward: total was -37.140000. running mean: -15.532609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2116: ep_len:79 episode reward: total was 1.050000. running mean: -15.366783\n",
      "ep 2116: ep_len:407 episode reward: total was -6.190000. running mean: -15.275015\n",
      "ep 2116: ep_len:3 episode reward: total was 0.000000. running mean: -15.122265\n",
      "ep 2116: ep_len:535 episode reward: total was -20.900000. running mean: -15.180043\n",
      "ep 2116: ep_len:327 episode reward: total was -13.290000. running mean: -15.161142\n",
      "epsilon:0.111029 episode_count: 14819. steps_count: 6552188.000000\n",
      "ep 2117: ep_len:555 episode reward: total was -11.710000. running mean: -15.126631\n",
      "ep 2117: ep_len:505 episode reward: total was -8.650000. running mean: -15.061864\n",
      "ep 2117: ep_len:540 episode reward: total was -25.910000. running mean: -15.170346\n",
      "ep 2117: ep_len:510 episode reward: total was -16.590000. running mean: -15.184542\n",
      "ep 2117: ep_len:3 episode reward: total was 0.000000. running mean: -15.032697\n",
      "ep 2117: ep_len:540 episode reward: total was -25.530000. running mean: -15.137670\n",
      "ep 2117: ep_len:204 episode reward: total was -5.850000. running mean: -15.044793\n",
      "epsilon:0.110893 episode_count: 14826. steps_count: 6555045.000000\n",
      "ep 2118: ep_len:630 episode reward: total was -12.970000. running mean: -15.024045\n",
      "ep 2118: ep_len:575 episode reward: total was -10.100000. running mean: -14.974805\n",
      "ep 2118: ep_len:500 episode reward: total was -12.950000. running mean: -14.954557\n",
      "ep 2118: ep_len:545 episode reward: total was -10.430000. running mean: -14.909311\n",
      "ep 2118: ep_len:3 episode reward: total was 0.000000. running mean: -14.760218\n",
      "ep 2118: ep_len:600 episode reward: total was -42.630000. running mean: -15.038916\n",
      "ep 2118: ep_len:298 episode reward: total was -3.810000. running mean: -14.926627\n",
      "epsilon:0.110756 episode_count: 14833. steps_count: 6558196.000000\n",
      "ep 2119: ep_len:540 episode reward: total was -11.970000. running mean: -14.897060\n",
      "ep 2119: ep_len:530 episode reward: total was -13.160000. running mean: -14.879690\n",
      "ep 2119: ep_len:530 episode reward: total was -12.660000. running mean: -14.857493\n",
      "ep 2119: ep_len:107 episode reward: total was 2.060000. running mean: -14.688318\n",
      "ep 2119: ep_len:105 episode reward: total was -4.430000. running mean: -14.585735\n",
      "ep 2119: ep_len:505 episode reward: total was -35.010000. running mean: -14.789978\n",
      "ep 2119: ep_len:500 episode reward: total was -24.490000. running mean: -14.886978\n",
      "epsilon:0.110620 episode_count: 14840. steps_count: 6561013.000000\n",
      "ep 2120: ep_len:650 episode reward: total was -33.760000. running mean: -15.075708\n",
      "ep 2120: ep_len:630 episode reward: total was -21.920000. running mean: -15.144151\n",
      "ep 2120: ep_len:444 episode reward: total was -1.750000. running mean: -15.010209\n",
      "ep 2120: ep_len:560 episode reward: total was 2.910000. running mean: -14.831007\n",
      "ep 2120: ep_len:48 episode reward: total was 4.500000. running mean: -14.637697\n",
      "ep 2120: ep_len:500 episode reward: total was -10.780000. running mean: -14.599120\n",
      "ep 2120: ep_len:545 episode reward: total was -41.680000. running mean: -14.869929\n",
      "epsilon:0.110483 episode_count: 14847. steps_count: 6564390.000000\n",
      "ep 2121: ep_len:205 episode reward: total was -5.930000. running mean: -14.780530\n",
      "ep 2121: ep_len:500 episode reward: total was -8.390000. running mean: -14.716624\n",
      "ep 2121: ep_len:500 episode reward: total was -14.540000. running mean: -14.714858\n",
      "ep 2121: ep_len:520 episode reward: total was -17.550000. running mean: -14.743210\n",
      "ep 2121: ep_len:3 episode reward: total was 0.000000. running mean: -14.595778\n",
      "ep 2121: ep_len:540 episode reward: total was -3.490000. running mean: -14.484720\n",
      "ep 2121: ep_len:500 episode reward: total was -14.610000. running mean: -14.485973\n",
      "epsilon:0.110347 episode_count: 14854. steps_count: 6567158.000000\n",
      "ep 2122: ep_len:500 episode reward: total was -10.260000. running mean: -14.443713\n",
      "ep 2122: ep_len:500 episode reward: total was -2.420000. running mean: -14.323476\n",
      "ep 2122: ep_len:565 episode reward: total was -19.480000. running mean: -14.375041\n",
      "ep 2122: ep_len:545 episode reward: total was -10.060000. running mean: -14.331891\n",
      "ep 2122: ep_len:3 episode reward: total was 0.000000. running mean: -14.188572\n",
      "ep 2122: ep_len:204 episode reward: total was 2.140000. running mean: -14.025286\n",
      "ep 2122: ep_len:590 episode reward: total was -34.700000. running mean: -14.232033\n",
      "epsilon:0.110210 episode_count: 14861. steps_count: 6570065.000000\n",
      "ep 2123: ep_len:610 episode reward: total was -23.010000. running mean: -14.319813\n",
      "ep 2123: ep_len:515 episode reward: total was 0.320000. running mean: -14.173415\n",
      "ep 2123: ep_len:585 episode reward: total was -30.520000. running mean: -14.336880\n",
      "ep 2123: ep_len:500 episode reward: total was 3.670000. running mean: -14.156812\n",
      "ep 2123: ep_len:3 episode reward: total was 0.000000. running mean: -14.015244\n",
      "ep 2123: ep_len:710 episode reward: total was -30.790000. running mean: -14.182991\n",
      "ep 2123: ep_len:500 episode reward: total was -27.570000. running mean: -14.316861\n",
      "epsilon:0.110074 episode_count: 14868. steps_count: 6573488.000000\n",
      "ep 2124: ep_len:525 episode reward: total was -31.480000. running mean: -14.488493\n",
      "ep 2124: ep_len:600 episode reward: total was 5.600000. running mean: -14.287608\n",
      "ep 2124: ep_len:555 episode reward: total was -19.820000. running mean: -14.342932\n",
      "ep 2124: ep_len:500 episode reward: total was -12.520000. running mean: -14.324702\n",
      "ep 2124: ep_len:3 episode reward: total was 0.000000. running mean: -14.181455\n",
      "ep 2124: ep_len:1065 episode reward: total was -91.570000. running mean: -14.955341\n",
      "ep 2124: ep_len:570 episode reward: total was -11.910000. running mean: -14.924887\n",
      "epsilon:0.109937 episode_count: 14875. steps_count: 6577306.000000\n",
      "ep 2125: ep_len:595 episode reward: total was -45.390000. running mean: -15.229538\n",
      "ep 2125: ep_len:500 episode reward: total was -12.060000. running mean: -15.197843\n",
      "ep 2125: ep_len:585 episode reward: total was -13.440000. running mean: -15.180265\n",
      "ep 2125: ep_len:545 episode reward: total was -7.500000. running mean: -15.103462\n",
      "ep 2125: ep_len:49 episode reward: total was 3.000000. running mean: -14.922427\n",
      "ep 2125: ep_len:530 episode reward: total was -32.060000. running mean: -15.093803\n",
      "ep 2125: ep_len:500 episode reward: total was -8.130000. running mean: -15.024165\n",
      "epsilon:0.109801 episode_count: 14882. steps_count: 6580610.000000\n",
      "ep 2126: ep_len:500 episode reward: total was -10.120000. running mean: -14.975123\n",
      "ep 2126: ep_len:590 episode reward: total was -11.410000. running mean: -14.939472\n",
      "ep 2126: ep_len:645 episode reward: total was -1.860000. running mean: -14.808677\n",
      "ep 2126: ep_len:505 episode reward: total was -33.680000. running mean: -14.997391\n",
      "ep 2126: ep_len:3 episode reward: total was 0.000000. running mean: -14.847417\n",
      "ep 2126: ep_len:630 episode reward: total was -10.700000. running mean: -14.805943\n",
      "ep 2126: ep_len:505 episode reward: total was -15.850000. running mean: -14.816383\n",
      "epsilon:0.109664 episode_count: 14889. steps_count: 6583988.000000\n",
      "ep 2127: ep_len:510 episode reward: total was -1.680000. running mean: -14.685019\n",
      "ep 2127: ep_len:640 episode reward: total was 9.330000. running mean: -14.444869\n",
      "ep 2127: ep_len:535 episode reward: total was -2.130000. running mean: -14.321720\n",
      "ep 2127: ep_len:500 episode reward: total was -10.990000. running mean: -14.288403\n",
      "ep 2127: ep_len:3 episode reward: total was 0.000000. running mean: -14.145519\n",
      "ep 2127: ep_len:500 episode reward: total was -31.290000. running mean: -14.316964\n",
      "ep 2127: ep_len:630 episode reward: total was -54.040000. running mean: -14.714194\n",
      "epsilon:0.109528 episode_count: 14896. steps_count: 6587306.000000\n",
      "ep 2128: ep_len:225 episode reward: total was -6.380000. running mean: -14.630852\n",
      "ep 2128: ep_len:570 episode reward: total was -20.370000. running mean: -14.688244\n",
      "ep 2128: ep_len:600 episode reward: total was -16.970000. running mean: -14.711061\n",
      "ep 2128: ep_len:570 episode reward: total was -22.000000. running mean: -14.783951\n",
      "ep 2128: ep_len:3 episode reward: total was 0.000000. running mean: -14.636111\n",
      "ep 2128: ep_len:600 episode reward: total was -27.810000. running mean: -14.767850\n",
      "ep 2128: ep_len:342 episode reward: total was -7.780000. running mean: -14.697972\n",
      "epsilon:0.109391 episode_count: 14903. steps_count: 6590216.000000\n",
      "ep 2129: ep_len:565 episode reward: total was -40.890000. running mean: -14.959892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2129: ep_len:525 episode reward: total was -3.350000. running mean: -14.843793\n",
      "ep 2129: ep_len:570 episode reward: total was -9.560000. running mean: -14.790955\n",
      "ep 2129: ep_len:535 episode reward: total was -9.100000. running mean: -14.734046\n",
      "ep 2129: ep_len:93 episode reward: total was -8.940000. running mean: -14.676105\n",
      "ep 2129: ep_len:500 episode reward: total was -7.640000. running mean: -14.605744\n",
      "ep 2129: ep_len:510 episode reward: total was -17.120000. running mean: -14.630887\n",
      "epsilon:0.109255 episode_count: 14910. steps_count: 6593514.000000\n",
      "ep 2130: ep_len:510 episode reward: total was -0.150000. running mean: -14.486078\n",
      "ep 2130: ep_len:525 episode reward: total was -24.520000. running mean: -14.586417\n",
      "ep 2130: ep_len:590 episode reward: total was -15.270000. running mean: -14.593253\n",
      "ep 2130: ep_len:505 episode reward: total was 3.400000. running mean: -14.413320\n",
      "ep 2130: ep_len:89 episode reward: total was -11.980000. running mean: -14.388987\n",
      "ep 2130: ep_len:500 episode reward: total was -4.500000. running mean: -14.290097\n",
      "ep 2130: ep_len:500 episode reward: total was -30.120000. running mean: -14.448396\n",
      "epsilon:0.109118 episode_count: 14917. steps_count: 6596733.000000\n",
      "ep 2131: ep_len:540 episode reward: total was -6.890000. running mean: -14.372812\n",
      "ep 2131: ep_len:505 episode reward: total was 6.330000. running mean: -14.165784\n",
      "ep 2131: ep_len:520 episode reward: total was -30.970000. running mean: -14.333826\n",
      "ep 2131: ep_len:500 episode reward: total was -19.640000. running mean: -14.386888\n",
      "ep 2131: ep_len:3 episode reward: total was 0.000000. running mean: -14.243019\n",
      "ep 2131: ep_len:660 episode reward: total was -80.730000. running mean: -14.907889\n",
      "ep 2131: ep_len:500 episode reward: total was -16.140000. running mean: -14.920210\n",
      "epsilon:0.108982 episode_count: 14924. steps_count: 6599961.000000\n",
      "ep 2132: ep_len:525 episode reward: total was -23.950000. running mean: -15.010508\n",
      "ep 2132: ep_len:500 episode reward: total was -2.200000. running mean: -14.882403\n",
      "ep 2132: ep_len:555 episode reward: total was -11.100000. running mean: -14.844579\n",
      "ep 2132: ep_len:500 episode reward: total was -22.180000. running mean: -14.917933\n",
      "ep 2132: ep_len:3 episode reward: total was 0.000000. running mean: -14.768754\n",
      "ep 2132: ep_len:535 episode reward: total was -18.640000. running mean: -14.807466\n",
      "ep 2132: ep_len:201 episode reward: total was -5.880000. running mean: -14.718192\n",
      "epsilon:0.108845 episode_count: 14931. steps_count: 6602780.000000\n",
      "ep 2133: ep_len:500 episode reward: total was 3.220000. running mean: -14.538810\n",
      "ep 2133: ep_len:555 episode reward: total was -5.910000. running mean: -14.452522\n",
      "ep 2133: ep_len:535 episode reward: total was -16.340000. running mean: -14.471396\n",
      "ep 2133: ep_len:610 episode reward: total was -3.490000. running mean: -14.361582\n",
      "ep 2133: ep_len:100 episode reward: total was 4.560000. running mean: -14.172367\n",
      "ep 2133: ep_len:505 episode reward: total was -5.080000. running mean: -14.081443\n",
      "ep 2133: ep_len:205 episode reward: total was -6.850000. running mean: -14.009128\n",
      "epsilon:0.108709 episode_count: 14938. steps_count: 6605790.000000\n",
      "ep 2134: ep_len:575 episode reward: total was -11.440000. running mean: -13.983437\n",
      "ep 2134: ep_len:525 episode reward: total was 3.170000. running mean: -13.811903\n",
      "ep 2134: ep_len:371 episode reward: total was 2.210000. running mean: -13.651684\n",
      "ep 2134: ep_len:505 episode reward: total was -15.010000. running mean: -13.665267\n",
      "ep 2134: ep_len:3 episode reward: total was 0.000000. running mean: -13.528614\n",
      "ep 2134: ep_len:505 episode reward: total was -21.270000. running mean: -13.606028\n",
      "ep 2134: ep_len:535 episode reward: total was -9.900000. running mean: -13.568968\n",
      "epsilon:0.108572 episode_count: 14945. steps_count: 6608809.000000\n",
      "ep 2135: ep_len:685 episode reward: total was -33.290000. running mean: -13.766178\n",
      "ep 2135: ep_len:305 episode reward: total was -12.300000. running mean: -13.751516\n",
      "ep 2135: ep_len:550 episode reward: total was 9.430000. running mean: -13.519701\n",
      "ep 2135: ep_len:510 episode reward: total was -6.630000. running mean: -13.450804\n",
      "ep 2135: ep_len:3 episode reward: total was 0.000000. running mean: -13.316296\n",
      "ep 2135: ep_len:535 episode reward: total was -30.060000. running mean: -13.483733\n",
      "ep 2135: ep_len:270 episode reward: total was -16.840000. running mean: -13.517296\n",
      "epsilon:0.108436 episode_count: 14952. steps_count: 6611667.000000\n",
      "ep 2136: ep_len:525 episode reward: total was -22.500000. running mean: -13.607123\n",
      "ep 2136: ep_len:550 episode reward: total was -14.480000. running mean: -13.615852\n",
      "ep 2136: ep_len:540 episode reward: total was -15.110000. running mean: -13.630793\n",
      "ep 2136: ep_len:525 episode reward: total was -3.450000. running mean: -13.528985\n",
      "ep 2136: ep_len:3 episode reward: total was 0.000000. running mean: -13.393695\n",
      "ep 2136: ep_len:605 episode reward: total was -7.570000. running mean: -13.335458\n",
      "ep 2136: ep_len:580 episode reward: total was -31.660000. running mean: -13.518704\n",
      "epsilon:0.108299 episode_count: 14959. steps_count: 6614995.000000\n",
      "ep 2137: ep_len:660 episode reward: total was -25.260000. running mean: -13.636117\n",
      "ep 2137: ep_len:550 episode reward: total was -14.730000. running mean: -13.647056\n",
      "ep 2137: ep_len:375 episode reward: total was -5.830000. running mean: -13.568885\n",
      "ep 2137: ep_len:530 episode reward: total was -57.130000. running mean: -14.004496\n",
      "ep 2137: ep_len:86 episode reward: total was 0.550000. running mean: -13.858951\n",
      "ep 2137: ep_len:284 episode reward: total was -11.860000. running mean: -13.838962\n",
      "ep 2137: ep_len:640 episode reward: total was -23.890000. running mean: -13.939472\n",
      "epsilon:0.108163 episode_count: 14966. steps_count: 6618120.000000\n",
      "ep 2138: ep_len:248 episode reward: total was 5.110000. running mean: -13.748977\n",
      "ep 2138: ep_len:665 episode reward: total was -47.930000. running mean: -14.090788\n",
      "ep 2138: ep_len:620 episode reward: total was -3.550000. running mean: -13.985380\n",
      "ep 2138: ep_len:600 episode reward: total was -20.470000. running mean: -14.050226\n",
      "ep 2138: ep_len:3 episode reward: total was 0.000000. running mean: -13.909724\n",
      "ep 2138: ep_len:580 episode reward: total was -19.120000. running mean: -13.961826\n",
      "ep 2138: ep_len:505 episode reward: total was -29.960000. running mean: -14.121808\n",
      "epsilon:0.108026 episode_count: 14973. steps_count: 6621341.000000\n",
      "ep 2139: ep_len:580 episode reward: total was -4.480000. running mean: -14.025390\n",
      "ep 2139: ep_len:560 episode reward: total was -7.460000. running mean: -13.959736\n",
      "ep 2139: ep_len:510 episode reward: total was -5.600000. running mean: -13.876139\n",
      "ep 2139: ep_len:585 episode reward: total was -10.950000. running mean: -13.846877\n",
      "ep 2139: ep_len:95 episode reward: total was 3.050000. running mean: -13.677909\n",
      "ep 2139: ep_len:505 episode reward: total was -8.800000. running mean: -13.629130\n",
      "ep 2139: ep_len:530 episode reward: total was -10.870000. running mean: -13.601538\n",
      "epsilon:0.107890 episode_count: 14980. steps_count: 6624706.000000\n",
      "ep 2140: ep_len:670 episode reward: total was -17.240000. running mean: -13.637923\n",
      "ep 2140: ep_len:500 episode reward: total was -3.490000. running mean: -13.536444\n",
      "ep 2140: ep_len:550 episode reward: total was -29.920000. running mean: -13.700279\n",
      "ep 2140: ep_len:575 episode reward: total was -1.070000. running mean: -13.573976\n",
      "ep 2140: ep_len:3 episode reward: total was 0.000000. running mean: -13.438237\n",
      "ep 2140: ep_len:645 episode reward: total was -37.550000. running mean: -13.679354\n",
      "ep 2140: ep_len:585 episode reward: total was -24.110000. running mean: -13.783661\n",
      "epsilon:0.107753 episode_count: 14987. steps_count: 6628234.000000\n",
      "ep 2141: ep_len:540 episode reward: total was -34.560000. running mean: -13.991424\n",
      "ep 2141: ep_len:585 episode reward: total was -26.710000. running mean: -14.118610\n",
      "ep 2141: ep_len:560 episode reward: total was -28.370000. running mean: -14.261124\n",
      "ep 2141: ep_len:102 episode reward: total was 1.560000. running mean: -14.102913\n",
      "ep 2141: ep_len:3 episode reward: total was 0.000000. running mean: -13.961883\n",
      "ep 2141: ep_len:335 episode reward: total was -10.370000. running mean: -13.925965\n",
      "ep 2141: ep_len:610 episode reward: total was -31.090000. running mean: -14.097605\n",
      "epsilon:0.107617 episode_count: 14994. steps_count: 6630969.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2142: ep_len:249 episode reward: total was -6.880000. running mean: -14.025429\n",
      "ep 2142: ep_len:580 episode reward: total was -28.990000. running mean: -14.175075\n",
      "ep 2142: ep_len:560 episode reward: total was -26.990000. running mean: -14.303224\n",
      "ep 2142: ep_len:510 episode reward: total was -9.540000. running mean: -14.255592\n",
      "ep 2142: ep_len:3 episode reward: total was 0.000000. running mean: -14.113036\n",
      "ep 2142: ep_len:515 episode reward: total was -22.630000. running mean: -14.198205\n",
      "ep 2142: ep_len:274 episode reward: total was -11.840000. running mean: -14.174623\n",
      "epsilon:0.107480 episode_count: 15001. steps_count: 6633660.000000\n",
      "ep 2143: ep_len:615 episode reward: total was -5.600000. running mean: -14.088877\n",
      "ep 2143: ep_len:500 episode reward: total was -8.020000. running mean: -14.028188\n",
      "ep 2143: ep_len:655 episode reward: total was -38.840000. running mean: -14.276306\n",
      "ep 2143: ep_len:114 episode reward: total was 0.110000. running mean: -14.132443\n",
      "ep 2143: ep_len:96 episode reward: total was 4.030000. running mean: -13.950819\n",
      "ep 2143: ep_len:255 episode reward: total was 4.670000. running mean: -13.764611\n",
      "ep 2143: ep_len:560 episode reward: total was -33.680000. running mean: -13.963765\n",
      "epsilon:0.107344 episode_count: 15008. steps_count: 6636455.000000\n",
      "ep 2144: ep_len:645 episode reward: total was -41.880000. running mean: -14.242927\n",
      "ep 2144: ep_len:555 episode reward: total was 8.360000. running mean: -14.016898\n",
      "ep 2144: ep_len:500 episode reward: total was -10.040000. running mean: -13.977129\n",
      "ep 2144: ep_len:525 episode reward: total was -27.130000. running mean: -14.108657\n",
      "ep 2144: ep_len:3 episode reward: total was 0.000000. running mean: -13.967571\n",
      "ep 2144: ep_len:500 episode reward: total was -5.470000. running mean: -13.882595\n",
      "ep 2144: ep_len:600 episode reward: total was -14.110000. running mean: -13.884869\n",
      "epsilon:0.107207 episode_count: 15015. steps_count: 6639783.000000\n",
      "ep 2145: ep_len:510 episode reward: total was -4.730000. running mean: -13.793321\n",
      "ep 2145: ep_len:580 episode reward: total was -31.130000. running mean: -13.966687\n",
      "ep 2145: ep_len:358 episode reward: total was -4.830000. running mean: -13.875320\n",
      "ep 2145: ep_len:145 episode reward: total was -0.390000. running mean: -13.740467\n",
      "ep 2145: ep_len:3 episode reward: total was 0.000000. running mean: -13.603063\n",
      "ep 2145: ep_len:500 episode reward: total was -9.600000. running mean: -13.563032\n",
      "ep 2145: ep_len:685 episode reward: total was -43.440000. running mean: -13.861802\n",
      "epsilon:0.107071 episode_count: 15022. steps_count: 6642564.000000\n",
      "ep 2146: ep_len:500 episode reward: total was -36.090000. running mean: -14.084084\n",
      "ep 2146: ep_len:505 episode reward: total was -40.150000. running mean: -14.344743\n",
      "ep 2146: ep_len:565 episode reward: total was -22.670000. running mean: -14.427995\n",
      "ep 2146: ep_len:126 episode reward: total was 0.110000. running mean: -14.282615\n",
      "ep 2146: ep_len:3 episode reward: total was 0.000000. running mean: -14.139789\n",
      "ep 2146: ep_len:525 episode reward: total was -18.600000. running mean: -14.184391\n",
      "ep 2146: ep_len:580 episode reward: total was -19.080000. running mean: -14.233347\n",
      "epsilon:0.106934 episode_count: 15029. steps_count: 6645368.000000\n",
      "ep 2147: ep_len:575 episode reward: total was -12.150000. running mean: -14.212514\n",
      "ep 2147: ep_len:620 episode reward: total was 22.380000. running mean: -13.846589\n",
      "ep 2147: ep_len:625 episode reward: total was -8.420000. running mean: -13.792323\n",
      "ep 2147: ep_len:525 episode reward: total was -25.550000. running mean: -13.909900\n",
      "ep 2147: ep_len:46 episode reward: total was -1.000000. running mean: -13.780801\n",
      "ep 2147: ep_len:500 episode reward: total was -7.260000. running mean: -13.715593\n",
      "ep 2147: ep_len:281 episode reward: total was -3.320000. running mean: -13.611637\n",
      "epsilon:0.106798 episode_count: 15036. steps_count: 6648540.000000\n",
      "ep 2148: ep_len:615 episode reward: total was -13.990000. running mean: -13.615420\n",
      "ep 2148: ep_len:520 episode reward: total was -16.400000. running mean: -13.643266\n",
      "ep 2148: ep_len:710 episode reward: total was -36.270000. running mean: -13.869534\n",
      "ep 2148: ep_len:55 episode reward: total was -1.420000. running mean: -13.745038\n",
      "ep 2148: ep_len:3 episode reward: total was 0.000000. running mean: -13.607588\n",
      "ep 2148: ep_len:505 episode reward: total was -17.610000. running mean: -13.647612\n",
      "ep 2148: ep_len:500 episode reward: total was -13.930000. running mean: -13.650436\n",
      "epsilon:0.106661 episode_count: 15043. steps_count: 6651448.000000\n",
      "ep 2149: ep_len:540 episode reward: total was -17.600000. running mean: -13.689931\n",
      "ep 2149: ep_len:600 episode reward: total was -2.660000. running mean: -13.579632\n",
      "ep 2149: ep_len:515 episode reward: total was -22.340000. running mean: -13.667236\n",
      "ep 2149: ep_len:509 episode reward: total was -10.010000. running mean: -13.630663\n",
      "ep 2149: ep_len:104 episode reward: total was 0.030000. running mean: -13.494057\n",
      "ep 2149: ep_len:560 episode reward: total was -18.140000. running mean: -13.540516\n",
      "ep 2149: ep_len:505 episode reward: total was -34.000000. running mean: -13.745111\n",
      "epsilon:0.106525 episode_count: 15050. steps_count: 6654781.000000\n",
      "ep 2150: ep_len:107 episode reward: total was -15.480000. running mean: -13.762460\n",
      "ep 2150: ep_len:560 episode reward: total was -7.510000. running mean: -13.699935\n",
      "ep 2150: ep_len:367 episode reward: total was 0.200000. running mean: -13.560936\n",
      "ep 2150: ep_len:520 episode reward: total was -30.610000. running mean: -13.731427\n",
      "ep 2150: ep_len:3 episode reward: total was 0.000000. running mean: -13.594112\n",
      "ep 2150: ep_len:510 episode reward: total was -34.630000. running mean: -13.804471\n",
      "ep 2150: ep_len:525 episode reward: total was -24.110000. running mean: -13.907527\n",
      "epsilon:0.106388 episode_count: 15057. steps_count: 6657373.000000\n",
      "ep 2151: ep_len:500 episode reward: total was -12.130000. running mean: -13.889751\n",
      "ep 2151: ep_len:515 episode reward: total was -4.990000. running mean: -13.800754\n",
      "ep 2151: ep_len:540 episode reward: total was -12.780000. running mean: -13.790546\n",
      "ep 2151: ep_len:500 episode reward: total was 1.710000. running mean: -13.635541\n",
      "ep 2151: ep_len:92 episode reward: total was 2.030000. running mean: -13.478885\n",
      "ep 2151: ep_len:595 episode reward: total was -23.580000. running mean: -13.579897\n",
      "ep 2151: ep_len:610 episode reward: total was -16.330000. running mean: -13.607398\n",
      "epsilon:0.106252 episode_count: 15064. steps_count: 6660725.000000\n",
      "ep 2152: ep_len:131 episode reward: total was 3.090000. running mean: -13.440424\n",
      "ep 2152: ep_len:500 episode reward: total was 3.720000. running mean: -13.268819\n",
      "ep 2152: ep_len:650 episode reward: total was -4.360000. running mean: -13.179731\n",
      "ep 2152: ep_len:556 episode reward: total was -34.430000. running mean: -13.392234\n",
      "ep 2152: ep_len:128 episode reward: total was -0.950000. running mean: -13.267812\n",
      "ep 2152: ep_len:610 episode reward: total was -48.750000. running mean: -13.622633\n",
      "ep 2152: ep_len:515 episode reward: total was -24.540000. running mean: -13.731807\n",
      "epsilon:0.106115 episode_count: 15071. steps_count: 6663815.000000\n",
      "ep 2153: ep_len:265 episode reward: total was -1.370000. running mean: -13.608189\n",
      "ep 2153: ep_len:620 episode reward: total was -30.060000. running mean: -13.772707\n",
      "ep 2153: ep_len:78 episode reward: total was -1.950000. running mean: -13.654480\n",
      "ep 2153: ep_len:595 episode reward: total was -9.520000. running mean: -13.613135\n",
      "ep 2153: ep_len:3 episode reward: total was 0.000000. running mean: -13.477004\n",
      "ep 2153: ep_len:232 episode reward: total was 2.660000. running mean: -13.315634\n",
      "ep 2153: ep_len:510 episode reward: total was -17.920000. running mean: -13.361678\n",
      "epsilon:0.105979 episode_count: 15078. steps_count: 6666118.000000\n",
      "ep 2154: ep_len:710 episode reward: total was -35.750000. running mean: -13.585561\n",
      "ep 2154: ep_len:550 episode reward: total was -2.470000. running mean: -13.474405\n",
      "ep 2154: ep_len:343 episode reward: total was -3.800000. running mean: -13.377661\n",
      "ep 2154: ep_len:515 episode reward: total was -14.070000. running mean: -13.384584\n",
      "ep 2154: ep_len:96 episode reward: total was 5.530000. running mean: -13.195439\n",
      "ep 2154: ep_len:575 episode reward: total was -27.270000. running mean: -13.336184\n",
      "ep 2154: ep_len:560 episode reward: total was -50.110000. running mean: -13.703922\n",
      "epsilon:0.105842 episode_count: 15085. steps_count: 6669467.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2155: ep_len:545 episode reward: total was -27.660000. running mean: -13.843483\n",
      "ep 2155: ep_len:510 episode reward: total was -16.300000. running mean: -13.868048\n",
      "ep 2155: ep_len:550 episode reward: total was -33.480000. running mean: -14.064168\n",
      "ep 2155: ep_len:695 episode reward: total was -43.920000. running mean: -14.362726\n",
      "ep 2155: ep_len:131 episode reward: total was 4.040000. running mean: -14.178699\n",
      "ep 2155: ep_len:650 episode reward: total was -19.280000. running mean: -14.229712\n",
      "ep 2155: ep_len:595 episode reward: total was -13.410000. running mean: -14.221515\n",
      "epsilon:0.105706 episode_count: 15092. steps_count: 6673143.000000\n",
      "ep 2156: ep_len:520 episode reward: total was -9.000000. running mean: -14.169300\n",
      "ep 2156: ep_len:515 episode reward: total was -24.000000. running mean: -14.267607\n",
      "ep 2156: ep_len:560 episode reward: total was -13.600000. running mean: -14.260931\n",
      "ep 2156: ep_len:505 episode reward: total was -21.690000. running mean: -14.335221\n",
      "ep 2156: ep_len:3 episode reward: total was 0.000000. running mean: -14.191869\n",
      "ep 2156: ep_len:500 episode reward: total was -27.260000. running mean: -14.322550\n",
      "ep 2156: ep_len:190 episode reward: total was -0.820000. running mean: -14.187525\n",
      "epsilon:0.105569 episode_count: 15099. steps_count: 6675936.000000\n",
      "ep 2157: ep_len:580 episode reward: total was -12.470000. running mean: -14.170350\n",
      "ep 2157: ep_len:575 episode reward: total was -12.030000. running mean: -14.148946\n",
      "ep 2157: ep_len:515 episode reward: total was -19.380000. running mean: -14.201257\n",
      "ep 2157: ep_len:510 episode reward: total was -31.650000. running mean: -14.375744\n",
      "ep 2157: ep_len:3 episode reward: total was 0.000000. running mean: -14.231987\n",
      "ep 2157: ep_len:755 episode reward: total was -77.480000. running mean: -14.864467\n",
      "ep 2157: ep_len:525 episode reward: total was -40.210000. running mean: -15.117922\n",
      "epsilon:0.105433 episode_count: 15106. steps_count: 6679399.000000\n",
      "ep 2158: ep_len:665 episode reward: total was -16.680000. running mean: -15.133543\n",
      "ep 2158: ep_len:500 episode reward: total was -26.340000. running mean: -15.245607\n",
      "ep 2158: ep_len:520 episode reward: total was -18.910000. running mean: -15.282251\n",
      "ep 2158: ep_len:530 episode reward: total was -27.050000. running mean: -15.399929\n",
      "ep 2158: ep_len:88 episode reward: total was 1.530000. running mean: -15.230630\n",
      "ep 2158: ep_len:610 episode reward: total was -32.920000. running mean: -15.407523\n",
      "ep 2158: ep_len:575 episode reward: total was -21.590000. running mean: -15.469348\n",
      "epsilon:0.105296 episode_count: 15113. steps_count: 6682887.000000\n",
      "ep 2159: ep_len:655 episode reward: total was -12.430000. running mean: -15.438955\n",
      "ep 2159: ep_len:585 episode reward: total was -23.040000. running mean: -15.514965\n",
      "ep 2159: ep_len:525 episode reward: total was -4.700000. running mean: -15.406815\n",
      "ep 2159: ep_len:500 episode reward: total was -1.980000. running mean: -15.272547\n",
      "ep 2159: ep_len:3 episode reward: total was 0.000000. running mean: -15.119822\n",
      "ep 2159: ep_len:224 episode reward: total was -11.350000. running mean: -15.082124\n",
      "ep 2159: ep_len:176 episode reward: total was -8.890000. running mean: -15.020202\n",
      "epsilon:0.105160 episode_count: 15120. steps_count: 6685555.000000\n",
      "ep 2160: ep_len:500 episode reward: total was -7.810000. running mean: -14.948100\n",
      "ep 2160: ep_len:505 episode reward: total was -30.510000. running mean: -15.103719\n",
      "ep 2160: ep_len:585 episode reward: total was -46.790000. running mean: -15.420582\n",
      "ep 2160: ep_len:550 episode reward: total was -34.100000. running mean: -15.607376\n",
      "ep 2160: ep_len:3 episode reward: total was 0.000000. running mean: -15.451303\n",
      "ep 2160: ep_len:500 episode reward: total was -8.960000. running mean: -15.386389\n",
      "ep 2160: ep_len:540 episode reward: total was -18.180000. running mean: -15.414326\n",
      "epsilon:0.105023 episode_count: 15127. steps_count: 6688738.000000\n",
      "ep 2161: ep_len:635 episode reward: total was -10.620000. running mean: -15.366382\n",
      "ep 2161: ep_len:525 episode reward: total was 0.680000. running mean: -15.205919\n",
      "ep 2161: ep_len:79 episode reward: total was -0.970000. running mean: -15.063559\n",
      "ep 2161: ep_len:500 episode reward: total was -2.520000. running mean: -14.938124\n",
      "ep 2161: ep_len:3 episode reward: total was 0.000000. running mean: -14.788743\n",
      "ep 2161: ep_len:500 episode reward: total was -24.520000. running mean: -14.886055\n",
      "ep 2161: ep_len:500 episode reward: total was -25.580000. running mean: -14.992995\n",
      "epsilon:0.104887 episode_count: 15134. steps_count: 6691480.000000\n",
      "ep 2162: ep_len:500 episode reward: total was -0.220000. running mean: -14.845265\n",
      "ep 2162: ep_len:500 episode reward: total was 6.730000. running mean: -14.629512\n",
      "ep 2162: ep_len:450 episode reward: total was -15.150000. running mean: -14.634717\n",
      "ep 2162: ep_len:520 episode reward: total was -19.020000. running mean: -14.678570\n",
      "ep 2162: ep_len:3 episode reward: total was 0.000000. running mean: -14.531784\n",
      "ep 2162: ep_len:500 episode reward: total was -10.250000. running mean: -14.488966\n",
      "ep 2162: ep_len:500 episode reward: total was -26.790000. running mean: -14.611976\n",
      "epsilon:0.104750 episode_count: 15141. steps_count: 6694453.000000\n",
      "ep 2163: ep_len:935 episode reward: total was -154.350000. running mean: -16.009357\n",
      "ep 2163: ep_len:530 episode reward: total was -57.320000. running mean: -16.422463\n",
      "ep 2163: ep_len:386 episode reward: total was -0.830000. running mean: -16.266538\n",
      "ep 2163: ep_len:575 episode reward: total was -4.560000. running mean: -16.149473\n",
      "ep 2163: ep_len:100 episode reward: total was -1.960000. running mean: -16.007578\n",
      "ep 2163: ep_len:520 episode reward: total was -67.270000. running mean: -16.520203\n",
      "ep 2163: ep_len:284 episode reward: total was -8.850000. running mean: -16.443501\n",
      "epsilon:0.104614 episode_count: 15148. steps_count: 6697783.000000\n",
      "ep 2164: ep_len:595 episode reward: total was -26.890000. running mean: -16.547966\n",
      "ep 2164: ep_len:500 episode reward: total was -29.370000. running mean: -16.676186\n",
      "ep 2164: ep_len:500 episode reward: total was 1.220000. running mean: -16.497224\n",
      "ep 2164: ep_len:108 episode reward: total was 3.580000. running mean: -16.296452\n",
      "ep 2164: ep_len:3 episode reward: total was 0.000000. running mean: -16.133487\n",
      "ep 2164: ep_len:620 episode reward: total was -30.250000. running mean: -16.274652\n",
      "ep 2164: ep_len:620 episode reward: total was -9.850000. running mean: -16.210406\n",
      "epsilon:0.104477 episode_count: 15155. steps_count: 6700729.000000\n",
      "ep 2165: ep_len:215 episode reward: total was -3.890000. running mean: -16.087202\n",
      "ep 2165: ep_len:330 episode reward: total was -8.810000. running mean: -16.014430\n",
      "ep 2165: ep_len:580 episode reward: total was -48.680000. running mean: -16.341086\n",
      "ep 2165: ep_len:56 episode reward: total was 0.550000. running mean: -16.172175\n",
      "ep 2165: ep_len:93 episode reward: total was 1.540000. running mean: -15.995053\n",
      "ep 2165: ep_len:500 episode reward: total was 0.350000. running mean: -15.831602\n",
      "ep 2165: ep_len:590 episode reward: total was -9.440000. running mean: -15.767686\n",
      "epsilon:0.104341 episode_count: 15162. steps_count: 6703093.000000\n",
      "ep 2166: ep_len:505 episode reward: total was -15.610000. running mean: -15.766109\n",
      "ep 2166: ep_len:500 episode reward: total was -26.070000. running mean: -15.869148\n",
      "ep 2166: ep_len:565 episode reward: total was -22.310000. running mean: -15.933557\n",
      "ep 2166: ep_len:655 episode reward: total was -45.070000. running mean: -16.224921\n",
      "ep 2166: ep_len:92 episode reward: total was 3.040000. running mean: -16.032272\n",
      "ep 2166: ep_len:329 episode reward: total was -7.360000. running mean: -15.945549\n",
      "ep 2166: ep_len:535 episode reward: total was -27.020000. running mean: -16.056294\n",
      "epsilon:0.104204 episode_count: 15169. steps_count: 6706274.000000\n",
      "ep 2167: ep_len:248 episode reward: total was -4.400000. running mean: -15.939731\n",
      "ep 2167: ep_len:620 episode reward: total was -30.110000. running mean: -16.081434\n",
      "ep 2167: ep_len:545 episode reward: total was -8.120000. running mean: -16.001819\n",
      "ep 2167: ep_len:125 episode reward: total was 1.120000. running mean: -15.830601\n",
      "ep 2167: ep_len:3 episode reward: total was 0.000000. running mean: -15.672295\n",
      "ep 2167: ep_len:520 episode reward: total was -18.160000. running mean: -15.697172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2167: ep_len:580 episode reward: total was -29.000000. running mean: -15.830200\n",
      "epsilon:0.104068 episode_count: 15176. steps_count: 6708915.000000\n",
      "ep 2168: ep_len:101 episode reward: total was -7.950000. running mean: -15.751398\n",
      "ep 2168: ep_len:535 episode reward: total was 0.010000. running mean: -15.593784\n",
      "ep 2168: ep_len:660 episode reward: total was -26.300000. running mean: -15.700847\n",
      "ep 2168: ep_len:535 episode reward: total was -8.190000. running mean: -15.625738\n",
      "ep 2168: ep_len:3 episode reward: total was 0.000000. running mean: -15.469481\n",
      "ep 2168: ep_len:500 episode reward: total was -21.610000. running mean: -15.530886\n",
      "ep 2168: ep_len:500 episode reward: total was -14.730000. running mean: -15.522877\n",
      "epsilon:0.103931 episode_count: 15183. steps_count: 6711749.000000\n",
      "ep 2169: ep_len:590 episode reward: total was -8.600000. running mean: -15.453648\n",
      "ep 2169: ep_len:297 episode reward: total was -40.870000. running mean: -15.707812\n",
      "ep 2169: ep_len:69 episode reward: total was -3.960000. running mean: -15.590334\n",
      "ep 2169: ep_len:525 episode reward: total was -9.690000. running mean: -15.531330\n",
      "ep 2169: ep_len:3 episode reward: total was 0.000000. running mean: -15.376017\n",
      "ep 2169: ep_len:232 episode reward: total was -7.870000. running mean: -15.300957\n",
      "ep 2169: ep_len:515 episode reward: total was -14.090000. running mean: -15.288847\n",
      "epsilon:0.103795 episode_count: 15190. steps_count: 6713980.000000\n",
      "ep 2170: ep_len:185 episode reward: total was -9.430000. running mean: -15.230259\n",
      "ep 2170: ep_len:287 episode reward: total was -11.360000. running mean: -15.191556\n",
      "ep 2170: ep_len:575 episode reward: total was -7.130000. running mean: -15.110941\n",
      "ep 2170: ep_len:505 episode reward: total was -2.540000. running mean: -14.985231\n",
      "ep 2170: ep_len:3 episode reward: total was 0.000000. running mean: -14.835379\n",
      "ep 2170: ep_len:565 episode reward: total was -40.000000. running mean: -15.087025\n",
      "ep 2170: ep_len:585 episode reward: total was -18.100000. running mean: -15.117155\n",
      "epsilon:0.103658 episode_count: 15197. steps_count: 6716685.000000\n",
      "ep 2171: ep_len:240 episode reward: total was -7.930000. running mean: -15.045283\n",
      "ep 2171: ep_len:615 episode reward: total was 6.020000. running mean: -14.834631\n",
      "ep 2171: ep_len:580 episode reward: total was -22.480000. running mean: -14.911084\n",
      "ep 2171: ep_len:610 episode reward: total was -8.740000. running mean: -14.849373\n",
      "ep 2171: ep_len:113 episode reward: total was 3.050000. running mean: -14.670380\n",
      "ep 2171: ep_len:500 episode reward: total was -29.250000. running mean: -14.816176\n",
      "ep 2171: ep_len:575 episode reward: total was -28.400000. running mean: -14.952014\n",
      "epsilon:0.103522 episode_count: 15204. steps_count: 6719918.000000\n",
      "ep 2172: ep_len:198 episode reward: total was -10.880000. running mean: -14.911294\n",
      "ep 2172: ep_len:575 episode reward: total was -12.960000. running mean: -14.891781\n",
      "ep 2172: ep_len:565 episode reward: total was -13.140000. running mean: -14.874263\n",
      "ep 2172: ep_len:505 episode reward: total was 0.560000. running mean: -14.719921\n",
      "ep 2172: ep_len:3 episode reward: total was 0.000000. running mean: -14.572721\n",
      "ep 2172: ep_len:630 episode reward: total was -0.840000. running mean: -14.435394\n",
      "ep 2172: ep_len:600 episode reward: total was -22.060000. running mean: -14.511640\n",
      "epsilon:0.103385 episode_count: 15211. steps_count: 6722994.000000\n",
      "ep 2173: ep_len:530 episode reward: total was -14.000000. running mean: -14.506524\n",
      "ep 2173: ep_len:560 episode reward: total was -17.470000. running mean: -14.536159\n",
      "ep 2173: ep_len:362 episode reward: total was -2.310000. running mean: -14.413897\n",
      "ep 2173: ep_len:580 episode reward: total was -28.150000. running mean: -14.551258\n",
      "ep 2173: ep_len:46 episode reward: total was 3.000000. running mean: -14.375745\n",
      "ep 2173: ep_len:560 episode reward: total was -24.560000. running mean: -14.477588\n",
      "ep 2173: ep_len:630 episode reward: total was -24.390000. running mean: -14.576712\n",
      "epsilon:0.103249 episode_count: 15218. steps_count: 6726262.000000\n",
      "ep 2174: ep_len:500 episode reward: total was -24.650000. running mean: -14.677445\n",
      "ep 2174: ep_len:500 episode reward: total was 0.550000. running mean: -14.525171\n",
      "ep 2174: ep_len:660 episode reward: total was -8.760000. running mean: -14.467519\n",
      "ep 2174: ep_len:560 episode reward: total was -4.130000. running mean: -14.364144\n",
      "ep 2174: ep_len:3 episode reward: total was 0.000000. running mean: -14.220502\n",
      "ep 2174: ep_len:555 episode reward: total was -5.250000. running mean: -14.130797\n",
      "ep 2174: ep_len:550 episode reward: total was -17.110000. running mean: -14.160589\n",
      "epsilon:0.103112 episode_count: 15225. steps_count: 6729590.000000\n",
      "ep 2175: ep_len:535 episode reward: total was -3.120000. running mean: -14.050183\n",
      "ep 2175: ep_len:590 episode reward: total was -0.940000. running mean: -13.919082\n",
      "ep 2175: ep_len:461 episode reward: total was -24.820000. running mean: -14.028091\n",
      "ep 2175: ep_len:56 episode reward: total was -1.470000. running mean: -13.902510\n",
      "ep 2175: ep_len:132 episode reward: total was 5.050000. running mean: -13.712985\n",
      "ep 2175: ep_len:530 episode reward: total was -32.470000. running mean: -13.900555\n",
      "ep 2175: ep_len:630 episode reward: total was -17.360000. running mean: -13.935149\n",
      "epsilon:0.102976 episode_count: 15232. steps_count: 6732524.000000\n",
      "ep 2176: ep_len:635 episode reward: total was -18.370000. running mean: -13.979498\n",
      "ep 2176: ep_len:590 episode reward: total was 16.870000. running mean: -13.671003\n",
      "ep 2176: ep_len:500 episode reward: total was -19.060000. running mean: -13.724893\n",
      "ep 2176: ep_len:505 episode reward: total was -17.660000. running mean: -13.764244\n",
      "ep 2176: ep_len:3 episode reward: total was 0.000000. running mean: -13.626601\n",
      "ep 2176: ep_len:500 episode reward: total was 1.980000. running mean: -13.470535\n",
      "ep 2176: ep_len:500 episode reward: total was -10.740000. running mean: -13.443230\n",
      "epsilon:0.102839 episode_count: 15239. steps_count: 6735757.000000\n",
      "ep 2177: ep_len:675 episode reward: total was -23.210000. running mean: -13.540898\n",
      "ep 2177: ep_len:181 episode reward: total was -0.350000. running mean: -13.408989\n",
      "ep 2177: ep_len:565 episode reward: total was -7.440000. running mean: -13.349299\n",
      "ep 2177: ep_len:535 episode reward: total was -44.180000. running mean: -13.657606\n",
      "ep 2177: ep_len:3 episode reward: total was 0.000000. running mean: -13.521030\n",
      "ep 2177: ep_len:580 episode reward: total was -6.430000. running mean: -13.450120\n",
      "ep 2177: ep_len:525 episode reward: total was -23.620000. running mean: -13.551818\n",
      "epsilon:0.102703 episode_count: 15246. steps_count: 6738821.000000\n",
      "ep 2178: ep_len:229 episode reward: total was 5.620000. running mean: -13.360100\n",
      "ep 2178: ep_len:301 episode reward: total was -6.280000. running mean: -13.289299\n",
      "ep 2178: ep_len:460 episode reward: total was -3.270000. running mean: -13.189106\n",
      "ep 2178: ep_len:118 episode reward: total was -1.960000. running mean: -13.076815\n",
      "ep 2178: ep_len:3 episode reward: total was 0.000000. running mean: -12.946047\n",
      "ep 2178: ep_len:500 episode reward: total was -8.260000. running mean: -12.899186\n",
      "ep 2178: ep_len:520 episode reward: total was -20.980000. running mean: -12.979995\n",
      "epsilon:0.102566 episode_count: 15253. steps_count: 6740952.000000\n",
      "ep 2179: ep_len:134 episode reward: total was 1.560000. running mean: -12.834595\n",
      "ep 2179: ep_len:590 episode reward: total was -28.520000. running mean: -12.991449\n",
      "ep 2179: ep_len:560 episode reward: total was -23.900000. running mean: -13.100534\n",
      "ep 2179: ep_len:56 episode reward: total was -1.940000. running mean: -12.988929\n",
      "ep 2179: ep_len:3 episode reward: total was 0.000000. running mean: -12.859040\n",
      "ep 2179: ep_len:500 episode reward: total was -4.030000. running mean: -12.770749\n",
      "ep 2179: ep_len:615 episode reward: total was -19.310000. running mean: -12.836142\n",
      "epsilon:0.102430 episode_count: 15260. steps_count: 6743410.000000\n",
      "ep 2180: ep_len:226 episode reward: total was -5.370000. running mean: -12.761480\n",
      "ep 2180: ep_len:347 episode reward: total was -26.760000. running mean: -12.901466\n",
      "ep 2180: ep_len:585 episode reward: total was -14.800000. running mean: -12.920451\n",
      "ep 2180: ep_len:565 episode reward: total was -4.570000. running mean: -12.836946\n",
      "ep 2180: ep_len:3 episode reward: total was 0.000000. running mean: -12.708577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2180: ep_len:660 episode reward: total was -44.420000. running mean: -13.025691\n",
      "ep 2180: ep_len:535 episode reward: total was -33.510000. running mean: -13.230534\n",
      "epsilon:0.102293 episode_count: 15267. steps_count: 6746331.000000\n",
      "ep 2181: ep_len:580 episode reward: total was -7.480000. running mean: -13.173029\n",
      "ep 2181: ep_len:354 episode reward: total was -12.810000. running mean: -13.169399\n",
      "ep 2181: ep_len:1120 episode reward: total was -146.720000. running mean: -14.504905\n",
      "ep 2181: ep_len:500 episode reward: total was -42.730000. running mean: -14.787156\n",
      "ep 2181: ep_len:87 episode reward: total was -7.940000. running mean: -14.718684\n",
      "ep 2181: ep_len:500 episode reward: total was -16.180000. running mean: -14.733297\n",
      "ep 2181: ep_len:585 episode reward: total was -32.070000. running mean: -14.906664\n",
      "epsilon:0.102157 episode_count: 15274. steps_count: 6750057.000000\n",
      "ep 2182: ep_len:540 episode reward: total was -32.490000. running mean: -15.082498\n",
      "ep 2182: ep_len:185 episode reward: total was -9.400000. running mean: -15.025673\n",
      "ep 2182: ep_len:635 episode reward: total was -23.610000. running mean: -15.111516\n",
      "ep 2182: ep_len:535 episode reward: total was -38.620000. running mean: -15.346601\n",
      "ep 2182: ep_len:3 episode reward: total was 0.000000. running mean: -15.193135\n",
      "ep 2182: ep_len:170 episode reward: total was -2.430000. running mean: -15.065503\n",
      "ep 2182: ep_len:540 episode reward: total was -24.650000. running mean: -15.161348\n",
      "epsilon:0.102020 episode_count: 15281. steps_count: 6752665.000000\n",
      "ep 2183: ep_len:595 episode reward: total was -47.560000. running mean: -15.485335\n",
      "ep 2183: ep_len:550 episode reward: total was -34.460000. running mean: -15.675081\n",
      "ep 2183: ep_len:635 episode reward: total was -10.220000. running mean: -15.620531\n",
      "ep 2183: ep_len:620 episode reward: total was -5.950000. running mean: -15.523825\n",
      "ep 2183: ep_len:3 episode reward: total was 0.000000. running mean: -15.368587\n",
      "ep 2183: ep_len:500 episode reward: total was -5.740000. running mean: -15.272301\n",
      "ep 2183: ep_len:500 episode reward: total was -19.120000. running mean: -15.310778\n",
      "epsilon:0.101884 episode_count: 15288. steps_count: 6756068.000000\n",
      "ep 2184: ep_len:500 episode reward: total was -11.320000. running mean: -15.270870\n",
      "ep 2184: ep_len:595 episode reward: total was -6.370000. running mean: -15.181862\n",
      "ep 2184: ep_len:388 episode reward: total was 0.710000. running mean: -15.022943\n",
      "ep 2184: ep_len:117 episode reward: total was 1.050000. running mean: -14.862214\n",
      "ep 2184: ep_len:3 episode reward: total was 0.000000. running mean: -14.713592\n",
      "ep 2184: ep_len:530 episode reward: total was -11.760000. running mean: -14.684056\n",
      "ep 2184: ep_len:515 episode reward: total was -30.650000. running mean: -14.843715\n",
      "epsilon:0.101747 episode_count: 15295. steps_count: 6758716.000000\n",
      "ep 2185: ep_len:605 episode reward: total was -23.040000. running mean: -14.925678\n",
      "ep 2185: ep_len:680 episode reward: total was -64.060000. running mean: -15.417021\n",
      "ep 2185: ep_len:515 episode reward: total was -9.640000. running mean: -15.359251\n",
      "ep 2185: ep_len:585 episode reward: total was -31.150000. running mean: -15.517158\n",
      "ep 2185: ep_len:3 episode reward: total was 0.000000. running mean: -15.361987\n",
      "ep 2185: ep_len:535 episode reward: total was -42.310000. running mean: -15.631467\n",
      "ep 2185: ep_len:585 episode reward: total was -33.950000. running mean: -15.814652\n",
      "epsilon:0.101611 episode_count: 15302. steps_count: 6762224.000000\n",
      "ep 2186: ep_len:515 episode reward: total was -3.130000. running mean: -15.687806\n",
      "ep 2186: ep_len:520 episode reward: total was -29.720000. running mean: -15.828128\n",
      "ep 2186: ep_len:690 episode reward: total was -38.880000. running mean: -16.058646\n",
      "ep 2186: ep_len:520 episode reward: total was -3.000000. running mean: -15.928060\n",
      "ep 2186: ep_len:48 episode reward: total was 0.000000. running mean: -15.768779\n",
      "ep 2186: ep_len:580 episode reward: total was -25.340000. running mean: -15.864492\n",
      "ep 2186: ep_len:170 episode reward: total was -13.410000. running mean: -15.839947\n",
      "epsilon:0.101474 episode_count: 15309. steps_count: 6765267.000000\n",
      "ep 2187: ep_len:565 episode reward: total was -16.150000. running mean: -15.843047\n",
      "ep 2187: ep_len:590 episode reward: total was -15.190000. running mean: -15.836517\n",
      "ep 2187: ep_len:545 episode reward: total was -6.940000. running mean: -15.747552\n",
      "ep 2187: ep_len:162 episode reward: total was -0.420000. running mean: -15.594276\n",
      "ep 2187: ep_len:3 episode reward: total was 0.000000. running mean: -15.438333\n",
      "ep 2187: ep_len:795 episode reward: total was -74.940000. running mean: -16.033350\n",
      "ep 2187: ep_len:560 episode reward: total was -19.510000. running mean: -16.068116\n",
      "epsilon:0.101338 episode_count: 15316. steps_count: 6768487.000000\n",
      "ep 2188: ep_len:645 episode reward: total was -27.280000. running mean: -16.180235\n",
      "ep 2188: ep_len:555 episode reward: total was -11.740000. running mean: -16.135833\n",
      "ep 2188: ep_len:595 episode reward: total was -7.120000. running mean: -16.045675\n",
      "ep 2188: ep_len:580 episode reward: total was -10.010000. running mean: -15.985318\n",
      "ep 2188: ep_len:3 episode reward: total was 0.000000. running mean: -15.825465\n",
      "ep 2188: ep_len:223 episode reward: total was -4.880000. running mean: -15.716010\n",
      "ep 2188: ep_len:208 episode reward: total was -14.410000. running mean: -15.702950\n",
      "epsilon:0.101201 episode_count: 15323. steps_count: 6771296.000000\n",
      "ep 2189: ep_len:212 episode reward: total was 4.610000. running mean: -15.499820\n",
      "ep 2189: ep_len:515 episode reward: total was -8.980000. running mean: -15.434622\n",
      "ep 2189: ep_len:500 episode reward: total was -6.040000. running mean: -15.340676\n",
      "ep 2189: ep_len:500 episode reward: total was -9.010000. running mean: -15.277369\n",
      "ep 2189: ep_len:97 episode reward: total was -9.940000. running mean: -15.223996\n",
      "ep 2189: ep_len:500 episode reward: total was -6.100000. running mean: -15.132756\n",
      "ep 2189: ep_len:500 episode reward: total was -16.950000. running mean: -15.150928\n",
      "epsilon:0.101065 episode_count: 15330. steps_count: 6774120.000000\n",
      "ep 2190: ep_len:126 episode reward: total was -6.960000. running mean: -15.069019\n",
      "ep 2190: ep_len:550 episode reward: total was -31.030000. running mean: -15.228629\n",
      "ep 2190: ep_len:376 episode reward: total was -14.320000. running mean: -15.219542\n",
      "ep 2190: ep_len:565 episode reward: total was -6.650000. running mean: -15.133847\n",
      "ep 2190: ep_len:3 episode reward: total was 0.000000. running mean: -14.982508\n",
      "ep 2190: ep_len:685 episode reward: total was -11.190000. running mean: -14.944583\n",
      "ep 2190: ep_len:570 episode reward: total was -21.100000. running mean: -15.006137\n",
      "epsilon:0.100928 episode_count: 15337. steps_count: 6776995.000000\n",
      "ep 2191: ep_len:590 episode reward: total was -10.180000. running mean: -14.957876\n",
      "ep 2191: ep_len:520 episode reward: total was -39.570000. running mean: -15.203997\n",
      "ep 2191: ep_len:78 episode reward: total was 1.050000. running mean: -15.041457\n",
      "ep 2191: ep_len:56 episode reward: total was 0.550000. running mean: -14.885543\n",
      "ep 2191: ep_len:90 episode reward: total was 3.560000. running mean: -14.701087\n",
      "ep 2191: ep_len:336 episode reward: total was -3.390000. running mean: -14.587976\n",
      "ep 2191: ep_len:278 episode reward: total was -9.880000. running mean: -14.540897\n",
      "epsilon:0.100792 episode_count: 15344. steps_count: 6778943.000000\n",
      "ep 2192: ep_len:560 episode reward: total was -2.960000. running mean: -14.425088\n",
      "ep 2192: ep_len:575 episode reward: total was 13.860000. running mean: -14.142237\n",
      "ep 2192: ep_len:605 episode reward: total was -37.870000. running mean: -14.379515\n",
      "ep 2192: ep_len:116 episode reward: total was 7.080000. running mean: -14.164919\n",
      "ep 2192: ep_len:3 episode reward: total was 0.000000. running mean: -14.023270\n",
      "ep 2192: ep_len:595 episode reward: total was -15.110000. running mean: -14.034137\n",
      "ep 2192: ep_len:515 episode reward: total was -13.870000. running mean: -14.032496\n",
      "epsilon:0.100655 episode_count: 15351. steps_count: 6781912.000000\n",
      "ep 2193: ep_len:585 episode reward: total was -1.630000. running mean: -13.908471\n",
      "ep 2193: ep_len:500 episode reward: total was -38.610000. running mean: -14.155486\n",
      "ep 2193: ep_len:635 episode reward: total was -12.060000. running mean: -14.134532\n",
      "ep 2193: ep_len:364 episode reward: total was -26.200000. running mean: -14.255186\n",
      "ep 2193: ep_len:3 episode reward: total was 0.000000. running mean: -14.112634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2193: ep_len:260 episode reward: total was -3.310000. running mean: -14.004608\n",
      "ep 2193: ep_len:500 episode reward: total was -21.760000. running mean: -14.082162\n",
      "epsilon:0.100519 episode_count: 15358. steps_count: 6784759.000000\n",
      "ep 2194: ep_len:500 episode reward: total was 4.920000. running mean: -13.892140\n",
      "ep 2194: ep_len:500 episode reward: total was -9.980000. running mean: -13.853019\n",
      "ep 2194: ep_len:442 episode reward: total was -4.780000. running mean: -13.762289\n",
      "ep 2194: ep_len:505 episode reward: total was -15.590000. running mean: -13.780566\n",
      "ep 2194: ep_len:3 episode reward: total was 0.000000. running mean: -13.642760\n",
      "ep 2194: ep_len:690 episode reward: total was -45.370000. running mean: -13.960033\n",
      "ep 2194: ep_len:520 episode reward: total was -33.680000. running mean: -14.157232\n",
      "epsilon:0.100382 episode_count: 15365. steps_count: 6787919.000000\n",
      "ep 2195: ep_len:500 episode reward: total was -2.810000. running mean: -14.043760\n",
      "ep 2195: ep_len:660 episode reward: total was -25.590000. running mean: -14.159222\n",
      "ep 2195: ep_len:615 episode reward: total was -10.050000. running mean: -14.118130\n",
      "ep 2195: ep_len:505 episode reward: total was -21.040000. running mean: -14.187349\n",
      "ep 2195: ep_len:54 episode reward: total was 5.000000. running mean: -13.995475\n",
      "ep 2195: ep_len:550 episode reward: total was -12.530000. running mean: -13.980821\n",
      "ep 2195: ep_len:550 episode reward: total was -31.340000. running mean: -14.154412\n",
      "epsilon:0.100246 episode_count: 15372. steps_count: 6791353.000000\n",
      "ep 2196: ep_len:500 episode reward: total was -29.700000. running mean: -14.309868\n",
      "ep 2196: ep_len:367 episode reward: total was -8.830000. running mean: -14.255070\n",
      "ep 2196: ep_len:355 episode reward: total was -12.840000. running mean: -14.240919\n",
      "ep 2196: ep_len:500 episode reward: total was -17.560000. running mean: -14.274110\n",
      "ep 2196: ep_len:64 episode reward: total was -6.480000. running mean: -14.196169\n",
      "ep 2196: ep_len:600 episode reward: total was 5.430000. running mean: -13.999907\n",
      "ep 2196: ep_len:500 episode reward: total was -21.790000. running mean: -14.077808\n",
      "epsilon:0.100109 episode_count: 15379. steps_count: 6794239.000000\n",
      "ep 2197: ep_len:500 episode reward: total was -4.240000. running mean: -13.979430\n",
      "ep 2197: ep_len:349 episode reward: total was -8.840000. running mean: -13.928035\n",
      "ep 2197: ep_len:440 episode reward: total was -17.310000. running mean: -13.961855\n",
      "ep 2197: ep_len:640 episode reward: total was -26.570000. running mean: -14.087937\n",
      "ep 2197: ep_len:56 episode reward: total was 2.500000. running mean: -13.922057\n",
      "ep 2197: ep_len:645 episode reward: total was -20.820000. running mean: -13.991037\n",
      "ep 2197: ep_len:500 episode reward: total was -8.120000. running mean: -13.932326\n",
      "epsilon:0.099973 episode_count: 15386. steps_count: 6797369.000000\n",
      "ep 2198: ep_len:565 episode reward: total was -38.870000. running mean: -14.181703\n",
      "ep 2198: ep_len:660 episode reward: total was 2.570000. running mean: -14.014186\n",
      "ep 2198: ep_len:665 episode reward: total was -24.290000. running mean: -14.116944\n",
      "ep 2198: ep_len:520 episode reward: total was -12.430000. running mean: -14.100075\n",
      "ep 2198: ep_len:3 episode reward: total was 0.000000. running mean: -13.959074\n",
      "ep 2198: ep_len:560 episode reward: total was -36.400000. running mean: -14.183483\n",
      "ep 2198: ep_len:630 episode reward: total was -22.780000. running mean: -14.269448\n",
      "epsilon:0.099836 episode_count: 15393. steps_count: 6800972.000000\n",
      "ep 2199: ep_len:590 episode reward: total was -17.590000. running mean: -14.302654\n",
      "ep 2199: ep_len:500 episode reward: total was -10.930000. running mean: -14.268927\n",
      "ep 2199: ep_len:660 episode reward: total was -41.360000. running mean: -14.539838\n",
      "ep 2199: ep_len:164 episode reward: total was -0.390000. running mean: -14.398340\n",
      "ep 2199: ep_len:84 episode reward: total was 4.550000. running mean: -14.208856\n",
      "ep 2199: ep_len:545 episode reward: total was -67.140000. running mean: -14.738168\n",
      "ep 2199: ep_len:189 episode reward: total was -4.350000. running mean: -14.634286\n",
      "epsilon:0.099700 episode_count: 15400. steps_count: 6803704.000000\n",
      "ep 2200: ep_len:565 episode reward: total was -19.770000. running mean: -14.685643\n",
      "ep 2200: ep_len:500 episode reward: total was -7.390000. running mean: -14.612687\n",
      "ep 2200: ep_len:79 episode reward: total was 0.040000. running mean: -14.466160\n",
      "ep 2200: ep_len:410 episode reward: total was -2.670000. running mean: -14.348198\n",
      "ep 2200: ep_len:3 episode reward: total was 0.000000. running mean: -14.204716\n",
      "ep 2200: ep_len:168 episode reward: total was 4.610000. running mean: -14.016569\n",
      "ep 2200: ep_len:520 episode reward: total was -14.130000. running mean: -14.017703\n",
      "epsilon:0.099563 episode_count: 15407. steps_count: 6805949.000000\n",
      "ep 2201: ep_len:500 episode reward: total was -61.200000. running mean: -14.489526\n",
      "ep 2201: ep_len:540 episode reward: total was -53.710000. running mean: -14.881731\n",
      "ep 2201: ep_len:545 episode reward: total was -13.780000. running mean: -14.870714\n",
      "ep 2201: ep_len:132 episode reward: total was 3.590000. running mean: -14.686107\n",
      "ep 2201: ep_len:3 episode reward: total was 0.000000. running mean: -14.539246\n",
      "ep 2201: ep_len:510 episode reward: total was 5.110000. running mean: -14.342753\n",
      "ep 2201: ep_len:268 episode reward: total was -11.370000. running mean: -14.313026\n",
      "epsilon:0.099427 episode_count: 15414. steps_count: 6808447.000000\n",
      "ep 2202: ep_len:500 episode reward: total was -11.240000. running mean: -14.282295\n",
      "ep 2202: ep_len:615 episode reward: total was -35.950000. running mean: -14.498972\n",
      "ep 2202: ep_len:635 episode reward: total was -23.490000. running mean: -14.588883\n",
      "ep 2202: ep_len:560 episode reward: total was 0.870000. running mean: -14.434294\n",
      "ep 2202: ep_len:106 episode reward: total was -15.980000. running mean: -14.449751\n",
      "ep 2202: ep_len:640 episode reward: total was -30.350000. running mean: -14.608753\n",
      "ep 2202: ep_len:500 episode reward: total was -13.770000. running mean: -14.600366\n",
      "epsilon:0.099290 episode_count: 15421. steps_count: 6812003.000000\n",
      "ep 2203: ep_len:890 episode reward: total was -125.810000. running mean: -15.712462\n",
      "ep 2203: ep_len:184 episode reward: total was -3.900000. running mean: -15.594338\n",
      "ep 2203: ep_len:79 episode reward: total was -1.460000. running mean: -15.452994\n",
      "ep 2203: ep_len:620 episode reward: total was -6.990000. running mean: -15.368364\n",
      "ep 2203: ep_len:3 episode reward: total was 0.000000. running mean: -15.214681\n",
      "ep 2203: ep_len:620 episode reward: total was -15.250000. running mean: -15.215034\n",
      "ep 2203: ep_len:505 episode reward: total was -14.870000. running mean: -15.211583\n",
      "epsilon:0.099154 episode_count: 15428. steps_count: 6814904.000000\n",
      "ep 2204: ep_len:515 episode reward: total was -13.430000. running mean: -15.193768\n",
      "ep 2204: ep_len:610 episode reward: total was 5.560000. running mean: -14.986230\n",
      "ep 2204: ep_len:515 episode reward: total was -9.680000. running mean: -14.933168\n",
      "ep 2204: ep_len:525 episode reward: total was -25.630000. running mean: -15.040136\n",
      "ep 2204: ep_len:114 episode reward: total was -10.950000. running mean: -14.999235\n",
      "ep 2204: ep_len:500 episode reward: total was -58.310000. running mean: -15.432342\n",
      "ep 2204: ep_len:585 episode reward: total was -19.490000. running mean: -15.472919\n",
      "epsilon:0.099017 episode_count: 15435. steps_count: 6818268.000000\n",
      "ep 2205: ep_len:545 episode reward: total was -33.920000. running mean: -15.657390\n",
      "ep 2205: ep_len:655 episode reward: total was -47.960000. running mean: -15.980416\n",
      "ep 2205: ep_len:640 episode reward: total was -33.090000. running mean: -16.151512\n",
      "ep 2205: ep_len:500 episode reward: total was -13.010000. running mean: -16.120097\n",
      "ep 2205: ep_len:87 episode reward: total was -3.460000. running mean: -15.993496\n",
      "ep 2205: ep_len:540 episode reward: total was -17.440000. running mean: -16.007961\n",
      "ep 2205: ep_len:500 episode reward: total was -8.540000. running mean: -15.933281\n",
      "epsilon:0.098881 episode_count: 15442. steps_count: 6821735.000000\n",
      "ep 2206: ep_len:605 episode reward: total was -12.000000. running mean: -15.893948\n",
      "ep 2206: ep_len:505 episode reward: total was -13.750000. running mean: -15.872509\n",
      "ep 2206: ep_len:79 episode reward: total was -2.990000. running mean: -15.743684\n",
      "ep 2206: ep_len:565 episode reward: total was -16.590000. running mean: -15.752147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2206: ep_len:3 episode reward: total was 0.000000. running mean: -15.594625\n",
      "ep 2206: ep_len:585 episode reward: total was -17.660000. running mean: -15.615279\n",
      "ep 2206: ep_len:500 episode reward: total was -25.990000. running mean: -15.719026\n",
      "epsilon:0.098744 episode_count: 15449. steps_count: 6824577.000000\n",
      "ep 2207: ep_len:525 episode reward: total was -47.720000. running mean: -16.039036\n",
      "ep 2207: ep_len:605 episode reward: total was -33.460000. running mean: -16.213246\n",
      "ep 2207: ep_len:560 episode reward: total was -15.910000. running mean: -16.210213\n",
      "ep 2207: ep_len:500 episode reward: total was -37.190000. running mean: -16.420011\n",
      "ep 2207: ep_len:2 episode reward: total was 0.000000. running mean: -16.255811\n",
      "ep 2207: ep_len:500 episode reward: total was -8.300000. running mean: -16.176253\n",
      "ep 2207: ep_len:575 episode reward: total was -2.020000. running mean: -16.034690\n",
      "epsilon:0.098608 episode_count: 15456. steps_count: 6827844.000000\n",
      "ep 2208: ep_len:240 episode reward: total was -20.830000. running mean: -16.082643\n",
      "ep 2208: ep_len:540 episode reward: total was -16.750000. running mean: -16.089317\n",
      "ep 2208: ep_len:505 episode reward: total was -2.610000. running mean: -15.954524\n",
      "ep 2208: ep_len:500 episode reward: total was 4.860000. running mean: -15.746379\n",
      "ep 2208: ep_len:48 episode reward: total was 4.500000. running mean: -15.543915\n",
      "ep 2208: ep_len:740 episode reward: total was -32.810000. running mean: -15.716576\n",
      "ep 2208: ep_len:500 episode reward: total was -5.720000. running mean: -15.616610\n",
      "epsilon:0.098471 episode_count: 15463. steps_count: 6830917.000000\n",
      "ep 2209: ep_len:510 episode reward: total was -25.970000. running mean: -15.720144\n",
      "ep 2209: ep_len:585 episode reward: total was -29.240000. running mean: -15.855342\n",
      "ep 2209: ep_len:79 episode reward: total was -0.970000. running mean: -15.706489\n",
      "ep 2209: ep_len:515 episode reward: total was -17.030000. running mean: -15.719724\n",
      "ep 2209: ep_len:101 episode reward: total was 7.040000. running mean: -15.492127\n",
      "ep 2209: ep_len:500 episode reward: total was -11.280000. running mean: -15.450006\n",
      "ep 2209: ep_len:540 episode reward: total was -5.500000. running mean: -15.350505\n",
      "epsilon:0.098335 episode_count: 15470. steps_count: 6833747.000000\n",
      "ep 2210: ep_len:645 episode reward: total was -1.850000. running mean: -15.215500\n",
      "ep 2210: ep_len:500 episode reward: total was -24.550000. running mean: -15.308845\n",
      "ep 2210: ep_len:61 episode reward: total was -2.960000. running mean: -15.185357\n",
      "ep 2210: ep_len:505 episode reward: total was -5.170000. running mean: -15.085203\n",
      "ep 2210: ep_len:3 episode reward: total was 0.000000. running mean: -14.934351\n",
      "ep 2210: ep_len:500 episode reward: total was -58.830000. running mean: -15.373308\n",
      "ep 2210: ep_len:610 episode reward: total was -50.230000. running mean: -15.721875\n",
      "epsilon:0.098198 episode_count: 15477. steps_count: 6836571.000000\n",
      "ep 2211: ep_len:710 episode reward: total was -42.900000. running mean: -15.993656\n",
      "ep 2211: ep_len:640 episode reward: total was 4.020000. running mean: -15.793519\n",
      "ep 2211: ep_len:540 episode reward: total was -4.290000. running mean: -15.678484\n",
      "ep 2211: ep_len:520 episode reward: total was -14.980000. running mean: -15.671499\n",
      "ep 2211: ep_len:3 episode reward: total was 0.000000. running mean: -15.514784\n",
      "ep 2211: ep_len:173 episode reward: total was 8.620000. running mean: -15.273437\n",
      "ep 2211: ep_len:730 episode reward: total was -64.970000. running mean: -15.770402\n",
      "epsilon:0.098062 episode_count: 15484. steps_count: 6839887.000000\n",
      "ep 2212: ep_len:500 episode reward: total was -17.880000. running mean: -15.791498\n",
      "ep 2212: ep_len:605 episode reward: total was -15.590000. running mean: -15.789483\n",
      "ep 2212: ep_len:520 episode reward: total was -18.200000. running mean: -15.813588\n",
      "ep 2212: ep_len:510 episode reward: total was -10.470000. running mean: -15.760152\n",
      "ep 2212: ep_len:97 episode reward: total was -9.450000. running mean: -15.697051\n",
      "ep 2212: ep_len:520 episode reward: total was -11.570000. running mean: -15.655780\n",
      "ep 2212: ep_len:625 episode reward: total was -11.250000. running mean: -15.611723\n",
      "epsilon:0.097925 episode_count: 15491. steps_count: 6843264.000000\n",
      "ep 2213: ep_len:247 episode reward: total was -19.810000. running mean: -15.653705\n",
      "ep 2213: ep_len:500 episode reward: total was -12.160000. running mean: -15.618768\n",
      "ep 2213: ep_len:79 episode reward: total was 0.560000. running mean: -15.456981\n",
      "ep 2213: ep_len:45 episode reward: total was 1.070000. running mean: -15.291711\n",
      "ep 2213: ep_len:3 episode reward: total was 0.000000. running mean: -15.138794\n",
      "ep 2213: ep_len:186 episode reward: total was 8.650000. running mean: -14.900906\n",
      "ep 2213: ep_len:520 episode reward: total was -12.850000. running mean: -14.880397\n",
      "epsilon:0.097789 episode_count: 15498. steps_count: 6844844.000000\n",
      "ep 2214: ep_len:680 episode reward: total was -23.690000. running mean: -14.968493\n",
      "ep 2214: ep_len:510 episode reward: total was 5.150000. running mean: -14.767308\n",
      "ep 2214: ep_len:585 episode reward: total was -41.940000. running mean: -15.039035\n",
      "ep 2214: ep_len:585 episode reward: total was -13.140000. running mean: -15.020044\n",
      "ep 2214: ep_len:125 episode reward: total was 3.550000. running mean: -14.834344\n",
      "ep 2214: ep_len:332 episode reward: total was -0.830000. running mean: -14.694301\n",
      "ep 2214: ep_len:590 episode reward: total was -47.720000. running mean: -15.024558\n",
      "epsilon:0.097652 episode_count: 15505. steps_count: 6848251.000000\n",
      "ep 2215: ep_len:545 episode reward: total was 3.040000. running mean: -14.843912\n",
      "ep 2215: ep_len:500 episode reward: total was 7.660000. running mean: -14.618873\n",
      "ep 2215: ep_len:555 episode reward: total was -10.010000. running mean: -14.572784\n",
      "ep 2215: ep_len:500 episode reward: total was 0.900000. running mean: -14.418056\n",
      "ep 2215: ep_len:3 episode reward: total was 0.000000. running mean: -14.273876\n",
      "ep 2215: ep_len:620 episode reward: total was -36.230000. running mean: -14.493437\n",
      "ep 2215: ep_len:610 episode reward: total was -14.340000. running mean: -14.491903\n",
      "epsilon:0.097516 episode_count: 15512. steps_count: 6851584.000000\n",
      "ep 2216: ep_len:650 episode reward: total was -32.290000. running mean: -14.669884\n",
      "ep 2216: ep_len:500 episode reward: total was 8.170000. running mean: -14.441485\n",
      "ep 2216: ep_len:500 episode reward: total was 0.930000. running mean: -14.287770\n",
      "ep 2216: ep_len:157 episode reward: total was 3.140000. running mean: -14.113492\n",
      "ep 2216: ep_len:88 episode reward: total was 2.540000. running mean: -13.946957\n",
      "ep 2216: ep_len:630 episode reward: total was -11.300000. running mean: -13.920488\n",
      "ep 2216: ep_len:520 episode reward: total was -32.990000. running mean: -14.111183\n",
      "epsilon:0.097379 episode_count: 15519. steps_count: 6854629.000000\n",
      "ep 2217: ep_len:505 episode reward: total was -31.490000. running mean: -14.284971\n",
      "ep 2217: ep_len:500 episode reward: total was -27.060000. running mean: -14.412721\n",
      "ep 2217: ep_len:560 episode reward: total was -19.920000. running mean: -14.467794\n",
      "ep 2217: ep_len:132 episode reward: total was -0.920000. running mean: -14.332316\n",
      "ep 2217: ep_len:3 episode reward: total was 0.000000. running mean: -14.188993\n",
      "ep 2217: ep_len:625 episode reward: total was -54.960000. running mean: -14.596703\n",
      "ep 2217: ep_len:630 episode reward: total was -23.640000. running mean: -14.687136\n",
      "epsilon:0.097243 episode_count: 15526. steps_count: 6857584.000000\n",
      "ep 2218: ep_len:580 episode reward: total was 1.990000. running mean: -14.520365\n",
      "ep 2218: ep_len:585 episode reward: total was -8.630000. running mean: -14.461461\n",
      "ep 2218: ep_len:505 episode reward: total was -3.110000. running mean: -14.347946\n",
      "ep 2218: ep_len:560 episode reward: total was -22.050000. running mean: -14.424967\n",
      "ep 2218: ep_len:3 episode reward: total was 0.000000. running mean: -14.280717\n",
      "ep 2218: ep_len:500 episode reward: total was -27.050000. running mean: -14.408410\n",
      "ep 2218: ep_len:679 episode reward: total was -65.490000. running mean: -14.919226\n",
      "epsilon:0.097106 episode_count: 15533. steps_count: 6860996.000000\n",
      "ep 2219: ep_len:540 episode reward: total was -32.940000. running mean: -15.099434\n",
      "ep 2219: ep_len:790 episode reward: total was -35.280000. running mean: -15.301239\n",
      "ep 2219: ep_len:520 episode reward: total was -8.320000. running mean: -15.231427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2219: ep_len:500 episode reward: total was 6.450000. running mean: -15.014613\n",
      "ep 2219: ep_len:95 episode reward: total was -5.450000. running mean: -14.918967\n",
      "ep 2219: ep_len:291 episode reward: total was -0.850000. running mean: -14.778277\n",
      "ep 2219: ep_len:615 episode reward: total was -19.840000. running mean: -14.828894\n",
      "epsilon:0.096970 episode_count: 15540. steps_count: 6864347.000000\n",
      "ep 2220: ep_len:128 episode reward: total was 6.600000. running mean: -14.614605\n",
      "ep 2220: ep_len:500 episode reward: total was -18.110000. running mean: -14.649559\n",
      "ep 2220: ep_len:585 episode reward: total was -13.260000. running mean: -14.635664\n",
      "ep 2220: ep_len:500 episode reward: total was -33.690000. running mean: -14.826207\n",
      "ep 2220: ep_len:3 episode reward: total was 0.000000. running mean: -14.677945\n",
      "ep 2220: ep_len:570 episode reward: total was -11.570000. running mean: -14.646865\n",
      "ep 2220: ep_len:505 episode reward: total was -20.420000. running mean: -14.704597\n",
      "epsilon:0.096833 episode_count: 15547. steps_count: 6867138.000000\n",
      "ep 2221: ep_len:580 episode reward: total was -3.520000. running mean: -14.592751\n",
      "ep 2221: ep_len:600 episode reward: total was -25.930000. running mean: -14.706123\n",
      "ep 2221: ep_len:460 episode reward: total was -16.810000. running mean: -14.727162\n",
      "ep 2221: ep_len:500 episode reward: total was -20.080000. running mean: -14.780690\n",
      "ep 2221: ep_len:82 episode reward: total was 6.540000. running mean: -14.567484\n",
      "ep 2221: ep_len:500 episode reward: total was -10.760000. running mean: -14.529409\n",
      "ep 2221: ep_len:585 episode reward: total was -29.100000. running mean: -14.675115\n",
      "epsilon:0.096697 episode_count: 15554. steps_count: 6870445.000000\n",
      "ep 2222: ep_len:640 episode reward: total was -19.240000. running mean: -14.720763\n",
      "ep 2222: ep_len:500 episode reward: total was -10.340000. running mean: -14.676956\n",
      "ep 2222: ep_len:565 episode reward: total was -10.070000. running mean: -14.630886\n",
      "ep 2222: ep_len:500 episode reward: total was -21.570000. running mean: -14.700277\n",
      "ep 2222: ep_len:103 episode reward: total was 5.050000. running mean: -14.502775\n",
      "ep 2222: ep_len:500 episode reward: total was -11.070000. running mean: -14.468447\n",
      "ep 2222: ep_len:590 episode reward: total was -24.940000. running mean: -14.573162\n",
      "epsilon:0.096560 episode_count: 15561. steps_count: 6873843.000000\n",
      "ep 2223: ep_len:510 episode reward: total was -20.080000. running mean: -14.628231\n",
      "ep 2223: ep_len:525 episode reward: total was -4.880000. running mean: -14.530748\n",
      "ep 2223: ep_len:580 episode reward: total was -12.760000. running mean: -14.513041\n",
      "ep 2223: ep_len:545 episode reward: total was -1.160000. running mean: -14.379511\n",
      "ep 2223: ep_len:3 episode reward: total was 0.000000. running mean: -14.235715\n",
      "ep 2223: ep_len:500 episode reward: total was -2.740000. running mean: -14.120758\n",
      "ep 2223: ep_len:640 episode reward: total was -32.430000. running mean: -14.303851\n",
      "epsilon:0.096424 episode_count: 15568. steps_count: 6877146.000000\n",
      "ep 2224: ep_len:570 episode reward: total was -6.060000. running mean: -14.221412\n",
      "ep 2224: ep_len:600 episode reward: total was 1.520000. running mean: -14.063998\n",
      "ep 2224: ep_len:382 episode reward: total was -4.330000. running mean: -13.966658\n",
      "ep 2224: ep_len:500 episode reward: total was -8.010000. running mean: -13.907092\n",
      "ep 2224: ep_len:3 episode reward: total was 0.000000. running mean: -13.768021\n",
      "ep 2224: ep_len:525 episode reward: total was -15.660000. running mean: -13.786940\n",
      "ep 2224: ep_len:316 episode reward: total was -14.340000. running mean: -13.792471\n",
      "epsilon:0.096287 episode_count: 15575. steps_count: 6880042.000000\n",
      "ep 2225: ep_len:630 episode reward: total was -30.800000. running mean: -13.962546\n",
      "ep 2225: ep_len:530 episode reward: total was 3.850000. running mean: -13.784421\n",
      "ep 2225: ep_len:500 episode reward: total was -1.610000. running mean: -13.662677\n",
      "ep 2225: ep_len:158 episode reward: total was 0.610000. running mean: -13.519950\n",
      "ep 2225: ep_len:3 episode reward: total was 0.000000. running mean: -13.384750\n",
      "ep 2225: ep_len:645 episode reward: total was -104.380000. running mean: -14.294703\n",
      "ep 2225: ep_len:565 episode reward: total was -34.130000. running mean: -14.493056\n",
      "epsilon:0.096151 episode_count: 15582. steps_count: 6883073.000000\n",
      "ep 2226: ep_len:243 episode reward: total was -36.910000. running mean: -14.717225\n",
      "ep 2226: ep_len:500 episode reward: total was -10.490000. running mean: -14.674953\n",
      "ep 2226: ep_len:625 episode reward: total was -16.760000. running mean: -14.695804\n",
      "ep 2226: ep_len:525 episode reward: total was -11.670000. running mean: -14.665545\n",
      "ep 2226: ep_len:3 episode reward: total was 0.000000. running mean: -14.518890\n",
      "ep 2226: ep_len:670 episode reward: total was -102.340000. running mean: -15.397101\n",
      "ep 2226: ep_len:545 episode reward: total was -15.450000. running mean: -15.397630\n",
      "epsilon:0.096014 episode_count: 15589. steps_count: 6886184.000000\n",
      "ep 2227: ep_len:565 episode reward: total was -2.630000. running mean: -15.269954\n",
      "ep 2227: ep_len:500 episode reward: total was -2.280000. running mean: -15.140054\n",
      "ep 2227: ep_len:845 episode reward: total was -72.330000. running mean: -15.711954\n",
      "ep 2227: ep_len:500 episode reward: total was -24.580000. running mean: -15.800634\n",
      "ep 2227: ep_len:3 episode reward: total was 0.000000. running mean: -15.642628\n",
      "ep 2227: ep_len:310 episode reward: total was -1.850000. running mean: -15.504702\n",
      "ep 2227: ep_len:500 episode reward: total was -16.380000. running mean: -15.513455\n",
      "epsilon:0.095878 episode_count: 15596. steps_count: 6889407.000000\n",
      "ep 2228: ep_len:545 episode reward: total was -31.500000. running mean: -15.673320\n",
      "ep 2228: ep_len:500 episode reward: total was -19.300000. running mean: -15.709587\n",
      "ep 2228: ep_len:575 episode reward: total was -20.930000. running mean: -15.761791\n",
      "ep 2228: ep_len:385 episode reward: total was 0.810000. running mean: -15.596073\n",
      "ep 2228: ep_len:3 episode reward: total was 0.000000. running mean: -15.440112\n",
      "ep 2228: ep_len:500 episode reward: total was -12.310000. running mean: -15.408811\n",
      "ep 2228: ep_len:585 episode reward: total was -29.600000. running mean: -15.550723\n",
      "epsilon:0.095741 episode_count: 15603. steps_count: 6892500.000000\n",
      "ep 2229: ep_len:575 episode reward: total was -13.980000. running mean: -15.535016\n",
      "ep 2229: ep_len:500 episode reward: total was -20.100000. running mean: -15.580666\n",
      "ep 2229: ep_len:650 episode reward: total was -36.060000. running mean: -15.785459\n",
      "ep 2229: ep_len:515 episode reward: total was 2.380000. running mean: -15.603804\n",
      "ep 2229: ep_len:3 episode reward: total was 0.000000. running mean: -15.447766\n",
      "ep 2229: ep_len:615 episode reward: total was -5.620000. running mean: -15.349489\n",
      "ep 2229: ep_len:545 episode reward: total was -26.640000. running mean: -15.462394\n",
      "epsilon:0.095605 episode_count: 15610. steps_count: 6895903.000000\n",
      "ep 2230: ep_len:500 episode reward: total was -14.770000. running mean: -15.455470\n",
      "ep 2230: ep_len:500 episode reward: total was 4.170000. running mean: -15.259215\n",
      "ep 2230: ep_len:580 episode reward: total was -16.710000. running mean: -15.273723\n",
      "ep 2230: ep_len:500 episode reward: total was -29.140000. running mean: -15.412386\n",
      "ep 2230: ep_len:3 episode reward: total was 0.000000. running mean: -15.258262\n",
      "ep 2230: ep_len:610 episode reward: total was -18.080000. running mean: -15.286479\n",
      "ep 2230: ep_len:500 episode reward: total was -25.440000. running mean: -15.388015\n",
      "epsilon:0.095468 episode_count: 15617. steps_count: 6899096.000000\n",
      "ep 2231: ep_len:118 episode reward: total was 2.090000. running mean: -15.213234\n",
      "ep 2231: ep_len:620 episode reward: total was 0.560000. running mean: -15.055502\n",
      "ep 2231: ep_len:575 episode reward: total was -30.540000. running mean: -15.210347\n",
      "ep 2231: ep_len:401 episode reward: total was -32.150000. running mean: -15.379744\n",
      "ep 2231: ep_len:3 episode reward: total was 0.000000. running mean: -15.225946\n",
      "ep 2231: ep_len:560 episode reward: total was -25.870000. running mean: -15.332387\n",
      "ep 2231: ep_len:610 episode reward: total was -6.530000. running mean: -15.244363\n",
      "epsilon:0.095332 episode_count: 15624. steps_count: 6901983.000000\n",
      "ep 2232: ep_len:500 episode reward: total was -30.310000. running mean: -15.395019\n",
      "ep 2232: ep_len:575 episode reward: total was -17.940000. running mean: -15.420469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2232: ep_len:625 episode reward: total was -36.680000. running mean: -15.633064\n",
      "ep 2232: ep_len:550 episode reward: total was -22.060000. running mean: -15.697334\n",
      "ep 2232: ep_len:100 episode reward: total was -10.450000. running mean: -15.644860\n",
      "ep 2232: ep_len:580 episode reward: total was -0.550000. running mean: -15.493912\n",
      "ep 2232: ep_len:560 episode reward: total was -18.360000. running mean: -15.522573\n",
      "epsilon:0.095195 episode_count: 15631. steps_count: 6905473.000000\n",
      "ep 2233: ep_len:630 episode reward: total was -7.890000. running mean: -15.446247\n",
      "ep 2233: ep_len:500 episode reward: total was 3.390000. running mean: -15.257884\n",
      "ep 2233: ep_len:500 episode reward: total was -29.550000. running mean: -15.400806\n",
      "ep 2233: ep_len:525 episode reward: total was -13.500000. running mean: -15.381797\n",
      "ep 2233: ep_len:3 episode reward: total was 0.000000. running mean: -15.227980\n",
      "ep 2233: ep_len:565 episode reward: total was -32.410000. running mean: -15.399800\n",
      "ep 2233: ep_len:500 episode reward: total was -13.770000. running mean: -15.383502\n",
      "epsilon:0.095059 episode_count: 15638. steps_count: 6908696.000000\n",
      "ep 2234: ep_len:555 episode reward: total was -21.550000. running mean: -15.445167\n",
      "ep 2234: ep_len:500 episode reward: total was -12.390000. running mean: -15.414615\n",
      "ep 2234: ep_len:550 episode reward: total was -3.730000. running mean: -15.297769\n",
      "ep 2234: ep_len:605 episode reward: total was -33.390000. running mean: -15.478691\n",
      "ep 2234: ep_len:3 episode reward: total was 0.000000. running mean: -15.323904\n",
      "ep 2234: ep_len:675 episode reward: total was -15.670000. running mean: -15.327365\n",
      "ep 2234: ep_len:329 episode reward: total was -24.860000. running mean: -15.422692\n",
      "epsilon:0.094922 episode_count: 15645. steps_count: 6911913.000000\n",
      "ep 2235: ep_len:500 episode reward: total was -54.330000. running mean: -15.811765\n",
      "ep 2235: ep_len:595 episode reward: total was -20.460000. running mean: -15.858247\n",
      "ep 2235: ep_len:680 episode reward: total was -41.530000. running mean: -16.114965\n",
      "ep 2235: ep_len:525 episode reward: total was -18.540000. running mean: -16.139215\n",
      "ep 2235: ep_len:3 episode reward: total was 0.000000. running mean: -15.977823\n",
      "ep 2235: ep_len:249 episode reward: total was 4.160000. running mean: -15.776445\n",
      "ep 2235: ep_len:500 episode reward: total was -45.330000. running mean: -16.071980\n",
      "epsilon:0.094786 episode_count: 15652. steps_count: 6914965.000000\n",
      "ep 2236: ep_len:715 episode reward: total was -34.320000. running mean: -16.254460\n",
      "ep 2236: ep_len:550 episode reward: total was -19.960000. running mean: -16.291516\n",
      "ep 2236: ep_len:545 episode reward: total was -3.620000. running mean: -16.164801\n",
      "ep 2236: ep_len:500 episode reward: total was -13.180000. running mean: -16.134953\n",
      "ep 2236: ep_len:3 episode reward: total was 0.000000. running mean: -15.973603\n",
      "ep 2236: ep_len:515 episode reward: total was -13.710000. running mean: -15.950967\n",
      "ep 2236: ep_len:310 episode reward: total was -29.430000. running mean: -16.085757\n",
      "epsilon:0.094649 episode_count: 15659. steps_count: 6918103.000000\n",
      "ep 2237: ep_len:253 episode reward: total was 1.090000. running mean: -15.914000\n",
      "ep 2237: ep_len:525 episode reward: total was -14.860000. running mean: -15.903460\n",
      "ep 2237: ep_len:500 episode reward: total was -32.560000. running mean: -16.070025\n",
      "ep 2237: ep_len:501 episode reward: total was 1.540000. running mean: -15.893925\n",
      "ep 2237: ep_len:3 episode reward: total was 0.000000. running mean: -15.734986\n",
      "ep 2237: ep_len:257 episode reward: total was -5.840000. running mean: -15.636036\n",
      "ep 2237: ep_len:575 episode reward: total was -32.630000. running mean: -15.805975\n",
      "epsilon:0.094513 episode_count: 15666. steps_count: 6920717.000000\n",
      "ep 2238: ep_len:204 episode reward: total was -1.900000. running mean: -15.666916\n",
      "ep 2238: ep_len:500 episode reward: total was 8.160000. running mean: -15.428647\n",
      "ep 2238: ep_len:720 episode reward: total was -54.810000. running mean: -15.822460\n",
      "ep 2238: ep_len:500 episode reward: total was -14.070000. running mean: -15.804935\n",
      "ep 2238: ep_len:3 episode reward: total was 0.000000. running mean: -15.646886\n",
      "ep 2238: ep_len:535 episode reward: total was -7.420000. running mean: -15.564617\n",
      "ep 2238: ep_len:550 episode reward: total was -11.620000. running mean: -15.525171\n",
      "epsilon:0.094376 episode_count: 15673. steps_count: 6923729.000000\n",
      "ep 2239: ep_len:500 episode reward: total was -33.800000. running mean: -15.707919\n",
      "ep 2239: ep_len:540 episode reward: total was -63.730000. running mean: -16.188140\n",
      "ep 2239: ep_len:500 episode reward: total was -11.510000. running mean: -16.141359\n",
      "ep 2239: ep_len:505 episode reward: total was -22.530000. running mean: -16.205245\n",
      "ep 2239: ep_len:54 episode reward: total was 3.500000. running mean: -16.008193\n",
      "ep 2239: ep_len:515 episode reward: total was 4.360000. running mean: -15.804511\n",
      "ep 2239: ep_len:505 episode reward: total was -27.970000. running mean: -15.926166\n",
      "epsilon:0.094240 episode_count: 15680. steps_count: 6926848.000000\n",
      "ep 2240: ep_len:590 episode reward: total was -17.140000. running mean: -15.938304\n",
      "ep 2240: ep_len:700 episode reward: total was -70.460000. running mean: -16.483521\n",
      "ep 2240: ep_len:500 episode reward: total was -13.380000. running mean: -16.452486\n",
      "ep 2240: ep_len:170 episode reward: total was 2.130000. running mean: -16.266661\n",
      "ep 2240: ep_len:3 episode reward: total was 0.000000. running mean: -16.103994\n",
      "ep 2240: ep_len:575 episode reward: total was -20.790000. running mean: -16.150854\n",
      "ep 2240: ep_len:326 episode reward: total was -15.370000. running mean: -16.143046\n",
      "epsilon:0.094103 episode_count: 15687. steps_count: 6929712.000000\n",
      "ep 2241: ep_len:550 episode reward: total was -2.060000. running mean: -16.002215\n",
      "ep 2241: ep_len:505 episode reward: total was -20.880000. running mean: -16.050993\n",
      "ep 2241: ep_len:555 episode reward: total was -19.610000. running mean: -16.086583\n",
      "ep 2241: ep_len:500 episode reward: total was -7.750000. running mean: -16.003217\n",
      "ep 2241: ep_len:109 episode reward: total was -4.950000. running mean: -15.892685\n",
      "ep 2241: ep_len:645 episode reward: total was -5.730000. running mean: -15.791058\n",
      "ep 2241: ep_len:590 episode reward: total was -19.100000. running mean: -15.824148\n",
      "epsilon:0.093967 episode_count: 15694. steps_count: 6933166.000000\n",
      "ep 2242: ep_len:605 episode reward: total was -30.480000. running mean: -15.970706\n",
      "ep 2242: ep_len:550 episode reward: total was -21.900000. running mean: -16.029999\n",
      "ep 2242: ep_len:359 episode reward: total was -29.890000. running mean: -16.168599\n",
      "ep 2242: ep_len:132 episode reward: total was 0.090000. running mean: -16.006013\n",
      "ep 2242: ep_len:74 episode reward: total was 5.020000. running mean: -15.795753\n",
      "ep 2242: ep_len:565 episode reward: total was -26.700000. running mean: -15.904796\n",
      "ep 2242: ep_len:555 episode reward: total was -13.630000. running mean: -15.882048\n",
      "epsilon:0.093830 episode_count: 15701. steps_count: 6936006.000000\n",
      "ep 2243: ep_len:630 episode reward: total was -3.380000. running mean: -15.757027\n",
      "ep 2243: ep_len:171 episode reward: total was -9.920000. running mean: -15.698657\n",
      "ep 2243: ep_len:615 episode reward: total was -4.420000. running mean: -15.585870\n",
      "ep 2243: ep_len:500 episode reward: total was -31.130000. running mean: -15.741312\n",
      "ep 2243: ep_len:3 episode reward: total was 0.000000. running mean: -15.583899\n",
      "ep 2243: ep_len:560 episode reward: total was -14.930000. running mean: -15.577360\n",
      "ep 2243: ep_len:575 episode reward: total was -51.820000. running mean: -15.939786\n",
      "epsilon:0.093694 episode_count: 15708. steps_count: 6939060.000000\n",
      "ep 2244: ep_len:505 episode reward: total was -0.150000. running mean: -15.781888\n",
      "ep 2244: ep_len:525 episode reward: total was -43.220000. running mean: -16.056269\n",
      "ep 2244: ep_len:600 episode reward: total was -42.960000. running mean: -16.325307\n",
      "ep 2244: ep_len:555 episode reward: total was 4.890000. running mean: -16.113153\n",
      "ep 2244: ep_len:56 episode reward: total was 4.000000. running mean: -15.912022\n",
      "ep 2244: ep_len:685 episode reward: total was -11.160000. running mean: -15.864502\n",
      "ep 2244: ep_len:545 episode reward: total was -13.620000. running mean: -15.842057\n",
      "epsilon:0.093557 episode_count: 15715. steps_count: 6942531.000000\n",
      "ep 2245: ep_len:565 episode reward: total was -2.430000. running mean: -15.707936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2245: ep_len:500 episode reward: total was -2.010000. running mean: -15.570957\n",
      "ep 2245: ep_len:535 episode reward: total was -7.280000. running mean: -15.488047\n",
      "ep 2245: ep_len:595 episode reward: total was -18.040000. running mean: -15.513567\n",
      "ep 2245: ep_len:3 episode reward: total was 0.000000. running mean: -15.358431\n",
      "ep 2245: ep_len:550 episode reward: total was -20.780000. running mean: -15.412647\n",
      "ep 2245: ep_len:191 episode reward: total was -6.360000. running mean: -15.322120\n",
      "epsilon:0.093421 episode_count: 15722. steps_count: 6945470.000000\n",
      "ep 2246: ep_len:640 episode reward: total was -31.270000. running mean: -15.481599\n",
      "ep 2246: ep_len:545 episode reward: total was 1.390000. running mean: -15.312883\n",
      "ep 2246: ep_len:79 episode reward: total was -0.970000. running mean: -15.169454\n",
      "ep 2246: ep_len:545 episode reward: total was -17.020000. running mean: -15.187960\n",
      "ep 2246: ep_len:3 episode reward: total was 0.000000. running mean: -15.036080\n",
      "ep 2246: ep_len:610 episode reward: total was 1.440000. running mean: -14.871319\n",
      "ep 2246: ep_len:505 episode reward: total was -24.070000. running mean: -14.963306\n",
      "epsilon:0.093284 episode_count: 15729. steps_count: 6948397.000000\n",
      "ep 2247: ep_len:500 episode reward: total was 2.440000. running mean: -14.789273\n",
      "ep 2247: ep_len:535 episode reward: total was -16.920000. running mean: -14.810580\n",
      "ep 2247: ep_len:500 episode reward: total was -7.060000. running mean: -14.733075\n",
      "ep 2247: ep_len:398 episode reward: total was -11.750000. running mean: -14.703244\n",
      "ep 2247: ep_len:3 episode reward: total was 0.000000. running mean: -14.556211\n",
      "ep 2247: ep_len:690 episode reward: total was -22.840000. running mean: -14.639049\n",
      "ep 2247: ep_len:595 episode reward: total was -18.080000. running mean: -14.673459\n",
      "epsilon:0.093148 episode_count: 15736. steps_count: 6951618.000000\n",
      "ep 2248: ep_len:204 episode reward: total was -2.910000. running mean: -14.555824\n",
      "ep 2248: ep_len:545 episode reward: total was -5.680000. running mean: -14.467066\n",
      "ep 2248: ep_len:650 episode reward: total was -27.000000. running mean: -14.592395\n",
      "ep 2248: ep_len:155 episode reward: total was 3.110000. running mean: -14.415371\n",
      "ep 2248: ep_len:3 episode reward: total was 0.000000. running mean: -14.271218\n",
      "ep 2248: ep_len:560 episode reward: total was -6.080000. running mean: -14.189305\n",
      "ep 2248: ep_len:322 episode reward: total was -15.410000. running mean: -14.201512\n",
      "epsilon:0.093011 episode_count: 15743. steps_count: 6954057.000000\n",
      "ep 2249: ep_len:545 episode reward: total was -43.500000. running mean: -14.494497\n",
      "ep 2249: ep_len:505 episode reward: total was -26.120000. running mean: -14.610752\n",
      "ep 2249: ep_len:635 episode reward: total was -33.510000. running mean: -14.799745\n",
      "ep 2249: ep_len:500 episode reward: total was -9.550000. running mean: -14.747247\n",
      "ep 2249: ep_len:89 episode reward: total was 6.520000. running mean: -14.534575\n",
      "ep 2249: ep_len:540 episode reward: total was -5.940000. running mean: -14.448629\n",
      "ep 2249: ep_len:515 episode reward: total was -23.150000. running mean: -14.535643\n",
      "epsilon:0.092875 episode_count: 15750. steps_count: 6957386.000000\n",
      "ep 2250: ep_len:560 episode reward: total was -28.810000. running mean: -14.678386\n",
      "ep 2250: ep_len:282 episode reward: total was -9.380000. running mean: -14.625402\n",
      "ep 2250: ep_len:620 episode reward: total was -9.920000. running mean: -14.578348\n",
      "ep 2250: ep_len:132 episode reward: total was 5.090000. running mean: -14.381665\n",
      "ep 2250: ep_len:3 episode reward: total was 0.000000. running mean: -14.237848\n",
      "ep 2250: ep_len:169 episode reward: total was -0.450000. running mean: -14.099970\n",
      "ep 2250: ep_len:600 episode reward: total was -10.290000. running mean: -14.061870\n",
      "epsilon:0.092738 episode_count: 15757. steps_count: 6959752.000000\n",
      "ep 2251: ep_len:550 episode reward: total was -18.230000. running mean: -14.103551\n",
      "ep 2251: ep_len:550 episode reward: total was 8.870000. running mean: -13.873816\n",
      "ep 2251: ep_len:500 episode reward: total was -29.570000. running mean: -14.030778\n",
      "ep 2251: ep_len:505 episode reward: total was 1.850000. running mean: -13.871970\n",
      "ep 2251: ep_len:111 episode reward: total was 7.550000. running mean: -13.657750\n",
      "ep 2251: ep_len:515 episode reward: total was 1.050000. running mean: -13.510673\n",
      "ep 2251: ep_len:585 episode reward: total was -11.600000. running mean: -13.491566\n",
      "epsilon:0.092602 episode_count: 15764. steps_count: 6963068.000000\n",
      "ep 2252: ep_len:585 episode reward: total was -7.110000. running mean: -13.427750\n",
      "ep 2252: ep_len:640 episode reward: total was -36.360000. running mean: -13.657073\n",
      "ep 2252: ep_len:590 episode reward: total was -18.060000. running mean: -13.701102\n",
      "ep 2252: ep_len:510 episode reward: total was -14.640000. running mean: -13.710491\n",
      "ep 2252: ep_len:3 episode reward: total was 0.000000. running mean: -13.573386\n",
      "ep 2252: ep_len:685 episode reward: total was -8.190000. running mean: -13.519552\n",
      "ep 2252: ep_len:510 episode reward: total was -18.690000. running mean: -13.571257\n",
      "epsilon:0.092465 episode_count: 15771. steps_count: 6966591.000000\n",
      "ep 2253: ep_len:122 episode reward: total was -6.480000. running mean: -13.500344\n",
      "ep 2253: ep_len:525 episode reward: total was -30.310000. running mean: -13.668441\n",
      "ep 2253: ep_len:610 episode reward: total was -15.810000. running mean: -13.689856\n",
      "ep 2253: ep_len:130 episode reward: total was -2.420000. running mean: -13.577158\n",
      "ep 2253: ep_len:3 episode reward: total was 0.000000. running mean: -13.441386\n",
      "ep 2253: ep_len:520 episode reward: total was 5.590000. running mean: -13.251072\n",
      "ep 2253: ep_len:346 episode reward: total was -13.370000. running mean: -13.252262\n",
      "epsilon:0.092329 episode_count: 15778. steps_count: 6968847.000000\n",
      "ep 2254: ep_len:175 episode reward: total was 1.580000. running mean: -13.103939\n",
      "ep 2254: ep_len:500 episode reward: total was -4.390000. running mean: -13.016800\n",
      "ep 2254: ep_len:545 episode reward: total was -8.030000. running mean: -12.966932\n",
      "ep 2254: ep_len:515 episode reward: total was -4.980000. running mean: -12.887062\n",
      "ep 2254: ep_len:3 episode reward: total was 0.000000. running mean: -12.758192\n",
      "ep 2254: ep_len:505 episode reward: total was -30.100000. running mean: -12.931610\n",
      "ep 2254: ep_len:520 episode reward: total was -15.610000. running mean: -12.958394\n",
      "epsilon:0.092192 episode_count: 15785. steps_count: 6971610.000000\n",
      "ep 2255: ep_len:229 episode reward: total was -23.430000. running mean: -13.063110\n",
      "ep 2255: ep_len:277 episode reward: total was -12.870000. running mean: -13.061179\n",
      "ep 2255: ep_len:550 episode reward: total was -29.950000. running mean: -13.230067\n",
      "ep 2255: ep_len:505 episode reward: total was -14.610000. running mean: -13.243866\n",
      "ep 2255: ep_len:99 episode reward: total was 1.030000. running mean: -13.101128\n",
      "ep 2255: ep_len:750 episode reward: total was -64.440000. running mean: -13.614516\n",
      "ep 2255: ep_len:520 episode reward: total was -15.990000. running mean: -13.638271\n",
      "epsilon:0.092056 episode_count: 15792. steps_count: 6974540.000000\n",
      "ep 2256: ep_len:515 episode reward: total was -30.890000. running mean: -13.810788\n",
      "ep 2256: ep_len:500 episode reward: total was 1.180000. running mean: -13.660881\n",
      "ep 2256: ep_len:79 episode reward: total was -3.970000. running mean: -13.563972\n",
      "ep 2256: ep_len:500 episode reward: total was -15.600000. running mean: -13.584332\n",
      "ep 2256: ep_len:3 episode reward: total was 0.000000. running mean: -13.448489\n",
      "ep 2256: ep_len:325 episode reward: total was -9.380000. running mean: -13.407804\n",
      "ep 2256: ep_len:169 episode reward: total was -6.890000. running mean: -13.342626\n",
      "epsilon:0.091919 episode_count: 15799. steps_count: 6976631.000000\n",
      "ep 2257: ep_len:590 episode reward: total was -0.070000. running mean: -13.209900\n",
      "ep 2257: ep_len:500 episode reward: total was -19.230000. running mean: -13.270101\n",
      "ep 2257: ep_len:595 episode reward: total was -17.570000. running mean: -13.313100\n",
      "ep 2257: ep_len:505 episode reward: total was -17.120000. running mean: -13.351169\n",
      "ep 2257: ep_len:79 episode reward: total was -4.440000. running mean: -13.262057\n",
      "ep 2257: ep_len:560 episode reward: total was -21.100000. running mean: -13.340436\n",
      "ep 2257: ep_len:570 episode reward: total was -16.070000. running mean: -13.367732\n",
      "epsilon:0.091783 episode_count: 15806. steps_count: 6980030.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2258: ep_len:209 episode reward: total was -17.410000. running mean: -13.408155\n",
      "ep 2258: ep_len:535 episode reward: total was 7.300000. running mean: -13.201073\n",
      "ep 2258: ep_len:515 episode reward: total was -8.930000. running mean: -13.158362\n",
      "ep 2258: ep_len:520 episode reward: total was -0.530000. running mean: -13.032079\n",
      "ep 2258: ep_len:76 episode reward: total was -8.450000. running mean: -12.986258\n",
      "ep 2258: ep_len:515 episode reward: total was -13.470000. running mean: -12.991095\n",
      "ep 2258: ep_len:335 episode reward: total was -21.900000. running mean: -13.080184\n",
      "epsilon:0.091646 episode_count: 15813. steps_count: 6982735.000000\n",
      "ep 2259: ep_len:505 episode reward: total was 0.390000. running mean: -12.945483\n",
      "ep 2259: ep_len:560 episode reward: total was 5.810000. running mean: -12.757928\n",
      "ep 2259: ep_len:575 episode reward: total was -9.770000. running mean: -12.728048\n",
      "ep 2259: ep_len:56 episode reward: total was -0.460000. running mean: -12.605368\n",
      "ep 2259: ep_len:3 episode reward: total was 0.000000. running mean: -12.479314\n",
      "ep 2259: ep_len:645 episode reward: total was -7.230000. running mean: -12.426821\n",
      "ep 2259: ep_len:725 episode reward: total was -48.730000. running mean: -12.789853\n",
      "epsilon:0.091510 episode_count: 15820. steps_count: 6985804.000000\n",
      "ep 2260: ep_len:535 episode reward: total was -29.440000. running mean: -12.956354\n",
      "ep 2260: ep_len:376 episode reward: total was -46.360000. running mean: -13.290391\n",
      "ep 2260: ep_len:725 episode reward: total was -70.900000. running mean: -13.866487\n",
      "ep 2260: ep_len:515 episode reward: total was -26.160000. running mean: -13.989422\n",
      "ep 2260: ep_len:3 episode reward: total was 0.000000. running mean: -13.849528\n",
      "ep 2260: ep_len:306 episode reward: total was -1.860000. running mean: -13.729633\n",
      "ep 2260: ep_len:595 episode reward: total was -24.840000. running mean: -13.840736\n",
      "epsilon:0.091373 episode_count: 15827. steps_count: 6988859.000000\n",
      "ep 2261: ep_len:500 episode reward: total was 9.380000. running mean: -13.608529\n",
      "ep 2261: ep_len:500 episode reward: total was -25.630000. running mean: -13.728744\n",
      "ep 2261: ep_len:520 episode reward: total was -18.880000. running mean: -13.780256\n",
      "ep 2261: ep_len:515 episode reward: total was -11.030000. running mean: -13.752754\n",
      "ep 2261: ep_len:3 episode reward: total was 0.000000. running mean: -13.615226\n",
      "ep 2261: ep_len:500 episode reward: total was -23.160000. running mean: -13.710674\n",
      "ep 2261: ep_len:560 episode reward: total was -28.050000. running mean: -13.854067\n",
      "epsilon:0.091237 episode_count: 15834. steps_count: 6991957.000000\n",
      "ep 2262: ep_len:620 episode reward: total was -38.330000. running mean: -14.098826\n",
      "ep 2262: ep_len:555 episode reward: total was -39.130000. running mean: -14.349138\n",
      "ep 2262: ep_len:570 episode reward: total was -30.350000. running mean: -14.509147\n",
      "ep 2262: ep_len:590 episode reward: total was 1.060000. running mean: -14.353455\n",
      "ep 2262: ep_len:95 episode reward: total was -12.450000. running mean: -14.334421\n",
      "ep 2262: ep_len:500 episode reward: total was -14.790000. running mean: -14.338977\n",
      "ep 2262: ep_len:575 episode reward: total was -47.700000. running mean: -14.672587\n",
      "epsilon:0.091100 episode_count: 15841. steps_count: 6995462.000000\n",
      "ep 2263: ep_len:500 episode reward: total was -41.690000. running mean: -14.942761\n",
      "ep 2263: ep_len:550 episode reward: total was -2.780000. running mean: -14.821133\n",
      "ep 2263: ep_len:565 episode reward: total was -16.970000. running mean: -14.842622\n",
      "ep 2263: ep_len:520 episode reward: total was -7.130000. running mean: -14.765496\n",
      "ep 2263: ep_len:67 episode reward: total was 1.520000. running mean: -14.602641\n",
      "ep 2263: ep_len:156 episode reward: total was 6.600000. running mean: -14.390614\n",
      "ep 2263: ep_len:630 episode reward: total was -35.200000. running mean: -14.598708\n",
      "epsilon:0.090964 episode_count: 15848. steps_count: 6998450.000000\n",
      "ep 2264: ep_len:505 episode reward: total was -16.260000. running mean: -14.615321\n",
      "ep 2264: ep_len:620 episode reward: total was -25.630000. running mean: -14.725468\n",
      "ep 2264: ep_len:620 episode reward: total was -7.980000. running mean: -14.658013\n",
      "ep 2264: ep_len:505 episode reward: total was 1.870000. running mean: -14.492733\n",
      "ep 2264: ep_len:3 episode reward: total was 0.000000. running mean: -14.347806\n",
      "ep 2264: ep_len:500 episode reward: total was -7.700000. running mean: -14.281328\n",
      "ep 2264: ep_len:580 episode reward: total was -9.810000. running mean: -14.236614\n",
      "epsilon:0.090827 episode_count: 15855. steps_count: 7001783.000000\n",
      "ep 2265: ep_len:605 episode reward: total was -60.470000. running mean: -14.698948\n",
      "ep 2265: ep_len:600 episode reward: total was -45.100000. running mean: -15.002959\n",
      "ep 2265: ep_len:359 episode reward: total was -14.840000. running mean: -15.001329\n",
      "ep 2265: ep_len:505 episode reward: total was -15.040000. running mean: -15.001716\n",
      "ep 2265: ep_len:3 episode reward: total was 0.000000. running mean: -14.851699\n",
      "ep 2265: ep_len:580 episode reward: total was -5.920000. running mean: -14.762382\n",
      "ep 2265: ep_len:210 episode reward: total was -3.320000. running mean: -14.647958\n",
      "epsilon:0.090691 episode_count: 15862. steps_count: 7004645.000000\n",
      "ep 2266: ep_len:510 episode reward: total was -14.020000. running mean: -14.641678\n",
      "ep 2266: ep_len:580 episode reward: total was -16.480000. running mean: -14.660062\n",
      "ep 2266: ep_len:510 episode reward: total was -15.350000. running mean: -14.666961\n",
      "ep 2266: ep_len:515 episode reward: total was -2.120000. running mean: -14.541491\n",
      "ep 2266: ep_len:3 episode reward: total was 0.000000. running mean: -14.396076\n",
      "ep 2266: ep_len:640 episode reward: total was -13.590000. running mean: -14.388016\n",
      "ep 2266: ep_len:625 episode reward: total was -10.760000. running mean: -14.351736\n",
      "epsilon:0.090554 episode_count: 15869. steps_count: 7008028.000000\n",
      "ep 2267: ep_len:535 episode reward: total was -9.430000. running mean: -14.302518\n",
      "ep 2267: ep_len:595 episode reward: total was -4.340000. running mean: -14.202893\n",
      "ep 2267: ep_len:500 episode reward: total was -17.040000. running mean: -14.231264\n",
      "ep 2267: ep_len:500 episode reward: total was -20.040000. running mean: -14.289351\n",
      "ep 2267: ep_len:123 episode reward: total was 2.040000. running mean: -14.126058\n",
      "ep 2267: ep_len:605 episode reward: total was 0.620000. running mean: -13.978597\n",
      "ep 2267: ep_len:535 episode reward: total was -25.150000. running mean: -14.090311\n",
      "epsilon:0.090418 episode_count: 15876. steps_count: 7011421.000000\n",
      "ep 2268: ep_len:595 episode reward: total was 0.470000. running mean: -13.944708\n",
      "ep 2268: ep_len:500 episode reward: total was -5.140000. running mean: -13.856661\n",
      "ep 2268: ep_len:432 episode reward: total was -25.270000. running mean: -13.970795\n",
      "ep 2268: ep_len:555 episode reward: total was -6.590000. running mean: -13.896987\n",
      "ep 2268: ep_len:3 episode reward: total was 0.000000. running mean: -13.758017\n",
      "ep 2268: ep_len:510 episode reward: total was -7.130000. running mean: -13.691737\n",
      "ep 2268: ep_len:298 episode reward: total was -14.910000. running mean: -13.703919\n",
      "epsilon:0.090281 episode_count: 15883. steps_count: 7014314.000000\n",
      "ep 2269: ep_len:196 episode reward: total was -11.920000. running mean: -13.686080\n",
      "ep 2269: ep_len:545 episode reward: total was 13.380000. running mean: -13.415419\n",
      "ep 2269: ep_len:640 episode reward: total was -9.420000. running mean: -13.375465\n",
      "ep 2269: ep_len:500 episode reward: total was -22.500000. running mean: -13.466710\n",
      "ep 2269: ep_len:92 episode reward: total was -11.450000. running mean: -13.446543\n",
      "ep 2269: ep_len:505 episode reward: total was -2.900000. running mean: -13.341078\n",
      "ep 2269: ep_len:510 episode reward: total was -7.520000. running mean: -13.282867\n",
      "epsilon:0.090145 episode_count: 15890. steps_count: 7017302.000000\n",
      "ep 2270: ep_len:500 episode reward: total was -19.140000. running mean: -13.341438\n",
      "ep 2270: ep_len:615 episode reward: total was 15.080000. running mean: -13.057224\n",
      "ep 2270: ep_len:630 episode reward: total was -12.040000. running mean: -13.047052\n",
      "ep 2270: ep_len:655 episode reward: total was -9.950000. running mean: -13.016081\n",
      "ep 2270: ep_len:2 episode reward: total was 0.000000. running mean: -12.885920\n",
      "ep 2270: ep_len:545 episode reward: total was -14.100000. running mean: -12.898061\n",
      "ep 2270: ep_len:610 episode reward: total was -32.500000. running mean: -13.094081\n",
      "epsilon:0.090008 episode_count: 15897. steps_count: 7020859.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2271: ep_len:535 episode reward: total was -15.550000. running mean: -13.118640\n",
      "ep 2271: ep_len:500 episode reward: total was 6.690000. running mean: -12.920553\n",
      "ep 2271: ep_len:570 episode reward: total was -7.600000. running mean: -12.867348\n",
      "ep 2271: ep_len:625 episode reward: total was 4.420000. running mean: -12.694474\n",
      "ep 2271: ep_len:3 episode reward: total was 0.000000. running mean: -12.567530\n",
      "ep 2271: ep_len:700 episode reward: total was -8.160000. running mean: -12.523454\n",
      "ep 2271: ep_len:505 episode reward: total was -8.890000. running mean: -12.487120\n",
      "epsilon:0.089872 episode_count: 15904. steps_count: 7024297.000000\n",
      "ep 2272: ep_len:626 episode reward: total was -47.440000. running mean: -12.836649\n",
      "ep 2272: ep_len:610 episode reward: total was -31.890000. running mean: -13.027182\n",
      "ep 2272: ep_len:525 episode reward: total was -33.440000. running mean: -13.231310\n",
      "ep 2272: ep_len:515 episode reward: total was -7.080000. running mean: -13.169797\n",
      "ep 2272: ep_len:3 episode reward: total was 0.000000. running mean: -13.038099\n",
      "ep 2272: ep_len:500 episode reward: total was -14.240000. running mean: -13.050118\n",
      "ep 2272: ep_len:293 episode reward: total was -7.770000. running mean: -12.997317\n",
      "epsilon:0.089735 episode_count: 15911. steps_count: 7027369.000000\n",
      "ep 2273: ep_len:125 episode reward: total was -2.460000. running mean: -12.891944\n",
      "ep 2273: ep_len:525 episode reward: total was -19.610000. running mean: -12.959124\n",
      "ep 2273: ep_len:605 episode reward: total was -22.140000. running mean: -13.050933\n",
      "ep 2273: ep_len:500 episode reward: total was -18.580000. running mean: -13.106224\n",
      "ep 2273: ep_len:80 episode reward: total was -0.470000. running mean: -12.979862\n",
      "ep 2273: ep_len:319 episode reward: total was -0.330000. running mean: -12.853363\n",
      "ep 2273: ep_len:505 episode reward: total was -23.550000. running mean: -12.960329\n",
      "epsilon:0.089599 episode_count: 15918. steps_count: 7030028.000000\n",
      "ep 2274: ep_len:585 episode reward: total was -17.610000. running mean: -13.006826\n",
      "ep 2274: ep_len:740 episode reward: total was -49.400000. running mean: -13.370758\n",
      "ep 2274: ep_len:585 episode reward: total was -5.910000. running mean: -13.296150\n",
      "ep 2274: ep_len:505 episode reward: total was -27.000000. running mean: -13.433189\n",
      "ep 2274: ep_len:76 episode reward: total was -4.480000. running mean: -13.343657\n",
      "ep 2274: ep_len:500 episode reward: total was -22.210000. running mean: -13.432320\n",
      "ep 2274: ep_len:530 episode reward: total was -31.000000. running mean: -13.607997\n",
      "epsilon:0.089462 episode_count: 15925. steps_count: 7033549.000000\n",
      "ep 2275: ep_len:600 episode reward: total was -34.380000. running mean: -13.815717\n",
      "ep 2275: ep_len:595 episode reward: total was -33.380000. running mean: -14.011360\n",
      "ep 2275: ep_len:610 episode reward: total was -32.510000. running mean: -14.196346\n",
      "ep 2275: ep_len:500 episode reward: total was -11.970000. running mean: -14.174083\n",
      "ep 2275: ep_len:88 episode reward: total was 3.550000. running mean: -13.996842\n",
      "ep 2275: ep_len:560 episode reward: total was -33.750000. running mean: -14.194374\n",
      "ep 2275: ep_len:620 episode reward: total was -3.510000. running mean: -14.087530\n",
      "epsilon:0.089326 episode_count: 15932. steps_count: 7037122.000000\n",
      "ep 2276: ep_len:590 episode reward: total was -14.940000. running mean: -14.096055\n",
      "ep 2276: ep_len:555 episode reward: total was -33.010000. running mean: -14.285194\n",
      "ep 2276: ep_len:500 episode reward: total was -24.690000. running mean: -14.389242\n",
      "ep 2276: ep_len:500 episode reward: total was 5.920000. running mean: -14.186150\n",
      "ep 2276: ep_len:3 episode reward: total was 0.000000. running mean: -14.044288\n",
      "ep 2276: ep_len:500 episode reward: total was -28.230000. running mean: -14.186145\n",
      "ep 2276: ep_len:595 episode reward: total was -39.690000. running mean: -14.441184\n",
      "epsilon:0.089189 episode_count: 15939. steps_count: 7040365.000000\n",
      "ep 2277: ep_len:580 episode reward: total was 2.600000. running mean: -14.270772\n",
      "ep 2277: ep_len:500 episode reward: total was -20.000000. running mean: -14.328064\n",
      "ep 2277: ep_len:725 episode reward: total was -44.290000. running mean: -14.627684\n",
      "ep 2277: ep_len:575 episode reward: total was -18.910000. running mean: -14.670507\n",
      "ep 2277: ep_len:3 episode reward: total was 0.000000. running mean: -14.523802\n",
      "ep 2277: ep_len:690 episode reward: total was -30.310000. running mean: -14.681664\n",
      "ep 2277: ep_len:570 episode reward: total was -47.540000. running mean: -15.010247\n",
      "epsilon:0.089053 episode_count: 15946. steps_count: 7044008.000000\n",
      "ep 2278: ep_len:535 episode reward: total was 3.890000. running mean: -14.821245\n",
      "ep 2278: ep_len:500 episode reward: total was -34.070000. running mean: -15.013732\n",
      "ep 2278: ep_len:545 episode reward: total was -14.380000. running mean: -15.007395\n",
      "ep 2278: ep_len:500 episode reward: total was -19.530000. running mean: -15.052621\n",
      "ep 2278: ep_len:3 episode reward: total was 0.000000. running mean: -14.902095\n",
      "ep 2278: ep_len:505 episode reward: total was -40.810000. running mean: -15.161174\n",
      "ep 2278: ep_len:530 episode reward: total was -16.560000. running mean: -15.175162\n",
      "epsilon:0.088916 episode_count: 15953. steps_count: 7047126.000000\n",
      "ep 2279: ep_len:505 episode reward: total was -11.690000. running mean: -15.140310\n",
      "ep 2279: ep_len:363 episode reward: total was -14.810000. running mean: -15.137007\n",
      "ep 2279: ep_len:79 episode reward: total was 1.050000. running mean: -14.975137\n",
      "ep 2279: ep_len:500 episode reward: total was -29.620000. running mean: -15.121586\n",
      "ep 2279: ep_len:100 episode reward: total was 0.550000. running mean: -14.964870\n",
      "ep 2279: ep_len:186 episode reward: total was -9.860000. running mean: -14.913821\n",
      "ep 2279: ep_len:550 episode reward: total was -4.030000. running mean: -14.804983\n",
      "epsilon:0.088780 episode_count: 15960. steps_count: 7049409.000000\n",
      "ep 2280: ep_len:630 episode reward: total was -17.920000. running mean: -14.836133\n",
      "ep 2280: ep_len:590 episode reward: total was 3.550000. running mean: -14.652272\n",
      "ep 2280: ep_len:530 episode reward: total was -10.920000. running mean: -14.614949\n",
      "ep 2280: ep_len:168 episode reward: total was -1.360000. running mean: -14.482400\n",
      "ep 2280: ep_len:71 episode reward: total was -1.960000. running mean: -14.357176\n",
      "ep 2280: ep_len:620 episode reward: total was 3.020000. running mean: -14.183404\n",
      "ep 2280: ep_len:190 episode reward: total was -16.950000. running mean: -14.211070\n",
      "epsilon:0.088643 episode_count: 15967. steps_count: 7052208.000000\n",
      "ep 2281: ep_len:525 episode reward: total was -2.440000. running mean: -14.093359\n",
      "ep 2281: ep_len:505 episode reward: total was -31.720000. running mean: -14.269626\n",
      "ep 2281: ep_len:560 episode reward: total was -34.760000. running mean: -14.474529\n",
      "ep 2281: ep_len:525 episode reward: total was -6.690000. running mean: -14.396684\n",
      "ep 2281: ep_len:3 episode reward: total was 0.000000. running mean: -14.252717\n",
      "ep 2281: ep_len:510 episode reward: total was -12.410000. running mean: -14.234290\n",
      "ep 2281: ep_len:555 episode reward: total was -11.420000. running mean: -14.206147\n",
      "epsilon:0.088507 episode_count: 15974. steps_count: 7055391.000000\n",
      "ep 2282: ep_len:500 episode reward: total was -2.270000. running mean: -14.086786\n",
      "ep 2282: ep_len:565 episode reward: total was -17.570000. running mean: -14.121618\n",
      "ep 2282: ep_len:510 episode reward: total was -16.470000. running mean: -14.145102\n",
      "ep 2282: ep_len:358 episode reward: total was -10.130000. running mean: -14.104951\n",
      "ep 2282: ep_len:109 episode reward: total was -0.940000. running mean: -13.973301\n",
      "ep 2282: ep_len:500 episode reward: total was -26.660000. running mean: -14.100168\n",
      "ep 2282: ep_len:505 episode reward: total was -25.170000. running mean: -14.210866\n",
      "epsilon:0.088370 episode_count: 15981. steps_count: 7058438.000000\n",
      "ep 2283: ep_len:132 episode reward: total was 4.100000. running mean: -14.027758\n",
      "ep 2283: ep_len:660 episode reward: total was -45.490000. running mean: -14.342380\n",
      "ep 2283: ep_len:555 episode reward: total was -8.500000. running mean: -14.283956\n",
      "ep 2283: ep_len:605 episode reward: total was -23.080000. running mean: -14.371917\n",
      "ep 2283: ep_len:3 episode reward: total was 0.000000. running mean: -14.228198\n",
      "ep 2283: ep_len:253 episode reward: total was -7.850000. running mean: -14.164416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2283: ep_len:540 episode reward: total was -36.560000. running mean: -14.388372\n",
      "epsilon:0.088234 episode_count: 15988. steps_count: 7061186.000000\n",
      "ep 2284: ep_len:540 episode reward: total was -39.960000. running mean: -14.644088\n",
      "ep 2284: ep_len:182 episode reward: total was -1.880000. running mean: -14.516447\n",
      "ep 2284: ep_len:505 episode reward: total was -20.040000. running mean: -14.571682\n",
      "ep 2284: ep_len:625 episode reward: total was -13.920000. running mean: -14.565166\n",
      "ep 2284: ep_len:3 episode reward: total was 0.000000. running mean: -14.419514\n",
      "ep 2284: ep_len:670 episode reward: total was -32.740000. running mean: -14.602719\n",
      "ep 2284: ep_len:535 episode reward: total was -18.450000. running mean: -14.641192\n",
      "epsilon:0.088097 episode_count: 15995. steps_count: 7064246.000000\n",
      "ep 2285: ep_len:500 episode reward: total was -36.290000. running mean: -14.857680\n",
      "ep 2285: ep_len:570 episode reward: total was 0.030000. running mean: -14.708803\n",
      "ep 2285: ep_len:379 episode reward: total was 1.690000. running mean: -14.544815\n",
      "ep 2285: ep_len:520 episode reward: total was 2.400000. running mean: -14.375367\n",
      "ep 2285: ep_len:117 episode reward: total was 3.550000. running mean: -14.196113\n",
      "ep 2285: ep_len:520 episode reward: total was -17.540000. running mean: -14.229552\n",
      "ep 2285: ep_len:515 episode reward: total was -9.830000. running mean: -14.185556\n",
      "epsilon:0.087961 episode_count: 16002. steps_count: 7067367.000000\n",
      "ep 2286: ep_len:500 episode reward: total was 12.240000. running mean: -13.921301\n",
      "ep 2286: ep_len:705 episode reward: total was -38.280000. running mean: -14.164888\n",
      "ep 2286: ep_len:545 episode reward: total was -10.370000. running mean: -14.126939\n",
      "ep 2286: ep_len:42 episode reward: total was 0.550000. running mean: -13.980170\n",
      "ep 2286: ep_len:3 episode reward: total was 0.000000. running mean: -13.840368\n",
      "ep 2286: ep_len:695 episode reward: total was -26.750000. running mean: -13.969464\n",
      "ep 2286: ep_len:615 episode reward: total was -30.100000. running mean: -14.130770\n",
      "epsilon:0.087824 episode_count: 16009. steps_count: 7070472.000000\n",
      "ep 2287: ep_len:500 episode reward: total was -1.810000. running mean: -14.007562\n",
      "ep 2287: ep_len:500 episode reward: total was 4.620000. running mean: -13.821286\n",
      "ep 2287: ep_len:545 episode reward: total was -8.110000. running mean: -13.764173\n",
      "ep 2287: ep_len:535 episode reward: total was 3.470000. running mean: -13.591832\n",
      "ep 2287: ep_len:3 episode reward: total was 0.000000. running mean: -13.455913\n",
      "ep 2287: ep_len:520 episode reward: total was -13.590000. running mean: -13.457254\n",
      "ep 2287: ep_len:555 episode reward: total was -25.330000. running mean: -13.575982\n",
      "epsilon:0.087688 episode_count: 16016. steps_count: 7073630.000000\n",
      "ep 2288: ep_len:500 episode reward: total was -8.960000. running mean: -13.529822\n",
      "ep 2288: ep_len:720 episode reward: total was -34.760000. running mean: -13.742124\n",
      "ep 2288: ep_len:615 episode reward: total was -12.800000. running mean: -13.732702\n",
      "ep 2288: ep_len:510 episode reward: total was -16.960000. running mean: -13.764975\n",
      "ep 2288: ep_len:97 episode reward: total was 5.560000. running mean: -13.571726\n",
      "ep 2288: ep_len:505 episode reward: total was -12.910000. running mean: -13.565108\n",
      "ep 2288: ep_len:535 episode reward: total was -7.550000. running mean: -13.504957\n",
      "epsilon:0.087551 episode_count: 16023. steps_count: 7077112.000000\n",
      "ep 2289: ep_len:121 episode reward: total was -6.410000. running mean: -13.434008\n",
      "ep 2289: ep_len:510 episode reward: total was -11.000000. running mean: -13.409668\n",
      "ep 2289: ep_len:79 episode reward: total was -0.970000. running mean: -13.285271\n",
      "ep 2289: ep_len:505 episode reward: total was -4.940000. running mean: -13.201818\n",
      "ep 2289: ep_len:105 episode reward: total was 4.540000. running mean: -13.024400\n",
      "ep 2289: ep_len:625 episode reward: total was -0.430000. running mean: -12.898456\n",
      "ep 2289: ep_len:500 episode reward: total was -16.930000. running mean: -12.938772\n",
      "epsilon:0.087415 episode_count: 16030. steps_count: 7079557.000000\n",
      "ep 2290: ep_len:730 episode reward: total was -37.660000. running mean: -13.185984\n",
      "ep 2290: ep_len:590 episode reward: total was -31.440000. running mean: -13.368524\n",
      "ep 2290: ep_len:640 episode reward: total was -7.880000. running mean: -13.313639\n",
      "ep 2290: ep_len:595 episode reward: total was -26.920000. running mean: -13.449702\n",
      "ep 2290: ep_len:3 episode reward: total was 0.000000. running mean: -13.315205\n",
      "ep 2290: ep_len:500 episode reward: total was -32.020000. running mean: -13.502253\n",
      "ep 2290: ep_len:565 episode reward: total was -39.530000. running mean: -13.762531\n",
      "epsilon:0.087278 episode_count: 16037. steps_count: 7083180.000000\n",
      "ep 2291: ep_len:500 episode reward: total was 0.280000. running mean: -13.622105\n",
      "ep 2291: ep_len:575 episode reward: total was -41.540000. running mean: -13.901284\n",
      "ep 2291: ep_len:570 episode reward: total was -7.240000. running mean: -13.834672\n",
      "ep 2291: ep_len:500 episode reward: total was -34.180000. running mean: -14.038125\n",
      "ep 2291: ep_len:3 episode reward: total was 0.000000. running mean: -13.897744\n",
      "ep 2291: ep_len:510 episode reward: total was -18.670000. running mean: -13.945466\n",
      "ep 2291: ep_len:560 episode reward: total was -15.060000. running mean: -13.956611\n",
      "epsilon:0.087142 episode_count: 16044. steps_count: 7086398.000000\n",
      "ep 2292: ep_len:590 episode reward: total was 1.080000. running mean: -13.806245\n",
      "ep 2292: ep_len:580 episode reward: total was -1.090000. running mean: -13.679083\n",
      "ep 2292: ep_len:570 episode reward: total was 4.430000. running mean: -13.497992\n",
      "ep 2292: ep_len:540 episode reward: total was 5.380000. running mean: -13.309212\n",
      "ep 2292: ep_len:110 episode reward: total was -4.470000. running mean: -13.220820\n",
      "ep 2292: ep_len:1245 episode reward: total was -180.260000. running mean: -14.891212\n",
      "ep 2292: ep_len:545 episode reward: total was -17.940000. running mean: -14.921700\n",
      "epsilon:0.087005 episode_count: 16051. steps_count: 7090578.000000\n",
      "ep 2293: ep_len:600 episode reward: total was -28.620000. running mean: -15.058683\n",
      "ep 2293: ep_len:555 episode reward: total was 2.370000. running mean: -14.884396\n",
      "ep 2293: ep_len:555 episode reward: total was -10.050000. running mean: -14.836052\n",
      "ep 2293: ep_len:615 episode reward: total was -4.980000. running mean: -14.737491\n",
      "ep 2293: ep_len:51 episode reward: total was 5.000000. running mean: -14.540116\n",
      "ep 2293: ep_len:535 episode reward: total was -19.430000. running mean: -14.589015\n",
      "ep 2293: ep_len:510 episode reward: total was -15.900000. running mean: -14.602125\n",
      "epsilon:0.086869 episode_count: 16058. steps_count: 7093999.000000\n",
      "ep 2294: ep_len:645 episode reward: total was -21.400000. running mean: -14.670104\n",
      "ep 2294: ep_len:540 episode reward: total was 7.890000. running mean: -14.444503\n",
      "ep 2294: ep_len:530 episode reward: total was -13.620000. running mean: -14.436258\n",
      "ep 2294: ep_len:500 episode reward: total was -25.220000. running mean: -14.544095\n",
      "ep 2294: ep_len:116 episode reward: total was 5.050000. running mean: -14.348154\n",
      "ep 2294: ep_len:500 episode reward: total was -5.620000. running mean: -14.260873\n",
      "ep 2294: ep_len:328 episode reward: total was -19.300000. running mean: -14.311264\n",
      "epsilon:0.086732 episode_count: 16065. steps_count: 7097158.000000\n",
      "ep 2295: ep_len:650 episode reward: total was -40.890000. running mean: -14.577051\n",
      "ep 2295: ep_len:640 episode reward: total was -12.470000. running mean: -14.555981\n",
      "ep 2295: ep_len:605 episode reward: total was -12.940000. running mean: -14.539821\n",
      "ep 2295: ep_len:515 episode reward: total was -17.560000. running mean: -14.570023\n",
      "ep 2295: ep_len:81 episode reward: total was 2.040000. running mean: -14.403923\n",
      "ep 2295: ep_len:308 episode reward: total was -4.830000. running mean: -14.308183\n",
      "ep 2295: ep_len:273 episode reward: total was -14.830000. running mean: -14.313402\n",
      "epsilon:0.086596 episode_count: 16072. steps_count: 7100230.000000\n",
      "ep 2296: ep_len:500 episode reward: total was -16.760000. running mean: -14.337868\n",
      "ep 2296: ep_len:525 episode reward: total was -13.850000. running mean: -14.332989\n",
      "ep 2296: ep_len:625 episode reward: total was -10.220000. running mean: -14.291859\n",
      "ep 2296: ep_len:500 episode reward: total was -0.640000. running mean: -14.155340\n",
      "ep 2296: ep_len:100 episode reward: total was 5.050000. running mean: -13.963287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2296: ep_len:555 episode reward: total was -19.590000. running mean: -14.019554\n",
      "ep 2296: ep_len:585 episode reward: total was -22.890000. running mean: -14.108259\n",
      "epsilon:0.086459 episode_count: 16079. steps_count: 7103620.000000\n",
      "ep 2297: ep_len:675 episode reward: total was -17.210000. running mean: -14.139276\n",
      "ep 2297: ep_len:590 episode reward: total was -27.460000. running mean: -14.272483\n",
      "ep 2297: ep_len:630 episode reward: total was -7.890000. running mean: -14.208658\n",
      "ep 2297: ep_len:500 episode reward: total was -28.150000. running mean: -14.348072\n",
      "ep 2297: ep_len:124 episode reward: total was 5.060000. running mean: -14.153991\n",
      "ep 2297: ep_len:500 episode reward: total was -8.260000. running mean: -14.095051\n",
      "ep 2297: ep_len:500 episode reward: total was -19.280000. running mean: -14.146901\n",
      "epsilon:0.086323 episode_count: 16086. steps_count: 7107139.000000\n",
      "ep 2298: ep_len:500 episode reward: total was -10.730000. running mean: -14.112732\n",
      "ep 2298: ep_len:640 episode reward: total was -43.190000. running mean: -14.403504\n",
      "ep 2298: ep_len:670 episode reward: total was -6.140000. running mean: -14.320869\n",
      "ep 2298: ep_len:555 episode reward: total was 2.400000. running mean: -14.153661\n",
      "ep 2298: ep_len:47 episode reward: total was 4.500000. running mean: -13.967124\n",
      "ep 2298: ep_len:615 episode reward: total was 4.490000. running mean: -13.782553\n",
      "ep 2298: ep_len:500 episode reward: total was -27.180000. running mean: -13.916527\n",
      "epsilon:0.086186 episode_count: 16093. steps_count: 7110666.000000\n",
      "ep 2299: ep_len:865 episode reward: total was -76.540000. running mean: -14.542762\n",
      "ep 2299: ep_len:530 episode reward: total was -28.810000. running mean: -14.685434\n",
      "ep 2299: ep_len:500 episode reward: total was -5.710000. running mean: -14.595680\n",
      "ep 2299: ep_len:500 episode reward: total was -36.620000. running mean: -14.815923\n",
      "ep 2299: ep_len:3 episode reward: total was 0.000000. running mean: -14.667764\n",
      "ep 2299: ep_len:500 episode reward: total was -7.470000. running mean: -14.595786\n",
      "ep 2299: ep_len:595 episode reward: total was -33.090000. running mean: -14.780728\n",
      "epsilon:0.086050 episode_count: 16100. steps_count: 7114159.000000\n",
      "ep 2300: ep_len:258 episode reward: total was -3.920000. running mean: -14.672121\n",
      "ep 2300: ep_len:525 episode reward: total was -30.980000. running mean: -14.835200\n",
      "ep 2300: ep_len:515 episode reward: total was -12.800000. running mean: -14.814848\n",
      "ep 2300: ep_len:560 episode reward: total was -3.040000. running mean: -14.697100\n",
      "ep 2300: ep_len:3 episode reward: total was 0.000000. running mean: -14.550129\n",
      "ep 2300: ep_len:610 episode reward: total was -4.620000. running mean: -14.450827\n",
      "ep 2300: ep_len:334 episode reward: total was -17.340000. running mean: -14.479719\n",
      "epsilon:0.085913 episode_count: 16107. steps_count: 7116964.000000\n",
      "ep 2301: ep_len:575 episode reward: total was -3.550000. running mean: -14.370422\n",
      "ep 2301: ep_len:500 episode reward: total was 6.130000. running mean: -14.165418\n",
      "ep 2301: ep_len:550 episode reward: total was -8.080000. running mean: -14.104563\n",
      "ep 2301: ep_len:510 episode reward: total was -31.160000. running mean: -14.275118\n",
      "ep 2301: ep_len:3 episode reward: total was 0.000000. running mean: -14.132367\n",
      "ep 2301: ep_len:515 episode reward: total was 6.900000. running mean: -13.922043\n",
      "ep 2301: ep_len:655 episode reward: total was -18.280000. running mean: -13.965622\n",
      "epsilon:0.085777 episode_count: 16114. steps_count: 7120272.000000\n",
      "ep 2302: ep_len:500 episode reward: total was -12.230000. running mean: -13.948266\n",
      "ep 2302: ep_len:500 episode reward: total was 4.650000. running mean: -13.762284\n",
      "ep 2302: ep_len:915 episode reward: total was -81.720000. running mean: -14.441861\n",
      "ep 2302: ep_len:500 episode reward: total was -25.620000. running mean: -14.553642\n",
      "ep 2302: ep_len:3 episode reward: total was 0.000000. running mean: -14.408106\n",
      "ep 2302: ep_len:680 episode reward: total was -7.650000. running mean: -14.340525\n",
      "ep 2302: ep_len:630 episode reward: total was -11.760000. running mean: -14.314719\n",
      "epsilon:0.085640 episode_count: 16121. steps_count: 7124000.000000\n",
      "ep 2303: ep_len:500 episode reward: total was 6.330000. running mean: -14.108272\n",
      "ep 2303: ep_len:500 episode reward: total was 10.730000. running mean: -13.859889\n",
      "ep 2303: ep_len:630 episode reward: total was -5.670000. running mean: -13.777991\n",
      "ep 2303: ep_len:600 episode reward: total was -20.360000. running mean: -13.843811\n",
      "ep 2303: ep_len:3 episode reward: total was 0.000000. running mean: -13.705373\n",
      "ep 2303: ep_len:515 episode reward: total was -15.510000. running mean: -13.723419\n",
      "ep 2303: ep_len:500 episode reward: total was -18.790000. running mean: -13.774085\n",
      "epsilon:0.085504 episode_count: 16128. steps_count: 7127248.000000\n",
      "ep 2304: ep_len:505 episode reward: total was 0.430000. running mean: -13.632044\n",
      "ep 2304: ep_len:600 episode reward: total was -77.790000. running mean: -14.273623\n",
      "ep 2304: ep_len:680 episode reward: total was -17.230000. running mean: -14.303187\n",
      "ep 2304: ep_len:505 episode reward: total was 1.870000. running mean: -14.141455\n",
      "ep 2304: ep_len:3 episode reward: total was 0.000000. running mean: -14.000041\n",
      "ep 2304: ep_len:525 episode reward: total was -18.290000. running mean: -14.042940\n",
      "ep 2304: ep_len:338 episode reward: total was -11.840000. running mean: -14.020911\n",
      "epsilon:0.085367 episode_count: 16135. steps_count: 7130404.000000\n",
      "ep 2305: ep_len:655 episode reward: total was -16.180000. running mean: -14.042502\n",
      "ep 2305: ep_len:500 episode reward: total was -24.080000. running mean: -14.142877\n",
      "ep 2305: ep_len:670 episode reward: total was -7.410000. running mean: -14.075548\n",
      "ep 2305: ep_len:500 episode reward: total was -9.400000. running mean: -14.028793\n",
      "ep 2305: ep_len:3 episode reward: total was 0.000000. running mean: -13.888505\n",
      "ep 2305: ep_len:530 episode reward: total was -5.070000. running mean: -13.800320\n",
      "ep 2305: ep_len:500 episode reward: total was -10.760000. running mean: -13.769916\n",
      "epsilon:0.085231 episode_count: 16142. steps_count: 7133762.000000\n",
      "ep 2306: ep_len:775 episode reward: total was -44.820000. running mean: -14.080417\n",
      "ep 2306: ep_len:505 episode reward: total was -4.510000. running mean: -13.984713\n",
      "ep 2306: ep_len:620 episode reward: total was -9.940000. running mean: -13.944266\n",
      "ep 2306: ep_len:392 episode reward: total was -11.690000. running mean: -13.921723\n",
      "ep 2306: ep_len:60 episode reward: total was -2.960000. running mean: -13.812106\n",
      "ep 2306: ep_len:545 episode reward: total was -20.590000. running mean: -13.879885\n",
      "ep 2306: ep_len:590 episode reward: total was -13.920000. running mean: -13.880286\n",
      "epsilon:0.085094 episode_count: 16149. steps_count: 7137249.000000\n",
      "ep 2307: ep_len:262 episode reward: total was 3.150000. running mean: -13.709983\n",
      "ep 2307: ep_len:500 episode reward: total was 4.690000. running mean: -13.525983\n",
      "ep 2307: ep_len:605 episode reward: total was -31.920000. running mean: -13.709924\n",
      "ep 2307: ep_len:500 episode reward: total was 5.200000. running mean: -13.520824\n",
      "ep 2307: ep_len:3 episode reward: total was 0.000000. running mean: -13.385616\n",
      "ep 2307: ep_len:600 episode reward: total was -26.060000. running mean: -13.512360\n",
      "ep 2307: ep_len:580 episode reward: total was -19.940000. running mean: -13.576636\n",
      "epsilon:0.084958 episode_count: 16156. steps_count: 7140299.000000\n",
      "ep 2308: ep_len:625 episode reward: total was -28.270000. running mean: -13.723570\n",
      "ep 2308: ep_len:540 episode reward: total was -27.190000. running mean: -13.858234\n",
      "ep 2308: ep_len:640 episode reward: total was -13.210000. running mean: -13.851752\n",
      "ep 2308: ep_len:500 episode reward: total was 8.290000. running mean: -13.630334\n",
      "ep 2308: ep_len:3 episode reward: total was 0.000000. running mean: -13.494031\n",
      "ep 2308: ep_len:635 episode reward: total was -2.850000. running mean: -13.387591\n",
      "ep 2308: ep_len:500 episode reward: total was -5.360000. running mean: -13.307315\n",
      "epsilon:0.084821 episode_count: 16163. steps_count: 7143742.000000\n",
      "ep 2309: ep_len:630 episode reward: total was -1.910000. running mean: -13.193342\n",
      "ep 2309: ep_len:500 episode reward: total was -35.820000. running mean: -13.419608\n",
      "ep 2309: ep_len:605 episode reward: total was -22.300000. running mean: -13.508412\n",
      "ep 2309: ep_len:500 episode reward: total was -17.600000. running mean: -13.549328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2309: ep_len:3 episode reward: total was 0.000000. running mean: -13.413835\n",
      "ep 2309: ep_len:660 episode reward: total was -75.680000. running mean: -14.036496\n",
      "ep 2309: ep_len:299 episode reward: total was -45.900000. running mean: -14.355131\n",
      "epsilon:0.084685 episode_count: 16170. steps_count: 7146939.000000\n",
      "ep 2310: ep_len:580 episode reward: total was 2.400000. running mean: -14.187580\n",
      "ep 2310: ep_len:500 episode reward: total was -24.140000. running mean: -14.287104\n",
      "ep 2310: ep_len:580 episode reward: total was -10.190000. running mean: -14.246133\n",
      "ep 2310: ep_len:500 episode reward: total was -0.710000. running mean: -14.110772\n",
      "ep 2310: ep_len:107 episode reward: total was 8.060000. running mean: -13.889064\n",
      "ep 2310: ep_len:500 episode reward: total was -10.820000. running mean: -13.858374\n",
      "ep 2310: ep_len:605 episode reward: total was -11.840000. running mean: -13.838190\n",
      "epsilon:0.084548 episode_count: 16177. steps_count: 7150311.000000\n",
      "ep 2311: ep_len:685 episode reward: total was -18.200000. running mean: -13.881808\n",
      "ep 2311: ep_len:565 episode reward: total was -88.830000. running mean: -14.631290\n",
      "ep 2311: ep_len:620 episode reward: total was -14.080000. running mean: -14.625777\n",
      "ep 2311: ep_len:404 episode reward: total was -33.680000. running mean: -14.816319\n",
      "ep 2311: ep_len:3 episode reward: total was 0.000000. running mean: -14.668156\n",
      "ep 2311: ep_len:545 episode reward: total was -28.670000. running mean: -14.808174\n",
      "ep 2311: ep_len:580 episode reward: total was -23.080000. running mean: -14.890893\n",
      "epsilon:0.084412 episode_count: 16184. steps_count: 7153713.000000\n",
      "ep 2312: ep_len:565 episode reward: total was -4.070000. running mean: -14.782684\n",
      "ep 2312: ep_len:500 episode reward: total was 8.700000. running mean: -14.547857\n",
      "ep 2312: ep_len:585 episode reward: total was -16.110000. running mean: -14.563478\n",
      "ep 2312: ep_len:520 episode reward: total was -2.660000. running mean: -14.444444\n",
      "ep 2312: ep_len:3 episode reward: total was 0.000000. running mean: -14.299999\n",
      "ep 2312: ep_len:505 episode reward: total was -6.930000. running mean: -14.226299\n",
      "ep 2312: ep_len:585 episode reward: total was -21.090000. running mean: -14.294936\n",
      "epsilon:0.084275 episode_count: 16191. steps_count: 7156976.000000\n",
      "ep 2313: ep_len:615 episode reward: total was 0.450000. running mean: -14.147487\n",
      "ep 2313: ep_len:288 episode reward: total was -40.370000. running mean: -14.409712\n",
      "ep 2313: ep_len:500 episode reward: total was -5.440000. running mean: -14.320015\n",
      "ep 2313: ep_len:500 episode reward: total was -12.540000. running mean: -14.302215\n",
      "ep 2313: ep_len:3 episode reward: total was 0.000000. running mean: -14.159193\n",
      "ep 2313: ep_len:670 episode reward: total was -65.550000. running mean: -14.673101\n",
      "ep 2313: ep_len:540 episode reward: total was -14.400000. running mean: -14.670370\n",
      "epsilon:0.084139 episode_count: 16198. steps_count: 7160092.000000\n",
      "ep 2314: ep_len:505 episode reward: total was -7.120000. running mean: -14.594866\n",
      "ep 2314: ep_len:190 episode reward: total was -9.920000. running mean: -14.548117\n",
      "ep 2314: ep_len:600 episode reward: total was -7.620000. running mean: -14.478836\n",
      "ep 2314: ep_len:505 episode reward: total was -12.530000. running mean: -14.459348\n",
      "ep 2314: ep_len:3 episode reward: total was 0.000000. running mean: -14.314754\n",
      "ep 2314: ep_len:545 episode reward: total was -23.900000. running mean: -14.410607\n",
      "ep 2314: ep_len:565 episode reward: total was -12.980000. running mean: -14.396301\n",
      "epsilon:0.084002 episode_count: 16205. steps_count: 7163005.000000\n",
      "ep 2315: ep_len:500 episode reward: total was 1.920000. running mean: -14.233138\n",
      "ep 2315: ep_len:505 episode reward: total was -26.720000. running mean: -14.358006\n",
      "ep 2315: ep_len:515 episode reward: total was -15.070000. running mean: -14.365126\n",
      "ep 2315: ep_len:500 episode reward: total was -5.070000. running mean: -14.272175\n",
      "ep 2315: ep_len:3 episode reward: total was 0.000000. running mean: -14.129453\n",
      "ep 2315: ep_len:590 episode reward: total was -8.100000. running mean: -14.069159\n",
      "ep 2315: ep_len:610 episode reward: total was -9.260000. running mean: -14.021067\n",
      "epsilon:0.083866 episode_count: 16212. steps_count: 7166228.000000\n",
      "ep 2316: ep_len:525 episode reward: total was -21.440000. running mean: -14.095256\n",
      "ep 2316: ep_len:500 episode reward: total was -48.710000. running mean: -14.441404\n",
      "ep 2316: ep_len:77 episode reward: total was -3.970000. running mean: -14.336690\n",
      "ep 2316: ep_len:545 episode reward: total was -12.390000. running mean: -14.317223\n",
      "ep 2316: ep_len:120 episode reward: total was 1.510000. running mean: -14.158951\n",
      "ep 2316: ep_len:186 episode reward: total was 0.080000. running mean: -14.016561\n",
      "ep 2316: ep_len:195 episode reward: total was -6.350000. running mean: -13.939896\n",
      "epsilon:0.083729 episode_count: 16219. steps_count: 7168376.000000\n",
      "ep 2317: ep_len:525 episode reward: total was -16.730000. running mean: -13.967797\n",
      "ep 2317: ep_len:352 episode reward: total was -9.330000. running mean: -13.921419\n",
      "ep 2317: ep_len:675 episode reward: total was -35.990000. running mean: -14.142104\n",
      "ep 2317: ep_len:620 episode reward: total was -3.010000. running mean: -14.030783\n",
      "ep 2317: ep_len:127 episode reward: total was 0.050000. running mean: -13.889976\n",
      "ep 2317: ep_len:246 episode reward: total was -26.920000. running mean: -14.020276\n",
      "ep 2317: ep_len:176 episode reward: total was 2.150000. running mean: -13.858573\n",
      "epsilon:0.083593 episode_count: 16226. steps_count: 7171097.000000\n",
      "ep 2318: ep_len:585 episode reward: total was -2.180000. running mean: -13.741787\n",
      "ep 2318: ep_len:500 episode reward: total was -27.460000. running mean: -13.878969\n",
      "ep 2318: ep_len:505 episode reward: total was -24.940000. running mean: -13.989580\n",
      "ep 2318: ep_len:600 episode reward: total was -10.030000. running mean: -13.949984\n",
      "ep 2318: ep_len:43 episode reward: total was 2.500000. running mean: -13.785484\n",
      "ep 2318: ep_len:294 episode reward: total was -4.370000. running mean: -13.691329\n",
      "ep 2318: ep_len:570 episode reward: total was -19.070000. running mean: -13.745116\n",
      "epsilon:0.083456 episode_count: 16233. steps_count: 7174194.000000\n",
      "ep 2319: ep_len:595 episode reward: total was -37.370000. running mean: -13.981365\n",
      "ep 2319: ep_len:500 episode reward: total was -12.950000. running mean: -13.971051\n",
      "ep 2319: ep_len:565 episode reward: total was -16.310000. running mean: -13.994441\n",
      "ep 2319: ep_len:500 episode reward: total was -23.630000. running mean: -14.090796\n",
      "ep 2319: ep_len:3 episode reward: total was 0.000000. running mean: -13.949888\n",
      "ep 2319: ep_len:555 episode reward: total was -17.620000. running mean: -13.986589\n",
      "ep 2319: ep_len:570 episode reward: total was -22.500000. running mean: -14.071724\n",
      "epsilon:0.083320 episode_count: 16240. steps_count: 7177482.000000\n",
      "ep 2320: ep_len:250 episode reward: total was -2.920000. running mean: -13.960206\n",
      "ep 2320: ep_len:685 episode reward: total was -55.000000. running mean: -14.370604\n",
      "ep 2320: ep_len:565 episode reward: total was -8.010000. running mean: -14.306998\n",
      "ep 2320: ep_len:520 episode reward: total was -12.560000. running mean: -14.289528\n",
      "ep 2320: ep_len:3 episode reward: total was 0.000000. running mean: -14.146633\n",
      "ep 2320: ep_len:500 episode reward: total was -55.280000. running mean: -14.557967\n",
      "ep 2320: ep_len:520 episode reward: total was -33.100000. running mean: -14.743387\n",
      "epsilon:0.083183 episode_count: 16247. steps_count: 7180525.000000\n",
      "ep 2321: ep_len:520 episode reward: total was -20.960000. running mean: -14.805553\n",
      "ep 2321: ep_len:595 episode reward: total was -32.680000. running mean: -14.984298\n",
      "ep 2321: ep_len:427 episode reward: total was -39.350000. running mean: -15.227955\n",
      "ep 2321: ep_len:375 episode reward: total was -9.620000. running mean: -15.171875\n",
      "ep 2321: ep_len:3 episode reward: total was 0.000000. running mean: -15.020156\n",
      "ep 2321: ep_len:540 episode reward: total was -22.590000. running mean: -15.095855\n",
      "ep 2321: ep_len:600 episode reward: total was -13.870000. running mean: -15.083596\n",
      "epsilon:0.083047 episode_count: 16254. steps_count: 7183585.000000\n",
      "ep 2322: ep_len:530 episode reward: total was -7.180000. running mean: -15.004560\n",
      "ep 2322: ep_len:570 episode reward: total was -18.760000. running mean: -15.042115\n",
      "ep 2322: ep_len:555 episode reward: total was 0.930000. running mean: -14.882393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2322: ep_len:505 episode reward: total was 5.890000. running mean: -14.674670\n",
      "ep 2322: ep_len:3 episode reward: total was 0.000000. running mean: -14.527923\n",
      "ep 2322: ep_len:500 episode reward: total was -13.010000. running mean: -14.512744\n",
      "ep 2322: ep_len:585 episode reward: total was -4.810000. running mean: -14.415716\n",
      "epsilon:0.082910 episode_count: 16261. steps_count: 7186833.000000\n",
      "ep 2323: ep_len:645 episode reward: total was -9.970000. running mean: -14.371259\n",
      "ep 2323: ep_len:575 episode reward: total was -10.370000. running mean: -14.331246\n",
      "ep 2323: ep_len:650 episode reward: total was -10.370000. running mean: -14.291634\n",
      "ep 2323: ep_len:112 episode reward: total was -0.430000. running mean: -14.153018\n",
      "ep 2323: ep_len:47 episode reward: total was 0.000000. running mean: -14.011487\n",
      "ep 2323: ep_len:500 episode reward: total was -23.590000. running mean: -14.107273\n",
      "ep 2323: ep_len:580 episode reward: total was -11.070000. running mean: -14.076900\n",
      "epsilon:0.082774 episode_count: 16268. steps_count: 7189942.000000\n",
      "ep 2324: ep_len:790 episode reward: total was -43.810000. running mean: -14.374231\n",
      "ep 2324: ep_len:500 episode reward: total was -6.850000. running mean: -14.298989\n",
      "ep 2324: ep_len:515 episode reward: total was -3.250000. running mean: -14.188499\n",
      "ep 2324: ep_len:500 episode reward: total was -3.680000. running mean: -14.083414\n",
      "ep 2324: ep_len:3 episode reward: total was 0.000000. running mean: -13.942579\n",
      "ep 2324: ep_len:500 episode reward: total was -9.100000. running mean: -13.894154\n",
      "ep 2324: ep_len:500 episode reward: total was -12.580000. running mean: -13.881012\n",
      "epsilon:0.082637 episode_count: 16275. steps_count: 7193250.000000\n",
      "ep 2325: ep_len:565 episode reward: total was -2.570000. running mean: -13.767902\n",
      "ep 2325: ep_len:590 episode reward: total was 13.290000. running mean: -13.497323\n",
      "ep 2325: ep_len:600 episode reward: total was -5.930000. running mean: -13.421650\n",
      "ep 2325: ep_len:500 episode reward: total was -11.040000. running mean: -13.397833\n",
      "ep 2325: ep_len:3 episode reward: total was 0.000000. running mean: -13.263855\n",
      "ep 2325: ep_len:640 episode reward: total was -35.920000. running mean: -13.490416\n",
      "ep 2325: ep_len:600 episode reward: total was -34.560000. running mean: -13.701112\n",
      "epsilon:0.082501 episode_count: 16282. steps_count: 7196748.000000\n",
      "ep 2326: ep_len:645 episode reward: total was -60.520000. running mean: -14.169301\n",
      "ep 2326: ep_len:500 episode reward: total was -10.610000. running mean: -14.133708\n",
      "ep 2326: ep_len:580 episode reward: total was -1.010000. running mean: -14.002471\n",
      "ep 2326: ep_len:132 episode reward: total was 2.610000. running mean: -13.836346\n",
      "ep 2326: ep_len:3 episode reward: total was 0.000000. running mean: -13.697983\n",
      "ep 2326: ep_len:540 episode reward: total was -49.270000. running mean: -14.053703\n",
      "ep 2326: ep_len:600 episode reward: total was -5.050000. running mean: -13.963666\n",
      "epsilon:0.082364 episode_count: 16289. steps_count: 7199748.000000\n",
      "ep 2327: ep_len:99 episode reward: total was -0.950000. running mean: -13.833529\n",
      "ep 2327: ep_len:600 episode reward: total was -46.610000. running mean: -14.161294\n",
      "ep 2327: ep_len:575 episode reward: total was -8.430000. running mean: -14.103981\n",
      "ep 2327: ep_len:515 episode reward: total was 5.880000. running mean: -13.904141\n",
      "ep 2327: ep_len:3 episode reward: total was 0.000000. running mean: -13.765100\n",
      "ep 2327: ep_len:685 episode reward: total was -18.780000. running mean: -13.815249\n",
      "ep 2327: ep_len:258 episode reward: total was -19.850000. running mean: -13.875596\n",
      "epsilon:0.082228 episode_count: 16296. steps_count: 7202483.000000\n",
      "ep 2328: ep_len:635 episode reward: total was -51.680000. running mean: -14.253640\n",
      "ep 2328: ep_len:570 episode reward: total was -7.480000. running mean: -14.185904\n",
      "ep 2328: ep_len:660 episode reward: total was -12.170000. running mean: -14.165745\n",
      "ep 2328: ep_len:590 episode reward: total was -8.030000. running mean: -14.104388\n",
      "ep 2328: ep_len:89 episode reward: total was 2.540000. running mean: -13.937944\n",
      "ep 2328: ep_len:690 episode reward: total was -71.460000. running mean: -14.513164\n",
      "ep 2328: ep_len:550 episode reward: total was -20.190000. running mean: -14.569933\n",
      "epsilon:0.082091 episode_count: 16303. steps_count: 7206267.000000\n",
      "ep 2329: ep_len:595 episode reward: total was -37.800000. running mean: -14.802233\n",
      "ep 2329: ep_len:545 episode reward: total was 0.830000. running mean: -14.645911\n",
      "ep 2329: ep_len:500 episode reward: total was -0.770000. running mean: -14.507152\n",
      "ep 2329: ep_len:431 episode reward: total was -12.570000. running mean: -14.487780\n",
      "ep 2329: ep_len:3 episode reward: total was 0.000000. running mean: -14.342902\n",
      "ep 2329: ep_len:500 episode reward: total was -21.810000. running mean: -14.417573\n",
      "ep 2329: ep_len:565 episode reward: total was -59.260000. running mean: -14.865998\n",
      "epsilon:0.081955 episode_count: 16310. steps_count: 7209406.000000\n",
      "ep 2330: ep_len:565 episode reward: total was -26.780000. running mean: -14.985138\n",
      "ep 2330: ep_len:595 episode reward: total was -5.170000. running mean: -14.886986\n",
      "ep 2330: ep_len:575 episode reward: total was -9.760000. running mean: -14.835717\n",
      "ep 2330: ep_len:156 episode reward: total was 3.100000. running mean: -14.656359\n",
      "ep 2330: ep_len:129 episode reward: total was 8.070000. running mean: -14.429096\n",
      "ep 2330: ep_len:510 episode reward: total was -16.910000. running mean: -14.453905\n",
      "ep 2330: ep_len:625 episode reward: total was -12.290000. running mean: -14.432266\n",
      "epsilon:0.081818 episode_count: 16317. steps_count: 7212561.000000\n",
      "ep 2331: ep_len:550 episode reward: total was -9.870000. running mean: -14.386643\n",
      "ep 2331: ep_len:560 episode reward: total was 14.340000. running mean: -14.099377\n",
      "ep 2331: ep_len:500 episode reward: total was -25.320000. running mean: -14.211583\n",
      "ep 2331: ep_len:41 episode reward: total was 0.030000. running mean: -14.069167\n",
      "ep 2331: ep_len:53 episode reward: total was 0.500000. running mean: -13.923475\n",
      "ep 2331: ep_len:565 episode reward: total was -15.820000. running mean: -13.942441\n",
      "ep 2331: ep_len:610 episode reward: total was -26.570000. running mean: -14.068716\n",
      "epsilon:0.081682 episode_count: 16324. steps_count: 7215440.000000\n",
      "ep 2332: ep_len:625 episode reward: total was 5.980000. running mean: -13.868229\n",
      "ep 2332: ep_len:500 episode reward: total was -7.320000. running mean: -13.802747\n",
      "ep 2332: ep_len:500 episode reward: total was -33.840000. running mean: -14.003119\n",
      "ep 2332: ep_len:520 episode reward: total was -4.160000. running mean: -13.904688\n",
      "ep 2332: ep_len:3 episode reward: total was 0.000000. running mean: -13.765641\n",
      "ep 2332: ep_len:260 episode reward: total was -0.890000. running mean: -13.636885\n",
      "ep 2332: ep_len:520 episode reward: total was -17.900000. running mean: -13.679516\n",
      "epsilon:0.081545 episode_count: 16331. steps_count: 7218368.000000\n",
      "ep 2333: ep_len:660 episode reward: total was -21.190000. running mean: -13.754621\n",
      "ep 2333: ep_len:580 episode reward: total was -5.430000. running mean: -13.671375\n",
      "ep 2333: ep_len:550 episode reward: total was -43.500000. running mean: -13.969661\n",
      "ep 2333: ep_len:620 episode reward: total was 5.390000. running mean: -13.776064\n",
      "ep 2333: ep_len:1 episode reward: total was 0.000000. running mean: -13.638304\n",
      "ep 2333: ep_len:650 episode reward: total was -16.250000. running mean: -13.664421\n",
      "ep 2333: ep_len:309 episode reward: total was -7.330000. running mean: -13.601076\n",
      "epsilon:0.081409 episode_count: 16338. steps_count: 7221738.000000\n",
      "ep 2334: ep_len:580 episode reward: total was -3.130000. running mean: -13.496366\n",
      "ep 2334: ep_len:500 episode reward: total was -2.310000. running mean: -13.384502\n",
      "ep 2334: ep_len:560 episode reward: total was -16.830000. running mean: -13.418957\n",
      "ep 2334: ep_len:56 episode reward: total was 2.570000. running mean: -13.259067\n",
      "ep 2334: ep_len:3 episode reward: total was 0.000000. running mean: -13.126477\n",
      "ep 2334: ep_len:500 episode reward: total was -12.980000. running mean: -13.125012\n",
      "ep 2334: ep_len:505 episode reward: total was -13.040000. running mean: -13.124162\n",
      "epsilon:0.081272 episode_count: 16345. steps_count: 7224442.000000\n",
      "ep 2335: ep_len:510 episode reward: total was -28.010000. running mean: -13.273020\n",
      "ep 2335: ep_len:640 episode reward: total was -21.090000. running mean: -13.351190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2335: ep_len:630 episode reward: total was -12.620000. running mean: -13.343878\n",
      "ep 2335: ep_len:555 episode reward: total was -11.390000. running mean: -13.324339\n",
      "ep 2335: ep_len:120 episode reward: total was 3.040000. running mean: -13.160696\n",
      "ep 2335: ep_len:560 episode reward: total was -3.450000. running mean: -13.063589\n",
      "ep 2335: ep_len:620 episode reward: total was -26.130000. running mean: -13.194253\n",
      "epsilon:0.081136 episode_count: 16352. steps_count: 7228077.000000\n",
      "ep 2336: ep_len:500 episode reward: total was -27.810000. running mean: -13.340411\n",
      "ep 2336: ep_len:555 episode reward: total was -62.220000. running mean: -13.829206\n",
      "ep 2336: ep_len:590 episode reward: total was -56.980000. running mean: -14.260714\n",
      "ep 2336: ep_len:500 episode reward: total was 2.340000. running mean: -14.094707\n",
      "ep 2336: ep_len:3 episode reward: total was 0.000000. running mean: -13.953760\n",
      "ep 2336: ep_len:615 episode reward: total was -2.500000. running mean: -13.839223\n",
      "ep 2336: ep_len:500 episode reward: total was -10.020000. running mean: -13.801030\n",
      "epsilon:0.080999 episode_count: 16359. steps_count: 7231340.000000\n",
      "ep 2337: ep_len:680 episode reward: total was -14.690000. running mean: -13.809920\n",
      "ep 2337: ep_len:500 episode reward: total was -23.720000. running mean: -13.909021\n",
      "ep 2337: ep_len:615 episode reward: total was -42.210000. running mean: -14.192031\n",
      "ep 2337: ep_len:505 episode reward: total was -16.050000. running mean: -14.210610\n",
      "ep 2337: ep_len:110 episode reward: total was -6.440000. running mean: -14.132904\n",
      "ep 2337: ep_len:685 episode reward: total was -16.810000. running mean: -14.159675\n",
      "ep 2337: ep_len:640 episode reward: total was -18.860000. running mean: -14.206678\n",
      "epsilon:0.080863 episode_count: 16366. steps_count: 7235075.000000\n",
      "ep 2338: ep_len:545 episode reward: total was -3.450000. running mean: -14.099112\n",
      "ep 2338: ep_len:625 episode reward: total was -3.530000. running mean: -13.993421\n",
      "ep 2338: ep_len:565 episode reward: total was -4.450000. running mean: -13.897986\n",
      "ep 2338: ep_len:395 episode reward: total was -15.620000. running mean: -13.915206\n",
      "ep 2338: ep_len:39 episode reward: total was 0.500000. running mean: -13.771054\n",
      "ep 2338: ep_len:520 episode reward: total was -21.310000. running mean: -13.846444\n",
      "ep 2338: ep_len:500 episode reward: total was -11.750000. running mean: -13.825479\n",
      "epsilon:0.080726 episode_count: 16373. steps_count: 7238264.000000\n",
      "ep 2339: ep_len:500 episode reward: total was -13.740000. running mean: -13.824625\n",
      "ep 2339: ep_len:500 episode reward: total was -0.340000. running mean: -13.689778\n",
      "ep 2339: ep_len:367 episode reward: total was -2.320000. running mean: -13.576081\n",
      "ep 2339: ep_len:595 episode reward: total was -21.050000. running mean: -13.650820\n",
      "ep 2339: ep_len:3 episode reward: total was 0.000000. running mean: -13.514312\n",
      "ep 2339: ep_len:500 episode reward: total was -18.300000. running mean: -13.562168\n",
      "ep 2339: ep_len:500 episode reward: total was -8.640000. running mean: -13.512947\n",
      "epsilon:0.080590 episode_count: 16380. steps_count: 7241229.000000\n",
      "ep 2340: ep_len:635 episode reward: total was -32.700000. running mean: -13.704817\n",
      "ep 2340: ep_len:585 episode reward: total was 10.320000. running mean: -13.464569\n",
      "ep 2340: ep_len:535 episode reward: total was -31.110000. running mean: -13.641023\n",
      "ep 2340: ep_len:398 episode reward: total was -13.130000. running mean: -13.635913\n",
      "ep 2340: ep_len:115 episode reward: total was 4.570000. running mean: -13.453854\n",
      "ep 2340: ep_len:650 episode reward: total was -33.850000. running mean: -13.657816\n",
      "ep 2340: ep_len:318 episode reward: total was -22.830000. running mean: -13.749537\n",
      "epsilon:0.080453 episode_count: 16387. steps_count: 7244465.000000\n",
      "ep 2341: ep_len:251 episode reward: total was -1.370000. running mean: -13.625742\n",
      "ep 2341: ep_len:520 episode reward: total was -7.290000. running mean: -13.562385\n",
      "ep 2341: ep_len:685 episode reward: total was -2.590000. running mean: -13.452661\n",
      "ep 2341: ep_len:515 episode reward: total was -21.570000. running mean: -13.533834\n",
      "ep 2341: ep_len:86 episode reward: total was -11.950000. running mean: -13.517996\n",
      "ep 2341: ep_len:615 episode reward: total was -11.470000. running mean: -13.497516\n",
      "ep 2341: ep_len:505 episode reward: total was -26.060000. running mean: -13.623141\n",
      "epsilon:0.080317 episode_count: 16394. steps_count: 7247642.000000\n",
      "ep 2342: ep_len:500 episode reward: total was -8.640000. running mean: -13.573309\n",
      "ep 2342: ep_len:500 episode reward: total was -0.860000. running mean: -13.446176\n",
      "ep 2342: ep_len:560 episode reward: total was -4.400000. running mean: -13.355714\n",
      "ep 2342: ep_len:500 episode reward: total was -2.380000. running mean: -13.245957\n",
      "ep 2342: ep_len:3 episode reward: total was 0.000000. running mean: -13.113498\n",
      "ep 2342: ep_len:585 episode reward: total was -18.690000. running mean: -13.169263\n",
      "ep 2342: ep_len:343 episode reward: total was -12.280000. running mean: -13.160370\n",
      "epsilon:0.080180 episode_count: 16401. steps_count: 7250633.000000\n",
      "ep 2343: ep_len:219 episode reward: total was 2.110000. running mean: -13.007666\n",
      "ep 2343: ep_len:274 episode reward: total was -20.300000. running mean: -13.080590\n",
      "ep 2343: ep_len:570 episode reward: total was -11.960000. running mean: -13.069384\n",
      "ep 2343: ep_len:580 episode reward: total was -10.970000. running mean: -13.048390\n",
      "ep 2343: ep_len:93 episode reward: total was 4.540000. running mean: -12.872506\n",
      "ep 2343: ep_len:665 episode reward: total was -27.170000. running mean: -13.015481\n",
      "ep 2343: ep_len:570 episode reward: total was -17.990000. running mean: -13.065226\n",
      "epsilon:0.080044 episode_count: 16408. steps_count: 7253604.000000\n",
      "ep 2344: ep_len:565 episode reward: total was -4.650000. running mean: -12.981074\n",
      "ep 2344: ep_len:500 episode reward: total was -3.370000. running mean: -12.884963\n",
      "ep 2344: ep_len:500 episode reward: total was -3.460000. running mean: -12.790714\n",
      "ep 2344: ep_len:500 episode reward: total was -20.620000. running mean: -12.869006\n",
      "ep 2344: ep_len:3 episode reward: total was 0.000000. running mean: -12.740316\n",
      "ep 2344: ep_len:169 episode reward: total was 5.090000. running mean: -12.562013\n",
      "ep 2344: ep_len:316 episode reward: total was -6.850000. running mean: -12.504893\n",
      "epsilon:0.079907 episode_count: 16415. steps_count: 7256157.000000\n",
      "ep 2345: ep_len:680 episode reward: total was -27.300000. running mean: -12.652844\n",
      "ep 2345: ep_len:500 episode reward: total was -2.870000. running mean: -12.555016\n",
      "ep 2345: ep_len:615 episode reward: total was -30.120000. running mean: -12.730666\n",
      "ep 2345: ep_len:610 episode reward: total was -12.480000. running mean: -12.728159\n",
      "ep 2345: ep_len:3 episode reward: total was 0.000000. running mean: -12.600877\n",
      "ep 2345: ep_len:690 episode reward: total was -14.240000. running mean: -12.617269\n",
      "ep 2345: ep_len:560 episode reward: total was -20.440000. running mean: -12.695496\n",
      "epsilon:0.079771 episode_count: 16422. steps_count: 7259815.000000\n",
      "ep 2346: ep_len:545 episode reward: total was -6.620000. running mean: -12.634741\n",
      "ep 2346: ep_len:580 episode reward: total was -15.540000. running mean: -12.663793\n",
      "ep 2346: ep_len:610 episode reward: total was -8.580000. running mean: -12.622956\n",
      "ep 2346: ep_len:53 episode reward: total was 1.550000. running mean: -12.481226\n",
      "ep 2346: ep_len:32 episode reward: total was 1.500000. running mean: -12.341414\n",
      "ep 2346: ep_len:620 episode reward: total was -23.720000. running mean: -12.455200\n",
      "ep 2346: ep_len:500 episode reward: total was -24.490000. running mean: -12.575548\n",
      "epsilon:0.079634 episode_count: 16429. steps_count: 7262755.000000\n",
      "ep 2347: ep_len:570 episode reward: total was -23.250000. running mean: -12.682292\n",
      "ep 2347: ep_len:347 episode reward: total was -41.310000. running mean: -12.968569\n",
      "ep 2347: ep_len:413 episode reward: total was 2.220000. running mean: -12.816684\n",
      "ep 2347: ep_len:505 episode reward: total was -9.490000. running mean: -12.783417\n",
      "ep 2347: ep_len:3 episode reward: total was 0.000000. running mean: -12.655583\n",
      "ep 2347: ep_len:500 episode reward: total was -19.280000. running mean: -12.721827\n",
      "ep 2347: ep_len:190 episode reward: total was -11.410000. running mean: -12.708708\n",
      "epsilon:0.079498 episode_count: 16436. steps_count: 7265283.000000\n",
      "ep 2348: ep_len:735 episode reward: total was -126.820000. running mean: -13.849821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2348: ep_len:500 episode reward: total was 7.660000. running mean: -13.634723\n",
      "ep 2348: ep_len:500 episode reward: total was -9.800000. running mean: -13.596376\n",
      "ep 2348: ep_len:590 episode reward: total was -3.530000. running mean: -13.495712\n",
      "ep 2348: ep_len:3 episode reward: total was 0.000000. running mean: -13.360755\n",
      "ep 2348: ep_len:575 episode reward: total was -12.740000. running mean: -13.354547\n",
      "ep 2348: ep_len:575 episode reward: total was -17.150000. running mean: -13.392502\n",
      "epsilon:0.079361 episode_count: 16443. steps_count: 7268761.000000\n",
      "ep 2349: ep_len:550 episode reward: total was -4.110000. running mean: -13.299677\n",
      "ep 2349: ep_len:500 episode reward: total was -3.220000. running mean: -13.198880\n",
      "ep 2349: ep_len:419 episode reward: total was -1.280000. running mean: -13.079691\n",
      "ep 2349: ep_len:580 episode reward: total was 0.840000. running mean: -12.940494\n",
      "ep 2349: ep_len:48 episode reward: total was 4.500000. running mean: -12.766090\n",
      "ep 2349: ep_len:520 episode reward: total was -7.730000. running mean: -12.715729\n",
      "ep 2349: ep_len:187 episode reward: total was -22.440000. running mean: -12.812971\n",
      "epsilon:0.079225 episode_count: 16450. steps_count: 7271565.000000\n",
      "ep 2350: ep_len:620 episode reward: total was -7.430000. running mean: -12.759142\n",
      "ep 2350: ep_len:530 episode reward: total was 9.870000. running mean: -12.532850\n",
      "ep 2350: ep_len:695 episode reward: total was -30.300000. running mean: -12.710522\n",
      "ep 2350: ep_len:595 episode reward: total was -58.210000. running mean: -13.165517\n",
      "ep 2350: ep_len:3 episode reward: total was 0.000000. running mean: -13.033861\n",
      "ep 2350: ep_len:685 episode reward: total was -24.260000. running mean: -13.146123\n",
      "ep 2350: ep_len:625 episode reward: total was -10.820000. running mean: -13.122861\n",
      "epsilon:0.079088 episode_count: 16457. steps_count: 7275318.000000\n",
      "ep 2351: ep_len:590 episode reward: total was -19.450000. running mean: -13.186133\n",
      "ep 2351: ep_len:530 episode reward: total was -28.280000. running mean: -13.337072\n",
      "ep 2351: ep_len:685 episode reward: total was -28.880000. running mean: -13.492501\n",
      "ep 2351: ep_len:505 episode reward: total was -28.740000. running mean: -13.644976\n",
      "ep 2351: ep_len:105 episode reward: total was 0.040000. running mean: -13.508126\n",
      "ep 2351: ep_len:291 episode reward: total was -3.390000. running mean: -13.406945\n",
      "ep 2351: ep_len:585 episode reward: total was -17.630000. running mean: -13.449175\n",
      "epsilon:0.078952 episode_count: 16464. steps_count: 7278609.000000\n",
      "ep 2352: ep_len:198 episode reward: total was -6.960000. running mean: -13.384284\n",
      "ep 2352: ep_len:520 episode reward: total was -2.490000. running mean: -13.275341\n",
      "ep 2352: ep_len:515 episode reward: total was -33.970000. running mean: -13.482287\n",
      "ep 2352: ep_len:530 episode reward: total was 6.330000. running mean: -13.284164\n",
      "ep 2352: ep_len:3 episode reward: total was 0.000000. running mean: -13.151323\n",
      "ep 2352: ep_len:510 episode reward: total was -9.460000. running mean: -13.114410\n",
      "ep 2352: ep_len:690 episode reward: total was -54.350000. running mean: -13.526766\n",
      "epsilon:0.078815 episode_count: 16471. steps_count: 7281575.000000\n",
      "ep 2353: ep_len:510 episode reward: total was 7.930000. running mean: -13.312198\n",
      "ep 2353: ep_len:500 episode reward: total was -43.650000. running mean: -13.615576\n",
      "ep 2353: ep_len:500 episode reward: total was 4.480000. running mean: -13.434620\n",
      "ep 2353: ep_len:510 episode reward: total was -27.090000. running mean: -13.571174\n",
      "ep 2353: ep_len:3 episode reward: total was 0.000000. running mean: -13.435462\n",
      "ep 2353: ep_len:630 episode reward: total was -9.530000. running mean: -13.396408\n",
      "ep 2353: ep_len:560 episode reward: total was -27.610000. running mean: -13.538543\n",
      "epsilon:0.078679 episode_count: 16478. steps_count: 7284788.000000\n",
      "ep 2354: ep_len:545 episode reward: total was -17.010000. running mean: -13.573258\n",
      "ep 2354: ep_len:540 episode reward: total was -5.090000. running mean: -13.488425\n",
      "ep 2354: ep_len:570 episode reward: total was 0.960000. running mean: -13.343941\n",
      "ep 2354: ep_len:530 episode reward: total was -14.960000. running mean: -13.360102\n",
      "ep 2354: ep_len:3 episode reward: total was 0.000000. running mean: -13.226501\n",
      "ep 2354: ep_len:690 episode reward: total was -41.910000. running mean: -13.513336\n",
      "ep 2354: ep_len:520 episode reward: total was -17.980000. running mean: -13.558002\n",
      "epsilon:0.078542 episode_count: 16485. steps_count: 7288186.000000\n",
      "ep 2355: ep_len:595 episode reward: total was 0.600000. running mean: -13.416422\n",
      "ep 2355: ep_len:540 episode reward: total was -10.440000. running mean: -13.386658\n",
      "ep 2355: ep_len:565 episode reward: total was -16.130000. running mean: -13.414092\n",
      "ep 2355: ep_len:500 episode reward: total was -18.520000. running mean: -13.465151\n",
      "ep 2355: ep_len:3 episode reward: total was 0.000000. running mean: -13.330499\n",
      "ep 2355: ep_len:555 episode reward: total was -28.010000. running mean: -13.477294\n",
      "ep 2355: ep_len:183 episode reward: total was -5.840000. running mean: -13.400921\n",
      "epsilon:0.078406 episode_count: 16492. steps_count: 7291127.000000\n",
      "ep 2356: ep_len:500 episode reward: total was -9.140000. running mean: -13.358312\n",
      "ep 2356: ep_len:367 episode reward: total was -35.810000. running mean: -13.582829\n",
      "ep 2356: ep_len:575 episode reward: total was -17.060000. running mean: -13.617601\n",
      "ep 2356: ep_len:560 episode reward: total was -29.190000. running mean: -13.773325\n",
      "ep 2356: ep_len:95 episode reward: total was 4.060000. running mean: -13.594991\n",
      "ep 2356: ep_len:640 episode reward: total was -15.540000. running mean: -13.614441\n",
      "ep 2356: ep_len:520 episode reward: total was -12.430000. running mean: -13.602597\n",
      "epsilon:0.078269 episode_count: 16499. steps_count: 7294384.000000\n",
      "ep 2357: ep_len:535 episode reward: total was -1.090000. running mean: -13.477471\n",
      "ep 2357: ep_len:640 episode reward: total was -26.050000. running mean: -13.603196\n",
      "ep 2357: ep_len:500 episode reward: total was -10.670000. running mean: -13.573864\n",
      "ep 2357: ep_len:168 episode reward: total was 4.120000. running mean: -13.396926\n",
      "ep 2357: ep_len:3 episode reward: total was 0.000000. running mean: -13.262956\n",
      "ep 2357: ep_len:580 episode reward: total was -18.980000. running mean: -13.320127\n",
      "ep 2357: ep_len:206 episode reward: total was -9.420000. running mean: -13.281126\n",
      "epsilon:0.078133 episode_count: 16506. steps_count: 7297016.000000\n",
      "ep 2358: ep_len:1000 episode reward: total was -148.350000. running mean: -14.631814\n",
      "ep 2358: ep_len:605 episode reward: total was -13.120000. running mean: -14.616696\n",
      "ep 2358: ep_len:645 episode reward: total was -30.080000. running mean: -14.771329\n",
      "ep 2358: ep_len:500 episode reward: total was -13.550000. running mean: -14.759116\n",
      "ep 2358: ep_len:3 episode reward: total was 0.000000. running mean: -14.611525\n",
      "ep 2358: ep_len:510 episode reward: total was -6.160000. running mean: -14.527010\n",
      "ep 2358: ep_len:530 episode reward: total was -27.950000. running mean: -14.661239\n",
      "epsilon:0.077996 episode_count: 16513. steps_count: 7300809.000000\n",
      "ep 2359: ep_len:535 episode reward: total was -21.420000. running mean: -14.728827\n",
      "ep 2359: ep_len:580 episode reward: total was -67.790000. running mean: -15.259439\n",
      "ep 2359: ep_len:700 episode reward: total was -25.330000. running mean: -15.360144\n",
      "ep 2359: ep_len:500 episode reward: total was -1.620000. running mean: -15.222743\n",
      "ep 2359: ep_len:3 episode reward: total was 0.000000. running mean: -15.070516\n",
      "ep 2359: ep_len:500 episode reward: total was -41.540000. running mean: -15.335210\n",
      "ep 2359: ep_len:525 episode reward: total was -9.810000. running mean: -15.279958\n",
      "epsilon:0.077860 episode_count: 16520. steps_count: 7304152.000000\n",
      "ep 2360: ep_len:515 episode reward: total was -23.970000. running mean: -15.366859\n",
      "ep 2360: ep_len:640 episode reward: total was -28.190000. running mean: -15.495090\n",
      "ep 2360: ep_len:441 episode reward: total was -8.270000. running mean: -15.422839\n",
      "ep 2360: ep_len:420 episode reward: total was -15.110000. running mean: -15.419711\n",
      "ep 2360: ep_len:3 episode reward: total was 0.000000. running mean: -15.265514\n",
      "ep 2360: ep_len:600 episode reward: total was -9.690000. running mean: -15.209759\n",
      "ep 2360: ep_len:585 episode reward: total was -34.170000. running mean: -15.399361\n",
      "epsilon:0.077723 episode_count: 16527. steps_count: 7307356.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2361: ep_len:505 episode reward: total was -15.750000. running mean: -15.402867\n",
      "ep 2361: ep_len:189 episode reward: total was -5.910000. running mean: -15.307939\n",
      "ep 2361: ep_len:560 episode reward: total was -2.120000. running mean: -15.176059\n",
      "ep 2361: ep_len:505 episode reward: total was -17.580000. running mean: -15.200099\n",
      "ep 2361: ep_len:47 episode reward: total was 0.000000. running mean: -15.048098\n",
      "ep 2361: ep_len:550 episode reward: total was -31.370000. running mean: -15.211317\n",
      "ep 2361: ep_len:620 episode reward: total was -28.070000. running mean: -15.339904\n",
      "epsilon:0.077587 episode_count: 16534. steps_count: 7310332.000000\n",
      "ep 2362: ep_len:213 episode reward: total was -14.950000. running mean: -15.336005\n",
      "ep 2362: ep_len:198 episode reward: total was -5.920000. running mean: -15.241845\n",
      "ep 2362: ep_len:575 episode reward: total was -21.830000. running mean: -15.307726\n",
      "ep 2362: ep_len:500 episode reward: total was -5.210000. running mean: -15.206749\n",
      "ep 2362: ep_len:95 episode reward: total was -5.970000. running mean: -15.114381\n",
      "ep 2362: ep_len:515 episode reward: total was -40.160000. running mean: -15.364838\n",
      "ep 2362: ep_len:545 episode reward: total was -15.370000. running mean: -15.364889\n",
      "epsilon:0.077450 episode_count: 16541. steps_count: 7312973.000000\n",
      "ep 2363: ep_len:680 episode reward: total was -12.180000. running mean: -15.333040\n",
      "ep 2363: ep_len:180 episode reward: total was -8.400000. running mean: -15.263710\n",
      "ep 2363: ep_len:1110 episode reward: total was -145.280000. running mean: -16.563873\n",
      "ep 2363: ep_len:550 episode reward: total was -3.080000. running mean: -16.429034\n",
      "ep 2363: ep_len:124 episode reward: total was 0.560000. running mean: -16.259144\n",
      "ep 2363: ep_len:535 episode reward: total was -1.880000. running mean: -16.115352\n",
      "ep 2363: ep_len:500 episode reward: total was -13.380000. running mean: -16.087999\n",
      "epsilon:0.077314 episode_count: 16548. steps_count: 7316652.000000\n",
      "ep 2364: ep_len:620 episode reward: total was -9.210000. running mean: -16.019219\n",
      "ep 2364: ep_len:500 episode reward: total was -23.160000. running mean: -16.090627\n",
      "ep 2364: ep_len:560 episode reward: total was -14.760000. running mean: -16.077320\n",
      "ep 2364: ep_len:595 episode reward: total was -18.020000. running mean: -16.096747\n",
      "ep 2364: ep_len:2 episode reward: total was 0.000000. running mean: -15.935780\n",
      "ep 2364: ep_len:535 episode reward: total was 6.390000. running mean: -15.712522\n",
      "ep 2364: ep_len:330 episode reward: total was -20.380000. running mean: -15.759197\n",
      "epsilon:0.077177 episode_count: 16555. steps_count: 7319794.000000\n",
      "ep 2365: ep_len:640 episode reward: total was -27.320000. running mean: -15.874805\n",
      "ep 2365: ep_len:615 episode reward: total was 4.500000. running mean: -15.671057\n",
      "ep 2365: ep_len:545 episode reward: total was -30.430000. running mean: -15.818646\n",
      "ep 2365: ep_len:112 episode reward: total was 0.060000. running mean: -15.659860\n",
      "ep 2365: ep_len:29 episode reward: total was 1.500000. running mean: -15.488261\n",
      "ep 2365: ep_len:590 episode reward: total was -86.320000. running mean: -16.196578\n",
      "ep 2365: ep_len:525 episode reward: total was -12.080000. running mean: -16.155413\n",
      "epsilon:0.077041 episode_count: 16562. steps_count: 7322850.000000\n",
      "ep 2366: ep_len:555 episode reward: total was -0.170000. running mean: -15.995558\n",
      "ep 2366: ep_len:520 episode reward: total was 8.180000. running mean: -15.753803\n",
      "ep 2366: ep_len:680 episode reward: total was -10.730000. running mean: -15.703565\n",
      "ep 2366: ep_len:505 episode reward: total was -1.190000. running mean: -15.558429\n",
      "ep 2366: ep_len:56 episode reward: total was -6.960000. running mean: -15.472445\n",
      "ep 2366: ep_len:580 episode reward: total was 7.930000. running mean: -15.238420\n",
      "ep 2366: ep_len:630 episode reward: total was -46.910000. running mean: -15.555136\n",
      "epsilon:0.076904 episode_count: 16569. steps_count: 7326376.000000\n",
      "ep 2367: ep_len:525 episode reward: total was -6.980000. running mean: -15.469385\n",
      "ep 2367: ep_len:500 episode reward: total was -24.610000. running mean: -15.560791\n",
      "ep 2367: ep_len:500 episode reward: total was 1.070000. running mean: -15.394483\n",
      "ep 2367: ep_len:590 episode reward: total was -3.600000. running mean: -15.276538\n",
      "ep 2367: ep_len:3 episode reward: total was 0.000000. running mean: -15.123773\n",
      "ep 2367: ep_len:710 episode reward: total was -78.040000. running mean: -15.752935\n",
      "ep 2367: ep_len:535 episode reward: total was -21.360000. running mean: -15.809006\n",
      "epsilon:0.076768 episode_count: 16576. steps_count: 7329739.000000\n",
      "ep 2368: ep_len:645 episode reward: total was -2.640000. running mean: -15.677316\n",
      "ep 2368: ep_len:610 episode reward: total was 8.550000. running mean: -15.435043\n",
      "ep 2368: ep_len:640 episode reward: total was -41.350000. running mean: -15.694192\n",
      "ep 2368: ep_len:525 episode reward: total was -0.990000. running mean: -15.547150\n",
      "ep 2368: ep_len:3 episode reward: total was 0.000000. running mean: -15.391679\n",
      "ep 2368: ep_len:655 episode reward: total was -2.190000. running mean: -15.259662\n",
      "ep 2368: ep_len:715 episode reward: total was -32.740000. running mean: -15.434465\n",
      "epsilon:0.076631 episode_count: 16583. steps_count: 7333532.000000\n",
      "ep 2369: ep_len:117 episode reward: total was -1.460000. running mean: -15.294721\n",
      "ep 2369: ep_len:500 episode reward: total was 8.290000. running mean: -15.058873\n",
      "ep 2369: ep_len:505 episode reward: total was -9.130000. running mean: -14.999585\n",
      "ep 2369: ep_len:408 episode reward: total was -1.650000. running mean: -14.866089\n",
      "ep 2369: ep_len:3 episode reward: total was 0.000000. running mean: -14.717428\n",
      "ep 2369: ep_len:535 episode reward: total was -18.820000. running mean: -14.758454\n",
      "ep 2369: ep_len:525 episode reward: total was -8.280000. running mean: -14.693669\n",
      "epsilon:0.076495 episode_count: 16590. steps_count: 7336125.000000\n",
      "ep 2370: ep_len:265 episode reward: total was -1.370000. running mean: -14.560433\n",
      "ep 2370: ep_len:525 episode reward: total was -40.250000. running mean: -14.817328\n",
      "ep 2370: ep_len:500 episode reward: total was -7.150000. running mean: -14.740655\n",
      "ep 2370: ep_len:25 episode reward: total was 0.020000. running mean: -14.593048\n",
      "ep 2370: ep_len:3 episode reward: total was 0.000000. running mean: -14.447118\n",
      "ep 2370: ep_len:540 episode reward: total was -28.790000. running mean: -14.590547\n",
      "ep 2370: ep_len:510 episode reward: total was -34.120000. running mean: -14.785841\n",
      "epsilon:0.076358 episode_count: 16597. steps_count: 7338493.000000\n",
      "ep 2371: ep_len:222 episode reward: total was -7.920000. running mean: -14.717183\n",
      "ep 2371: ep_len:1025 episode reward: total was -105.530000. running mean: -15.625311\n",
      "ep 2371: ep_len:570 episode reward: total was -15.750000. running mean: -15.626558\n",
      "ep 2371: ep_len:500 episode reward: total was -14.500000. running mean: -15.615292\n",
      "ep 2371: ep_len:3 episode reward: total was 0.000000. running mean: -15.459139\n",
      "ep 2371: ep_len:317 episode reward: total was -22.850000. running mean: -15.533048\n",
      "ep 2371: ep_len:500 episode reward: total was -13.800000. running mean: -15.515717\n",
      "epsilon:0.076222 episode_count: 16604. steps_count: 7341630.000000\n",
      "ep 2372: ep_len:500 episode reward: total was -3.110000. running mean: -15.391660\n",
      "ep 2372: ep_len:270 episode reward: total was -15.340000. running mean: -15.391144\n",
      "ep 2372: ep_len:625 episode reward: total was -11.740000. running mean: -15.354632\n",
      "ep 2372: ep_len:515 episode reward: total was -11.990000. running mean: -15.320986\n",
      "ep 2372: ep_len:3 episode reward: total was 0.000000. running mean: -15.167776\n",
      "ep 2372: ep_len:505 episode reward: total was -26.490000. running mean: -15.280998\n",
      "ep 2372: ep_len:505 episode reward: total was -19.980000. running mean: -15.327988\n",
      "epsilon:0.076085 episode_count: 16611. steps_count: 7344553.000000\n",
      "ep 2373: ep_len:500 episode reward: total was -20.950000. running mean: -15.384208\n",
      "ep 2373: ep_len:575 episode reward: total was 19.810000. running mean: -15.032266\n",
      "ep 2373: ep_len:530 episode reward: total was -3.100000. running mean: -14.912944\n",
      "ep 2373: ep_len:500 episode reward: total was -1.260000. running mean: -14.776414\n",
      "ep 2373: ep_len:3 episode reward: total was 0.000000. running mean: -14.628650\n",
      "ep 2373: ep_len:555 episode reward: total was -12.360000. running mean: -14.605964\n",
      "ep 2373: ep_len:500 episode reward: total was -15.790000. running mean: -14.617804\n",
      "epsilon:0.075949 episode_count: 16618. steps_count: 7347716.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2374: ep_len:520 episode reward: total was 9.930000. running mean: -14.372326\n",
      "ep 2374: ep_len:570 episode reward: total was -34.730000. running mean: -14.575903\n",
      "ep 2374: ep_len:565 episode reward: total was -22.830000. running mean: -14.658444\n",
      "ep 2374: ep_len:505 episode reward: total was -10.080000. running mean: -14.612659\n",
      "ep 2374: ep_len:3 episode reward: total was 0.000000. running mean: -14.466533\n",
      "ep 2374: ep_len:555 episode reward: total was -2.560000. running mean: -14.347467\n",
      "ep 2374: ep_len:615 episode reward: total was -41.000000. running mean: -14.613993\n",
      "epsilon:0.075812 episode_count: 16625. steps_count: 7351049.000000\n",
      "ep 2375: ep_len:535 episode reward: total was -5.100000. running mean: -14.518853\n",
      "ep 2375: ep_len:525 episode reward: total was -34.470000. running mean: -14.718364\n",
      "ep 2375: ep_len:387 episode reward: total was 0.670000. running mean: -14.564481\n",
      "ep 2375: ep_len:415 episode reward: total was -7.170000. running mean: -14.490536\n",
      "ep 2375: ep_len:3 episode reward: total was 0.000000. running mean: -14.345630\n",
      "ep 2375: ep_len:665 episode reward: total was -45.830000. running mean: -14.660474\n",
      "ep 2375: ep_len:311 episode reward: total was -15.900000. running mean: -14.672869\n",
      "epsilon:0.075676 episode_count: 16632. steps_count: 7353890.000000\n",
      "ep 2376: ep_len:680 episode reward: total was -17.690000. running mean: -14.703041\n",
      "ep 2376: ep_len:890 episode reward: total was -74.330000. running mean: -15.299310\n",
      "ep 2376: ep_len:590 episode reward: total was -8.500000. running mean: -15.231317\n",
      "ep 2376: ep_len:570 episode reward: total was -2.980000. running mean: -15.108804\n",
      "ep 2376: ep_len:88 episode reward: total was 2.540000. running mean: -14.932316\n",
      "ep 2376: ep_len:685 episode reward: total was -10.210000. running mean: -14.885093\n",
      "ep 2376: ep_len:310 episode reward: total was -14.910000. running mean: -14.885342\n",
      "epsilon:0.075539 episode_count: 16639. steps_count: 7357703.000000\n",
      "ep 2377: ep_len:625 episode reward: total was -22.300000. running mean: -14.959488\n",
      "ep 2377: ep_len:500 episode reward: total was 12.620000. running mean: -14.683694\n",
      "ep 2377: ep_len:630 episode reward: total was -9.040000. running mean: -14.627257\n",
      "ep 2377: ep_len:56 episode reward: total was 0.550000. running mean: -14.475484\n",
      "ep 2377: ep_len:96 episode reward: total was 6.540000. running mean: -14.265329\n",
      "ep 2377: ep_len:515 episode reward: total was -23.150000. running mean: -14.354176\n",
      "ep 2377: ep_len:630 episode reward: total was -96.320000. running mean: -15.173834\n",
      "epsilon:0.075403 episode_count: 16646. steps_count: 7360755.000000\n",
      "ep 2378: ep_len:193 episode reward: total was -4.400000. running mean: -15.066096\n",
      "ep 2378: ep_len:500 episode reward: total was -14.480000. running mean: -15.060235\n",
      "ep 2378: ep_len:413 episode reward: total was -6.800000. running mean: -14.977633\n",
      "ep 2378: ep_len:550 episode reward: total was -16.470000. running mean: -14.992556\n",
      "ep 2378: ep_len:3 episode reward: total was 0.000000. running mean: -14.842631\n",
      "ep 2378: ep_len:685 episode reward: total was -16.780000. running mean: -14.862004\n",
      "ep 2378: ep_len:570 episode reward: total was -16.490000. running mean: -14.878284\n",
      "epsilon:0.075266 episode_count: 16653. steps_count: 7363669.000000\n",
      "ep 2379: ep_len:500 episode reward: total was -27.110000. running mean: -15.000601\n",
      "ep 2379: ep_len:500 episode reward: total was -5.540000. running mean: -14.905995\n",
      "ep 2379: ep_len:69 episode reward: total was 0.540000. running mean: -14.751535\n",
      "ep 2379: ep_len:535 episode reward: total was -16.210000. running mean: -14.766120\n",
      "ep 2379: ep_len:3 episode reward: total was 0.000000. running mean: -14.618459\n",
      "ep 2379: ep_len:550 episode reward: total was -46.320000. running mean: -14.935474\n",
      "ep 2379: ep_len:191 episode reward: total was -5.900000. running mean: -14.845120\n",
      "epsilon:0.075130 episode_count: 16660. steps_count: 7366017.000000\n",
      "ep 2380: ep_len:128 episode reward: total was -15.950000. running mean: -14.856168\n",
      "ep 2380: ep_len:615 episode reward: total was -16.410000. running mean: -14.871707\n",
      "ep 2380: ep_len:530 episode reward: total was -2.610000. running mean: -14.749090\n",
      "ep 2380: ep_len:565 episode reward: total was 2.040000. running mean: -14.581199\n",
      "ep 2380: ep_len:3 episode reward: total was 0.000000. running mean: -14.435387\n",
      "ep 2380: ep_len:520 episode reward: total was -34.850000. running mean: -14.639533\n",
      "ep 2380: ep_len:193 episode reward: total was -8.900000. running mean: -14.582138\n",
      "epsilon:0.074993 episode_count: 16667. steps_count: 7368571.000000\n",
      "ep 2381: ep_len:635 episode reward: total was -13.020000. running mean: -14.566516\n",
      "ep 2381: ep_len:500 episode reward: total was -20.240000. running mean: -14.623251\n",
      "ep 2381: ep_len:560 episode reward: total was -7.140000. running mean: -14.548418\n",
      "ep 2381: ep_len:515 episode reward: total was -16.190000. running mean: -14.564834\n",
      "ep 2381: ep_len:101 episode reward: total was -0.950000. running mean: -14.428686\n",
      "ep 2381: ep_len:525 episode reward: total was -42.050000. running mean: -14.704899\n",
      "ep 2381: ep_len:525 episode reward: total was -42.230000. running mean: -14.980150\n",
      "epsilon:0.074857 episode_count: 16674. steps_count: 7371932.000000\n",
      "ep 2382: ep_len:252 episode reward: total was -3.870000. running mean: -14.869049\n",
      "ep 2382: ep_len:665 episode reward: total was -44.230000. running mean: -15.162658\n",
      "ep 2382: ep_len:575 episode reward: total was -8.700000. running mean: -15.098032\n",
      "ep 2382: ep_len:570 episode reward: total was -28.590000. running mean: -15.232951\n",
      "ep 2382: ep_len:91 episode reward: total was 4.540000. running mean: -15.035222\n",
      "ep 2382: ep_len:710 episode reward: total was -22.770000. running mean: -15.112570\n",
      "ep 2382: ep_len:245 episode reward: total was -5.380000. running mean: -15.015244\n",
      "epsilon:0.074720 episode_count: 16681. steps_count: 7375040.000000\n",
      "ep 2383: ep_len:500 episode reward: total was 4.250000. running mean: -14.822591\n",
      "ep 2383: ep_len:505 episode reward: total was -4.180000. running mean: -14.716165\n",
      "ep 2383: ep_len:640 episode reward: total was -5.710000. running mean: -14.626104\n",
      "ep 2383: ep_len:409 episode reward: total was -6.720000. running mean: -14.547043\n",
      "ep 2383: ep_len:3 episode reward: total was 0.000000. running mean: -14.401572\n",
      "ep 2383: ep_len:570 episode reward: total was -10.240000. running mean: -14.359957\n",
      "ep 2383: ep_len:605 episode reward: total was -24.700000. running mean: -14.463357\n",
      "epsilon:0.074584 episode_count: 16688. steps_count: 7378272.000000\n",
      "ep 2384: ep_len:191 episode reward: total was -1.860000. running mean: -14.337323\n",
      "ep 2384: ep_len:500 episode reward: total was 2.150000. running mean: -14.172450\n",
      "ep 2384: ep_len:615 episode reward: total was -4.230000. running mean: -14.073026\n",
      "ep 2384: ep_len:620 episode reward: total was -8.930000. running mean: -14.021595\n",
      "ep 2384: ep_len:125 episode reward: total was 4.060000. running mean: -13.840780\n",
      "ep 2384: ep_len:500 episode reward: total was -5.300000. running mean: -13.755372\n",
      "ep 2384: ep_len:335 episode reward: total was -21.870000. running mean: -13.836518\n",
      "epsilon:0.074447 episode_count: 16695. steps_count: 7381158.000000\n",
      "ep 2385: ep_len:545 episode reward: total was -21.400000. running mean: -13.912153\n",
      "ep 2385: ep_len:500 episode reward: total was -8.200000. running mean: -13.855031\n",
      "ep 2385: ep_len:79 episode reward: total was 1.050000. running mean: -13.705981\n",
      "ep 2385: ep_len:140 episode reward: total was 3.600000. running mean: -13.532921\n",
      "ep 2385: ep_len:3 episode reward: total was 0.000000. running mean: -13.397592\n",
      "ep 2385: ep_len:585 episode reward: total was -18.220000. running mean: -13.445816\n",
      "ep 2385: ep_len:194 episode reward: total was -8.930000. running mean: -13.400658\n",
      "epsilon:0.074311 episode_count: 16702. steps_count: 7383204.000000\n",
      "ep 2386: ep_len:216 episode reward: total was -14.830000. running mean: -13.414951\n",
      "ep 2386: ep_len:281 episode reward: total was -14.850000. running mean: -13.429302\n",
      "ep 2386: ep_len:555 episode reward: total was -4.090000. running mean: -13.335909\n",
      "ep 2386: ep_len:49 episode reward: total was 1.540000. running mean: -13.187150\n",
      "ep 2386: ep_len:96 episode reward: total was 4.030000. running mean: -13.014978\n",
      "ep 2386: ep_len:500 episode reward: total was -5.750000. running mean: -12.942328\n",
      "ep 2386: ep_len:300 episode reward: total was -7.840000. running mean: -12.891305\n",
      "epsilon:0.074174 episode_count: 16709. steps_count: 7385201.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2387: ep_len:500 episode reward: total was 5.690000. running mean: -12.705492\n",
      "ep 2387: ep_len:300 episode reward: total was -9.370000. running mean: -12.672137\n",
      "ep 2387: ep_len:605 episode reward: total was -7.470000. running mean: -12.620116\n",
      "ep 2387: ep_len:560 episode reward: total was -16.970000. running mean: -12.663615\n",
      "ep 2387: ep_len:3 episode reward: total was 0.000000. running mean: -12.536978\n",
      "ep 2387: ep_len:630 episode reward: total was -32.750000. running mean: -12.739109\n",
      "ep 2387: ep_len:302 episode reward: total was -42.360000. running mean: -13.035318\n",
      "epsilon:0.074038 episode_count: 16716. steps_count: 7388101.000000\n",
      "ep 2388: ep_len:560 episode reward: total was -15.180000. running mean: -13.056764\n",
      "ep 2388: ep_len:500 episode reward: total was -23.660000. running mean: -13.162797\n",
      "ep 2388: ep_len:545 episode reward: total was -2.210000. running mean: -13.053269\n",
      "ep 2388: ep_len:500 episode reward: total was -32.680000. running mean: -13.249536\n",
      "ep 2388: ep_len:3 episode reward: total was 0.000000. running mean: -13.117041\n",
      "ep 2388: ep_len:695 episode reward: total was -5.140000. running mean: -13.037270\n",
      "ep 2388: ep_len:725 episode reward: total was -81.510000. running mean: -13.721998\n",
      "epsilon:0.073901 episode_count: 16723. steps_count: 7391629.000000\n",
      "ep 2389: ep_len:665 episode reward: total was -8.170000. running mean: -13.666478\n",
      "ep 2389: ep_len:605 episode reward: total was 13.330000. running mean: -13.396513\n",
      "ep 2389: ep_len:339 episode reward: total was 4.240000. running mean: -13.220148\n",
      "ep 2389: ep_len:500 episode reward: total was -3.080000. running mean: -13.118746\n",
      "ep 2389: ep_len:96 episode reward: total was 3.020000. running mean: -12.957359\n",
      "ep 2389: ep_len:575 episode reward: total was -13.140000. running mean: -12.959185\n",
      "ep 2389: ep_len:340 episode reward: total was -21.310000. running mean: -13.042693\n",
      "epsilon:0.073765 episode_count: 16730. steps_count: 7394749.000000\n",
      "ep 2390: ep_len:585 episode reward: total was -14.680000. running mean: -13.059066\n",
      "ep 2390: ep_len:560 episode reward: total was 1.820000. running mean: -12.910276\n",
      "ep 2390: ep_len:580 episode reward: total was -4.590000. running mean: -12.827073\n",
      "ep 2390: ep_len:515 episode reward: total was 3.880000. running mean: -12.660002\n",
      "ep 2390: ep_len:3 episode reward: total was 0.000000. running mean: -12.533402\n",
      "ep 2390: ep_len:500 episode reward: total was -10.780000. running mean: -12.515868\n",
      "ep 2390: ep_len:570 episode reward: total was -28.200000. running mean: -12.672710\n",
      "epsilon:0.073628 episode_count: 16737. steps_count: 7398062.000000\n",
      "ep 2391: ep_len:660 episode reward: total was -13.200000. running mean: -12.677982\n",
      "ep 2391: ep_len:630 episode reward: total was -9.620000. running mean: -12.647403\n",
      "ep 2391: ep_len:500 episode reward: total was -1.060000. running mean: -12.531529\n",
      "ep 2391: ep_len:545 episode reward: total was -28.120000. running mean: -12.687413\n",
      "ep 2391: ep_len:3 episode reward: total was 0.000000. running mean: -12.560539\n",
      "ep 2391: ep_len:560 episode reward: total was -6.740000. running mean: -12.502334\n",
      "ep 2391: ep_len:540 episode reward: total was -16.330000. running mean: -12.540610\n",
      "epsilon:0.073492 episode_count: 16744. steps_count: 7401500.000000\n",
      "ep 2392: ep_len:500 episode reward: total was -16.280000. running mean: -12.578004\n",
      "ep 2392: ep_len:500 episode reward: total was -2.370000. running mean: -12.475924\n",
      "ep 2392: ep_len:570 episode reward: total was -51.040000. running mean: -12.861565\n",
      "ep 2392: ep_len:500 episode reward: total was -17.620000. running mean: -12.909149\n",
      "ep 2392: ep_len:1 episode reward: total was 0.000000. running mean: -12.780058\n",
      "ep 2392: ep_len:735 episode reward: total was -65.480000. running mean: -13.307057\n",
      "ep 2392: ep_len:500 episode reward: total was -29.240000. running mean: -13.466387\n",
      "epsilon:0.073355 episode_count: 16751. steps_count: 7404806.000000\n",
      "ep 2393: ep_len:625 episode reward: total was 4.680000. running mean: -13.284923\n",
      "ep 2393: ep_len:500 episode reward: total was -8.040000. running mean: -13.232474\n",
      "ep 2393: ep_len:715 episode reward: total was -16.750000. running mean: -13.267649\n",
      "ep 2393: ep_len:510 episode reward: total was -23.110000. running mean: -13.366072\n",
      "ep 2393: ep_len:3 episode reward: total was 0.000000. running mean: -13.232412\n",
      "ep 2393: ep_len:610 episode reward: total was -33.040000. running mean: -13.430488\n",
      "ep 2393: ep_len:555 episode reward: total was -29.590000. running mean: -13.592083\n",
      "epsilon:0.073219 episode_count: 16758. steps_count: 7408324.000000\n",
      "ep 2394: ep_len:505 episode reward: total was -20.910000. running mean: -13.665262\n",
      "ep 2394: ep_len:550 episode reward: total was -19.490000. running mean: -13.723509\n",
      "ep 2394: ep_len:535 episode reward: total was -46.520000. running mean: -14.051474\n",
      "ep 2394: ep_len:500 episode reward: total was -12.730000. running mean: -14.038259\n",
      "ep 2394: ep_len:3 episode reward: total was 0.000000. running mean: -13.897877\n",
      "ep 2394: ep_len:785 episode reward: total was -96.880000. running mean: -14.727698\n",
      "ep 2394: ep_len:740 episode reward: total was -73.900000. running mean: -15.319421\n",
      "epsilon:0.073082 episode_count: 16765. steps_count: 7411942.000000\n",
      "ep 2395: ep_len:640 episode reward: total was -23.310000. running mean: -15.399327\n",
      "ep 2395: ep_len:605 episode reward: total was 8.810000. running mean: -15.157234\n",
      "ep 2395: ep_len:540 episode reward: total was -33.420000. running mean: -15.339861\n",
      "ep 2395: ep_len:550 episode reward: total was -43.890000. running mean: -15.625363\n",
      "ep 2395: ep_len:41 episode reward: total was -2.000000. running mean: -15.489109\n",
      "ep 2395: ep_len:500 episode reward: total was -10.930000. running mean: -15.443518\n",
      "ep 2395: ep_len:570 episode reward: total was -67.720000. running mean: -15.966283\n",
      "epsilon:0.072946 episode_count: 16772. steps_count: 7415388.000000\n",
      "ep 2396: ep_len:595 episode reward: total was -17.170000. running mean: -15.978320\n",
      "ep 2396: ep_len:545 episode reward: total was -10.840000. running mean: -15.926937\n",
      "ep 2396: ep_len:545 episode reward: total was -3.120000. running mean: -15.798867\n",
      "ep 2396: ep_len:500 episode reward: total was 11.380000. running mean: -15.527079\n",
      "ep 2396: ep_len:3 episode reward: total was 0.000000. running mean: -15.371808\n",
      "ep 2396: ep_len:520 episode reward: total was 0.910000. running mean: -15.208990\n",
      "ep 2396: ep_len:184 episode reward: total was -6.930000. running mean: -15.126200\n",
      "epsilon:0.072809 episode_count: 16779. steps_count: 7418280.000000\n",
      "ep 2397: ep_len:600 episode reward: total was -10.680000. running mean: -15.081738\n",
      "ep 2397: ep_len:610 episode reward: total was -9.590000. running mean: -15.026821\n",
      "ep 2397: ep_len:407 episode reward: total was 1.230000. running mean: -14.864252\n",
      "ep 2397: ep_len:530 episode reward: total was 6.890000. running mean: -14.646710\n",
      "ep 2397: ep_len:3 episode reward: total was 0.000000. running mean: -14.500243\n",
      "ep 2397: ep_len:500 episode reward: total was -1.760000. running mean: -14.372840\n",
      "ep 2397: ep_len:555 episode reward: total was -36.600000. running mean: -14.595112\n",
      "epsilon:0.072673 episode_count: 16786. steps_count: 7421485.000000\n",
      "ep 2398: ep_len:630 episode reward: total was -101.810000. running mean: -15.467261\n",
      "ep 2398: ep_len:500 episode reward: total was -12.890000. running mean: -15.441488\n",
      "ep 2398: ep_len:590 episode reward: total was -14.290000. running mean: -15.429973\n",
      "ep 2398: ep_len:515 episode reward: total was 1.370000. running mean: -15.261974\n",
      "ep 2398: ep_len:109 episode reward: total was 7.540000. running mean: -15.033954\n",
      "ep 2398: ep_len:605 episode reward: total was -17.640000. running mean: -15.060014\n",
      "ep 2398: ep_len:545 episode reward: total was -12.090000. running mean: -15.030314\n",
      "epsilon:0.072536 episode_count: 16793. steps_count: 7424979.000000\n",
      "ep 2399: ep_len:600 episode reward: total was -6.600000. running mean: -14.946011\n",
      "ep 2399: ep_len:525 episode reward: total was -16.060000. running mean: -14.957151\n",
      "ep 2399: ep_len:710 episode reward: total was -47.850000. running mean: -15.286079\n",
      "ep 2399: ep_len:730 episode reward: total was -60.680000. running mean: -15.740019\n",
      "ep 2399: ep_len:3 episode reward: total was 0.000000. running mean: -15.582618\n",
      "ep 2399: ep_len:595 episode reward: total was -21.210000. running mean: -15.638892\n",
      "ep 2399: ep_len:500 episode reward: total was -31.070000. running mean: -15.793203\n",
      "epsilon:0.072400 episode_count: 16800. steps_count: 7428642.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2400: ep_len:545 episode reward: total was -9.510000. running mean: -15.730371\n",
      "ep 2400: ep_len:296 episode reward: total was -3.290000. running mean: -15.605968\n",
      "ep 2400: ep_len:530 episode reward: total was -6.010000. running mean: -15.510008\n",
      "ep 2400: ep_len:565 episode reward: total was -42.940000. running mean: -15.784308\n",
      "ep 2400: ep_len:3 episode reward: total was 0.000000. running mean: -15.626465\n",
      "ep 2400: ep_len:635 episode reward: total was -19.410000. running mean: -15.664300\n",
      "ep 2400: ep_len:286 episode reward: total was -4.320000. running mean: -15.550857\n",
      "epsilon:0.072263 episode_count: 16807. steps_count: 7431502.000000\n",
      "ep 2401: ep_len:560 episode reward: total was -33.340000. running mean: -15.728749\n",
      "ep 2401: ep_len:530 episode reward: total was -15.370000. running mean: -15.725161\n",
      "ep 2401: ep_len:565 episode reward: total was -1.770000. running mean: -15.585609\n",
      "ep 2401: ep_len:615 episode reward: total was 10.460000. running mean: -15.325153\n",
      "ep 2401: ep_len:88 episode reward: total was -11.950000. running mean: -15.291402\n",
      "ep 2401: ep_len:515 episode reward: total was -57.100000. running mean: -15.709488\n",
      "ep 2401: ep_len:565 episode reward: total was -40.650000. running mean: -15.958893\n",
      "epsilon:0.072127 episode_count: 16814. steps_count: 7434940.000000\n",
      "ep 2402: ep_len:615 episode reward: total was -1.080000. running mean: -15.810104\n",
      "ep 2402: ep_len:500 episode reward: total was -16.640000. running mean: -15.818403\n",
      "ep 2402: ep_len:620 episode reward: total was -18.270000. running mean: -15.842919\n",
      "ep 2402: ep_len:500 episode reward: total was -2.150000. running mean: -15.705990\n",
      "ep 2402: ep_len:3 episode reward: total was 0.000000. running mean: -15.548930\n",
      "ep 2402: ep_len:595 episode reward: total was -41.510000. running mean: -15.808541\n",
      "ep 2402: ep_len:590 episode reward: total was -14.570000. running mean: -15.796155\n",
      "epsilon:0.071990 episode_count: 16821. steps_count: 7438363.000000\n",
      "ep 2403: ep_len:605 episode reward: total was -26.620000. running mean: -15.904394\n",
      "ep 2403: ep_len:525 episode reward: total was 8.910000. running mean: -15.656250\n",
      "ep 2403: ep_len:560 episode reward: total was -42.170000. running mean: -15.921387\n",
      "ep 2403: ep_len:56 episode reward: total was 1.560000. running mean: -15.746573\n",
      "ep 2403: ep_len:83 episode reward: total was 0.540000. running mean: -15.583708\n",
      "ep 2403: ep_len:500 episode reward: total was -12.290000. running mean: -15.550770\n",
      "ep 2403: ep_len:615 episode reward: total was -31.540000. running mean: -15.710663\n",
      "epsilon:0.071854 episode_count: 16828. steps_count: 7441307.000000\n",
      "ep 2404: ep_len:505 episode reward: total was 1.900000. running mean: -15.534556\n",
      "ep 2404: ep_len:745 episode reward: total was -59.490000. running mean: -15.974111\n",
      "ep 2404: ep_len:595 episode reward: total was -51.130000. running mean: -16.325669\n",
      "ep 2404: ep_len:500 episode reward: total was -18.600000. running mean: -16.348413\n",
      "ep 2404: ep_len:125 episode reward: total was 7.550000. running mean: -16.109429\n",
      "ep 2404: ep_len:500 episode reward: total was -15.600000. running mean: -16.104334\n",
      "ep 2404: ep_len:299 episode reward: total was -6.840000. running mean: -16.011691\n",
      "epsilon:0.071717 episode_count: 16835. steps_count: 7444576.000000\n",
      "ep 2405: ep_len:125 episode reward: total was -4.450000. running mean: -15.896074\n",
      "ep 2405: ep_len:565 episode reward: total was -12.430000. running mean: -15.861413\n",
      "ep 2405: ep_len:525 episode reward: total was -19.880000. running mean: -15.901599\n",
      "ep 2405: ep_len:530 episode reward: total was -28.040000. running mean: -16.022983\n",
      "ep 2405: ep_len:62 episode reward: total was -1.980000. running mean: -15.882553\n",
      "ep 2405: ep_len:575 episode reward: total was -11.150000. running mean: -15.835228\n",
      "ep 2405: ep_len:236 episode reward: total was -6.360000. running mean: -15.740476\n",
      "epsilon:0.071581 episode_count: 16842. steps_count: 7447194.000000\n",
      "ep 2406: ep_len:535 episode reward: total was -5.690000. running mean: -15.639971\n",
      "ep 2406: ep_len:500 episode reward: total was -3.890000. running mean: -15.522471\n",
      "ep 2406: ep_len:451 episode reward: total was -5.770000. running mean: -15.424946\n",
      "ep 2406: ep_len:505 episode reward: total was 11.420000. running mean: -15.156497\n",
      "ep 2406: ep_len:3 episode reward: total was 0.000000. running mean: -15.004932\n",
      "ep 2406: ep_len:535 episode reward: total was -4.420000. running mean: -14.899083\n",
      "ep 2406: ep_len:178 episode reward: total was -5.860000. running mean: -14.808692\n",
      "epsilon:0.071444 episode_count: 16849. steps_count: 7449901.000000\n",
      "ep 2407: ep_len:595 episode reward: total was -5.740000. running mean: -14.718005\n",
      "ep 2407: ep_len:540 episode reward: total was -4.080000. running mean: -14.611625\n",
      "ep 2407: ep_len:555 episode reward: total was -11.540000. running mean: -14.580909\n",
      "ep 2407: ep_len:500 episode reward: total was -3.640000. running mean: -14.471500\n",
      "ep 2407: ep_len:3 episode reward: total was 0.000000. running mean: -14.326785\n",
      "ep 2407: ep_len:640 episode reward: total was -15.050000. running mean: -14.334017\n",
      "ep 2407: ep_len:585 episode reward: total was -14.580000. running mean: -14.336477\n",
      "epsilon:0.071308 episode_count: 16856. steps_count: 7453319.000000\n",
      "ep 2408: ep_len:560 episode reward: total was 1.920000. running mean: -14.173912\n",
      "ep 2408: ep_len:345 episode reward: total was -25.770000. running mean: -14.289873\n",
      "ep 2408: ep_len:775 episode reward: total was -43.740000. running mean: -14.584374\n",
      "ep 2408: ep_len:500 episode reward: total was -20.060000. running mean: -14.639130\n",
      "ep 2408: ep_len:3 episode reward: total was 0.000000. running mean: -14.492739\n",
      "ep 2408: ep_len:625 episode reward: total was -12.640000. running mean: -14.474211\n",
      "ep 2408: ep_len:545 episode reward: total was -26.480000. running mean: -14.594269\n",
      "epsilon:0.071171 episode_count: 16863. steps_count: 7456672.000000\n",
      "ep 2409: ep_len:565 episode reward: total was 4.440000. running mean: -14.403927\n",
      "ep 2409: ep_len:383 episode reward: total was -15.820000. running mean: -14.418087\n",
      "ep 2409: ep_len:72 episode reward: total was -1.470000. running mean: -14.288607\n",
      "ep 2409: ep_len:500 episode reward: total was -31.160000. running mean: -14.457320\n",
      "ep 2409: ep_len:3 episode reward: total was 0.000000. running mean: -14.312747\n",
      "ep 2409: ep_len:243 episode reward: total was 3.630000. running mean: -14.133320\n",
      "ep 2409: ep_len:605 episode reward: total was -8.090000. running mean: -14.072887\n",
      "epsilon:0.071035 episode_count: 16870. steps_count: 7459043.000000\n",
      "ep 2410: ep_len:635 episode reward: total was -19.930000. running mean: -14.131458\n",
      "ep 2410: ep_len:500 episode reward: total was -15.140000. running mean: -14.141543\n",
      "ep 2410: ep_len:500 episode reward: total was -11.360000. running mean: -14.113728\n",
      "ep 2410: ep_len:500 episode reward: total was 4.400000. running mean: -13.928590\n",
      "ep 2410: ep_len:3 episode reward: total was 0.000000. running mean: -13.789305\n",
      "ep 2410: ep_len:535 episode reward: total was -18.710000. running mean: -13.838511\n",
      "ep 2410: ep_len:600 episode reward: total was -64.830000. running mean: -14.348426\n",
      "epsilon:0.070898 episode_count: 16877. steps_count: 7462316.000000\n",
      "ep 2411: ep_len:205 episode reward: total was -1.890000. running mean: -14.223842\n",
      "ep 2411: ep_len:545 episode reward: total was -5.040000. running mean: -14.132004\n",
      "ep 2411: ep_len:423 episode reward: total was -0.290000. running mean: -13.993584\n",
      "ep 2411: ep_len:575 episode reward: total was -37.810000. running mean: -14.231748\n",
      "ep 2411: ep_len:3 episode reward: total was 0.000000. running mean: -14.089430\n",
      "ep 2411: ep_len:670 episode reward: total was -46.170000. running mean: -14.410236\n",
      "ep 2411: ep_len:640 episode reward: total was -23.830000. running mean: -14.504434\n",
      "epsilon:0.070762 episode_count: 16884. steps_count: 7465377.000000\n",
      "ep 2412: ep_len:500 episode reward: total was -15.230000. running mean: -14.511689\n",
      "ep 2412: ep_len:500 episode reward: total was -3.750000. running mean: -14.404072\n",
      "ep 2412: ep_len:520 episode reward: total was -9.300000. running mean: -14.353032\n",
      "ep 2412: ep_len:505 episode reward: total was -12.140000. running mean: -14.330901\n",
      "ep 2412: ep_len:49 episode reward: total was 4.500000. running mean: -14.142592\n",
      "ep 2412: ep_len:510 episode reward: total was -33.470000. running mean: -14.335866\n",
      "ep 2412: ep_len:710 episode reward: total was -71.370000. running mean: -14.906208\n",
      "epsilon:0.070625 episode_count: 16891. steps_count: 7468671.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2413: ep_len:580 episode reward: total was -4.120000. running mean: -14.798346\n",
      "ep 2413: ep_len:615 episode reward: total was -36.810000. running mean: -15.018462\n",
      "ep 2413: ep_len:500 episode reward: total was -12.230000. running mean: -14.990578\n",
      "ep 2413: ep_len:525 episode reward: total was -10.470000. running mean: -14.945372\n",
      "ep 2413: ep_len:3 episode reward: total was 0.000000. running mean: -14.795918\n",
      "ep 2413: ep_len:515 episode reward: total was -16.890000. running mean: -14.816859\n",
      "ep 2413: ep_len:500 episode reward: total was -5.070000. running mean: -14.719390\n",
      "epsilon:0.070489 episode_count: 16898. steps_count: 7471909.000000\n",
      "ep 2414: ep_len:515 episode reward: total was -5.960000. running mean: -14.631796\n",
      "ep 2414: ep_len:500 episode reward: total was -22.040000. running mean: -14.705879\n",
      "ep 2414: ep_len:500 episode reward: total was -12.460000. running mean: -14.683420\n",
      "ep 2414: ep_len:500 episode reward: total was -28.640000. running mean: -14.822986\n",
      "ep 2414: ep_len:3 episode reward: total was 0.000000. running mean: -14.674756\n",
      "ep 2414: ep_len:520 episode reward: total was -5.580000. running mean: -14.583808\n",
      "ep 2414: ep_len:505 episode reward: total was -24.510000. running mean: -14.683070\n",
      "epsilon:0.070352 episode_count: 16905. steps_count: 7474952.000000\n",
      "ep 2415: ep_len:620 episode reward: total was 5.160000. running mean: -14.484639\n",
      "ep 2415: ep_len:500 episode reward: total was -0.460000. running mean: -14.344393\n",
      "ep 2415: ep_len:510 episode reward: total was -12.090000. running mean: -14.321849\n",
      "ep 2415: ep_len:600 episode reward: total was 1.020000. running mean: -14.168431\n",
      "ep 2415: ep_len:3 episode reward: total was 0.000000. running mean: -14.026746\n",
      "ep 2415: ep_len:500 episode reward: total was -12.690000. running mean: -14.013379\n",
      "ep 2415: ep_len:320 episode reward: total was -4.780000. running mean: -13.921045\n",
      "epsilon:0.070216 episode_count: 16912. steps_count: 7478005.000000\n",
      "ep 2416: ep_len:500 episode reward: total was -3.510000. running mean: -13.816935\n",
      "ep 2416: ep_len:575 episode reward: total was -6.160000. running mean: -13.740365\n",
      "ep 2416: ep_len:385 episode reward: total was -24.320000. running mean: -13.846162\n",
      "ep 2416: ep_len:500 episode reward: total was -18.610000. running mean: -13.893800\n",
      "ep 2416: ep_len:3 episode reward: total was 0.000000. running mean: -13.754862\n",
      "ep 2416: ep_len:520 episode reward: total was 0.140000. running mean: -13.615913\n",
      "ep 2416: ep_len:282 episode reward: total was -8.340000. running mean: -13.563154\n",
      "epsilon:0.070079 episode_count: 16919. steps_count: 7480770.000000\n",
      "ep 2417: ep_len:595 episode reward: total was -3.000000. running mean: -13.457523\n",
      "ep 2417: ep_len:535 episode reward: total was -29.580000. running mean: -13.618747\n",
      "ep 2417: ep_len:640 episode reward: total was -14.910000. running mean: -13.631660\n",
      "ep 2417: ep_len:500 episode reward: total was -22.060000. running mean: -13.715943\n",
      "ep 2417: ep_len:3 episode reward: total was 0.000000. running mean: -13.578784\n",
      "ep 2417: ep_len:500 episode reward: total was -9.340000. running mean: -13.536396\n",
      "ep 2417: ep_len:625 episode reward: total was -26.860000. running mean: -13.669632\n",
      "epsilon:0.069943 episode_count: 16926. steps_count: 7484168.000000\n",
      "ep 2418: ep_len:525 episode reward: total was -21.920000. running mean: -13.752136\n",
      "ep 2418: ep_len:500 episode reward: total was -11.900000. running mean: -13.733614\n",
      "ep 2418: ep_len:575 episode reward: total was -9.680000. running mean: -13.693078\n",
      "ep 2418: ep_len:505 episode reward: total was -20.120000. running mean: -13.757347\n",
      "ep 2418: ep_len:3 episode reward: total was 0.000000. running mean: -13.619774\n",
      "ep 2418: ep_len:500 episode reward: total was -14.930000. running mean: -13.632876\n",
      "ep 2418: ep_len:575 episode reward: total was -16.400000. running mean: -13.660547\n",
      "epsilon:0.069806 episode_count: 16933. steps_count: 7487351.000000\n",
      "ep 2419: ep_len:740 episode reward: total was -48.160000. running mean: -14.005542\n",
      "ep 2419: ep_len:500 episode reward: total was 8.680000. running mean: -13.778687\n",
      "ep 2419: ep_len:500 episode reward: total was 9.540000. running mean: -13.545500\n",
      "ep 2419: ep_len:535 episode reward: total was -32.480000. running mean: -13.734845\n",
      "ep 2419: ep_len:3 episode reward: total was 0.000000. running mean: -13.597496\n",
      "ep 2419: ep_len:565 episode reward: total was -10.630000. running mean: -13.567821\n",
      "ep 2419: ep_len:500 episode reward: total was 4.430000. running mean: -13.387843\n",
      "epsilon:0.069670 episode_count: 16940. steps_count: 7490694.000000\n",
      "ep 2420: ep_len:545 episode reward: total was -18.740000. running mean: -13.441365\n",
      "ep 2420: ep_len:645 episode reward: total was 3.090000. running mean: -13.276051\n",
      "ep 2420: ep_len:575 episode reward: total was -8.670000. running mean: -13.229991\n",
      "ep 2420: ep_len:510 episode reward: total was -30.110000. running mean: -13.398791\n",
      "ep 2420: ep_len:3 episode reward: total was 0.000000. running mean: -13.264803\n",
      "ep 2420: ep_len:620 episode reward: total was -17.780000. running mean: -13.309955\n",
      "ep 2420: ep_len:500 episode reward: total was -22.960000. running mean: -13.406455\n",
      "epsilon:0.069533 episode_count: 16947. steps_count: 7494092.000000\n",
      "ep 2421: ep_len:1005 episode reward: total was -125.370000. running mean: -14.526091\n",
      "ep 2421: ep_len:590 episode reward: total was -7.370000. running mean: -14.454530\n",
      "ep 2421: ep_len:570 episode reward: total was -15.800000. running mean: -14.467984\n",
      "ep 2421: ep_len:505 episode reward: total was -19.600000. running mean: -14.519305\n",
      "ep 2421: ep_len:3 episode reward: total was 0.000000. running mean: -14.374111\n",
      "ep 2421: ep_len:645 episode reward: total was -38.670000. running mean: -14.617070\n",
      "ep 2421: ep_len:615 episode reward: total was -24.400000. running mean: -14.714900\n",
      "epsilon:0.069397 episode_count: 16954. steps_count: 7498025.000000\n",
      "ep 2422: ep_len:265 episode reward: total was -24.820000. running mean: -14.815951\n",
      "ep 2422: ep_len:500 episode reward: total was -33.320000. running mean: -15.000991\n",
      "ep 2422: ep_len:352 episode reward: total was -8.340000. running mean: -14.934381\n",
      "ep 2422: ep_len:500 episode reward: total was -14.090000. running mean: -14.925937\n",
      "ep 2422: ep_len:127 episode reward: total was -11.440000. running mean: -14.891078\n",
      "ep 2422: ep_len:500 episode reward: total was -21.080000. running mean: -14.952967\n",
      "ep 2422: ep_len:203 episode reward: total was -7.380000. running mean: -14.877238\n",
      "epsilon:0.069260 episode_count: 16961. steps_count: 7500472.000000\n",
      "ep 2423: ep_len:505 episode reward: total was -3.150000. running mean: -14.759965\n",
      "ep 2423: ep_len:625 episode reward: total was 8.530000. running mean: -14.527066\n",
      "ep 2423: ep_len:545 episode reward: total was -5.060000. running mean: -14.432395\n",
      "ep 2423: ep_len:406 episode reward: total was -11.170000. running mean: -14.399771\n",
      "ep 2423: ep_len:3 episode reward: total was 0.000000. running mean: -14.255773\n",
      "ep 2423: ep_len:505 episode reward: total was -35.030000. running mean: -14.463516\n",
      "ep 2423: ep_len:580 episode reward: total was -13.060000. running mean: -14.449480\n",
      "epsilon:0.069124 episode_count: 16968. steps_count: 7503641.000000\n",
      "ep 2424: ep_len:555 episode reward: total was -30.350000. running mean: -14.608486\n",
      "ep 2424: ep_len:535 episode reward: total was -12.900000. running mean: -14.591401\n",
      "ep 2424: ep_len:500 episode reward: total was -15.060000. running mean: -14.596087\n",
      "ep 2424: ep_len:126 episode reward: total was 0.600000. running mean: -14.444126\n",
      "ep 2424: ep_len:3 episode reward: total was 0.000000. running mean: -14.299685\n",
      "ep 2424: ep_len:610 episode reward: total was -22.620000. running mean: -14.382888\n",
      "ep 2424: ep_len:500 episode reward: total was -9.640000. running mean: -14.335459\n",
      "epsilon:0.068987 episode_count: 16975. steps_count: 7506470.000000\n",
      "ep 2425: ep_len:595 episode reward: total was 3.430000. running mean: -14.157804\n",
      "ep 2425: ep_len:200 episode reward: total was -9.370000. running mean: -14.109926\n",
      "ep 2425: ep_len:545 episode reward: total was -2.090000. running mean: -13.989727\n",
      "ep 2425: ep_len:510 episode reward: total was -8.060000. running mean: -13.930430\n",
      "ep 2425: ep_len:3 episode reward: total was 0.000000. running mean: -13.791125\n",
      "ep 2425: ep_len:300 episode reward: total was -1.840000. running mean: -13.671614\n",
      "ep 2425: ep_len:320 episode reward: total was -15.320000. running mean: -13.688098\n",
      "epsilon:0.068851 episode_count: 16982. steps_count: 7508943.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2426: ep_len:238 episode reward: total was 5.120000. running mean: -13.500017\n",
      "ep 2426: ep_len:635 episode reward: total was 9.060000. running mean: -13.274417\n",
      "ep 2426: ep_len:555 episode reward: total was -27.870000. running mean: -13.420373\n",
      "ep 2426: ep_len:500 episode reward: total was 8.930000. running mean: -13.196869\n",
      "ep 2426: ep_len:3 episode reward: total was 0.000000. running mean: -13.064900\n",
      "ep 2426: ep_len:620 episode reward: total was -23.000000. running mean: -13.164251\n",
      "ep 2426: ep_len:635 episode reward: total was -31.040000. running mean: -13.343009\n",
      "epsilon:0.068714 episode_count: 16989. steps_count: 7512129.000000\n",
      "ep 2427: ep_len:248 episode reward: total was -0.390000. running mean: -13.213479\n",
      "ep 2427: ep_len:500 episode reward: total was -37.570000. running mean: -13.457044\n",
      "ep 2427: ep_len:560 episode reward: total was -7.400000. running mean: -13.396473\n",
      "ep 2427: ep_len:500 episode reward: total was -15.540000. running mean: -13.417909\n",
      "ep 2427: ep_len:95 episode reward: total was 1.030000. running mean: -13.273430\n",
      "ep 2427: ep_len:550 episode reward: total was -12.960000. running mean: -13.270295\n",
      "ep 2427: ep_len:545 episode reward: total was -22.410000. running mean: -13.361692\n",
      "epsilon:0.068578 episode_count: 16996. steps_count: 7515127.000000\n",
      "ep 2428: ep_len:640 episode reward: total was -3.350000. running mean: -13.261575\n",
      "ep 2428: ep_len:650 episode reward: total was -34.120000. running mean: -13.470160\n",
      "ep 2428: ep_len:500 episode reward: total was -6.600000. running mean: -13.401458\n",
      "ep 2428: ep_len:575 episode reward: total was -14.130000. running mean: -13.408744\n",
      "ep 2428: ep_len:84 episode reward: total was -9.450000. running mean: -13.369156\n",
      "ep 2428: ep_len:500 episode reward: total was -19.000000. running mean: -13.425465\n",
      "ep 2428: ep_len:183 episode reward: total was -5.870000. running mean: -13.349910\n",
      "epsilon:0.068441 episode_count: 17003. steps_count: 7518259.000000\n",
      "ep 2429: ep_len:505 episode reward: total was -1.870000. running mean: -13.235111\n",
      "ep 2429: ep_len:525 episode reward: total was -10.500000. running mean: -13.207760\n",
      "ep 2429: ep_len:575 episode reward: total was -1.050000. running mean: -13.086182\n",
      "ep 2429: ep_len:56 episode reward: total was -2.480000. running mean: -12.980120\n",
      "ep 2429: ep_len:38 episode reward: total was 3.500000. running mean: -12.815319\n",
      "ep 2429: ep_len:545 episode reward: total was -20.940000. running mean: -12.896566\n",
      "ep 2429: ep_len:600 episode reward: total was -20.880000. running mean: -12.976400\n",
      "epsilon:0.068305 episode_count: 17010. steps_count: 7521103.000000\n",
      "ep 2430: ep_len:125 episode reward: total was -3.470000. running mean: -12.881336\n",
      "ep 2430: ep_len:545 episode reward: total was 1.360000. running mean: -12.738923\n",
      "ep 2430: ep_len:600 episode reward: total was -10.080000. running mean: -12.712334\n",
      "ep 2430: ep_len:520 episode reward: total was -30.120000. running mean: -12.886410\n",
      "ep 2430: ep_len:3 episode reward: total was 0.000000. running mean: -12.757546\n",
      "ep 2430: ep_len:530 episode reward: total was -27.200000. running mean: -12.901971\n",
      "ep 2430: ep_len:505 episode reward: total was -27.560000. running mean: -13.048551\n",
      "epsilon:0.068168 episode_count: 17017. steps_count: 7523931.000000\n",
      "ep 2431: ep_len:615 episode reward: total was -32.800000. running mean: -13.246065\n",
      "ep 2431: ep_len:625 episode reward: total was -9.640000. running mean: -13.210005\n",
      "ep 2431: ep_len:630 episode reward: total was -26.850000. running mean: -13.346405\n",
      "ep 2431: ep_len:570 episode reward: total was -2.570000. running mean: -13.238641\n",
      "ep 2431: ep_len:3 episode reward: total was 0.000000. running mean: -13.106254\n",
      "ep 2431: ep_len:505 episode reward: total was -14.980000. running mean: -13.124992\n",
      "ep 2431: ep_len:590 episode reward: total was -20.520000. running mean: -13.198942\n",
      "epsilon:0.068032 episode_count: 17024. steps_count: 7527469.000000\n",
      "ep 2432: ep_len:650 episode reward: total was -20.000000. running mean: -13.266952\n",
      "ep 2432: ep_len:530 episode reward: total was -3.860000. running mean: -13.172883\n",
      "ep 2432: ep_len:570 episode reward: total was -10.730000. running mean: -13.148454\n",
      "ep 2432: ep_len:565 episode reward: total was 6.060000. running mean: -12.956370\n",
      "ep 2432: ep_len:3 episode reward: total was 0.000000. running mean: -12.826806\n",
      "ep 2432: ep_len:500 episode reward: total was 4.560000. running mean: -12.652938\n",
      "ep 2432: ep_len:595 episode reward: total was -50.150000. running mean: -13.027908\n",
      "epsilon:0.067895 episode_count: 17031. steps_count: 7530882.000000\n",
      "ep 2433: ep_len:565 episode reward: total was 0.630000. running mean: -12.891329\n",
      "ep 2433: ep_len:540 episode reward: total was 16.860000. running mean: -12.593816\n",
      "ep 2433: ep_len:510 episode reward: total was -23.600000. running mean: -12.703878\n",
      "ep 2433: ep_len:620 episode reward: total was -13.440000. running mean: -12.711239\n",
      "ep 2433: ep_len:98 episode reward: total was -7.950000. running mean: -12.663627\n",
      "ep 2433: ep_len:625 episode reward: total was -20.780000. running mean: -12.744790\n",
      "ep 2433: ep_len:339 episode reward: total was -7.300000. running mean: -12.690343\n",
      "epsilon:0.067759 episode_count: 17038. steps_count: 7534179.000000\n",
      "ep 2434: ep_len:565 episode reward: total was -10.710000. running mean: -12.670539\n",
      "ep 2434: ep_len:191 episode reward: total was -2.870000. running mean: -12.572534\n",
      "ep 2434: ep_len:630 episode reward: total was -15.240000. running mean: -12.599208\n",
      "ep 2434: ep_len:630 episode reward: total was -10.950000. running mean: -12.582716\n",
      "ep 2434: ep_len:3 episode reward: total was 0.000000. running mean: -12.456889\n",
      "ep 2434: ep_len:525 episode reward: total was -5.880000. running mean: -12.391120\n",
      "ep 2434: ep_len:530 episode reward: total was -11.580000. running mean: -12.383009\n",
      "epsilon:0.067622 episode_count: 17045. steps_count: 7537253.000000\n",
      "ep 2435: ep_len:259 episode reward: total was -0.400000. running mean: -12.263179\n",
      "ep 2435: ep_len:540 episode reward: total was -13.150000. running mean: -12.272047\n",
      "ep 2435: ep_len:510 episode reward: total was -34.040000. running mean: -12.489727\n",
      "ep 2435: ep_len:625 episode reward: total was -1.380000. running mean: -12.378629\n",
      "ep 2435: ep_len:3 episode reward: total was 0.000000. running mean: -12.254843\n",
      "ep 2435: ep_len:835 episode reward: total was -56.710000. running mean: -12.699395\n",
      "ep 2435: ep_len:580 episode reward: total was -13.390000. running mean: -12.706301\n",
      "epsilon:0.067486 episode_count: 17052. steps_count: 7540605.000000\n",
      "ep 2436: ep_len:600 episode reward: total was -15.770000. running mean: -12.736938\n",
      "ep 2436: ep_len:286 episode reward: total was -9.400000. running mean: -12.703568\n",
      "ep 2436: ep_len:389 episode reward: total was -38.840000. running mean: -12.964933\n",
      "ep 2436: ep_len:580 episode reward: total was 12.090000. running mean: -12.714383\n",
      "ep 2436: ep_len:3 episode reward: total was 0.000000. running mean: -12.587240\n",
      "ep 2436: ep_len:540 episode reward: total was -16.330000. running mean: -12.624667\n",
      "ep 2436: ep_len:500 episode reward: total was -22.110000. running mean: -12.719520\n",
      "epsilon:0.067349 episode_count: 17059. steps_count: 7543503.000000\n",
      "ep 2437: ep_len:500 episode reward: total was -2.680000. running mean: -12.619125\n",
      "ep 2437: ep_len:589 episode reward: total was -39.470000. running mean: -12.887634\n",
      "ep 2437: ep_len:630 episode reward: total was -8.270000. running mean: -12.841458\n",
      "ep 2437: ep_len:500 episode reward: total was 1.060000. running mean: -12.702443\n",
      "ep 2437: ep_len:53 episode reward: total was 3.500000. running mean: -12.540419\n",
      "ep 2437: ep_len:575 episode reward: total was -13.610000. running mean: -12.551114\n",
      "ep 2437: ep_len:500 episode reward: total was -24.270000. running mean: -12.668303\n",
      "epsilon:0.067213 episode_count: 17066. steps_count: 7546850.000000\n",
      "ep 2438: ep_len:500 episode reward: total was -11.790000. running mean: -12.659520\n",
      "ep 2438: ep_len:535 episode reward: total was 2.980000. running mean: -12.503125\n",
      "ep 2438: ep_len:620 episode reward: total was -5.370000. running mean: -12.431794\n",
      "ep 2438: ep_len:403 episode reward: total was -9.630000. running mean: -12.403776\n",
      "ep 2438: ep_len:106 episode reward: total was -10.960000. running mean: -12.389338\n",
      "ep 2438: ep_len:625 episode reward: total was -22.430000. running mean: -12.489745\n",
      "ep 2438: ep_len:600 episode reward: total was -40.980000. running mean: -12.774647\n",
      "epsilon:0.067076 episode_count: 17073. steps_count: 7550239.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2439: ep_len:646 episode reward: total was -59.960000. running mean: -13.246501\n",
      "ep 2439: ep_len:500 episode reward: total was -19.840000. running mean: -13.312436\n",
      "ep 2439: ep_len:570 episode reward: total was -0.520000. running mean: -13.184511\n",
      "ep 2439: ep_len:565 episode reward: total was -43.760000. running mean: -13.490266\n",
      "ep 2439: ep_len:83 episode reward: total was -10.460000. running mean: -13.459964\n",
      "ep 2439: ep_len:500 episode reward: total was -0.110000. running mean: -13.326464\n",
      "ep 2439: ep_len:605 episode reward: total was -18.030000. running mean: -13.373499\n",
      "epsilon:0.066940 episode_count: 17080. steps_count: 7553708.000000\n",
      "ep 2440: ep_len:220 episode reward: total was -0.380000. running mean: -13.243564\n",
      "ep 2440: ep_len:605 episode reward: total was -31.250000. running mean: -13.423629\n",
      "ep 2440: ep_len:625 episode reward: total was -13.930000. running mean: -13.428692\n",
      "ep 2440: ep_len:570 episode reward: total was -6.540000. running mean: -13.359806\n",
      "ep 2440: ep_len:3 episode reward: total was 0.000000. running mean: -13.226208\n",
      "ep 2440: ep_len:705 episode reward: total was -2.630000. running mean: -13.120245\n",
      "ep 2440: ep_len:600 episode reward: total was -16.380000. running mean: -13.152843\n",
      "epsilon:0.066803 episode_count: 17087. steps_count: 7557036.000000\n",
      "ep 2441: ep_len:590 episode reward: total was 7.470000. running mean: -12.946615\n",
      "ep 2441: ep_len:500 episode reward: total was -7.640000. running mean: -12.893548\n",
      "ep 2441: ep_len:545 episode reward: total was 2.440000. running mean: -12.740213\n",
      "ep 2441: ep_len:500 episode reward: total was -19.610000. running mean: -12.808911\n",
      "ep 2441: ep_len:3 episode reward: total was 0.000000. running mean: -12.680822\n",
      "ep 2441: ep_len:530 episode reward: total was -0.390000. running mean: -12.557913\n",
      "ep 2441: ep_len:500 episode reward: total was 3.450000. running mean: -12.397834\n",
      "epsilon:0.066667 episode_count: 17094. steps_count: 7560204.000000\n",
      "ep 2442: ep_len:500 episode reward: total was 4.740000. running mean: -12.226456\n",
      "ep 2442: ep_len:505 episode reward: total was -0.390000. running mean: -12.108091\n",
      "ep 2442: ep_len:600 episode reward: total was -16.070000. running mean: -12.147711\n",
      "ep 2442: ep_len:500 episode reward: total was -6.500000. running mean: -12.091233\n",
      "ep 2442: ep_len:3 episode reward: total was 0.000000. running mean: -11.970321\n",
      "ep 2442: ep_len:317 episode reward: total was 3.150000. running mean: -11.819118\n",
      "ep 2442: ep_len:545 episode reward: total was -19.780000. running mean: -11.898727\n",
      "epsilon:0.066530 episode_count: 17101. steps_count: 7563174.000000\n",
      "ep 2443: ep_len:550 episode reward: total was -27.370000. running mean: -12.053439\n",
      "ep 2443: ep_len:510 episode reward: total was -17.200000. running mean: -12.104905\n",
      "ep 2443: ep_len:590 episode reward: total was -26.460000. running mean: -12.248456\n",
      "ep 2443: ep_len:500 episode reward: total was 6.450000. running mean: -12.061471\n",
      "ep 2443: ep_len:128 episode reward: total was 3.060000. running mean: -11.910257\n",
      "ep 2443: ep_len:590 episode reward: total was 1.990000. running mean: -11.771254\n",
      "ep 2443: ep_len:525 episode reward: total was -9.380000. running mean: -11.747342\n",
      "epsilon:0.066394 episode_count: 17108. steps_count: 7566567.000000\n",
      "ep 2444: ep_len:540 episode reward: total was -23.420000. running mean: -11.864068\n",
      "ep 2444: ep_len:500 episode reward: total was -37.780000. running mean: -12.123227\n",
      "ep 2444: ep_len:660 episode reward: total was -18.740000. running mean: -12.189395\n",
      "ep 2444: ep_len:500 episode reward: total was -18.700000. running mean: -12.254501\n",
      "ep 2444: ep_len:117 episode reward: total was 1.530000. running mean: -12.116656\n",
      "ep 2444: ep_len:207 episode reward: total was 0.100000. running mean: -11.994490\n",
      "ep 2444: ep_len:326 episode reward: total was -7.340000. running mean: -11.947945\n",
      "epsilon:0.066257 episode_count: 17115. steps_count: 7569417.000000\n",
      "ep 2445: ep_len:625 episode reward: total was -5.930000. running mean: -11.887765\n",
      "ep 2445: ep_len:525 episode reward: total was -18.930000. running mean: -11.958188\n",
      "ep 2445: ep_len:630 episode reward: total was -16.950000. running mean: -12.008106\n",
      "ep 2445: ep_len:515 episode reward: total was 10.990000. running mean: -11.778125\n",
      "ep 2445: ep_len:3 episode reward: total was 0.000000. running mean: -11.660344\n",
      "ep 2445: ep_len:530 episode reward: total was -10.370000. running mean: -11.647440\n",
      "ep 2445: ep_len:610 episode reward: total was -20.430000. running mean: -11.735266\n",
      "epsilon:0.066121 episode_count: 17122. steps_count: 7572855.000000\n",
      "ep 2446: ep_len:500 episode reward: total was -1.670000. running mean: -11.634613\n",
      "ep 2446: ep_len:500 episode reward: total was -13.580000. running mean: -11.654067\n",
      "ep 2446: ep_len:570 episode reward: total was -11.590000. running mean: -11.653426\n",
      "ep 2446: ep_len:570 episode reward: total was -9.640000. running mean: -11.633292\n",
      "ep 2446: ep_len:105 episode reward: total was 2.550000. running mean: -11.491459\n",
      "ep 2446: ep_len:500 episode reward: total was -17.990000. running mean: -11.556444\n",
      "ep 2446: ep_len:530 episode reward: total was -18.190000. running mean: -11.622780\n",
      "epsilon:0.065984 episode_count: 17129. steps_count: 7576130.000000\n",
      "ep 2447: ep_len:525 episode reward: total was 5.770000. running mean: -11.448852\n",
      "ep 2447: ep_len:500 episode reward: total was -27.530000. running mean: -11.609664\n",
      "ep 2447: ep_len:540 episode reward: total was -7.580000. running mean: -11.569367\n",
      "ep 2447: ep_len:575 episode reward: total was 2.570000. running mean: -11.427973\n",
      "ep 2447: ep_len:111 episode reward: total was 4.520000. running mean: -11.268494\n",
      "ep 2447: ep_len:500 episode reward: total was -5.310000. running mean: -11.208909\n",
      "ep 2447: ep_len:525 episode reward: total was -11.490000. running mean: -11.211720\n",
      "epsilon:0.065848 episode_count: 17136. steps_count: 7579406.000000\n",
      "ep 2448: ep_len:640 episode reward: total was -25.790000. running mean: -11.357502\n",
      "ep 2448: ep_len:500 episode reward: total was -6.890000. running mean: -11.312827\n",
      "ep 2448: ep_len:550 episode reward: total was 4.650000. running mean: -11.153199\n",
      "ep 2448: ep_len:500 episode reward: total was -7.530000. running mean: -11.116967\n",
      "ep 2448: ep_len:3 episode reward: total was 0.000000. running mean: -11.005797\n",
      "ep 2448: ep_len:515 episode reward: total was -27.070000. running mean: -11.166439\n",
      "ep 2448: ep_len:515 episode reward: total was -38.450000. running mean: -11.439275\n",
      "epsilon:0.065711 episode_count: 17143. steps_count: 7582629.000000\n",
      "ep 2449: ep_len:510 episode reward: total was -5.630000. running mean: -11.381182\n",
      "ep 2449: ep_len:371 episode reward: total was -13.870000. running mean: -11.406071\n",
      "ep 2449: ep_len:505 episode reward: total was -21.010000. running mean: -11.502110\n",
      "ep 2449: ep_len:565 episode reward: total was -7.010000. running mean: -11.457189\n",
      "ep 2449: ep_len:94 episode reward: total was 4.050000. running mean: -11.302117\n",
      "ep 2449: ep_len:695 episode reward: total was -54.830000. running mean: -11.737396\n",
      "ep 2449: ep_len:580 episode reward: total was -5.500000. running mean: -11.675022\n",
      "epsilon:0.065575 episode_count: 17150. steps_count: 7585949.000000\n",
      "ep 2450: ep_len:590 episode reward: total was -18.910000. running mean: -11.747371\n",
      "ep 2450: ep_len:585 episode reward: total was -8.690000. running mean: -11.716798\n",
      "ep 2450: ep_len:500 episode reward: total was -0.220000. running mean: -11.601830\n",
      "ep 2450: ep_len:500 episode reward: total was -12.570000. running mean: -11.611511\n",
      "ep 2450: ep_len:3 episode reward: total was 0.000000. running mean: -11.495396\n",
      "ep 2450: ep_len:530 episode reward: total was -7.780000. running mean: -11.458242\n",
      "ep 2450: ep_len:625 episode reward: total was -22.430000. running mean: -11.567960\n",
      "epsilon:0.065438 episode_count: 17157. steps_count: 7589282.000000\n",
      "ep 2451: ep_len:665 episode reward: total was -45.830000. running mean: -11.910580\n",
      "ep 2451: ep_len:500 episode reward: total was -12.370000. running mean: -11.915175\n",
      "ep 2451: ep_len:366 episode reward: total was -19.360000. running mean: -11.989623\n",
      "ep 2451: ep_len:500 episode reward: total was 3.530000. running mean: -11.834427\n",
      "ep 2451: ep_len:3 episode reward: total was 0.000000. running mean: -11.716082\n",
      "ep 2451: ep_len:244 episode reward: total was -8.850000. running mean: -11.687422\n",
      "ep 2451: ep_len:187 episode reward: total was -16.410000. running mean: -11.734647\n",
      "epsilon:0.065302 episode_count: 17164. steps_count: 7591747.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2452: ep_len:500 episode reward: total was -14.660000. running mean: -11.763901\n",
      "ep 2452: ep_len:540 episode reward: total was 10.870000. running mean: -11.537562\n",
      "ep 2452: ep_len:630 episode reward: total was -40.580000. running mean: -11.827986\n",
      "ep 2452: ep_len:500 episode reward: total was -16.560000. running mean: -11.875306\n",
      "ep 2452: ep_len:43 episode reward: total was -2.000000. running mean: -11.776553\n",
      "ep 2452: ep_len:555 episode reward: total was -5.620000. running mean: -11.714988\n",
      "ep 2452: ep_len:298 episode reward: total was -3.750000. running mean: -11.635338\n",
      "epsilon:0.065165 episode_count: 17171. steps_count: 7594813.000000\n",
      "ep 2453: ep_len:545 episode reward: total was -0.390000. running mean: -11.522884\n",
      "ep 2453: ep_len:700 episode reward: total was -54.360000. running mean: -11.951256\n",
      "ep 2453: ep_len:620 episode reward: total was -5.540000. running mean: -11.887143\n",
      "ep 2453: ep_len:500 episode reward: total was -7.020000. running mean: -11.838472\n",
      "ep 2453: ep_len:3 episode reward: total was 0.000000. running mean: -11.720087\n",
      "ep 2453: ep_len:695 episode reward: total was -41.790000. running mean: -12.020786\n",
      "ep 2453: ep_len:530 episode reward: total was -11.580000. running mean: -12.016378\n",
      "epsilon:0.065029 episode_count: 17178. steps_count: 7598406.000000\n",
      "ep 2454: ep_len:615 episode reward: total was 1.630000. running mean: -11.879914\n",
      "ep 2454: ep_len:510 episode reward: total was 15.410000. running mean: -11.607015\n",
      "ep 2454: ep_len:500 episode reward: total was -2.620000. running mean: -11.517145\n",
      "ep 2454: ep_len:565 episode reward: total was -2.590000. running mean: -11.427874\n",
      "ep 2454: ep_len:3 episode reward: total was 0.000000. running mean: -11.313595\n",
      "ep 2454: ep_len:590 episode reward: total was -15.180000. running mean: -11.352259\n",
      "ep 2454: ep_len:540 episode reward: total was -10.820000. running mean: -11.346936\n",
      "epsilon:0.064892 episode_count: 17185. steps_count: 7601729.000000\n",
      "ep 2455: ep_len:605 episode reward: total was -13.090000. running mean: -11.364367\n",
      "ep 2455: ep_len:680 episode reward: total was -24.360000. running mean: -11.494323\n",
      "ep 2455: ep_len:500 episode reward: total was -33.050000. running mean: -11.709880\n",
      "ep 2455: ep_len:500 episode reward: total was -1.200000. running mean: -11.604781\n",
      "ep 2455: ep_len:78 episode reward: total was 4.050000. running mean: -11.448234\n",
      "ep 2455: ep_len:510 episode reward: total was 0.460000. running mean: -11.329151\n",
      "ep 2455: ep_len:500 episode reward: total was -0.210000. running mean: -11.217960\n",
      "epsilon:0.064756 episode_count: 17192. steps_count: 7605102.000000\n",
      "ep 2456: ep_len:615 episode reward: total was -8.500000. running mean: -11.190780\n",
      "ep 2456: ep_len:366 episode reward: total was -2.770000. running mean: -11.106572\n",
      "ep 2456: ep_len:525 episode reward: total was -15.510000. running mean: -11.150607\n",
      "ep 2456: ep_len:110 episode reward: total was -2.430000. running mean: -11.063400\n",
      "ep 2456: ep_len:71 episode reward: total was -4.470000. running mean: -10.997466\n",
      "ep 2456: ep_len:635 episode reward: total was -1.880000. running mean: -10.906292\n",
      "ep 2456: ep_len:500 episode reward: total was -11.730000. running mean: -10.914529\n",
      "epsilon:0.064619 episode_count: 17199. steps_count: 7607924.000000\n",
      "ep 2457: ep_len:795 episode reward: total was -49.720000. running mean: -11.302584\n",
      "ep 2457: ep_len:760 episode reward: total was -76.170000. running mean: -11.951258\n",
      "ep 2457: ep_len:625 episode reward: total was -13.760000. running mean: -11.969345\n",
      "ep 2457: ep_len:615 episode reward: total was 8.930000. running mean: -11.760352\n",
      "ep 2457: ep_len:3 episode reward: total was 0.000000. running mean: -11.642748\n",
      "ep 2457: ep_len:565 episode reward: total was -20.120000. running mean: -11.727521\n",
      "ep 2457: ep_len:500 episode reward: total was -8.720000. running mean: -11.697446\n",
      "epsilon:0.064483 episode_count: 17206. steps_count: 7611787.000000\n",
      "ep 2458: ep_len:505 episode reward: total was -2.920000. running mean: -11.609671\n",
      "ep 2458: ep_len:765 episode reward: total was -35.270000. running mean: -11.846274\n",
      "ep 2458: ep_len:810 episode reward: total was -33.620000. running mean: -12.064012\n",
      "ep 2458: ep_len:500 episode reward: total was -27.570000. running mean: -12.219072\n",
      "ep 2458: ep_len:94 episode reward: total was 3.040000. running mean: -12.066481\n",
      "ep 2458: ep_len:540 episode reward: total was -31.450000. running mean: -12.260316\n",
      "ep 2458: ep_len:500 episode reward: total was -38.650000. running mean: -12.524213\n",
      "epsilon:0.064346 episode_count: 17213. steps_count: 7615501.000000\n",
      "ep 2459: ep_len:500 episode reward: total was -1.090000. running mean: -12.409871\n",
      "ep 2459: ep_len:575 episode reward: total was 3.490000. running mean: -12.250872\n",
      "ep 2459: ep_len:565 episode reward: total was -28.370000. running mean: -12.412063\n",
      "ep 2459: ep_len:555 episode reward: total was 2.020000. running mean: -12.267743\n",
      "ep 2459: ep_len:3 episode reward: total was 0.000000. running mean: -12.145065\n",
      "ep 2459: ep_len:169 episode reward: total was 2.090000. running mean: -12.002715\n",
      "ep 2459: ep_len:500 episode reward: total was -14.880000. running mean: -12.031487\n",
      "epsilon:0.064210 episode_count: 17220. steps_count: 7618368.000000\n",
      "ep 2460: ep_len:510 episode reward: total was 1.850000. running mean: -11.892673\n",
      "ep 2460: ep_len:555 episode reward: total was -25.940000. running mean: -12.033146\n",
      "ep 2460: ep_len:500 episode reward: total was 0.560000. running mean: -11.907214\n",
      "ep 2460: ep_len:56 episode reward: total was 1.560000. running mean: -11.772542\n",
      "ep 2460: ep_len:133 episode reward: total was 6.060000. running mean: -11.594217\n",
      "ep 2460: ep_len:585 episode reward: total was -23.080000. running mean: -11.709075\n",
      "ep 2460: ep_len:555 episode reward: total was -54.260000. running mean: -12.134584\n",
      "epsilon:0.064073 episode_count: 17227. steps_count: 7621262.000000\n",
      "ep 2461: ep_len:610 episode reward: total was -0.990000. running mean: -12.023138\n",
      "ep 2461: ep_len:505 episode reward: total was -28.370000. running mean: -12.186607\n",
      "ep 2461: ep_len:500 episode reward: total was -1.640000. running mean: -12.081141\n",
      "ep 2461: ep_len:143 episode reward: total was 2.590000. running mean: -11.934429\n",
      "ep 2461: ep_len:96 episode reward: total was -2.450000. running mean: -11.839585\n",
      "ep 2461: ep_len:550 episode reward: total was -17.400000. running mean: -11.895189\n",
      "ep 2461: ep_len:545 episode reward: total was -12.060000. running mean: -11.896837\n",
      "epsilon:0.063937 episode_count: 17234. steps_count: 7624211.000000\n",
      "ep 2462: ep_len:615 episode reward: total was -46.420000. running mean: -12.242069\n",
      "ep 2462: ep_len:655 episode reward: total was -61.490000. running mean: -12.734548\n",
      "ep 2462: ep_len:630 episode reward: total was -11.440000. running mean: -12.721603\n",
      "ep 2462: ep_len:42 episode reward: total was -2.970000. running mean: -12.624087\n",
      "ep 2462: ep_len:95 episode reward: total was 3.540000. running mean: -12.462446\n",
      "ep 2462: ep_len:164 episode reward: total was 4.590000. running mean: -12.291921\n",
      "ep 2462: ep_len:555 episode reward: total was -10.110000. running mean: -12.270102\n",
      "epsilon:0.063800 episode_count: 17241. steps_count: 7626967.000000\n",
      "ep 2463: ep_len:560 episode reward: total was -33.610000. running mean: -12.483501\n",
      "ep 2463: ep_len:500 episode reward: total was -21.610000. running mean: -12.574766\n",
      "ep 2463: ep_len:580 episode reward: total was -31.550000. running mean: -12.764518\n",
      "ep 2463: ep_len:500 episode reward: total was -25.600000. running mean: -12.892873\n",
      "ep 2463: ep_len:55 episode reward: total was 5.010000. running mean: -12.713844\n",
      "ep 2463: ep_len:590 episode reward: total was -27.160000. running mean: -12.858306\n",
      "ep 2463: ep_len:182 episode reward: total was -6.410000. running mean: -12.793823\n",
      "epsilon:0.063664 episode_count: 17248. steps_count: 7629934.000000\n",
      "ep 2464: ep_len:585 episode reward: total was -32.800000. running mean: -12.993885\n",
      "ep 2464: ep_len:500 episode reward: total was 1.110000. running mean: -12.852846\n",
      "ep 2464: ep_len:665 episode reward: total was -30.790000. running mean: -13.032217\n",
      "ep 2464: ep_len:610 episode reward: total was -4.460000. running mean: -12.946495\n",
      "ep 2464: ep_len:125 episode reward: total was 1.550000. running mean: -12.801530\n",
      "ep 2464: ep_len:550 episode reward: total was -7.420000. running mean: -12.747715\n",
      "ep 2464: ep_len:630 episode reward: total was -27.000000. running mean: -12.890238\n",
      "epsilon:0.063527 episode_count: 17255. steps_count: 7633599.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2465: ep_len:605 episode reward: total was 1.110000. running mean: -12.750235\n",
      "ep 2465: ep_len:367 episode reward: total was -20.850000. running mean: -12.831233\n",
      "ep 2465: ep_len:500 episode reward: total was -7.430000. running mean: -12.777221\n",
      "ep 2465: ep_len:565 episode reward: total was -6.020000. running mean: -12.709649\n",
      "ep 2465: ep_len:3 episode reward: total was 0.000000. running mean: -12.582552\n",
      "ep 2465: ep_len:500 episode reward: total was -14.130000. running mean: -12.598027\n",
      "ep 2465: ep_len:500 episode reward: total was -6.210000. running mean: -12.534146\n",
      "epsilon:0.063391 episode_count: 17262. steps_count: 7636639.000000\n",
      "ep 2466: ep_len:134 episode reward: total was 2.570000. running mean: -12.383105\n",
      "ep 2466: ep_len:575 episode reward: total was 2.330000. running mean: -12.235974\n",
      "ep 2466: ep_len:765 episode reward: total was -51.980000. running mean: -12.633414\n",
      "ep 2466: ep_len:555 episode reward: total was 6.390000. running mean: -12.443180\n",
      "ep 2466: ep_len:3 episode reward: total was 0.000000. running mean: -12.318748\n",
      "ep 2466: ep_len:500 episode reward: total was -7.430000. running mean: -12.269861\n",
      "ep 2466: ep_len:605 episode reward: total was -12.080000. running mean: -12.267962\n",
      "epsilon:0.063254 episode_count: 17269. steps_count: 7639776.000000\n",
      "ep 2467: ep_len:610 episode reward: total was -7.830000. running mean: -12.223582\n",
      "ep 2467: ep_len:500 episode reward: total was -45.300000. running mean: -12.554347\n",
      "ep 2467: ep_len:570 episode reward: total was -9.370000. running mean: -12.522503\n",
      "ep 2467: ep_len:500 episode reward: total was 1.310000. running mean: -12.384178\n",
      "ep 2467: ep_len:3 episode reward: total was 0.000000. running mean: -12.260336\n",
      "ep 2467: ep_len:500 episode reward: total was 5.540000. running mean: -12.082333\n",
      "ep 2467: ep_len:610 episode reward: total was -3.810000. running mean: -11.999610\n",
      "epsilon:0.063118 episode_count: 17276. steps_count: 7643069.000000\n",
      "ep 2468: ep_len:660 episode reward: total was -22.260000. running mean: -12.102214\n",
      "ep 2468: ep_len:590 episode reward: total was -37.990000. running mean: -12.361091\n",
      "ep 2468: ep_len:500 episode reward: total was -5.650000. running mean: -12.293980\n",
      "ep 2468: ep_len:575 episode reward: total was -5.000000. running mean: -12.221041\n",
      "ep 2468: ep_len:95 episode reward: total was -2.450000. running mean: -12.123330\n",
      "ep 2468: ep_len:590 episode reward: total was -13.200000. running mean: -12.134097\n",
      "ep 2468: ep_len:210 episode reward: total was -6.850000. running mean: -12.081256\n",
      "epsilon:0.062981 episode_count: 17283. steps_count: 7646289.000000\n",
      "ep 2469: ep_len:760 episode reward: total was -38.130000. running mean: -12.341743\n",
      "ep 2469: ep_len:500 episode reward: total was -20.900000. running mean: -12.427326\n",
      "ep 2469: ep_len:645 episode reward: total was -28.270000. running mean: -12.585753\n",
      "ep 2469: ep_len:505 episode reward: total was -40.240000. running mean: -12.862295\n",
      "ep 2469: ep_len:53 episode reward: total was 5.000000. running mean: -12.683672\n",
      "ep 2469: ep_len:635 episode reward: total was -22.050000. running mean: -12.777336\n",
      "ep 2469: ep_len:650 episode reward: total was -42.200000. running mean: -13.071562\n",
      "epsilon:0.062845 episode_count: 17290. steps_count: 7650037.000000\n",
      "ep 2470: ep_len:211 episode reward: total was -15.870000. running mean: -13.099547\n",
      "ep 2470: ep_len:545 episode reward: total was -37.500000. running mean: -13.343551\n",
      "ep 2470: ep_len:605 episode reward: total was -13.280000. running mean: -13.342916\n",
      "ep 2470: ep_len:505 episode reward: total was -20.010000. running mean: -13.409586\n",
      "ep 2470: ep_len:76 episode reward: total was -0.960000. running mean: -13.285091\n",
      "ep 2470: ep_len:505 episode reward: total was -38.800000. running mean: -13.540240\n",
      "ep 2470: ep_len:183 episode reward: total was -13.420000. running mean: -13.539037\n",
      "epsilon:0.062708 episode_count: 17297. steps_count: 7652667.000000\n",
      "ep 2471: ep_len:550 episode reward: total was -19.540000. running mean: -13.599047\n",
      "ep 2471: ep_len:545 episode reward: total was -4.110000. running mean: -13.504156\n",
      "ep 2471: ep_len:75 episode reward: total was 0.040000. running mean: -13.368715\n",
      "ep 2471: ep_len:545 episode reward: total was -17.580000. running mean: -13.410828\n",
      "ep 2471: ep_len:88 episode reward: total was 2.050000. running mean: -13.256219\n",
      "ep 2471: ep_len:560 episode reward: total was -23.820000. running mean: -13.361857\n",
      "ep 2471: ep_len:615 episode reward: total was -22.590000. running mean: -13.454139\n",
      "epsilon:0.062572 episode_count: 17304. steps_count: 7655645.000000\n",
      "ep 2472: ep_len:500 episode reward: total was 7.800000. running mean: -13.241597\n",
      "ep 2472: ep_len:605 episode reward: total was -37.640000. running mean: -13.485581\n",
      "ep 2472: ep_len:690 episode reward: total was -18.680000. running mean: -13.537525\n",
      "ep 2472: ep_len:500 episode reward: total was -22.550000. running mean: -13.627650\n",
      "ep 2472: ep_len:3 episode reward: total was 0.000000. running mean: -13.491374\n",
      "ep 2472: ep_len:610 episode reward: total was -32.890000. running mean: -13.685360\n",
      "ep 2472: ep_len:595 episode reward: total was -18.350000. running mean: -13.732006\n",
      "epsilon:0.062435 episode_count: 17311. steps_count: 7659148.000000\n",
      "ep 2473: ep_len:231 episode reward: total was -7.930000. running mean: -13.673986\n",
      "ep 2473: ep_len:580 episode reward: total was -23.160000. running mean: -13.768846\n",
      "ep 2473: ep_len:620 episode reward: total was -2.510000. running mean: -13.656258\n",
      "ep 2473: ep_len:610 episode reward: total was 1.390000. running mean: -13.505795\n",
      "ep 2473: ep_len:88 episode reward: total was 5.020000. running mean: -13.320537\n",
      "ep 2473: ep_len:510 episode reward: total was 0.610000. running mean: -13.181232\n",
      "ep 2473: ep_len:600 episode reward: total was -24.360000. running mean: -13.293020\n",
      "epsilon:0.062299 episode_count: 17318. steps_count: 7662387.000000\n",
      "ep 2474: ep_len:525 episode reward: total was -3.430000. running mean: -13.194390\n",
      "ep 2474: ep_len:675 episode reward: total was -45.950000. running mean: -13.521946\n",
      "ep 2474: ep_len:500 episode reward: total was -6.600000. running mean: -13.452726\n",
      "ep 2474: ep_len:525 episode reward: total was 6.930000. running mean: -13.248899\n",
      "ep 2474: ep_len:91 episode reward: total was 4.540000. running mean: -13.071010\n",
      "ep 2474: ep_len:500 episode reward: total was -15.740000. running mean: -13.097700\n",
      "ep 2474: ep_len:505 episode reward: total was -24.500000. running mean: -13.211723\n",
      "epsilon:0.062162 episode_count: 17325. steps_count: 7665708.000000\n",
      "ep 2475: ep_len:199 episode reward: total was 7.580000. running mean: -13.003806\n",
      "ep 2475: ep_len:500 episode reward: total was -6.520000. running mean: -12.938968\n",
      "ep 2475: ep_len:600 episode reward: total was -4.700000. running mean: -12.856578\n",
      "ep 2475: ep_len:590 episode reward: total was -2.940000. running mean: -12.757412\n",
      "ep 2475: ep_len:3 episode reward: total was 0.000000. running mean: -12.629838\n",
      "ep 2475: ep_len:535 episode reward: total was -34.030000. running mean: -12.843840\n",
      "ep 2475: ep_len:585 episode reward: total was -3.940000. running mean: -12.754801\n",
      "epsilon:0.062026 episode_count: 17332. steps_count: 7668720.000000\n",
      "ep 2476: ep_len:510 episode reward: total was -17.400000. running mean: -12.801253\n",
      "ep 2476: ep_len:615 episode reward: total was -27.140000. running mean: -12.944641\n",
      "ep 2476: ep_len:605 episode reward: total was -13.720000. running mean: -12.952394\n",
      "ep 2476: ep_len:550 episode reward: total was -11.510000. running mean: -12.937970\n",
      "ep 2476: ep_len:3 episode reward: total was 0.000000. running mean: -12.808591\n",
      "ep 2476: ep_len:520 episode reward: total was -16.690000. running mean: -12.847405\n",
      "ep 2476: ep_len:590 episode reward: total was -12.600000. running mean: -12.844931\n",
      "epsilon:0.061889 episode_count: 17339. steps_count: 7672113.000000\n",
      "ep 2477: ep_len:218 episode reward: total was 2.080000. running mean: -12.695681\n",
      "ep 2477: ep_len:500 episode reward: total was -11.280000. running mean: -12.681525\n",
      "ep 2477: ep_len:605 episode reward: total was -49.450000. running mean: -13.049209\n",
      "ep 2477: ep_len:505 episode reward: total was -14.580000. running mean: -13.064517\n",
      "ep 2477: ep_len:3 episode reward: total was 0.000000. running mean: -12.933872\n",
      "ep 2477: ep_len:745 episode reward: total was -57.820000. running mean: -13.382733\n",
      "ep 2477: ep_len:550 episode reward: total was -21.560000. running mean: -13.464506\n",
      "epsilon:0.061753 episode_count: 17346. steps_count: 7675239.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2478: ep_len:550 episode reward: total was -9.600000. running mean: -13.425861\n",
      "ep 2478: ep_len:510 episode reward: total was -16.310000. running mean: -13.454702\n",
      "ep 2478: ep_len:356 episode reward: total was 3.700000. running mean: -13.283155\n",
      "ep 2478: ep_len:520 episode reward: total was -6.520000. running mean: -13.215524\n",
      "ep 2478: ep_len:130 episode reward: total was 3.060000. running mean: -13.052769\n",
      "ep 2478: ep_len:575 episode reward: total was -13.230000. running mean: -13.054541\n",
      "ep 2478: ep_len:510 episode reward: total was -21.350000. running mean: -13.137495\n",
      "epsilon:0.061616 episode_count: 17353. steps_count: 7678390.000000\n",
      "ep 2479: ep_len:545 episode reward: total was -16.030000. running mean: -13.166420\n",
      "ep 2479: ep_len:605 episode reward: total was -15.150000. running mean: -13.186256\n",
      "ep 2479: ep_len:500 episode reward: total was -2.630000. running mean: -13.080694\n",
      "ep 2479: ep_len:520 episode reward: total was -1.080000. running mean: -12.960687\n",
      "ep 2479: ep_len:3 episode reward: total was 0.000000. running mean: -12.831080\n",
      "ep 2479: ep_len:580 episode reward: total was -12.620000. running mean: -12.828969\n",
      "ep 2479: ep_len:580 episode reward: total was -6.100000. running mean: -12.761679\n",
      "epsilon:0.061480 episode_count: 17360. steps_count: 7681723.000000\n",
      "ep 2480: ep_len:565 episode reward: total was -17.200000. running mean: -12.806063\n",
      "ep 2480: ep_len:510 episode reward: total was -19.820000. running mean: -12.876202\n",
      "ep 2480: ep_len:500 episode reward: total was -13.410000. running mean: -12.881540\n",
      "ep 2480: ep_len:560 episode reward: total was -18.470000. running mean: -12.937425\n",
      "ep 2480: ep_len:87 episode reward: total was 2.540000. running mean: -12.782650\n",
      "ep 2480: ep_len:525 episode reward: total was -32.260000. running mean: -12.977424\n",
      "ep 2480: ep_len:535 episode reward: total was -3.950000. running mean: -12.887150\n",
      "epsilon:0.061343 episode_count: 17367. steps_count: 7685005.000000\n",
      "ep 2481: ep_len:575 episode reward: total was -1.400000. running mean: -12.772278\n",
      "ep 2481: ep_len:525 episode reward: total was -17.400000. running mean: -12.818555\n",
      "ep 2481: ep_len:79 episode reward: total was 0.040000. running mean: -12.689970\n",
      "ep 2481: ep_len:520 episode reward: total was -8.520000. running mean: -12.648270\n",
      "ep 2481: ep_len:3 episode reward: total was 0.000000. running mean: -12.521787\n",
      "ep 2481: ep_len:500 episode reward: total was -22.800000. running mean: -12.624569\n",
      "ep 2481: ep_len:570 episode reward: total was -6.830000. running mean: -12.566624\n",
      "epsilon:0.061207 episode_count: 17374. steps_count: 7687777.000000\n",
      "ep 2482: ep_len:585 episode reward: total was -22.720000. running mean: -12.668158\n",
      "ep 2482: ep_len:500 episode reward: total was -16.870000. running mean: -12.710176\n",
      "ep 2482: ep_len:540 episode reward: total was -6.750000. running mean: -12.650574\n",
      "ep 2482: ep_len:560 episode reward: total was 5.030000. running mean: -12.473768\n",
      "ep 2482: ep_len:53 episode reward: total was 3.500000. running mean: -12.314031\n",
      "ep 2482: ep_len:172 episode reward: total was 2.590000. running mean: -12.164990\n",
      "ep 2482: ep_len:500 episode reward: total was -16.120000. running mean: -12.204541\n",
      "epsilon:0.061070 episode_count: 17381. steps_count: 7690687.000000\n",
      "ep 2483: ep_len:500 episode reward: total was -23.400000. running mean: -12.316495\n",
      "ep 2483: ep_len:585 episode reward: total was -9.860000. running mean: -12.291930\n",
      "ep 2483: ep_len:500 episode reward: total was -9.510000. running mean: -12.264111\n",
      "ep 2483: ep_len:650 episode reward: total was -34.450000. running mean: -12.485970\n",
      "ep 2483: ep_len:3 episode reward: total was 0.000000. running mean: -12.361110\n",
      "ep 2483: ep_len:165 episode reward: total was 5.610000. running mean: -12.181399\n",
      "ep 2483: ep_len:525 episode reward: total was -1.300000. running mean: -12.072585\n",
      "epsilon:0.060934 episode_count: 17388. steps_count: 7693615.000000\n",
      "ep 2484: ep_len:500 episode reward: total was 10.280000. running mean: -11.849059\n",
      "ep 2484: ep_len:590 episode reward: total was 0.680000. running mean: -11.723769\n",
      "ep 2484: ep_len:450 episode reward: total was -4.270000. running mean: -11.649231\n",
      "ep 2484: ep_len:740 episode reward: total was -22.860000. running mean: -11.761339\n",
      "ep 2484: ep_len:3 episode reward: total was 0.000000. running mean: -11.643725\n",
      "ep 2484: ep_len:535 episode reward: total was -28.890000. running mean: -11.816188\n",
      "ep 2484: ep_len:550 episode reward: total was -25.830000. running mean: -11.956326\n",
      "epsilon:0.060797 episode_count: 17395. steps_count: 7696983.000000\n",
      "ep 2485: ep_len:640 episode reward: total was -22.820000. running mean: -12.064963\n",
      "ep 2485: ep_len:500 episode reward: total was -13.490000. running mean: -12.079213\n",
      "ep 2485: ep_len:640 episode reward: total was -29.830000. running mean: -12.256721\n",
      "ep 2485: ep_len:535 episode reward: total was 9.370000. running mean: -12.040454\n",
      "ep 2485: ep_len:3 episode reward: total was 0.000000. running mean: -11.920049\n",
      "ep 2485: ep_len:505 episode reward: total was -7.220000. running mean: -11.873049\n",
      "ep 2485: ep_len:600 episode reward: total was -27.540000. running mean: -12.029718\n",
      "epsilon:0.060661 episode_count: 17402. steps_count: 7700406.000000\n",
      "ep 2486: ep_len:645 episode reward: total was -8.150000. running mean: -11.990921\n",
      "ep 2486: ep_len:500 episode reward: total was -30.360000. running mean: -12.174612\n",
      "ep 2486: ep_len:625 episode reward: total was -12.950000. running mean: -12.182366\n",
      "ep 2486: ep_len:130 episode reward: total was 0.590000. running mean: -12.054642\n",
      "ep 2486: ep_len:97 episode reward: total was 4.550000. running mean: -11.888596\n",
      "ep 2486: ep_len:515 episode reward: total was -1.770000. running mean: -11.787410\n",
      "ep 2486: ep_len:575 episode reward: total was -20.480000. running mean: -11.874336\n",
      "epsilon:0.060524 episode_count: 17409. steps_count: 7703493.000000\n",
      "ep 2487: ep_len:750 episode reward: total was -26.630000. running mean: -12.021892\n",
      "ep 2487: ep_len:500 episode reward: total was 9.190000. running mean: -11.809773\n",
      "ep 2487: ep_len:605 episode reward: total was -11.510000. running mean: -11.806776\n",
      "ep 2487: ep_len:545 episode reward: total was -3.500000. running mean: -11.723708\n",
      "ep 2487: ep_len:3 episode reward: total was 0.000000. running mean: -11.606471\n",
      "ep 2487: ep_len:550 episode reward: total was -7.110000. running mean: -11.561506\n",
      "ep 2487: ep_len:630 episode reward: total was -3.220000. running mean: -11.478091\n",
      "epsilon:0.060388 episode_count: 17416. steps_count: 7707076.000000\n",
      "ep 2488: ep_len:550 episode reward: total was -31.830000. running mean: -11.681610\n",
      "ep 2488: ep_len:510 episode reward: total was 0.180000. running mean: -11.562994\n",
      "ep 2488: ep_len:580 episode reward: total was -17.710000. running mean: -11.624464\n",
      "ep 2488: ep_len:170 episode reward: total was 3.660000. running mean: -11.471619\n",
      "ep 2488: ep_len:48 episode reward: total was 3.000000. running mean: -11.326903\n",
      "ep 2488: ep_len:580 episode reward: total was -3.570000. running mean: -11.249334\n",
      "ep 2488: ep_len:500 episode reward: total was -18.890000. running mean: -11.325741\n",
      "epsilon:0.060251 episode_count: 17423. steps_count: 7710014.000000\n",
      "ep 2489: ep_len:675 episode reward: total was -17.240000. running mean: -11.384883\n",
      "ep 2489: ep_len:505 episode reward: total was -15.840000. running mean: -11.429435\n",
      "ep 2489: ep_len:318 episode reward: total was -6.360000. running mean: -11.378740\n",
      "ep 2489: ep_len:720 episode reward: total was -52.010000. running mean: -11.785053\n",
      "ep 2489: ep_len:113 episode reward: total was -12.450000. running mean: -11.791702\n",
      "ep 2489: ep_len:520 episode reward: total was -14.740000. running mean: -11.821185\n",
      "ep 2489: ep_len:510 episode reward: total was -4.270000. running mean: -11.745673\n",
      "epsilon:0.060115 episode_count: 17430. steps_count: 7713375.000000\n",
      "ep 2490: ep_len:835 episode reward: total was -139.370000. running mean: -13.021917\n",
      "ep 2490: ep_len:550 episode reward: total was -12.850000. running mean: -13.020198\n",
      "ep 2490: ep_len:550 episode reward: total was -13.280000. running mean: -13.022796\n",
      "ep 2490: ep_len:528 episode reward: total was -10.910000. running mean: -13.001668\n",
      "ep 2490: ep_len:93 episode reward: total was 4.540000. running mean: -12.826251\n",
      "ep 2490: ep_len:515 episode reward: total was 4.830000. running mean: -12.649688\n",
      "ep 2490: ep_len:259 episode reward: total was -5.850000. running mean: -12.581692\n",
      "epsilon:0.059978 episode_count: 17437. steps_count: 7716705.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2491: ep_len:570 episode reward: total was -16.320000. running mean: -12.619075\n",
      "ep 2491: ep_len:309 episode reward: total was -26.330000. running mean: -12.756184\n",
      "ep 2491: ep_len:500 episode reward: total was -11.010000. running mean: -12.738722\n",
      "ep 2491: ep_len:540 episode reward: total was 3.420000. running mean: -12.577135\n",
      "ep 2491: ep_len:3 episode reward: total was 0.000000. running mean: -12.451364\n",
      "ep 2491: ep_len:585 episode reward: total was -36.580000. running mean: -12.692650\n",
      "ep 2491: ep_len:500 episode reward: total was -25.170000. running mean: -12.817423\n",
      "epsilon:0.059842 episode_count: 17444. steps_count: 7719712.000000\n",
      "ep 2492: ep_len:600 episode reward: total was 10.500000. running mean: -12.584249\n",
      "ep 2492: ep_len:500 episode reward: total was -8.540000. running mean: -12.543807\n",
      "ep 2492: ep_len:505 episode reward: total was -6.980000. running mean: -12.488169\n",
      "ep 2492: ep_len:610 episode reward: total was 1.070000. running mean: -12.352587\n",
      "ep 2492: ep_len:3 episode reward: total was 0.000000. running mean: -12.229061\n",
      "ep 2492: ep_len:560 episode reward: total was -11.210000. running mean: -12.218870\n",
      "ep 2492: ep_len:510 episode reward: total was -15.410000. running mean: -12.250782\n",
      "epsilon:0.059705 episode_count: 17451. steps_count: 7723000.000000\n",
      "ep 2493: ep_len:570 episode reward: total was -18.070000. running mean: -12.308974\n",
      "ep 2493: ep_len:525 episode reward: total was 9.140000. running mean: -12.094484\n",
      "ep 2493: ep_len:610 episode reward: total was -11.130000. running mean: -12.084839\n",
      "ep 2493: ep_len:132 episode reward: total was 1.100000. running mean: -11.952991\n",
      "ep 2493: ep_len:87 episode reward: total was 6.060000. running mean: -11.772861\n",
      "ep 2493: ep_len:500 episode reward: total was 1.240000. running mean: -11.642732\n",
      "ep 2493: ep_len:505 episode reward: total was -25.600000. running mean: -11.782305\n",
      "epsilon:0.059569 episode_count: 17458. steps_count: 7725929.000000\n",
      "ep 2494: ep_len:535 episode reward: total was -23.920000. running mean: -11.903682\n",
      "ep 2494: ep_len:515 episode reward: total was -14.330000. running mean: -11.927945\n",
      "ep 2494: ep_len:670 episode reward: total was -11.190000. running mean: -11.920566\n",
      "ep 2494: ep_len:570 episode reward: total was 4.540000. running mean: -11.755960\n",
      "ep 2494: ep_len:3 episode reward: total was 0.000000. running mean: -11.638400\n",
      "ep 2494: ep_len:600 episode reward: total was -22.870000. running mean: -11.750716\n",
      "ep 2494: ep_len:595 episode reward: total was -9.430000. running mean: -11.727509\n",
      "epsilon:0.059432 episode_count: 17465. steps_count: 7729417.000000\n",
      "ep 2495: ep_len:134 episode reward: total was 1.070000. running mean: -11.599534\n",
      "ep 2495: ep_len:347 episode reward: total was -33.830000. running mean: -11.821839\n",
      "ep 2495: ep_len:585 episode reward: total was -19.950000. running mean: -11.903120\n",
      "ep 2495: ep_len:500 episode reward: total was 3.410000. running mean: -11.749989\n",
      "ep 2495: ep_len:105 episode reward: total was 4.080000. running mean: -11.591689\n",
      "ep 2495: ep_len:500 episode reward: total was 3.450000. running mean: -11.441273\n",
      "ep 2495: ep_len:500 episode reward: total was -17.420000. running mean: -11.501060\n",
      "epsilon:0.059296 episode_count: 17472. steps_count: 7732088.000000\n",
      "ep 2496: ep_len:635 episode reward: total was -22.410000. running mean: -11.610149\n",
      "ep 2496: ep_len:505 episode reward: total was 11.720000. running mean: -11.376848\n",
      "ep 2496: ep_len:845 episode reward: total was -46.680000. running mean: -11.729879\n",
      "ep 2496: ep_len:555 episode reward: total was -40.580000. running mean: -12.018380\n",
      "ep 2496: ep_len:3 episode reward: total was 0.000000. running mean: -11.898197\n",
      "ep 2496: ep_len:500 episode reward: total was 1.020000. running mean: -11.769015\n",
      "ep 2496: ep_len:585 episode reward: total was -7.290000. running mean: -11.724225\n",
      "epsilon:0.059159 episode_count: 17479. steps_count: 7735716.000000\n",
      "ep 2497: ep_len:114 episode reward: total was -1.910000. running mean: -11.626082\n",
      "ep 2497: ep_len:545 episode reward: total was -11.850000. running mean: -11.628321\n",
      "ep 2497: ep_len:600 episode reward: total was -24.370000. running mean: -11.755738\n",
      "ep 2497: ep_len:141 episode reward: total was 0.630000. running mean: -11.631881\n",
      "ep 2497: ep_len:121 episode reward: total was 6.040000. running mean: -11.455162\n",
      "ep 2497: ep_len:500 episode reward: total was 8.630000. running mean: -11.254310\n",
      "ep 2497: ep_len:723 episode reward: total was -59.430000. running mean: -11.736067\n",
      "epsilon:0.059023 episode_count: 17486. steps_count: 7738460.000000\n",
      "ep 2498: ep_len:630 episode reward: total was -25.290000. running mean: -11.871607\n",
      "ep 2498: ep_len:342 episode reward: total was -14.310000. running mean: -11.895991\n",
      "ep 2498: ep_len:575 episode reward: total was -12.250000. running mean: -11.899531\n",
      "ep 2498: ep_len:530 episode reward: total was -3.630000. running mean: -11.816835\n",
      "ep 2498: ep_len:3 episode reward: total was 0.000000. running mean: -11.698667\n",
      "ep 2498: ep_len:175 episode reward: total was 3.610000. running mean: -11.545580\n",
      "ep 2498: ep_len:510 episode reward: total was -37.690000. running mean: -11.807025\n",
      "epsilon:0.058886 episode_count: 17493. steps_count: 7741225.000000\n",
      "ep 2499: ep_len:675 episode reward: total was -24.770000. running mean: -11.936654\n",
      "ep 2499: ep_len:505 episode reward: total was -21.970000. running mean: -12.036988\n",
      "ep 2499: ep_len:540 episode reward: total was -9.940000. running mean: -12.016018\n",
      "ep 2499: ep_len:515 episode reward: total was -26.650000. running mean: -12.162358\n",
      "ep 2499: ep_len:53 episode reward: total was 3.500000. running mean: -12.005734\n",
      "ep 2499: ep_len:530 episode reward: total was -9.050000. running mean: -11.976177\n",
      "ep 2499: ep_len:575 episode reward: total was -36.450000. running mean: -12.220915\n",
      "epsilon:0.058750 episode_count: 17500. steps_count: 7744618.000000\n",
      "ep 2500: ep_len:620 episode reward: total was -22.080000. running mean: -12.319506\n",
      "ep 2500: ep_len:500 episode reward: total was 14.710000. running mean: -12.049211\n",
      "ep 2500: ep_len:515 episode reward: total was -4.780000. running mean: -11.976519\n",
      "ep 2500: ep_len:515 episode reward: total was -17.530000. running mean: -12.032053\n",
      "ep 2500: ep_len:126 episode reward: total was 4.060000. running mean: -11.871133\n",
      "ep 2500: ep_len:500 episode reward: total was -11.590000. running mean: -11.868322\n",
      "ep 2500: ep_len:550 episode reward: total was -26.100000. running mean: -12.010638\n",
      "epsilon:0.058613 episode_count: 17507. steps_count: 7747944.000000\n",
      "ep 2501: ep_len:134 episode reward: total was -12.920000. running mean: -12.019732\n",
      "ep 2501: ep_len:500 episode reward: total was -27.990000. running mean: -12.179435\n",
      "ep 2501: ep_len:500 episode reward: total was -4.150000. running mean: -12.099140\n",
      "ep 2501: ep_len:595 episode reward: total was -7.040000. running mean: -12.048549\n",
      "ep 2501: ep_len:50 episode reward: total was 2.000000. running mean: -11.908063\n",
      "ep 2501: ep_len:500 episode reward: total was 1.390000. running mean: -11.775083\n",
      "ep 2501: ep_len:316 episode reward: total was -8.810000. running mean: -11.745432\n",
      "epsilon:0.058477 episode_count: 17514. steps_count: 7750539.000000\n",
      "ep 2502: ep_len:500 episode reward: total was 1.210000. running mean: -11.615878\n",
      "ep 2502: ep_len:500 episode reward: total was 2.240000. running mean: -11.477319\n",
      "ep 2502: ep_len:570 episode reward: total was -17.250000. running mean: -11.535046\n",
      "ep 2502: ep_len:510 episode reward: total was 3.490000. running mean: -11.384795\n",
      "ep 2502: ep_len:49 episode reward: total was 4.500000. running mean: -11.225947\n",
      "ep 2502: ep_len:500 episode reward: total was -22.930000. running mean: -11.342988\n",
      "ep 2502: ep_len:510 episode reward: total was -19.470000. running mean: -11.424258\n",
      "epsilon:0.058340 episode_count: 17521. steps_count: 7753678.000000\n",
      "ep 2503: ep_len:580 episode reward: total was -6.580000. running mean: -11.375815\n",
      "ep 2503: ep_len:500 episode reward: total was -5.360000. running mean: -11.315657\n",
      "ep 2503: ep_len:655 episode reward: total was -12.200000. running mean: -11.324501\n",
      "ep 2503: ep_len:500 episode reward: total was -18.640000. running mean: -11.397656\n",
      "ep 2503: ep_len:3 episode reward: total was 0.000000. running mean: -11.283679\n",
      "ep 2503: ep_len:311 episode reward: total was 1.150000. running mean: -11.159342\n",
      "ep 2503: ep_len:510 episode reward: total was -13.410000. running mean: -11.181849\n",
      "epsilon:0.058204 episode_count: 17528. steps_count: 7756737.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2504: ep_len:192 episode reward: total was -1.860000. running mean: -11.088630\n",
      "ep 2504: ep_len:570 episode reward: total was -5.830000. running mean: -11.036044\n",
      "ep 2504: ep_len:615 episode reward: total was -8.950000. running mean: -11.015184\n",
      "ep 2504: ep_len:560 episode reward: total was -3.110000. running mean: -10.936132\n",
      "ep 2504: ep_len:3 episode reward: total was 0.000000. running mean: -10.826770\n",
      "ep 2504: ep_len:515 episode reward: total was 8.940000. running mean: -10.629103\n",
      "ep 2504: ep_len:500 episode reward: total was -6.560000. running mean: -10.588412\n",
      "epsilon:0.058067 episode_count: 17535. steps_count: 7759692.000000\n",
      "ep 2505: ep_len:575 episode reward: total was -16.500000. running mean: -10.647528\n",
      "ep 2505: ep_len:500 episode reward: total was -34.370000. running mean: -10.884752\n",
      "ep 2505: ep_len:630 episode reward: total was -17.300000. running mean: -10.948905\n",
      "ep 2505: ep_len:610 episode reward: total was -1.930000. running mean: -10.858716\n",
      "ep 2505: ep_len:90 episode reward: total was 2.550000. running mean: -10.724629\n",
      "ep 2505: ep_len:540 episode reward: total was -4.380000. running mean: -10.661182\n",
      "ep 2505: ep_len:210 episode reward: total was -11.400000. running mean: -10.668571\n",
      "epsilon:0.057931 episode_count: 17542. steps_count: 7762847.000000\n",
      "ep 2506: ep_len:625 episode reward: total was 0.030000. running mean: -10.561585\n",
      "ep 2506: ep_len:364 episode reward: total was -22.890000. running mean: -10.684869\n",
      "ep 2506: ep_len:545 episode reward: total was -6.610000. running mean: -10.644120\n",
      "ep 2506: ep_len:372 episode reward: total was -28.170000. running mean: -10.819379\n",
      "ep 2506: ep_len:97 episode reward: total was 6.050000. running mean: -10.650685\n",
      "ep 2506: ep_len:515 episode reward: total was -15.310000. running mean: -10.697278\n",
      "ep 2506: ep_len:510 episode reward: total was -10.120000. running mean: -10.691506\n",
      "epsilon:0.057794 episode_count: 17549. steps_count: 7765875.000000\n",
      "ep 2507: ep_len:515 episode reward: total was 3.730000. running mean: -10.547291\n",
      "ep 2507: ep_len:530 episode reward: total was -5.130000. running mean: -10.493118\n",
      "ep 2507: ep_len:385 episode reward: total was -11.800000. running mean: -10.506187\n",
      "ep 2507: ep_len:500 episode reward: total was -13.030000. running mean: -10.531425\n",
      "ep 2507: ep_len:3 episode reward: total was 0.000000. running mean: -10.426110\n",
      "ep 2507: ep_len:635 episode reward: total was -4.630000. running mean: -10.368149\n",
      "ep 2507: ep_len:545 episode reward: total was -11.060000. running mean: -10.375068\n",
      "epsilon:0.057658 episode_count: 17556. steps_count: 7768988.000000\n",
      "ep 2508: ep_len:580 episode reward: total was -2.660000. running mean: -10.297917\n",
      "ep 2508: ep_len:540 episode reward: total was -7.070000. running mean: -10.265638\n",
      "ep 2508: ep_len:590 episode reward: total was -3.640000. running mean: -10.199382\n",
      "ep 2508: ep_len:590 episode reward: total was 0.570000. running mean: -10.091688\n",
      "ep 2508: ep_len:3 episode reward: total was 0.000000. running mean: -9.990771\n",
      "ep 2508: ep_len:585 episode reward: total was -5.060000. running mean: -9.941463\n",
      "ep 2508: ep_len:590 episode reward: total was -4.470000. running mean: -9.886749\n",
      "epsilon:0.057521 episode_count: 17563. steps_count: 7772466.000000\n",
      "ep 2509: ep_len:206 episode reward: total was -3.880000. running mean: -9.826681\n",
      "ep 2509: ep_len:545 episode reward: total was -19.670000. running mean: -9.925114\n",
      "ep 2509: ep_len:760 episode reward: total was -52.190000. running mean: -10.347763\n",
      "ep 2509: ep_len:510 episode reward: total was -2.620000. running mean: -10.270485\n",
      "ep 2509: ep_len:3 episode reward: total was 0.000000. running mean: -10.167781\n",
      "ep 2509: ep_len:640 episode reward: total was -7.760000. running mean: -10.143703\n",
      "ep 2509: ep_len:575 episode reward: total was -21.880000. running mean: -10.261066\n",
      "epsilon:0.057385 episode_count: 17570. steps_count: 7775705.000000\n",
      "ep 2510: ep_len:500 episode reward: total was 4.750000. running mean: -10.110955\n",
      "ep 2510: ep_len:575 episode reward: total was -7.340000. running mean: -10.083246\n",
      "ep 2510: ep_len:78 episode reward: total was 1.050000. running mean: -9.971913\n",
      "ep 2510: ep_len:525 episode reward: total was 3.450000. running mean: -9.837694\n",
      "ep 2510: ep_len:3 episode reward: total was 0.000000. running mean: -9.739317\n",
      "ep 2510: ep_len:510 episode reward: total was -36.560000. running mean: -10.007524\n",
      "ep 2510: ep_len:310 episode reward: total was -12.820000. running mean: -10.035649\n",
      "epsilon:0.057248 episode_count: 17577. steps_count: 7778206.000000\n",
      "ep 2511: ep_len:134 episode reward: total was 4.590000. running mean: -9.889392\n",
      "ep 2511: ep_len:635 episode reward: total was -19.040000. running mean: -9.980898\n",
      "ep 2511: ep_len:500 episode reward: total was -10.540000. running mean: -9.986489\n",
      "ep 2511: ep_len:381 episode reward: total was -7.650000. running mean: -9.963124\n",
      "ep 2511: ep_len:120 episode reward: total was -8.940000. running mean: -9.952893\n",
      "ep 2511: ep_len:515 episode reward: total was -27.730000. running mean: -10.130664\n",
      "ep 2511: ep_len:530 episode reward: total was -29.590000. running mean: -10.325258\n",
      "epsilon:0.057112 episode_count: 17584. steps_count: 7781021.000000\n",
      "ep 2512: ep_len:580 episode reward: total was -23.230000. running mean: -10.454305\n",
      "ep 2512: ep_len:181 episode reward: total was 1.120000. running mean: -10.338562\n",
      "ep 2512: ep_len:585 episode reward: total was -2.060000. running mean: -10.255776\n",
      "ep 2512: ep_len:500 episode reward: total was -29.560000. running mean: -10.448819\n",
      "ep 2512: ep_len:50 episode reward: total was 3.010000. running mean: -10.314230\n",
      "ep 2512: ep_len:500 episode reward: total was -6.780000. running mean: -10.278888\n",
      "ep 2512: ep_len:560 episode reward: total was -19.870000. running mean: -10.374799\n",
      "epsilon:0.056975 episode_count: 17591. steps_count: 7783977.000000\n",
      "ep 2513: ep_len:710 episode reward: total was -42.700000. running mean: -10.698051\n",
      "ep 2513: ep_len:550 episode reward: total was -22.790000. running mean: -10.818971\n",
      "ep 2513: ep_len:515 episode reward: total was -7.580000. running mean: -10.786581\n",
      "ep 2513: ep_len:500 episode reward: total was 9.420000. running mean: -10.584515\n",
      "ep 2513: ep_len:3 episode reward: total was 0.000000. running mean: -10.478670\n",
      "ep 2513: ep_len:675 episode reward: total was -1.170000. running mean: -10.385583\n",
      "ep 2513: ep_len:305 episode reward: total was -10.830000. running mean: -10.390027\n",
      "epsilon:0.056839 episode_count: 17598. steps_count: 7787235.000000\n",
      "ep 2514: ep_len:580 episode reward: total was 2.040000. running mean: -10.265727\n",
      "ep 2514: ep_len:525 episode reward: total was 6.820000. running mean: -10.094870\n",
      "ep 2514: ep_len:570 episode reward: total was -19.080000. running mean: -10.184721\n",
      "ep 2514: ep_len:550 episode reward: total was -3.560000. running mean: -10.118474\n",
      "ep 2514: ep_len:3 episode reward: total was 0.000000. running mean: -10.017289\n",
      "ep 2514: ep_len:500 episode reward: total was -1.710000. running mean: -9.934216\n",
      "ep 2514: ep_len:500 episode reward: total was -23.990000. running mean: -10.074774\n",
      "epsilon:0.056702 episode_count: 17605. steps_count: 7790463.000000\n",
      "ep 2515: ep_len:635 episode reward: total was -2.830000. running mean: -10.002326\n",
      "ep 2515: ep_len:323 episode reward: total was -16.340000. running mean: -10.065703\n",
      "ep 2515: ep_len:500 episode reward: total was -4.000000. running mean: -10.005046\n",
      "ep 2515: ep_len:132 episode reward: total was -0.430000. running mean: -9.909296\n",
      "ep 2515: ep_len:3 episode reward: total was 0.000000. running mean: -9.810203\n",
      "ep 2515: ep_len:625 episode reward: total was -3.210000. running mean: -9.744201\n",
      "ep 2515: ep_len:570 episode reward: total was -30.870000. running mean: -9.955459\n",
      "epsilon:0.056566 episode_count: 17612. steps_count: 7793251.000000\n",
      "ep 2516: ep_len:595 episode reward: total was 2.970000. running mean: -9.826204\n",
      "ep 2516: ep_len:505 episode reward: total was -5.460000. running mean: -9.782542\n",
      "ep 2516: ep_len:615 episode reward: total was -32.860000. running mean: -10.013317\n",
      "ep 2516: ep_len:880 episode reward: total was -99.800000. running mean: -10.911183\n",
      "ep 2516: ep_len:3 episode reward: total was 0.000000. running mean: -10.802072\n",
      "ep 2516: ep_len:530 episode reward: total was -13.030000. running mean: -10.824351\n",
      "ep 2516: ep_len:525 episode reward: total was -22.500000. running mean: -10.941107\n",
      "epsilon:0.056429 episode_count: 17619. steps_count: 7796904.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2517: ep_len:210 episode reward: total was 7.640000. running mean: -10.755296\n",
      "ep 2517: ep_len:600 episode reward: total was -4.620000. running mean: -10.693943\n",
      "ep 2517: ep_len:660 episode reward: total was -2.900000. running mean: -10.616004\n",
      "ep 2517: ep_len:500 episode reward: total was 6.410000. running mean: -10.445744\n",
      "ep 2517: ep_len:3 episode reward: total was 0.000000. running mean: -10.341286\n",
      "ep 2517: ep_len:610 episode reward: total was -7.360000. running mean: -10.311474\n",
      "ep 2517: ep_len:172 episode reward: total was -2.880000. running mean: -10.237159\n",
      "epsilon:0.056293 episode_count: 17626. steps_count: 7799659.000000\n",
      "ep 2518: ep_len:183 episode reward: total was 3.120000. running mean: -10.103587\n",
      "ep 2518: ep_len:590 episode reward: total was -1.430000. running mean: -10.016851\n",
      "ep 2518: ep_len:645 episode reward: total was -22.900000. running mean: -10.145683\n",
      "ep 2518: ep_len:170 episode reward: total was 1.650000. running mean: -10.027726\n",
      "ep 2518: ep_len:126 episode reward: total was -16.950000. running mean: -10.096949\n",
      "ep 2518: ep_len:590 episode reward: total was -20.350000. running mean: -10.199479\n",
      "ep 2518: ep_len:620 episode reward: total was -25.830000. running mean: -10.355785\n",
      "epsilon:0.056156 episode_count: 17633. steps_count: 7802583.000000\n",
      "ep 2519: ep_len:865 episode reward: total was -57.230000. running mean: -10.824527\n",
      "ep 2519: ep_len:535 episode reward: total was -22.830000. running mean: -10.944581\n",
      "ep 2519: ep_len:63 episode reward: total was 3.070000. running mean: -10.804436\n",
      "ep 2519: ep_len:154 episode reward: total was 1.110000. running mean: -10.685291\n",
      "ep 2519: ep_len:3 episode reward: total was 0.000000. running mean: -10.578438\n",
      "ep 2519: ep_len:605 episode reward: total was -4.570000. running mean: -10.518354\n",
      "ep 2519: ep_len:565 episode reward: total was -9.570000. running mean: -10.508870\n",
      "epsilon:0.056020 episode_count: 17640. steps_count: 7805373.000000\n",
      "ep 2520: ep_len:500 episode reward: total was -58.370000. running mean: -10.987482\n",
      "ep 2520: ep_len:575 episode reward: total was 9.410000. running mean: -10.783507\n",
      "ep 2520: ep_len:362 episode reward: total was -0.790000. running mean: -10.683572\n",
      "ep 2520: ep_len:500 episode reward: total was -10.490000. running mean: -10.681636\n",
      "ep 2520: ep_len:3 episode reward: total was 0.000000. running mean: -10.574820\n",
      "ep 2520: ep_len:160 episode reward: total was 2.600000. running mean: -10.443072\n",
      "ep 2520: ep_len:500 episode reward: total was -17.040000. running mean: -10.509041\n",
      "epsilon:0.055883 episode_count: 17647. steps_count: 7807973.000000\n",
      "ep 2521: ep_len:600 episode reward: total was 2.510000. running mean: -10.378850\n",
      "ep 2521: ep_len:545 episode reward: total was 7.450000. running mean: -10.200562\n",
      "ep 2521: ep_len:398 episode reward: total was -19.300000. running mean: -10.291556\n",
      "ep 2521: ep_len:575 episode reward: total was -15.880000. running mean: -10.347441\n",
      "ep 2521: ep_len:3 episode reward: total was 0.000000. running mean: -10.243966\n",
      "ep 2521: ep_len:535 episode reward: total was -2.260000. running mean: -10.164127\n",
      "ep 2521: ep_len:500 episode reward: total was -48.800000. running mean: -10.550485\n",
      "epsilon:0.055747 episode_count: 17654. steps_count: 7811129.000000\n",
      "ep 2522: ep_len:540 episode reward: total was -25.940000. running mean: -10.704381\n",
      "ep 2522: ep_len:161 episode reward: total was -9.450000. running mean: -10.691837\n",
      "ep 2522: ep_len:555 episode reward: total was -7.730000. running mean: -10.662218\n",
      "ep 2522: ep_len:89 episode reward: total was 2.050000. running mean: -10.535096\n",
      "ep 2522: ep_len:100 episode reward: total was 5.050000. running mean: -10.379245\n",
      "ep 2522: ep_len:540 episode reward: total was -30.920000. running mean: -10.584653\n",
      "ep 2522: ep_len:565 episode reward: total was -18.980000. running mean: -10.668606\n",
      "epsilon:0.055610 episode_count: 17661. steps_count: 7813679.000000\n",
      "ep 2523: ep_len:570 episode reward: total was -18.570000. running mean: -10.747620\n",
      "ep 2523: ep_len:368 episode reward: total was -40.320000. running mean: -11.043344\n",
      "ep 2523: ep_len:555 episode reward: total was 5.980000. running mean: -10.873111\n",
      "ep 2523: ep_len:500 episode reward: total was -18.050000. running mean: -10.944879\n",
      "ep 2523: ep_len:3 episode reward: total was 0.000000. running mean: -10.835431\n",
      "ep 2523: ep_len:530 episode reward: total was 9.620000. running mean: -10.630876\n",
      "ep 2523: ep_len:570 episode reward: total was -39.520000. running mean: -10.919768\n",
      "epsilon:0.055474 episode_count: 17668. steps_count: 7816775.000000\n",
      "ep 2524: ep_len:265 episode reward: total was 2.120000. running mean: -10.789370\n",
      "ep 2524: ep_len:590 episode reward: total was 13.880000. running mean: -10.542676\n",
      "ep 2524: ep_len:690 episode reward: total was -10.690000. running mean: -10.544149\n",
      "ep 2524: ep_len:56 episode reward: total was -0.460000. running mean: -10.443308\n",
      "ep 2524: ep_len:101 episode reward: total was -0.460000. running mean: -10.343475\n",
      "ep 2524: ep_len:720 episode reward: total was -72.580000. running mean: -10.965840\n",
      "ep 2524: ep_len:575 episode reward: total was -11.730000. running mean: -10.973482\n",
      "epsilon:0.055337 episode_count: 17675. steps_count: 7819772.000000\n",
      "ep 2525: ep_len:222 episode reward: total was -3.410000. running mean: -10.897847\n",
      "ep 2525: ep_len:725 episode reward: total was -33.810000. running mean: -11.126968\n",
      "ep 2525: ep_len:500 episode reward: total was 9.990000. running mean: -10.915799\n",
      "ep 2525: ep_len:505 episode reward: total was 3.920000. running mean: -10.767441\n",
      "ep 2525: ep_len:3 episode reward: total was 0.000000. running mean: -10.659766\n",
      "ep 2525: ep_len:595 episode reward: total was -30.270000. running mean: -10.855869\n",
      "ep 2525: ep_len:540 episode reward: total was -17.830000. running mean: -10.925610\n",
      "epsilon:0.055201 episode_count: 17682. steps_count: 7822862.000000\n",
      "ep 2526: ep_len:500 episode reward: total was -0.080000. running mean: -10.817154\n",
      "ep 2526: ep_len:510 episode reward: total was 6.300000. running mean: -10.645982\n",
      "ep 2526: ep_len:570 episode reward: total was -4.520000. running mean: -10.584723\n",
      "ep 2526: ep_len:500 episode reward: total was -0.130000. running mean: -10.480175\n",
      "ep 2526: ep_len:3 episode reward: total was 0.000000. running mean: -10.375374\n",
      "ep 2526: ep_len:500 episode reward: total was -14.280000. running mean: -10.414420\n",
      "ep 2526: ep_len:580 episode reward: total was -14.890000. running mean: -10.459176\n",
      "epsilon:0.055064 episode_count: 17689. steps_count: 7826025.000000\n",
      "ep 2527: ep_len:610 episode reward: total was 5.430000. running mean: -10.300284\n",
      "ep 2527: ep_len:525 episode reward: total was -16.480000. running mean: -10.362081\n",
      "ep 2527: ep_len:520 episode reward: total was -48.030000. running mean: -10.738760\n",
      "ep 2527: ep_len:622 episode reward: total was -31.930000. running mean: -10.950673\n",
      "ep 2527: ep_len:3 episode reward: total was 0.000000. running mean: -10.841166\n",
      "ep 2527: ep_len:580 episode reward: total was -3.520000. running mean: -10.767954\n",
      "ep 2527: ep_len:600 episode reward: total was -19.110000. running mean: -10.851375\n",
      "epsilon:0.054928 episode_count: 17696. steps_count: 7829485.000000\n",
      "ep 2528: ep_len:740 episode reward: total was -27.750000. running mean: -11.020361\n",
      "ep 2528: ep_len:500 episode reward: total was 4.120000. running mean: -10.868957\n",
      "ep 2528: ep_len:435 episode reward: total was -2.800000. running mean: -10.788268\n",
      "ep 2528: ep_len:525 episode reward: total was -14.070000. running mean: -10.821085\n",
      "ep 2528: ep_len:3 episode reward: total was 0.000000. running mean: -10.712874\n",
      "ep 2528: ep_len:231 episode reward: total was 4.590000. running mean: -10.559845\n",
      "ep 2528: ep_len:500 episode reward: total was -17.670000. running mean: -10.630947\n",
      "epsilon:0.054791 episode_count: 17703. steps_count: 7832419.000000\n",
      "ep 2529: ep_len:111 episode reward: total was -2.940000. running mean: -10.554038\n",
      "ep 2529: ep_len:535 episode reward: total was 12.880000. running mean: -10.319697\n",
      "ep 2529: ep_len:620 episode reward: total was -30.740000. running mean: -10.523900\n",
      "ep 2529: ep_len:690 episode reward: total was -53.080000. running mean: -10.949461\n",
      "ep 2529: ep_len:3 episode reward: total was 0.000000. running mean: -10.839967\n",
      "ep 2529: ep_len:525 episode reward: total was 9.560000. running mean: -10.635967\n",
      "ep 2529: ep_len:500 episode reward: total was -12.450000. running mean: -10.654107\n",
      "epsilon:0.054655 episode_count: 17710. steps_count: 7835403.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2530: ep_len:134 episode reward: total was -1.920000. running mean: -10.566766\n",
      "ep 2530: ep_len:500 episode reward: total was -1.890000. running mean: -10.479999\n",
      "ep 2530: ep_len:500 episode reward: total was -18.510000. running mean: -10.560299\n",
      "ep 2530: ep_len:130 episode reward: total was 1.100000. running mean: -10.443696\n",
      "ep 2530: ep_len:2 episode reward: total was 0.000000. running mean: -10.339259\n",
      "ep 2530: ep_len:250 episode reward: total was 2.610000. running mean: -10.209766\n",
      "ep 2530: ep_len:545 episode reward: total was -22.100000. running mean: -10.328668\n",
      "epsilon:0.054518 episode_count: 17717. steps_count: 7837464.000000\n",
      "ep 2531: ep_len:207 episode reward: total was -19.840000. running mean: -10.423782\n",
      "ep 2531: ep_len:565 episode reward: total was -8.490000. running mean: -10.404444\n",
      "ep 2531: ep_len:590 episode reward: total was -13.310000. running mean: -10.433499\n",
      "ep 2531: ep_len:56 episode reward: total was -5.950000. running mean: -10.388664\n",
      "ep 2531: ep_len:3 episode reward: total was 0.000000. running mean: -10.284778\n",
      "ep 2531: ep_len:640 episode reward: total was -11.850000. running mean: -10.300430\n",
      "ep 2531: ep_len:500 episode reward: total was -27.950000. running mean: -10.476926\n",
      "epsilon:0.054382 episode_count: 17724. steps_count: 7840025.000000\n",
      "ep 2532: ep_len:213 episode reward: total was -6.930000. running mean: -10.441456\n",
      "ep 2532: ep_len:640 episode reward: total was -34.850000. running mean: -10.685542\n",
      "ep 2532: ep_len:590 episode reward: total was -18.740000. running mean: -10.766086\n",
      "ep 2532: ep_len:610 episode reward: total was -3.550000. running mean: -10.693926\n",
      "ep 2532: ep_len:81 episode reward: total was -0.960000. running mean: -10.596586\n",
      "ep 2532: ep_len:600 episode reward: total was -49.160000. running mean: -10.982220\n",
      "ep 2532: ep_len:585 episode reward: total was -13.570000. running mean: -11.008098\n",
      "epsilon:0.054245 episode_count: 17731. steps_count: 7843344.000000\n",
      "ep 2533: ep_len:208 episode reward: total was 3.130000. running mean: -10.866717\n",
      "ep 2533: ep_len:580 episode reward: total was -10.580000. running mean: -10.863850\n",
      "ep 2533: ep_len:455 episode reward: total was -2.270000. running mean: -10.777912\n",
      "ep 2533: ep_len:500 episode reward: total was -23.070000. running mean: -10.900833\n",
      "ep 2533: ep_len:3 episode reward: total was 0.000000. running mean: -10.791824\n",
      "ep 2533: ep_len:174 episode reward: total was 1.580000. running mean: -10.668106\n",
      "ep 2533: ep_len:555 episode reward: total was -3.340000. running mean: -10.594825\n",
      "epsilon:0.054109 episode_count: 17738. steps_count: 7845819.000000\n",
      "ep 2534: ep_len:585 episode reward: total was 0.100000. running mean: -10.487877\n",
      "ep 2534: ep_len:500 episode reward: total was -7.960000. running mean: -10.462598\n",
      "ep 2534: ep_len:635 episode reward: total was -4.960000. running mean: -10.407572\n",
      "ep 2534: ep_len:505 episode reward: total was -3.120000. running mean: -10.334696\n",
      "ep 2534: ep_len:103 episode reward: total was -0.450000. running mean: -10.235849\n",
      "ep 2534: ep_len:735 episode reward: total was -16.200000. running mean: -10.295491\n",
      "ep 2534: ep_len:505 episode reward: total was -7.070000. running mean: -10.263236\n",
      "epsilon:0.053972 episode_count: 17745. steps_count: 7849387.000000\n",
      "ep 2535: ep_len:540 episode reward: total was -14.970000. running mean: -10.310303\n",
      "ep 2535: ep_len:555 episode reward: total was 15.880000. running mean: -10.048400\n",
      "ep 2535: ep_len:550 episode reward: total was -21.590000. running mean: -10.163816\n",
      "ep 2535: ep_len:560 episode reward: total was 1.930000. running mean: -10.042878\n",
      "ep 2535: ep_len:3 episode reward: total was 0.000000. running mean: -9.942449\n",
      "ep 2535: ep_len:675 episode reward: total was -59.460000. running mean: -10.437625\n",
      "ep 2535: ep_len:570 episode reward: total was -10.890000. running mean: -10.442149\n",
      "epsilon:0.053836 episode_count: 17752. steps_count: 7852840.000000\n",
      "ep 2536: ep_len:530 episode reward: total was -27.940000. running mean: -10.617127\n",
      "ep 2536: ep_len:860 episode reward: total was -40.680000. running mean: -10.917756\n",
      "ep 2536: ep_len:660 episode reward: total was -12.740000. running mean: -10.935978\n",
      "ep 2536: ep_len:500 episode reward: total was -7.430000. running mean: -10.900919\n",
      "ep 2536: ep_len:128 episode reward: total was 5.070000. running mean: -10.741209\n",
      "ep 2536: ep_len:525 episode reward: total was -13.520000. running mean: -10.768997\n",
      "ep 2536: ep_len:590 episode reward: total was -17.620000. running mean: -10.837507\n",
      "epsilon:0.053699 episode_count: 17759. steps_count: 7856633.000000\n",
      "ep 2537: ep_len:580 episode reward: total was 4.500000. running mean: -10.684132\n",
      "ep 2537: ep_len:500 episode reward: total was 8.190000. running mean: -10.495391\n",
      "ep 2537: ep_len:500 episode reward: total was -2.660000. running mean: -10.417037\n",
      "ep 2537: ep_len:385 episode reward: total was -29.640000. running mean: -10.609267\n",
      "ep 2537: ep_len:3 episode reward: total was 0.000000. running mean: -10.503174\n",
      "ep 2537: ep_len:500 episode reward: total was -22.380000. running mean: -10.621942\n",
      "ep 2537: ep_len:575 episode reward: total was -1.280000. running mean: -10.528523\n",
      "epsilon:0.053563 episode_count: 17766. steps_count: 7859676.000000\n",
      "ep 2538: ep_len:223 episode reward: total was -15.830000. running mean: -10.581538\n",
      "ep 2538: ep_len:175 episode reward: total was -12.440000. running mean: -10.600122\n",
      "ep 2538: ep_len:500 episode reward: total was -22.440000. running mean: -10.718521\n",
      "ep 2538: ep_len:570 episode reward: total was 3.820000. running mean: -10.573136\n",
      "ep 2538: ep_len:78 episode reward: total was 1.540000. running mean: -10.452004\n",
      "ep 2538: ep_len:530 episode reward: total was -6.280000. running mean: -10.410284\n",
      "ep 2538: ep_len:590 episode reward: total was -4.760000. running mean: -10.353782\n",
      "epsilon:0.053426 episode_count: 17773. steps_count: 7862342.000000\n",
      "ep 2539: ep_len:500 episode reward: total was -14.920000. running mean: -10.399444\n",
      "ep 2539: ep_len:530 episode reward: total was 14.220000. running mean: -10.153249\n",
      "ep 2539: ep_len:570 episode reward: total was -2.070000. running mean: -10.072417\n",
      "ep 2539: ep_len:402 episode reward: total was -28.140000. running mean: -10.253093\n",
      "ep 2539: ep_len:3 episode reward: total was 0.000000. running mean: -10.150562\n",
      "ep 2539: ep_len:615 episode reward: total was -15.050000. running mean: -10.199556\n",
      "ep 2539: ep_len:500 episode reward: total was -33.550000. running mean: -10.433061\n",
      "epsilon:0.053290 episode_count: 17780. steps_count: 7865462.000000\n",
      "ep 2540: ep_len:520 episode reward: total was -2.370000. running mean: -10.352430\n",
      "ep 2540: ep_len:545 episode reward: total was 7.100000. running mean: -10.177906\n",
      "ep 2540: ep_len:353 episode reward: total was 7.730000. running mean: -9.998827\n",
      "ep 2540: ep_len:56 episode reward: total was -3.440000. running mean: -9.933238\n",
      "ep 2540: ep_len:3 episode reward: total was 0.000000. running mean: -9.833906\n",
      "ep 2540: ep_len:252 episode reward: total was 4.630000. running mean: -9.689267\n",
      "ep 2540: ep_len:500 episode reward: total was -20.560000. running mean: -9.797974\n",
      "epsilon:0.053153 episode_count: 17787. steps_count: 7867691.000000\n",
      "ep 2541: ep_len:630 episode reward: total was -21.250000. running mean: -9.912494\n",
      "ep 2541: ep_len:610 episode reward: total was -13.070000. running mean: -9.944070\n",
      "ep 2541: ep_len:500 episode reward: total was -7.120000. running mean: -9.915829\n",
      "ep 2541: ep_len:390 episode reward: total was -37.190000. running mean: -10.188571\n",
      "ep 2541: ep_len:3 episode reward: total was 0.000000. running mean: -10.086685\n",
      "ep 2541: ep_len:298 episode reward: total was -3.370000. running mean: -10.019518\n",
      "ep 2541: ep_len:505 episode reward: total was -23.030000. running mean: -10.149623\n",
      "epsilon:0.053017 episode_count: 17794. steps_count: 7870627.000000\n",
      "ep 2542: ep_len:535 episode reward: total was -14.840000. running mean: -10.196527\n",
      "ep 2542: ep_len:500 episode reward: total was 2.150000. running mean: -10.073061\n",
      "ep 2542: ep_len:605 episode reward: total was -30.990000. running mean: -10.282231\n",
      "ep 2542: ep_len:575 episode reward: total was -18.360000. running mean: -10.363008\n",
      "ep 2542: ep_len:119 episode reward: total was 0.030000. running mean: -10.259078\n",
      "ep 2542: ep_len:520 episode reward: total was -34.950000. running mean: -10.505988\n",
      "ep 2542: ep_len:590 episode reward: total was -17.380000. running mean: -10.574728\n",
      "epsilon:0.052880 episode_count: 17801. steps_count: 7874071.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2543: ep_len:233 episode reward: total was 4.590000. running mean: -10.423080\n",
      "ep 2543: ep_len:570 episode reward: total was -29.640000. running mean: -10.615250\n",
      "ep 2543: ep_len:366 episode reward: total was 5.220000. running mean: -10.456897\n",
      "ep 2543: ep_len:542 episode reward: total was -3.400000. running mean: -10.386328\n",
      "ep 2543: ep_len:47 episode reward: total was -1.000000. running mean: -10.292465\n",
      "ep 2543: ep_len:590 episode reward: total was 1.470000. running mean: -10.174840\n",
      "ep 2543: ep_len:193 episode reward: total was -10.920000. running mean: -10.182292\n",
      "epsilon:0.052744 episode_count: 17808. steps_count: 7876612.000000\n",
      "ep 2544: ep_len:500 episode reward: total was -13.370000. running mean: -10.214169\n",
      "ep 2544: ep_len:500 episode reward: total was 2.670000. running mean: -10.085327\n",
      "ep 2544: ep_len:434 episode reward: total was -12.800000. running mean: -10.112474\n",
      "ep 2544: ep_len:525 episode reward: total was 1.410000. running mean: -9.997249\n",
      "ep 2544: ep_len:3 episode reward: total was 0.000000. running mean: -9.897277\n",
      "ep 2544: ep_len:605 episode reward: total was -22.920000. running mean: -10.027504\n",
      "ep 2544: ep_len:194 episode reward: total was -3.340000. running mean: -9.960629\n",
      "epsilon:0.052607 episode_count: 17815. steps_count: 7879373.000000\n",
      "ep 2545: ep_len:251 episode reward: total was -17.800000. running mean: -10.039023\n",
      "ep 2545: ep_len:500 episode reward: total was -20.770000. running mean: -10.146332\n",
      "ep 2545: ep_len:451 episode reward: total was 0.230000. running mean: -10.042569\n",
      "ep 2545: ep_len:143 episode reward: total was -2.900000. running mean: -9.971143\n",
      "ep 2545: ep_len:127 episode reward: total was 7.060000. running mean: -9.800832\n",
      "ep 2545: ep_len:287 episode reward: total was -4.370000. running mean: -9.746524\n",
      "ep 2545: ep_len:580 episode reward: total was -35.240000. running mean: -10.001458\n",
      "epsilon:0.052471 episode_count: 17822. steps_count: 7881712.000000\n",
      "ep 2546: ep_len:570 episode reward: total was -8.480000. running mean: -9.986244\n",
      "ep 2546: ep_len:500 episode reward: total was 0.830000. running mean: -9.878081\n",
      "ep 2546: ep_len:600 episode reward: total was -42.470000. running mean: -10.204001\n",
      "ep 2546: ep_len:540 episode reward: total was 8.430000. running mean: -10.017661\n",
      "ep 2546: ep_len:84 episode reward: total was 4.520000. running mean: -9.872284\n",
      "ep 2546: ep_len:525 episode reward: total was -12.460000. running mean: -9.898161\n",
      "ep 2546: ep_len:565 episode reward: total was -18.110000. running mean: -9.980279\n",
      "epsilon:0.052334 episode_count: 17829. steps_count: 7885096.000000\n",
      "ep 2547: ep_len:630 episode reward: total was -14.730000. running mean: -10.027777\n",
      "ep 2547: ep_len:500 episode reward: total was -26.260000. running mean: -10.190099\n",
      "ep 2547: ep_len:605 episode reward: total was -8.780000. running mean: -10.175998\n",
      "ep 2547: ep_len:500 episode reward: total was -8.980000. running mean: -10.164038\n",
      "ep 2547: ep_len:108 episode reward: total was -9.460000. running mean: -10.156998\n",
      "ep 2547: ep_len:221 episode reward: total was 4.630000. running mean: -10.009128\n",
      "ep 2547: ep_len:605 episode reward: total was -23.410000. running mean: -10.143136\n",
      "epsilon:0.052198 episode_count: 17836. steps_count: 7888265.000000\n",
      "ep 2548: ep_len:134 episode reward: total was 2.570000. running mean: -10.016005\n",
      "ep 2548: ep_len:168 episode reward: total was -3.900000. running mean: -9.954845\n",
      "ep 2548: ep_len:640 episode reward: total was -36.350000. running mean: -10.218796\n",
      "ep 2548: ep_len:510 episode reward: total was -18.490000. running mean: -10.301508\n",
      "ep 2548: ep_len:87 episode reward: total was -9.960000. running mean: -10.298093\n",
      "ep 2548: ep_len:220 episode reward: total was 5.120000. running mean: -10.143912\n",
      "ep 2548: ep_len:312 episode reward: total was -17.880000. running mean: -10.221273\n",
      "epsilon:0.052061 episode_count: 17843. steps_count: 7890336.000000\n",
      "ep 2549: ep_len:223 episode reward: total was -16.830000. running mean: -10.287361\n",
      "ep 2549: ep_len:580 episode reward: total was -29.950000. running mean: -10.483987\n",
      "ep 2549: ep_len:660 episode reward: total was -33.420000. running mean: -10.713347\n",
      "ep 2549: ep_len:163 episode reward: total was -0.380000. running mean: -10.610014\n",
      "ep 2549: ep_len:3 episode reward: total was 0.000000. running mean: -10.503914\n",
      "ep 2549: ep_len:595 episode reward: total was -0.580000. running mean: -10.404674\n",
      "ep 2549: ep_len:610 episode reward: total was -18.530000. running mean: -10.485928\n",
      "epsilon:0.051925 episode_count: 17850. steps_count: 7893170.000000\n",
      "ep 2550: ep_len:580 episode reward: total was -9.640000. running mean: -10.477468\n",
      "ep 2550: ep_len:675 episode reward: total was -26.250000. running mean: -10.635194\n",
      "ep 2550: ep_len:444 episode reward: total was 1.710000. running mean: -10.511742\n",
      "ep 2550: ep_len:132 episode reward: total was 4.600000. running mean: -10.360624\n",
      "ep 2550: ep_len:83 episode reward: total was 3.510000. running mean: -10.221918\n",
      "ep 2550: ep_len:315 episode reward: total was 4.190000. running mean: -10.077799\n",
      "ep 2550: ep_len:500 episode reward: total was -5.330000. running mean: -10.030321\n",
      "epsilon:0.051788 episode_count: 17857. steps_count: 7895899.000000\n",
      "ep 2551: ep_len:134 episode reward: total was -0.920000. running mean: -9.939218\n",
      "ep 2551: ep_len:500 episode reward: total was -19.070000. running mean: -10.030526\n",
      "ep 2551: ep_len:500 episode reward: total was -0.160000. running mean: -9.931820\n",
      "ep 2551: ep_len:382 episode reward: total was -7.090000. running mean: -9.903402\n",
      "ep 2551: ep_len:56 episode reward: total was 5.500000. running mean: -9.749368\n",
      "ep 2551: ep_len:505 episode reward: total was -55.650000. running mean: -10.208374\n",
      "ep 2551: ep_len:500 episode reward: total was -12.700000. running mean: -10.233291\n",
      "epsilon:0.051652 episode_count: 17864. steps_count: 7898476.000000\n",
      "ep 2552: ep_len:124 episode reward: total was -13.940000. running mean: -10.270358\n",
      "ep 2552: ep_len:167 episode reward: total was -10.950000. running mean: -10.277154\n",
      "ep 2552: ep_len:525 episode reward: total was -5.800000. running mean: -10.232383\n",
      "ep 2552: ep_len:505 episode reward: total was -24.660000. running mean: -10.376659\n",
      "ep 2552: ep_len:127 episode reward: total was 7.550000. running mean: -10.197392\n",
      "ep 2552: ep_len:680 episode reward: total was -44.320000. running mean: -10.538618\n",
      "ep 2552: ep_len:324 episode reward: total was -7.760000. running mean: -10.510832\n",
      "epsilon:0.051515 episode_count: 17871. steps_count: 7900928.000000\n",
      "ep 2553: ep_len:515 episode reward: total was -17.300000. running mean: -10.578724\n",
      "ep 2553: ep_len:800 episode reward: total was -39.150000. running mean: -10.864437\n",
      "ep 2553: ep_len:675 episode reward: total was -18.710000. running mean: -10.942892\n",
      "ep 2553: ep_len:590 episode reward: total was 5.100000. running mean: -10.782463\n",
      "ep 2553: ep_len:3 episode reward: total was 0.000000. running mean: -10.674639\n",
      "ep 2553: ep_len:550 episode reward: total was -13.660000. running mean: -10.704492\n",
      "ep 2553: ep_len:500 episode reward: total was -33.000000. running mean: -10.927447\n",
      "epsilon:0.051379 episode_count: 17878. steps_count: 7904561.000000\n",
      "ep 2554: ep_len:217 episode reward: total was 4.130000. running mean: -10.776873\n",
      "ep 2554: ep_len:585 episode reward: total was 3.580000. running mean: -10.633304\n",
      "ep 2554: ep_len:392 episode reward: total was -3.780000. running mean: -10.564771\n",
      "ep 2554: ep_len:500 episode reward: total was -26.160000. running mean: -10.720723\n",
      "ep 2554: ep_len:3 episode reward: total was 0.000000. running mean: -10.613516\n",
      "ep 2554: ep_len:205 episode reward: total was 5.130000. running mean: -10.456081\n",
      "ep 2554: ep_len:505 episode reward: total was -8.380000. running mean: -10.435320\n",
      "epsilon:0.051242 episode_count: 17885. steps_count: 7906968.000000\n",
      "ep 2555: ep_len:500 episode reward: total was -5.300000. running mean: -10.383967\n",
      "ep 2555: ep_len:700 episode reward: total was -96.840000. running mean: -11.248527\n",
      "ep 2555: ep_len:384 episode reward: total was -1.280000. running mean: -11.148842\n",
      "ep 2555: ep_len:600 episode reward: total was -7.420000. running mean: -11.111554\n",
      "ep 2555: ep_len:3 episode reward: total was 0.000000. running mean: -11.000438\n",
      "ep 2555: ep_len:695 episode reward: total was -54.480000. running mean: -11.435234\n",
      "ep 2555: ep_len:515 episode reward: total was -15.400000. running mean: -11.474881\n",
      "epsilon:0.051106 episode_count: 17892. steps_count: 7910365.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2556: ep_len:210 episode reward: total was 1.090000. running mean: -11.349232\n",
      "ep 2556: ep_len:525 episode reward: total was 13.400000. running mean: -11.101740\n",
      "ep 2556: ep_len:510 episode reward: total was -8.970000. running mean: -11.080423\n",
      "ep 2556: ep_len:500 episode reward: total was -13.560000. running mean: -11.105219\n",
      "ep 2556: ep_len:3 episode reward: total was 0.000000. running mean: -10.994166\n",
      "ep 2556: ep_len:575 episode reward: total was -39.600000. running mean: -11.280225\n",
      "ep 2556: ep_len:292 episode reward: total was -1.800000. running mean: -11.185422\n",
      "epsilon:0.050969 episode_count: 17899. steps_count: 7912980.000000\n",
      "ep 2557: ep_len:500 episode reward: total was -15.380000. running mean: -11.227368\n",
      "ep 2557: ep_len:515 episode reward: total was 14.950000. running mean: -10.965595\n",
      "ep 2557: ep_len:585 episode reward: total was -5.750000. running mean: -10.913439\n",
      "ep 2557: ep_len:102 episode reward: total was 3.110000. running mean: -10.773204\n",
      "ep 2557: ep_len:3 episode reward: total was 0.000000. running mean: -10.665472\n",
      "ep 2557: ep_len:510 episode reward: total was -40.090000. running mean: -10.959717\n",
      "ep 2557: ep_len:510 episode reward: total was -10.090000. running mean: -10.951020\n",
      "epsilon:0.050833 episode_count: 17906. steps_count: 7915705.000000\n",
      "ep 2558: ep_len:600 episode reward: total was -20.300000. running mean: -11.044510\n",
      "ep 2558: ep_len:500 episode reward: total was -35.580000. running mean: -11.289865\n",
      "ep 2558: ep_len:500 episode reward: total was -12.510000. running mean: -11.302066\n",
      "ep 2558: ep_len:500 episode reward: total was -13.140000. running mean: -11.320446\n",
      "ep 2558: ep_len:85 episode reward: total was 0.520000. running mean: -11.202041\n",
      "ep 2558: ep_len:635 episode reward: total was -82.750000. running mean: -11.917521\n",
      "ep 2558: ep_len:295 episode reward: total was -40.840000. running mean: -12.206746\n",
      "epsilon:0.050696 episode_count: 17913. steps_count: 7918820.000000\n",
      "ep 2559: ep_len:630 episode reward: total was -18.700000. running mean: -12.271678\n",
      "ep 2559: ep_len:575 episode reward: total was -5.030000. running mean: -12.199261\n",
      "ep 2559: ep_len:565 episode reward: total was -12.440000. running mean: -12.201669\n",
      "ep 2559: ep_len:610 episode reward: total was -27.900000. running mean: -12.358652\n",
      "ep 2559: ep_len:3 episode reward: total was 0.000000. running mean: -12.235066\n",
      "ep 2559: ep_len:615 episode reward: total was -12.350000. running mean: -12.236215\n",
      "ep 2559: ep_len:505 episode reward: total was -28.570000. running mean: -12.399553\n",
      "epsilon:0.050560 episode_count: 17920. steps_count: 7922323.000000\n",
      "ep 2560: ep_len:575 episode reward: total was 1.600000. running mean: -12.259557\n",
      "ep 2560: ep_len:600 episode reward: total was -3.160000. running mean: -12.168562\n",
      "ep 2560: ep_len:363 episode reward: total was 7.200000. running mean: -11.974876\n",
      "ep 2560: ep_len:120 episode reward: total was 1.090000. running mean: -11.844227\n",
      "ep 2560: ep_len:3 episode reward: total was 0.000000. running mean: -11.725785\n",
      "ep 2560: ep_len:500 episode reward: total was -4.900000. running mean: -11.657527\n",
      "ep 2560: ep_len:266 episode reward: total was -35.880000. running mean: -11.899752\n",
      "epsilon:0.050423 episode_count: 17927. steps_count: 7924750.000000\n",
      "ep 2561: ep_len:555 episode reward: total was 1.400000. running mean: -11.766754\n",
      "ep 2561: ep_len:530 episode reward: total was -29.510000. running mean: -11.944187\n",
      "ep 2561: ep_len:630 episode reward: total was -18.830000. running mean: -12.013045\n",
      "ep 2561: ep_len:88 episode reward: total was 3.090000. running mean: -11.862014\n",
      "ep 2561: ep_len:88 episode reward: total was 5.050000. running mean: -11.692894\n",
      "ep 2561: ep_len:247 episode reward: total was -4.320000. running mean: -11.619165\n",
      "ep 2561: ep_len:203 episode reward: total was -3.340000. running mean: -11.536374\n",
      "epsilon:0.050287 episode_count: 17934. steps_count: 7927091.000000\n",
      "ep 2562: ep_len:211 episode reward: total was 1.590000. running mean: -11.405110\n",
      "ep 2562: ep_len:500 episode reward: total was -8.150000. running mean: -11.372559\n",
      "ep 2562: ep_len:78 episode reward: total was 0.530000. running mean: -11.253533\n",
      "ep 2562: ep_len:500 episode reward: total was 15.410000. running mean: -10.986898\n",
      "ep 2562: ep_len:3 episode reward: total was 0.000000. running mean: -10.877029\n",
      "ep 2562: ep_len:760 episode reward: total was -40.820000. running mean: -11.176459\n",
      "ep 2562: ep_len:500 episode reward: total was 0.340000. running mean: -11.061294\n",
      "epsilon:0.050150 episode_count: 17941. steps_count: 7929643.000000\n",
      "ep 2563: ep_len:254 episode reward: total was 2.670000. running mean: -10.923981\n",
      "ep 2563: ep_len:595 episode reward: total was -19.810000. running mean: -11.012841\n",
      "ep 2563: ep_len:580 episode reward: total was -15.780000. running mean: -11.060513\n",
      "ep 2563: ep_len:550 episode reward: total was -7.610000. running mean: -11.026008\n",
      "ep 2563: ep_len:95 episode reward: total was 4.060000. running mean: -10.875148\n",
      "ep 2563: ep_len:765 episode reward: total was -55.350000. running mean: -11.319896\n",
      "ep 2563: ep_len:590 episode reward: total was -24.010000. running mean: -11.446797\n",
      "epsilon:0.050014 episode_count: 17948. steps_count: 7933072.000000\n",
      "ep 2564: ep_len:625 episode reward: total was 1.700000. running mean: -11.315329\n",
      "ep 2564: ep_len:500 episode reward: total was -30.720000. running mean: -11.509376\n",
      "ep 2564: ep_len:590 episode reward: total was -18.540000. running mean: -11.579682\n",
      "ep 2564: ep_len:530 episode reward: total was 8.030000. running mean: -11.383585\n",
      "ep 2564: ep_len:45 episode reward: total was 4.010000. running mean: -11.229650\n",
      "ep 2564: ep_len:545 episode reward: total was -6.800000. running mean: -11.185353\n",
      "ep 2564: ep_len:505 episode reward: total was -8.110000. running mean: -11.154600\n",
      "epsilon:0.049877 episode_count: 17955. steps_count: 7936412.000000\n",
      "ep 2565: ep_len:535 episode reward: total was -31.450000. running mean: -11.357554\n",
      "ep 2565: ep_len:500 episode reward: total was 5.810000. running mean: -11.185878\n",
      "ep 2565: ep_len:565 episode reward: total was -3.150000. running mean: -11.105519\n",
      "ep 2565: ep_len:530 episode reward: total was -4.480000. running mean: -11.039264\n",
      "ep 2565: ep_len:3 episode reward: total was 0.000000. running mean: -10.928871\n",
      "ep 2565: ep_len:500 episode reward: total was -28.240000. running mean: -11.101983\n",
      "ep 2565: ep_len:590 episode reward: total was -28.060000. running mean: -11.271563\n",
      "epsilon:0.049741 episode_count: 17962. steps_count: 7939635.000000\n",
      "ep 2566: ep_len:550 episode reward: total was 4.420000. running mean: -11.114647\n",
      "ep 2566: ep_len:500 episode reward: total was -18.290000. running mean: -11.186401\n",
      "ep 2566: ep_len:500 episode reward: total was 0.560000. running mean: -11.068937\n",
      "ep 2566: ep_len:525 episode reward: total was 8.920000. running mean: -10.869047\n",
      "ep 2566: ep_len:81 episode reward: total was 5.560000. running mean: -10.704757\n",
      "ep 2566: ep_len:695 episode reward: total was -52.320000. running mean: -11.120909\n",
      "ep 2566: ep_len:615 episode reward: total was -4.560000. running mean: -11.055300\n",
      "epsilon:0.049604 episode_count: 17969. steps_count: 7943101.000000\n",
      "ep 2567: ep_len:550 episode reward: total was 11.930000. running mean: -10.825447\n",
      "ep 2567: ep_len:500 episode reward: total was -9.490000. running mean: -10.812093\n",
      "ep 2567: ep_len:500 episode reward: total was -7.640000. running mean: -10.780372\n",
      "ep 2567: ep_len:555 episode reward: total was -16.540000. running mean: -10.837968\n",
      "ep 2567: ep_len:86 episode reward: total was -0.460000. running mean: -10.734188\n",
      "ep 2567: ep_len:695 episode reward: total was -69.640000. running mean: -11.323247\n",
      "ep 2567: ep_len:287 episode reward: total was -0.310000. running mean: -11.213114\n",
      "epsilon:0.049468 episode_count: 17976. steps_count: 7946274.000000\n",
      "ep 2568: ep_len:525 episode reward: total was 6.950000. running mean: -11.031483\n",
      "ep 2568: ep_len:500 episode reward: total was -12.030000. running mean: -11.041468\n",
      "ep 2568: ep_len:525 episode reward: total was -8.820000. running mean: -11.019253\n",
      "ep 2568: ep_len:500 episode reward: total was -18.600000. running mean: -11.095061\n",
      "ep 2568: ep_len:3 episode reward: total was 0.000000. running mean: -10.984110\n",
      "ep 2568: ep_len:515 episode reward: total was -22.520000. running mean: -11.099469\n",
      "ep 2568: ep_len:510 episode reward: total was -12.410000. running mean: -11.112575\n",
      "epsilon:0.049331 episode_count: 17983. steps_count: 7949352.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2569: ep_len:500 episode reward: total was 3.730000. running mean: -10.964149\n",
      "ep 2569: ep_len:735 episode reward: total was -39.370000. running mean: -11.248207\n",
      "ep 2569: ep_len:500 episode reward: total was -25.590000. running mean: -11.391625\n",
      "ep 2569: ep_len:56 episode reward: total was 1.560000. running mean: -11.262109\n",
      "ep 2569: ep_len:3 episode reward: total was 0.000000. running mean: -11.149488\n",
      "ep 2569: ep_len:234 episode reward: total was 2.150000. running mean: -11.016493\n",
      "ep 2569: ep_len:560 episode reward: total was -14.850000. running mean: -11.054828\n",
      "epsilon:0.049195 episode_count: 17990. steps_count: 7951940.000000\n",
      "ep 2570: ep_len:127 episode reward: total was -0.960000. running mean: -10.953880\n",
      "ep 2570: ep_len:339 episode reward: total was -8.370000. running mean: -10.928041\n",
      "ep 2570: ep_len:371 episode reward: total was 0.680000. running mean: -10.811961\n",
      "ep 2570: ep_len:515 episode reward: total was -4.970000. running mean: -10.753541\n",
      "ep 2570: ep_len:95 episode reward: total was 2.530000. running mean: -10.620706\n",
      "ep 2570: ep_len:645 episode reward: total was -76.610000. running mean: -11.280599\n",
      "ep 2570: ep_len:500 episode reward: total was -6.270000. running mean: -11.230493\n",
      "epsilon:0.049058 episode_count: 17997. steps_count: 7954532.000000\n",
      "ep 2571: ep_len:540 episode reward: total was -0.430000. running mean: -11.122488\n",
      "ep 2571: ep_len:565 episode reward: total was -8.660000. running mean: -11.097863\n",
      "ep 2571: ep_len:500 episode reward: total was 5.900000. running mean: -10.927884\n",
      "ep 2571: ep_len:510 episode reward: total was -10.530000. running mean: -10.923905\n",
      "ep 2571: ep_len:3 episode reward: total was 0.000000. running mean: -10.814666\n",
      "ep 2571: ep_len:605 episode reward: total was -8.520000. running mean: -10.791720\n",
      "ep 2571: ep_len:525 episode reward: total was -19.090000. running mean: -10.874702\n",
      "epsilon:0.048922 episode_count: 18004. steps_count: 7957780.000000\n",
      "ep 2572: ep_len:197 episode reward: total was -19.420000. running mean: -10.960155\n",
      "ep 2572: ep_len:630 episode reward: total was -18.100000. running mean: -11.031554\n",
      "ep 2572: ep_len:535 episode reward: total was -20.890000. running mean: -11.130138\n",
      "ep 2572: ep_len:520 episode reward: total was -15.040000. running mean: -11.169237\n",
      "ep 2572: ep_len:3 episode reward: total was 0.000000. running mean: -11.057544\n",
      "ep 2572: ep_len:500 episode reward: total was -55.860000. running mean: -11.505569\n",
      "ep 2572: ep_len:540 episode reward: total was -7.090000. running mean: -11.461413\n",
      "epsilon:0.048785 episode_count: 18011. steps_count: 7960705.000000\n",
      "ep 2573: ep_len:620 episode reward: total was -1.930000. running mean: -11.366099\n",
      "ep 2573: ep_len:500 episode reward: total was -10.530000. running mean: -11.357738\n",
      "ep 2573: ep_len:580 episode reward: total was -12.700000. running mean: -11.371161\n",
      "ep 2573: ep_len:590 episode reward: total was 6.080000. running mean: -11.196649\n",
      "ep 2573: ep_len:3 episode reward: total was 0.000000. running mean: -11.084683\n",
      "ep 2573: ep_len:525 episode reward: total was 3.890000. running mean: -10.934936\n",
      "ep 2573: ep_len:530 episode reward: total was -11.150000. running mean: -10.937087\n",
      "epsilon:0.048649 episode_count: 18018. steps_count: 7964053.000000\n",
      "ep 2574: ep_len:91 episode reward: total was -5.960000. running mean: -10.887316\n",
      "ep 2574: ep_len:550 episode reward: total was 11.360000. running mean: -10.664843\n",
      "ep 2574: ep_len:585 episode reward: total was -43.560000. running mean: -10.993794\n",
      "ep 2574: ep_len:575 episode reward: total was -32.600000. running mean: -11.209856\n",
      "ep 2574: ep_len:3 episode reward: total was 0.000000. running mean: -11.097758\n",
      "ep 2574: ep_len:500 episode reward: total was -40.820000. running mean: -11.394980\n",
      "ep 2574: ep_len:520 episode reward: total was -16.400000. running mean: -11.445030\n",
      "epsilon:0.048512 episode_count: 18025. steps_count: 7966877.000000\n",
      "ep 2575: ep_len:500 episode reward: total was -1.780000. running mean: -11.348380\n",
      "ep 2575: ep_len:500 episode reward: total was -2.280000. running mean: -11.257696\n",
      "ep 2575: ep_len:595 episode reward: total was -9.890000. running mean: -11.244019\n",
      "ep 2575: ep_len:715 episode reward: total was -37.380000. running mean: -11.505379\n",
      "ep 2575: ep_len:87 episode reward: total was -10.480000. running mean: -11.495125\n",
      "ep 2575: ep_len:570 episode reward: total was -26.570000. running mean: -11.645874\n",
      "ep 2575: ep_len:510 episode reward: total was -51.270000. running mean: -12.042115\n",
      "epsilon:0.048376 episode_count: 18032. steps_count: 7970354.000000\n",
      "ep 2576: ep_len:640 episode reward: total was -8.710000. running mean: -12.008794\n",
      "ep 2576: ep_len:505 episode reward: total was -25.630000. running mean: -12.145006\n",
      "ep 2576: ep_len:645 episode reward: total was -24.420000. running mean: -12.267756\n",
      "ep 2576: ep_len:391 episode reward: total was 0.310000. running mean: -12.141978\n",
      "ep 2576: ep_len:97 episode reward: total was 5.040000. running mean: -11.970159\n",
      "ep 2576: ep_len:610 episode reward: total was -31.190000. running mean: -12.162357\n",
      "ep 2576: ep_len:545 episode reward: total was -15.120000. running mean: -12.191934\n",
      "epsilon:0.048239 episode_count: 18039. steps_count: 7973787.000000\n",
      "ep 2577: ep_len:590 episode reward: total was -9.700000. running mean: -12.167014\n",
      "ep 2577: ep_len:345 episode reward: total was -13.300000. running mean: -12.178344\n",
      "ep 2577: ep_len:690 episode reward: total was -13.690000. running mean: -12.193461\n",
      "ep 2577: ep_len:29 episode reward: total was 1.030000. running mean: -12.061226\n",
      "ep 2577: ep_len:3 episode reward: total was 0.000000. running mean: -11.940614\n",
      "ep 2577: ep_len:535 episode reward: total was 4.610000. running mean: -11.775108\n",
      "ep 2577: ep_len:510 episode reward: total was -7.330000. running mean: -11.730657\n",
      "epsilon:0.048103 episode_count: 18046. steps_count: 7976489.000000\n",
      "ep 2578: ep_len:585 episode reward: total was -20.190000. running mean: -11.815250\n",
      "ep 2578: ep_len:525 episode reward: total was -19.330000. running mean: -11.890397\n",
      "ep 2578: ep_len:660 episode reward: total was -7.690000. running mean: -11.848393\n",
      "ep 2578: ep_len:920 episode reward: total was -84.130000. running mean: -12.571210\n",
      "ep 2578: ep_len:52 episode reward: total was 5.000000. running mean: -12.395497\n",
      "ep 2578: ep_len:575 episode reward: total was -15.770000. running mean: -12.429242\n",
      "ep 2578: ep_len:595 episode reward: total was -20.450000. running mean: -12.509450\n",
      "epsilon:0.047966 episode_count: 18053. steps_count: 7980401.000000\n",
      "ep 2579: ep_len:565 episode reward: total was -19.250000. running mean: -12.576856\n",
      "ep 2579: ep_len:625 episode reward: total was -15.500000. running mean: -12.606087\n",
      "ep 2579: ep_len:452 episode reward: total was 2.770000. running mean: -12.452326\n",
      "ep 2579: ep_len:525 episode reward: total was -16.040000. running mean: -12.488203\n",
      "ep 2579: ep_len:103 episode reward: total was 6.550000. running mean: -12.297821\n",
      "ep 2579: ep_len:625 episode reward: total was -22.300000. running mean: -12.397843\n",
      "ep 2579: ep_len:560 episode reward: total was -20.870000. running mean: -12.482564\n",
      "epsilon:0.047830 episode_count: 18060. steps_count: 7983856.000000\n",
      "ep 2580: ep_len:500 episode reward: total was -9.860000. running mean: -12.456339\n",
      "ep 2580: ep_len:265 episode reward: total was -13.360000. running mean: -12.465375\n",
      "ep 2580: ep_len:575 episode reward: total was -13.320000. running mean: -12.473921\n",
      "ep 2580: ep_len:505 episode reward: total was -2.770000. running mean: -12.376882\n",
      "ep 2580: ep_len:82 episode reward: total was 5.520000. running mean: -12.197913\n",
      "ep 2580: ep_len:590 episode reward: total was 3.620000. running mean: -12.039734\n",
      "ep 2580: ep_len:505 episode reward: total was -14.350000. running mean: -12.062837\n",
      "epsilon:0.047693 episode_count: 18067. steps_count: 7986878.000000\n",
      "ep 2581: ep_len:610 episode reward: total was -4.470000. running mean: -11.986909\n",
      "ep 2581: ep_len:545 episode reward: total was -41.630000. running mean: -12.283339\n",
      "ep 2581: ep_len:560 episode reward: total was -22.190000. running mean: -12.382406\n",
      "ep 2581: ep_len:615 episode reward: total was 5.410000. running mean: -12.204482\n",
      "ep 2581: ep_len:3 episode reward: total was 0.000000. running mean: -12.082437\n",
      "ep 2581: ep_len:555 episode reward: total was -7.990000. running mean: -12.041513\n",
      "ep 2581: ep_len:500 episode reward: total was -22.840000. running mean: -12.149498\n",
      "epsilon:0.047557 episode_count: 18074. steps_count: 7990266.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2582: ep_len:665 episode reward: total was -12.120000. running mean: -12.149203\n",
      "ep 2582: ep_len:500 episode reward: total was -10.870000. running mean: -12.136411\n",
      "ep 2582: ep_len:500 episode reward: total was 2.400000. running mean: -11.991047\n",
      "ep 2582: ep_len:520 episode reward: total was 6.370000. running mean: -11.807436\n",
      "ep 2582: ep_len:3 episode reward: total was 0.000000. running mean: -11.689362\n",
      "ep 2582: ep_len:505 episode reward: total was -20.320000. running mean: -11.775668\n",
      "ep 2582: ep_len:505 episode reward: total was -23.620000. running mean: -11.894111\n",
      "epsilon:0.047420 episode_count: 18081. steps_count: 7993464.000000\n",
      "ep 2583: ep_len:500 episode reward: total was -29.680000. running mean: -12.071970\n",
      "ep 2583: ep_len:500 episode reward: total was -49.160000. running mean: -12.442851\n",
      "ep 2583: ep_len:620 episode reward: total was -15.710000. running mean: -12.475522\n",
      "ep 2583: ep_len:500 episode reward: total was -19.090000. running mean: -12.541667\n",
      "ep 2583: ep_len:3 episode reward: total was 0.000000. running mean: -12.416250\n",
      "ep 2583: ep_len:500 episode reward: total was -9.840000. running mean: -12.390488\n",
      "ep 2583: ep_len:620 episode reward: total was -16.370000. running mean: -12.430283\n",
      "epsilon:0.047284 episode_count: 18088. steps_count: 7996707.000000\n",
      "ep 2584: ep_len:210 episode reward: total was -4.940000. running mean: -12.355380\n",
      "ep 2584: ep_len:565 episode reward: total was -17.120000. running mean: -12.403026\n",
      "ep 2584: ep_len:500 episode reward: total was -1.320000. running mean: -12.292196\n",
      "ep 2584: ep_len:500 episode reward: total was 1.910000. running mean: -12.150174\n",
      "ep 2584: ep_len:3 episode reward: total was 0.000000. running mean: -12.028672\n",
      "ep 2584: ep_len:600 episode reward: total was -12.850000. running mean: -12.036886\n",
      "ep 2584: ep_len:540 episode reward: total was -18.020000. running mean: -12.096717\n",
      "epsilon:0.047147 episode_count: 18095. steps_count: 7999625.000000\n",
      "ep 2585: ep_len:625 episode reward: total was -13.700000. running mean: -12.112750\n",
      "ep 2585: ep_len:575 episode reward: total was -5.970000. running mean: -12.051322\n",
      "ep 2585: ep_len:650 episode reward: total was -21.380000. running mean: -12.144609\n",
      "ep 2585: ep_len:520 episode reward: total was 1.460000. running mean: -12.008563\n",
      "ep 2585: ep_len:79 episode reward: total was -8.450000. running mean: -11.972977\n",
      "ep 2585: ep_len:520 episode reward: total was -6.620000. running mean: -11.919447\n",
      "ep 2585: ep_len:680 episode reward: total was -9.210000. running mean: -11.892353\n",
      "epsilon:0.047011 episode_count: 18102. steps_count: 8003274.000000\n",
      "ep 2586: ep_len:615 episode reward: total was -12.710000. running mean: -11.900529\n",
      "ep 2586: ep_len:500 episode reward: total was 9.170000. running mean: -11.689824\n",
      "ep 2586: ep_len:560 episode reward: total was -4.170000. running mean: -11.614626\n",
      "ep 2586: ep_len:500 episode reward: total was -0.510000. running mean: -11.503580\n",
      "ep 2586: ep_len:3 episode reward: total was 0.000000. running mean: -11.388544\n",
      "ep 2586: ep_len:595 episode reward: total was 0.430000. running mean: -11.270358\n",
      "ep 2586: ep_len:540 episode reward: total was -14.580000. running mean: -11.303455\n",
      "epsilon:0.046874 episode_count: 18109. steps_count: 8006587.000000\n",
      "ep 2587: ep_len:545 episode reward: total was -24.430000. running mean: -11.434720\n",
      "ep 2587: ep_len:500 episode reward: total was -31.320000. running mean: -11.633573\n",
      "ep 2587: ep_len:66 episode reward: total was -2.980000. running mean: -11.547037\n",
      "ep 2587: ep_len:395 episode reward: total was -7.170000. running mean: -11.503267\n",
      "ep 2587: ep_len:94 episode reward: total was 6.040000. running mean: -11.327834\n",
      "ep 2587: ep_len:505 episode reward: total was -12.190000. running mean: -11.336456\n",
      "ep 2587: ep_len:570 episode reward: total was -2.900000. running mean: -11.252091\n",
      "epsilon:0.046738 episode_count: 18116. steps_count: 8009262.000000\n",
      "ep 2588: ep_len:580 episode reward: total was -9.990000. running mean: -11.239470\n",
      "ep 2588: ep_len:353 episode reward: total was -36.830000. running mean: -11.495376\n",
      "ep 2588: ep_len:505 episode reward: total was -10.580000. running mean: -11.486222\n",
      "ep 2588: ep_len:515 episode reward: total was -15.600000. running mean: -11.527360\n",
      "ep 2588: ep_len:45 episode reward: total was 3.000000. running mean: -11.382086\n",
      "ep 2588: ep_len:725 episode reward: total was -47.780000. running mean: -11.746065\n",
      "ep 2588: ep_len:535 episode reward: total was -12.500000. running mean: -11.753605\n",
      "epsilon:0.046601 episode_count: 18123. steps_count: 8012520.000000\n",
      "ep 2589: ep_len:630 episode reward: total was -7.650000. running mean: -11.712569\n",
      "ep 2589: ep_len:500 episode reward: total was -11.510000. running mean: -11.710543\n",
      "ep 2589: ep_len:500 episode reward: total was 1.080000. running mean: -11.582637\n",
      "ep 2589: ep_len:170 episode reward: total was 3.120000. running mean: -11.435611\n",
      "ep 2589: ep_len:3 episode reward: total was 0.000000. running mean: -11.321255\n",
      "ep 2589: ep_len:590 episode reward: total was -13.750000. running mean: -11.345542\n",
      "ep 2589: ep_len:245 episode reward: total was -8.870000. running mean: -11.320787\n",
      "epsilon:0.046465 episode_count: 18130. steps_count: 8015158.000000\n",
      "ep 2590: ep_len:525 episode reward: total was -12.230000. running mean: -11.329879\n",
      "ep 2590: ep_len:610 episode reward: total was 2.070000. running mean: -11.195880\n",
      "ep 2590: ep_len:424 episode reward: total was -2.780000. running mean: -11.111722\n",
      "ep 2590: ep_len:519 episode reward: total was -26.980000. running mean: -11.270404\n",
      "ep 2590: ep_len:90 episode reward: total was -10.440000. running mean: -11.262100\n",
      "ep 2590: ep_len:334 episode reward: total was -6.800000. running mean: -11.217479\n",
      "ep 2590: ep_len:555 episode reward: total was -12.990000. running mean: -11.235204\n",
      "epsilon:0.046328 episode_count: 18137. steps_count: 8018215.000000\n",
      "ep 2591: ep_len:560 episode reward: total was -2.370000. running mean: -11.146552\n",
      "ep 2591: ep_len:535 episode reward: total was -14.470000. running mean: -11.179787\n",
      "ep 2591: ep_len:600 episode reward: total was -2.910000. running mean: -11.097089\n",
      "ep 2591: ep_len:500 episode reward: total was -17.110000. running mean: -11.157218\n",
      "ep 2591: ep_len:50 episode reward: total was 4.510000. running mean: -11.000546\n",
      "ep 2591: ep_len:610 episode reward: total was -8.020000. running mean: -10.970740\n",
      "ep 2591: ep_len:211 episode reward: total was -8.400000. running mean: -10.945033\n",
      "epsilon:0.046192 episode_count: 18144. steps_count: 8021281.000000\n",
      "ep 2592: ep_len:248 episode reward: total was -0.910000. running mean: -10.844683\n",
      "ep 2592: ep_len:297 episode reward: total was -16.850000. running mean: -10.904736\n",
      "ep 2592: ep_len:560 episode reward: total was -9.090000. running mean: -10.886589\n",
      "ep 2592: ep_len:520 episode reward: total was -9.530000. running mean: -10.873023\n",
      "ep 2592: ep_len:3 episode reward: total was 0.000000. running mean: -10.764292\n",
      "ep 2592: ep_len:278 episode reward: total was -23.860000. running mean: -10.895250\n",
      "ep 2592: ep_len:1145 episode reward: total was -168.230000. running mean: -12.468597\n",
      "epsilon:0.046055 episode_count: 18151. steps_count: 8024332.000000\n",
      "ep 2593: ep_len:695 episode reward: total was -21.640000. running mean: -12.560311\n",
      "ep 2593: ep_len:630 episode reward: total was -11.970000. running mean: -12.554408\n",
      "ep 2593: ep_len:74 episode reward: total was -0.950000. running mean: -12.438364\n",
      "ep 2593: ep_len:520 episode reward: total was 9.050000. running mean: -12.223480\n",
      "ep 2593: ep_len:3 episode reward: total was 0.000000. running mean: -12.101245\n",
      "ep 2593: ep_len:163 episode reward: total was 1.590000. running mean: -11.964333\n",
      "ep 2593: ep_len:510 episode reward: total was -25.880000. running mean: -12.103490\n",
      "epsilon:0.045919 episode_count: 18158. steps_count: 8026927.000000\n",
      "ep 2594: ep_len:530 episode reward: total was -17.910000. running mean: -12.161555\n",
      "ep 2594: ep_len:640 episode reward: total was -6.090000. running mean: -12.100839\n",
      "ep 2594: ep_len:535 episode reward: total was -4.190000. running mean: -12.021731\n",
      "ep 2594: ep_len:500 episode reward: total was 11.350000. running mean: -11.788014\n",
      "ep 2594: ep_len:85 episode reward: total was -11.950000. running mean: -11.789633\n",
      "ep 2594: ep_len:540 episode reward: total was -6.230000. running mean: -11.734037\n",
      "ep 2594: ep_len:600 episode reward: total was -5.410000. running mean: -11.670797\n",
      "epsilon:0.045782 episode_count: 18165. steps_count: 8030357.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2595: ep_len:560 episode reward: total was -2.090000. running mean: -11.574989\n",
      "ep 2595: ep_len:188 episode reward: total was -9.430000. running mean: -11.553539\n",
      "ep 2595: ep_len:397 episode reward: total was 0.230000. running mean: -11.435703\n",
      "ep 2595: ep_len:510 episode reward: total was -9.950000. running mean: -11.420846\n",
      "ep 2595: ep_len:3 episode reward: total was 0.000000. running mean: -11.306638\n",
      "ep 2595: ep_len:530 episode reward: total was -9.170000. running mean: -11.285272\n",
      "ep 2595: ep_len:520 episode reward: total was -39.550000. running mean: -11.567919\n",
      "epsilon:0.045646 episode_count: 18172. steps_count: 8033065.000000\n",
      "ep 2596: ep_len:525 episode reward: total was 1.910000. running mean: -11.433140\n",
      "ep 2596: ep_len:500 episode reward: total was 3.660000. running mean: -11.282208\n",
      "ep 2596: ep_len:540 episode reward: total was -13.750000. running mean: -11.306886\n",
      "ep 2596: ep_len:625 episode reward: total was -8.980000. running mean: -11.283617\n",
      "ep 2596: ep_len:3 episode reward: total was 0.000000. running mean: -11.170781\n",
      "ep 2596: ep_len:620 episode reward: total was -2.060000. running mean: -11.079673\n",
      "ep 2596: ep_len:500 episode reward: total was -19.150000. running mean: -11.160377\n",
      "epsilon:0.045509 episode_count: 18179. steps_count: 8036378.000000\n",
      "ep 2597: ep_len:500 episode reward: total was 4.770000. running mean: -11.001073\n",
      "ep 2597: ep_len:500 episode reward: total was -7.520000. running mean: -10.966262\n",
      "ep 2597: ep_len:590 episode reward: total was -30.240000. running mean: -11.158999\n",
      "ep 2597: ep_len:500 episode reward: total was -17.070000. running mean: -11.218109\n",
      "ep 2597: ep_len:3 episode reward: total was 0.000000. running mean: -11.105928\n",
      "ep 2597: ep_len:655 episode reward: total was -17.220000. running mean: -11.167069\n",
      "ep 2597: ep_len:510 episode reward: total was -4.580000. running mean: -11.101198\n",
      "epsilon:0.045373 episode_count: 18186. steps_count: 8039636.000000\n",
      "ep 2598: ep_len:510 episode reward: total was -22.970000. running mean: -11.219886\n",
      "ep 2598: ep_len:595 episode reward: total was -22.990000. running mean: -11.337588\n",
      "ep 2598: ep_len:520 episode reward: total was -5.080000. running mean: -11.275012\n",
      "ep 2598: ep_len:575 episode reward: total was -2.050000. running mean: -11.182762\n",
      "ep 2598: ep_len:3 episode reward: total was 0.000000. running mean: -11.070934\n",
      "ep 2598: ep_len:535 episode reward: total was -16.740000. running mean: -11.127625\n",
      "ep 2598: ep_len:280 episode reward: total was -16.870000. running mean: -11.185048\n",
      "epsilon:0.045236 episode_count: 18193. steps_count: 8042654.000000\n",
      "ep 2599: ep_len:515 episode reward: total was -17.150000. running mean: -11.244698\n",
      "ep 2599: ep_len:515 episode reward: total was -26.050000. running mean: -11.392751\n",
      "ep 2599: ep_len:393 episode reward: total was -22.340000. running mean: -11.502223\n",
      "ep 2599: ep_len:132 episode reward: total was 2.110000. running mean: -11.366101\n",
      "ep 2599: ep_len:3 episode reward: total was 0.000000. running mean: -11.252440\n",
      "ep 2599: ep_len:231 episode reward: total was 4.130000. running mean: -11.098616\n",
      "ep 2599: ep_len:550 episode reward: total was -7.800000. running mean: -11.065630\n",
      "epsilon:0.045100 episode_count: 18200. steps_count: 8044993.000000\n",
      "ep 2600: ep_len:565 episode reward: total was -7.140000. running mean: -11.026373\n",
      "ep 2600: ep_len:515 episode reward: total was -19.930000. running mean: -11.115410\n",
      "ep 2600: ep_len:645 episode reward: total was -22.460000. running mean: -11.228855\n",
      "ep 2600: ep_len:595 episode reward: total was -0.110000. running mean: -11.117667\n",
      "ep 2600: ep_len:113 episode reward: total was 4.060000. running mean: -10.965890\n",
      "ep 2600: ep_len:545 episode reward: total was -17.640000. running mean: -11.032631\n",
      "ep 2600: ep_len:520 episode reward: total was -57.800000. running mean: -11.500305\n",
      "epsilon:0.044963 episode_count: 18207. steps_count: 8048491.000000\n",
      "ep 2601: ep_len:610 episode reward: total was -7.460000. running mean: -11.459902\n",
      "ep 2601: ep_len:610 episode reward: total was -19.090000. running mean: -11.536203\n",
      "ep 2601: ep_len:510 episode reward: total was -6.400000. running mean: -11.484841\n",
      "ep 2601: ep_len:56 episode reward: total was 1.560000. running mean: -11.354393\n",
      "ep 2601: ep_len:3 episode reward: total was 0.000000. running mean: -11.240849\n",
      "ep 2601: ep_len:610 episode reward: total was -3.030000. running mean: -11.158740\n",
      "ep 2601: ep_len:500 episode reward: total was -3.240000. running mean: -11.079553\n",
      "epsilon:0.044827 episode_count: 18214. steps_count: 8051390.000000\n",
      "ep 2602: ep_len:560 episode reward: total was -15.580000. running mean: -11.124557\n",
      "ep 2602: ep_len:640 episode reward: total was -15.050000. running mean: -11.163812\n",
      "ep 2602: ep_len:75 episode reward: total was 1.050000. running mean: -11.041673\n",
      "ep 2602: ep_len:500 episode reward: total was -9.980000. running mean: -11.031057\n",
      "ep 2602: ep_len:3 episode reward: total was 0.000000. running mean: -10.920746\n",
      "ep 2602: ep_len:540 episode reward: total was 2.660000. running mean: -10.784939\n",
      "ep 2602: ep_len:500 episode reward: total was -12.240000. running mean: -10.799489\n",
      "epsilon:0.044690 episode_count: 18221. steps_count: 8054208.000000\n",
      "ep 2603: ep_len:600 episode reward: total was -22.730000. running mean: -10.918794\n",
      "ep 2603: ep_len:278 episode reward: total was -3.330000. running mean: -10.842906\n",
      "ep 2603: ep_len:645 episode reward: total was -15.450000. running mean: -10.888977\n",
      "ep 2603: ep_len:500 episode reward: total was 9.390000. running mean: -10.686188\n",
      "ep 2603: ep_len:3 episode reward: total was 0.000000. running mean: -10.579326\n",
      "ep 2603: ep_len:505 episode reward: total was 4.470000. running mean: -10.428833\n",
      "ep 2603: ep_len:500 episode reward: total was -15.500000. running mean: -10.479544\n",
      "epsilon:0.044554 episode_count: 18228. steps_count: 8057239.000000\n",
      "ep 2604: ep_len:620 episode reward: total was -25.800000. running mean: -10.632749\n",
      "ep 2604: ep_len:540 episode reward: total was -12.930000. running mean: -10.655721\n",
      "ep 2604: ep_len:73 episode reward: total was -2.450000. running mean: -10.573664\n",
      "ep 2604: ep_len:392 episode reward: total was -2.190000. running mean: -10.489827\n",
      "ep 2604: ep_len:3 episode reward: total was 0.000000. running mean: -10.384929\n",
      "ep 2604: ep_len:500 episode reward: total was -6.720000. running mean: -10.348280\n",
      "ep 2604: ep_len:625 episode reward: total was -1.480000. running mean: -10.259597\n",
      "epsilon:0.044417 episode_count: 18235. steps_count: 8059992.000000\n",
      "ep 2605: ep_len:610 episode reward: total was -8.920000. running mean: -10.246201\n",
      "ep 2605: ep_len:630 episode reward: total was -14.090000. running mean: -10.284639\n",
      "ep 2605: ep_len:880 episode reward: total was -61.670000. running mean: -10.798493\n",
      "ep 2605: ep_len:351 episode reward: total was -35.740000. running mean: -11.047908\n",
      "ep 2605: ep_len:3 episode reward: total was 0.000000. running mean: -10.937429\n",
      "ep 2605: ep_len:515 episode reward: total was -4.870000. running mean: -10.876754\n",
      "ep 2605: ep_len:630 episode reward: total was -1.720000. running mean: -10.785187\n",
      "epsilon:0.044281 episode_count: 18242. steps_count: 8063611.000000\n",
      "ep 2606: ep_len:595 episode reward: total was -6.930000. running mean: -10.746635\n",
      "ep 2606: ep_len:281 episode reward: total was 1.710000. running mean: -10.622069\n",
      "ep 2606: ep_len:500 episode reward: total was -8.150000. running mean: -10.597348\n",
      "ep 2606: ep_len:550 episode reward: total was -1.640000. running mean: -10.507774\n",
      "ep 2606: ep_len:3 episode reward: total was 0.000000. running mean: -10.402697\n",
      "ep 2606: ep_len:500 episode reward: total was -29.280000. running mean: -10.591470\n",
      "ep 2606: ep_len:630 episode reward: total was -15.930000. running mean: -10.644855\n",
      "epsilon:0.044144 episode_count: 18249. steps_count: 8066670.000000\n",
      "ep 2607: ep_len:580 episode reward: total was -3.110000. running mean: -10.569507\n",
      "ep 2607: ep_len:580 episode reward: total was -13.330000. running mean: -10.597111\n",
      "ep 2607: ep_len:550 episode reward: total was -9.520000. running mean: -10.586340\n",
      "ep 2607: ep_len:500 episode reward: total was -0.500000. running mean: -10.485477\n",
      "ep 2607: ep_len:113 episode reward: total was -9.440000. running mean: -10.475022\n",
      "ep 2607: ep_len:685 episode reward: total was -40.300000. running mean: -10.773272\n",
      "ep 2607: ep_len:615 episode reward: total was -22.630000. running mean: -10.891839\n",
      "epsilon:0.044008 episode_count: 18256. steps_count: 8070293.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2608: ep_len:525 episode reward: total was -3.950000. running mean: -10.822421\n",
      "ep 2608: ep_len:500 episode reward: total was -3.050000. running mean: -10.744697\n",
      "ep 2608: ep_len:605 episode reward: total was -6.550000. running mean: -10.702750\n",
      "ep 2608: ep_len:500 episode reward: total was 7.310000. running mean: -10.522622\n",
      "ep 2608: ep_len:3 episode reward: total was 0.000000. running mean: -10.417396\n",
      "ep 2608: ep_len:530 episode reward: total was -15.040000. running mean: -10.463622\n",
      "ep 2608: ep_len:510 episode reward: total was -3.750000. running mean: -10.396486\n",
      "epsilon:0.043871 episode_count: 18263. steps_count: 8073466.000000\n",
      "ep 2609: ep_len:500 episode reward: total was -36.350000. running mean: -10.656021\n",
      "ep 2609: ep_len:500 episode reward: total was -2.450000. running mean: -10.573961\n",
      "ep 2609: ep_len:570 episode reward: total was 2.120000. running mean: -10.447021\n",
      "ep 2609: ep_len:540 episode reward: total was -38.460000. running mean: -10.727151\n",
      "ep 2609: ep_len:3 episode reward: total was 0.000000. running mean: -10.619879\n",
      "ep 2609: ep_len:500 episode reward: total was -18.690000. running mean: -10.700581\n",
      "ep 2609: ep_len:565 episode reward: total was -22.070000. running mean: -10.814275\n",
      "epsilon:0.043735 episode_count: 18270. steps_count: 8076644.000000\n",
      "ep 2610: ep_len:605 episode reward: total was -14.150000. running mean: -10.847632\n",
      "ep 2610: ep_len:500 episode reward: total was -8.910000. running mean: -10.828256\n",
      "ep 2610: ep_len:565 episode reward: total was 0.570000. running mean: -10.714273\n",
      "ep 2610: ep_len:500 episode reward: total was -6.560000. running mean: -10.672730\n",
      "ep 2610: ep_len:80 episode reward: total was -0.470000. running mean: -10.570703\n",
      "ep 2610: ep_len:165 episode reward: total was -6.900000. running mean: -10.533996\n",
      "ep 2610: ep_len:505 episode reward: total was -14.660000. running mean: -10.575256\n",
      "epsilon:0.043598 episode_count: 18277. steps_count: 8079564.000000\n",
      "ep 2611: ep_len:113 episode reward: total was 4.090000. running mean: -10.428604\n",
      "ep 2611: ep_len:500 episode reward: total was -24.530000. running mean: -10.569618\n",
      "ep 2611: ep_len:615 episode reward: total was -11.960000. running mean: -10.583521\n",
      "ep 2611: ep_len:56 episode reward: total was -0.460000. running mean: -10.482286\n",
      "ep 2611: ep_len:91 episode reward: total was 3.040000. running mean: -10.347063\n",
      "ep 2611: ep_len:500 episode reward: total was -5.160000. running mean: -10.295193\n",
      "ep 2611: ep_len:560 episode reward: total was -16.480000. running mean: -10.357041\n",
      "epsilon:0.043462 episode_count: 18284. steps_count: 8081999.000000\n",
      "ep 2612: ep_len:650 episode reward: total was -8.690000. running mean: -10.340370\n",
      "ep 2612: ep_len:357 episode reward: total was -10.350000. running mean: -10.340467\n",
      "ep 2612: ep_len:500 episode reward: total was -2.970000. running mean: -10.266762\n",
      "ep 2612: ep_len:56 episode reward: total was -1.940000. running mean: -10.183494\n",
      "ep 2612: ep_len:82 episode reward: total was -0.500000. running mean: -10.086659\n",
      "ep 2612: ep_len:500 episode reward: total was 2.540000. running mean: -9.960393\n",
      "ep 2612: ep_len:615 episode reward: total was -16.350000. running mean: -10.024289\n",
      "epsilon:0.043325 episode_count: 18291. steps_count: 8084759.000000\n",
      "ep 2613: ep_len:500 episode reward: total was 6.900000. running mean: -9.855046\n",
      "ep 2613: ep_len:530 episode reward: total was -32.340000. running mean: -10.079895\n",
      "ep 2613: ep_len:620 episode reward: total was -7.560000. running mean: -10.054697\n",
      "ep 2613: ep_len:585 episode reward: total was 8.960000. running mean: -9.864550\n",
      "ep 2613: ep_len:88 episode reward: total was -14.980000. running mean: -9.915704\n",
      "ep 2613: ep_len:605 episode reward: total was 1.400000. running mean: -9.802547\n",
      "ep 2613: ep_len:510 episode reward: total was -42.780000. running mean: -10.132322\n",
      "epsilon:0.043189 episode_count: 18298. steps_count: 8088197.000000\n",
      "ep 2614: ep_len:134 episode reward: total was 3.580000. running mean: -9.995198\n",
      "ep 2614: ep_len:510 episode reward: total was -34.550000. running mean: -10.240746\n",
      "ep 2614: ep_len:650 episode reward: total was 1.380000. running mean: -10.124539\n",
      "ep 2614: ep_len:170 episode reward: total was 1.130000. running mean: -10.011994\n",
      "ep 2614: ep_len:3 episode reward: total was 0.000000. running mean: -9.911874\n",
      "ep 2614: ep_len:675 episode reward: total was -31.760000. running mean: -10.130355\n",
      "ep 2614: ep_len:500 episode reward: total was -18.020000. running mean: -10.209251\n",
      "epsilon:0.043052 episode_count: 18305. steps_count: 8090839.000000\n",
      "ep 2615: ep_len:500 episode reward: total was -11.340000. running mean: -10.220559\n",
      "ep 2615: ep_len:620 episode reward: total was -8.960000. running mean: -10.207953\n",
      "ep 2615: ep_len:500 episode reward: total was 3.870000. running mean: -10.067174\n",
      "ep 2615: ep_len:500 episode reward: total was -0.160000. running mean: -9.968102\n",
      "ep 2615: ep_len:3 episode reward: total was 0.000000. running mean: -9.868421\n",
      "ep 2615: ep_len:595 episode reward: total was -8.810000. running mean: -9.857837\n",
      "ep 2615: ep_len:540 episode reward: total was -14.620000. running mean: -9.905458\n",
      "epsilon:0.042916 episode_count: 18312. steps_count: 8094097.000000\n",
      "ep 2616: ep_len:530 episode reward: total was -10.840000. running mean: -9.914804\n",
      "ep 2616: ep_len:625 episode reward: total was -18.720000. running mean: -10.002856\n",
      "ep 2616: ep_len:540 episode reward: total was -4.700000. running mean: -9.949827\n",
      "ep 2616: ep_len:500 episode reward: total was -14.140000. running mean: -9.991729\n",
      "ep 2616: ep_len:129 episode reward: total was 5.560000. running mean: -9.836212\n",
      "ep 2616: ep_len:695 episode reward: total was -7.220000. running mean: -9.810049\n",
      "ep 2616: ep_len:520 episode reward: total was -10.920000. running mean: -9.821149\n",
      "epsilon:0.042779 episode_count: 18319. steps_count: 8097636.000000\n",
      "ep 2617: ep_len:500 episode reward: total was 3.390000. running mean: -9.689037\n",
      "ep 2617: ep_len:500 episode reward: total was -20.450000. running mean: -9.796647\n",
      "ep 2617: ep_len:373 episode reward: total was -24.940000. running mean: -9.948081\n",
      "ep 2617: ep_len:555 episode reward: total was 2.320000. running mean: -9.825400\n",
      "ep 2617: ep_len:108 episode reward: total was 6.560000. running mean: -9.661546\n",
      "ep 2617: ep_len:308 episode reward: total was -15.870000. running mean: -9.723630\n",
      "ep 2617: ep_len:610 episode reward: total was -14.610000. running mean: -9.772494\n",
      "epsilon:0.042643 episode_count: 18326. steps_count: 8100590.000000\n",
      "ep 2618: ep_len:214 episode reward: total was 2.100000. running mean: -9.653769\n",
      "ep 2618: ep_len:535 episode reward: total was 0.240000. running mean: -9.554831\n",
      "ep 2618: ep_len:500 episode reward: total was -30.130000. running mean: -9.760583\n",
      "ep 2618: ep_len:379 episode reward: total was 1.780000. running mean: -9.645177\n",
      "ep 2618: ep_len:3 episode reward: total was 0.000000. running mean: -9.548726\n",
      "ep 2618: ep_len:525 episode reward: total was 7.900000. running mean: -9.374238\n",
      "ep 2618: ep_len:276 episode reward: total was -6.320000. running mean: -9.343696\n",
      "epsilon:0.042506 episode_count: 18333. steps_count: 8103022.000000\n",
      "ep 2619: ep_len:500 episode reward: total was -21.920000. running mean: -9.469459\n",
      "ep 2619: ep_len:193 episode reward: total was -9.910000. running mean: -9.473864\n",
      "ep 2619: ep_len:575 episode reward: total was -10.620000. running mean: -9.485326\n",
      "ep 2619: ep_len:500 episode reward: total was -7.010000. running mean: -9.460572\n",
      "ep 2619: ep_len:89 episode reward: total was 1.530000. running mean: -9.350667\n",
      "ep 2619: ep_len:640 episode reward: total was -35.600000. running mean: -9.613160\n",
      "ep 2619: ep_len:500 episode reward: total was -14.510000. running mean: -9.662128\n",
      "epsilon:0.042370 episode_count: 18340. steps_count: 8106019.000000\n",
      "ep 2620: ep_len:580 episode reward: total was -21.230000. running mean: -9.777807\n",
      "ep 2620: ep_len:610 episode reward: total was -12.170000. running mean: -9.801729\n",
      "ep 2620: ep_len:570 episode reward: total was -19.730000. running mean: -9.901012\n",
      "ep 2620: ep_len:600 episode reward: total was -27.080000. running mean: -10.072802\n",
      "ep 2620: ep_len:3 episode reward: total was 0.000000. running mean: -9.972074\n",
      "ep 2620: ep_len:580 episode reward: total was -2.950000. running mean: -9.901853\n",
      "ep 2620: ep_len:625 episode reward: total was -9.830000. running mean: -9.901134\n",
      "epsilon:0.042233 episode_count: 18347. steps_count: 8109587.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2621: ep_len:126 episode reward: total was 0.080000. running mean: -9.801323\n",
      "ep 2621: ep_len:745 episode reward: total was -109.420000. running mean: -10.797510\n",
      "ep 2621: ep_len:600 episode reward: total was -34.150000. running mean: -11.031035\n",
      "ep 2621: ep_len:500 episode reward: total was 4.020000. running mean: -10.880524\n",
      "ep 2621: ep_len:3 episode reward: total was 0.000000. running mean: -10.771719\n",
      "ep 2621: ep_len:650 episode reward: total was -58.930000. running mean: -11.253302\n",
      "ep 2621: ep_len:560 episode reward: total was -15.530000. running mean: -11.296069\n",
      "epsilon:0.042097 episode_count: 18354. steps_count: 8112771.000000\n",
      "ep 2622: ep_len:260 episode reward: total was 4.650000. running mean: -11.136608\n",
      "ep 2622: ep_len:500 episode reward: total was 0.700000. running mean: -11.018242\n",
      "ep 2622: ep_len:795 episode reward: total was -44.730000. running mean: -11.355360\n",
      "ep 2622: ep_len:580 episode reward: total was 3.090000. running mean: -11.210906\n",
      "ep 2622: ep_len:39 episode reward: total was 1.000000. running mean: -11.088797\n",
      "ep 2622: ep_len:515 episode reward: total was -4.580000. running mean: -11.023709\n",
      "ep 2622: ep_len:180 episode reward: total was -10.390000. running mean: -11.017372\n",
      "epsilon:0.041960 episode_count: 18361. steps_count: 8115640.000000\n",
      "ep 2623: ep_len:520 episode reward: total was -13.110000. running mean: -11.038298\n",
      "ep 2623: ep_len:590 episode reward: total was 0.130000. running mean: -10.926615\n",
      "ep 2623: ep_len:65 episode reward: total was 1.550000. running mean: -10.801849\n",
      "ep 2623: ep_len:500 episode reward: total was 3.000000. running mean: -10.663831\n",
      "ep 2623: ep_len:3 episode reward: total was 0.000000. running mean: -10.557192\n",
      "ep 2623: ep_len:550 episode reward: total was -0.870000. running mean: -10.460320\n",
      "ep 2623: ep_len:520 episode reward: total was -20.170000. running mean: -10.557417\n",
      "epsilon:0.041824 episode_count: 18368. steps_count: 8118388.000000\n",
      "ep 2624: ep_len:265 episode reward: total was 5.640000. running mean: -10.395443\n",
      "ep 2624: ep_len:545 episode reward: total was -11.830000. running mean: -10.409789\n",
      "ep 2624: ep_len:455 episode reward: total was -16.330000. running mean: -10.468991\n",
      "ep 2624: ep_len:520 episode reward: total was -0.000000. running mean: -10.364301\n",
      "ep 2624: ep_len:112 episode reward: total was 4.560000. running mean: -10.215058\n",
      "ep 2624: ep_len:500 episode reward: total was -0.720000. running mean: -10.120107\n",
      "ep 2624: ep_len:500 episode reward: total was -14.720000. running mean: -10.166106\n",
      "epsilon:0.041687 episode_count: 18375. steps_count: 8121285.000000\n",
      "ep 2625: ep_len:575 episode reward: total was -5.410000. running mean: -10.118545\n",
      "ep 2625: ep_len:610 episode reward: total was 14.410000. running mean: -9.873260\n",
      "ep 2625: ep_len:525 episode reward: total was -31.430000. running mean: -10.088827\n",
      "ep 2625: ep_len:500 episode reward: total was -39.270000. running mean: -10.380639\n",
      "ep 2625: ep_len:3 episode reward: total was 0.000000. running mean: -10.276832\n",
      "ep 2625: ep_len:540 episode reward: total was -35.290000. running mean: -10.526964\n",
      "ep 2625: ep_len:500 episode reward: total was -4.630000. running mean: -10.467994\n",
      "epsilon:0.041551 episode_count: 18382. steps_count: 8124538.000000\n",
      "ep 2626: ep_len:595 episode reward: total was -16.390000. running mean: -10.527214\n",
      "ep 2626: ep_len:590 episode reward: total was -24.520000. running mean: -10.667142\n",
      "ep 2626: ep_len:386 episode reward: total was -13.280000. running mean: -10.693271\n",
      "ep 2626: ep_len:500 episode reward: total was -16.550000. running mean: -10.751838\n",
      "ep 2626: ep_len:129 episode reward: total was 6.050000. running mean: -10.583820\n",
      "ep 2626: ep_len:500 episode reward: total was -20.540000. running mean: -10.683382\n",
      "ep 2626: ep_len:545 episode reward: total was -45.220000. running mean: -11.028748\n",
      "epsilon:0.041414 episode_count: 18389. steps_count: 8127783.000000\n",
      "ep 2627: ep_len:570 episode reward: total was -10.600000. running mean: -11.024460\n",
      "ep 2627: ep_len:500 episode reward: total was -3.000000. running mean: -10.944216\n",
      "ep 2627: ep_len:389 episode reward: total was -10.360000. running mean: -10.938374\n",
      "ep 2627: ep_len:595 episode reward: total was -1.470000. running mean: -10.843690\n",
      "ep 2627: ep_len:99 episode reward: total was -2.970000. running mean: -10.764953\n",
      "ep 2627: ep_len:600 episode reward: total was -3.470000. running mean: -10.692003\n",
      "ep 2627: ep_len:555 episode reward: total was -7.020000. running mean: -10.655283\n",
      "epsilon:0.041278 episode_count: 18396. steps_count: 8131091.000000\n",
      "ep 2628: ep_len:525 episode reward: total was -3.490000. running mean: -10.583631\n",
      "ep 2628: ep_len:500 episode reward: total was 1.670000. running mean: -10.461094\n",
      "ep 2628: ep_len:505 episode reward: total was -14.460000. running mean: -10.501083\n",
      "ep 2628: ep_len:520 episode reward: total was -2.930000. running mean: -10.425372\n",
      "ep 2628: ep_len:105 episode reward: total was 4.540000. running mean: -10.275719\n",
      "ep 2628: ep_len:625 episode reward: total was -14.900000. running mean: -10.321962\n",
      "ep 2628: ep_len:520 episode reward: total was -39.070000. running mean: -10.609442\n",
      "epsilon:0.041141 episode_count: 18403. steps_count: 8134391.000000\n",
      "ep 2629: ep_len:635 episode reward: total was -26.600000. running mean: -10.769348\n",
      "ep 2629: ep_len:515 episode reward: total was -26.440000. running mean: -10.926054\n",
      "ep 2629: ep_len:500 episode reward: total was -2.960000. running mean: -10.846394\n",
      "ep 2629: ep_len:535 episode reward: total was -2.610000. running mean: -10.764030\n",
      "ep 2629: ep_len:76 episode reward: total was 3.010000. running mean: -10.626289\n",
      "ep 2629: ep_len:515 episode reward: total was 1.060000. running mean: -10.509426\n",
      "ep 2629: ep_len:515 episode reward: total was -48.230000. running mean: -10.886632\n",
      "epsilon:0.041005 episode_count: 18410. steps_count: 8137682.000000\n",
      "ep 2630: ep_len:248 episode reward: total was -23.910000. running mean: -11.016866\n",
      "ep 2630: ep_len:515 episode reward: total was -0.150000. running mean: -10.908197\n",
      "ep 2630: ep_len:500 episode reward: total was -18.670000. running mean: -10.985815\n",
      "ep 2630: ep_len:520 episode reward: total was -64.760000. running mean: -11.523557\n",
      "ep 2630: ep_len:3 episode reward: total was 0.000000. running mean: -11.408321\n",
      "ep 2630: ep_len:505 episode reward: total was -19.950000. running mean: -11.493738\n",
      "ep 2630: ep_len:595 episode reward: total was -29.080000. running mean: -11.669601\n",
      "epsilon:0.040868 episode_count: 18417. steps_count: 8140568.000000\n",
      "ep 2631: ep_len:605 episode reward: total was -12.070000. running mean: -11.673605\n",
      "ep 2631: ep_len:500 episode reward: total was -7.000000. running mean: -11.626869\n",
      "ep 2631: ep_len:448 episode reward: total was 0.260000. running mean: -11.508000\n",
      "ep 2631: ep_len:510 episode reward: total was -5.960000. running mean: -11.452520\n",
      "ep 2631: ep_len:3 episode reward: total was 0.000000. running mean: -11.337995\n",
      "ep 2631: ep_len:795 episode reward: total was -75.790000. running mean: -11.982515\n",
      "ep 2631: ep_len:590 episode reward: total was -14.350000. running mean: -12.006190\n",
      "epsilon:0.040732 episode_count: 18424. steps_count: 8144019.000000\n",
      "ep 2632: ep_len:530 episode reward: total was 5.860000. running mean: -11.827528\n",
      "ep 2632: ep_len:525 episode reward: total was 11.820000. running mean: -11.591053\n",
      "ep 2632: ep_len:640 episode reward: total was -0.950000. running mean: -11.484642\n",
      "ep 2632: ep_len:515 episode reward: total was -22.090000. running mean: -11.590696\n",
      "ep 2632: ep_len:95 episode reward: total was 5.040000. running mean: -11.424389\n",
      "ep 2632: ep_len:505 episode reward: total was -4.740000. running mean: -11.357545\n",
      "ep 2632: ep_len:610 episode reward: total was -22.550000. running mean: -11.469469\n",
      "epsilon:0.040595 episode_count: 18431. steps_count: 8147439.000000\n",
      "ep 2633: ep_len:600 episode reward: total was 3.440000. running mean: -11.320375\n",
      "ep 2633: ep_len:650 episode reward: total was -54.010000. running mean: -11.747271\n",
      "ep 2633: ep_len:500 episode reward: total was -37.600000. running mean: -12.005798\n",
      "ep 2633: ep_len:515 episode reward: total was -9.560000. running mean: -11.981340\n",
      "ep 2633: ep_len:3 episode reward: total was 0.000000. running mean: -11.861527\n",
      "ep 2633: ep_len:635 episode reward: total was -7.530000. running mean: -11.818212\n",
      "ep 2633: ep_len:590 episode reward: total was -43.060000. running mean: -12.130629\n",
      "epsilon:0.040459 episode_count: 18438. steps_count: 8150932.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2634: ep_len:630 episode reward: total was 2.640000. running mean: -11.982923\n",
      "ep 2634: ep_len:560 episode reward: total was -20.580000. running mean: -12.068894\n",
      "ep 2634: ep_len:755 episode reward: total was -60.310000. running mean: -12.551305\n",
      "ep 2634: ep_len:500 episode reward: total was -17.620000. running mean: -12.601992\n",
      "ep 2634: ep_len:3 episode reward: total was 0.000000. running mean: -12.475972\n",
      "ep 2634: ep_len:650 episode reward: total was -30.910000. running mean: -12.660312\n",
      "ep 2634: ep_len:530 episode reward: total was -6.090000. running mean: -12.594609\n",
      "epsilon:0.040322 episode_count: 18445. steps_count: 8154560.000000\n",
      "ep 2635: ep_len:132 episode reward: total was 0.090000. running mean: -12.467763\n",
      "ep 2635: ep_len:500 episode reward: total was -7.150000. running mean: -12.414585\n",
      "ep 2635: ep_len:645 episode reward: total was -8.200000. running mean: -12.372440\n",
      "ep 2635: ep_len:565 episode reward: total was 4.420000. running mean: -12.204515\n",
      "ep 2635: ep_len:93 episode reward: total was -7.960000. running mean: -12.162070\n",
      "ep 2635: ep_len:500 episode reward: total was -28.730000. running mean: -12.327749\n",
      "ep 2635: ep_len:500 episode reward: total was -34.590000. running mean: -12.550372\n",
      "epsilon:0.040186 episode_count: 18452. steps_count: 8157495.000000\n",
      "ep 2636: ep_len:197 episode reward: total was 0.120000. running mean: -12.423668\n",
      "ep 2636: ep_len:575 episode reward: total was 10.360000. running mean: -12.195831\n",
      "ep 2636: ep_len:700 episode reward: total was -50.260000. running mean: -12.576473\n",
      "ep 2636: ep_len:515 episode reward: total was 1.440000. running mean: -12.436308\n",
      "ep 2636: ep_len:85 episode reward: total was -10.940000. running mean: -12.421345\n",
      "ep 2636: ep_len:520 episode reward: total was -0.760000. running mean: -12.304732\n",
      "ep 2636: ep_len:211 episode reward: total was -7.880000. running mean: -12.260485\n",
      "epsilon:0.040049 episode_count: 18459. steps_count: 8160298.000000\n",
      "ep 2637: ep_len:1005 episode reward: total was -103.900000. running mean: -13.176880\n",
      "ep 2637: ep_len:585 episode reward: total was -23.830000. running mean: -13.283411\n",
      "ep 2637: ep_len:575 episode reward: total was -8.780000. running mean: -13.238377\n",
      "ep 2637: ep_len:570 episode reward: total was -32.280000. running mean: -13.428793\n",
      "ep 2637: ep_len:3 episode reward: total was 0.000000. running mean: -13.294505\n",
      "ep 2637: ep_len:595 episode reward: total was 3.460000. running mean: -13.126960\n",
      "ep 2637: ep_len:281 episode reward: total was -7.290000. running mean: -13.068590\n",
      "epsilon:0.039913 episode_count: 18466. steps_count: 8163912.000000\n",
      "ep 2638: ep_len:505 episode reward: total was -5.920000. running mean: -12.997105\n",
      "ep 2638: ep_len:630 episode reward: total was -8.060000. running mean: -12.947734\n",
      "ep 2638: ep_len:455 episode reward: total was -23.790000. running mean: -13.056156\n",
      "ep 2638: ep_len:570 episode reward: total was -45.260000. running mean: -13.378195\n",
      "ep 2638: ep_len:105 episode reward: total was 5.030000. running mean: -13.194113\n",
      "ep 2638: ep_len:261 episode reward: total was 8.170000. running mean: -12.980472\n",
      "ep 2638: ep_len:515 episode reward: total was -25.440000. running mean: -13.105067\n",
      "epsilon:0.039776 episode_count: 18473. steps_count: 8166953.000000\n",
      "ep 2639: ep_len:214 episode reward: total was 6.170000. running mean: -12.912316\n",
      "ep 2639: ep_len:363 episode reward: total was -40.330000. running mean: -13.186493\n",
      "ep 2639: ep_len:560 episode reward: total was -4.400000. running mean: -13.098628\n",
      "ep 2639: ep_len:500 episode reward: total was 6.390000. running mean: -12.903742\n",
      "ep 2639: ep_len:3 episode reward: total was 0.000000. running mean: -12.774704\n",
      "ep 2639: ep_len:510 episode reward: total was 0.400000. running mean: -12.642957\n",
      "ep 2639: ep_len:510 episode reward: total was -29.140000. running mean: -12.807928\n",
      "epsilon:0.039640 episode_count: 18480. steps_count: 8169613.000000\n",
      "ep 2640: ep_len:615 episode reward: total was 1.670000. running mean: -12.663148\n",
      "ep 2640: ep_len:900 episode reward: total was -56.640000. running mean: -13.102917\n",
      "ep 2640: ep_len:500 episode reward: total was 2.060000. running mean: -12.951288\n",
      "ep 2640: ep_len:565 episode reward: total was -11.060000. running mean: -12.932375\n",
      "ep 2640: ep_len:3 episode reward: total was 0.000000. running mean: -12.803051\n",
      "ep 2640: ep_len:560 episode reward: total was -11.110000. running mean: -12.786121\n",
      "ep 2640: ep_len:284 episode reward: total was -9.350000. running mean: -12.751759\n",
      "epsilon:0.039503 episode_count: 18487. steps_count: 8173040.000000\n",
      "ep 2641: ep_len:565 episode reward: total was -18.930000. running mean: -12.813542\n",
      "ep 2641: ep_len:615 episode reward: total was 24.420000. running mean: -12.441206\n",
      "ep 2641: ep_len:412 episode reward: total was -10.850000. running mean: -12.425294\n",
      "ep 2641: ep_len:500 episode reward: total was -24.630000. running mean: -12.547341\n",
      "ep 2641: ep_len:3 episode reward: total was 0.000000. running mean: -12.421868\n",
      "ep 2641: ep_len:600 episode reward: total was -7.530000. running mean: -12.372949\n",
      "ep 2641: ep_len:525 episode reward: total was -15.810000. running mean: -12.407320\n",
      "epsilon:0.039367 episode_count: 18494. steps_count: 8176260.000000\n",
      "ep 2642: ep_len:575 episode reward: total was 6.380000. running mean: -12.219447\n",
      "ep 2642: ep_len:590 episode reward: total was -24.940000. running mean: -12.346652\n",
      "ep 2642: ep_len:620 episode reward: total was -43.320000. running mean: -12.656386\n",
      "ep 2642: ep_len:505 episode reward: total was -1.650000. running mean: -12.546322\n",
      "ep 2642: ep_len:3 episode reward: total was 0.000000. running mean: -12.420859\n",
      "ep 2642: ep_len:595 episode reward: total was -0.490000. running mean: -12.301550\n",
      "ep 2642: ep_len:550 episode reward: total was -21.070000. running mean: -12.389235\n",
      "epsilon:0.039230 episode_count: 18501. steps_count: 8179698.000000\n",
      "ep 2643: ep_len:555 episode reward: total was -23.740000. running mean: -12.502742\n",
      "ep 2643: ep_len:500 episode reward: total was 3.680000. running mean: -12.340915\n",
      "ep 2643: ep_len:500 episode reward: total was -25.620000. running mean: -12.473706\n",
      "ep 2643: ep_len:500 episode reward: total was 3.020000. running mean: -12.318769\n",
      "ep 2643: ep_len:3 episode reward: total was 0.000000. running mean: -12.195581\n",
      "ep 2643: ep_len:620 episode reward: total was -28.690000. running mean: -12.360525\n",
      "ep 2643: ep_len:620 episode reward: total was -22.950000. running mean: -12.466420\n",
      "epsilon:0.039094 episode_count: 18508. steps_count: 8182996.000000\n",
      "ep 2644: ep_len:545 episode reward: total was -12.120000. running mean: -12.462956\n",
      "ep 2644: ep_len:540 episode reward: total was -7.960000. running mean: -12.417926\n",
      "ep 2644: ep_len:500 episode reward: total was -3.940000. running mean: -12.333147\n",
      "ep 2644: ep_len:500 episode reward: total was 5.930000. running mean: -12.150515\n",
      "ep 2644: ep_len:84 episode reward: total was -10.950000. running mean: -12.138510\n",
      "ep 2644: ep_len:540 episode reward: total was 9.150000. running mean: -11.925625\n",
      "ep 2644: ep_len:328 episode reward: total was -17.830000. running mean: -11.984669\n",
      "epsilon:0.038957 episode_count: 18515. steps_count: 8186033.000000\n",
      "ep 2645: ep_len:650 episode reward: total was -0.870000. running mean: -11.873522\n",
      "ep 2645: ep_len:570 episode reward: total was -48.050000. running mean: -12.235287\n",
      "ep 2645: ep_len:635 episode reward: total was -6.410000. running mean: -12.177034\n",
      "ep 2645: ep_len:560 episode reward: total was 4.060000. running mean: -12.014664\n",
      "ep 2645: ep_len:3 episode reward: total was 0.000000. running mean: -11.894517\n",
      "ep 2645: ep_len:560 episode reward: total was -5.890000. running mean: -11.834472\n",
      "ep 2645: ep_len:190 episode reward: total was -8.350000. running mean: -11.799627\n",
      "epsilon:0.038821 episode_count: 18522. steps_count: 8189201.000000\n",
      "ep 2646: ep_len:655 episode reward: total was -19.210000. running mean: -11.873731\n",
      "ep 2646: ep_len:500 episode reward: total was 10.210000. running mean: -11.652894\n",
      "ep 2646: ep_len:79 episode reward: total was -1.460000. running mean: -11.550965\n",
      "ep 2646: ep_len:41 episode reward: total was 2.540000. running mean: -11.410055\n",
      "ep 2646: ep_len:3 episode reward: total was 0.000000. running mean: -11.295954\n",
      "ep 2646: ep_len:625 episode reward: total was -9.430000. running mean: -11.277295\n",
      "ep 2646: ep_len:620 episode reward: total was -7.190000. running mean: -11.236422\n",
      "epsilon:0.038684 episode_count: 18529. steps_count: 8191724.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2647: ep_len:545 episode reward: total was -3.130000. running mean: -11.155358\n",
      "ep 2647: ep_len:505 episode reward: total was -30.880000. running mean: -11.352604\n",
      "ep 2647: ep_len:550 episode reward: total was -20.920000. running mean: -11.448278\n",
      "ep 2647: ep_len:529 episode reward: total was -27.910000. running mean: -11.612895\n",
      "ep 2647: ep_len:3 episode reward: total was 0.000000. running mean: -11.496766\n",
      "ep 2647: ep_len:570 episode reward: total was 5.920000. running mean: -11.322599\n",
      "ep 2647: ep_len:339 episode reward: total was -16.790000. running mean: -11.377273\n",
      "epsilon:0.038548 episode_count: 18536. steps_count: 8194765.000000\n",
      "ep 2648: ep_len:515 episode reward: total was -6.800000. running mean: -11.331500\n",
      "ep 2648: ep_len:500 episode reward: total was 2.030000. running mean: -11.197885\n",
      "ep 2648: ep_len:555 episode reward: total was -10.770000. running mean: -11.193606\n",
      "ep 2648: ep_len:500 episode reward: total was 8.940000. running mean: -10.992270\n",
      "ep 2648: ep_len:3 episode reward: total was 0.000000. running mean: -10.882347\n",
      "ep 2648: ep_len:500 episode reward: total was 1.920000. running mean: -10.754324\n",
      "ep 2648: ep_len:630 episode reward: total was -29.600000. running mean: -10.942781\n",
      "epsilon:0.038411 episode_count: 18543. steps_count: 8197968.000000\n",
      "ep 2649: ep_len:605 episode reward: total was -28.040000. running mean: -11.113753\n",
      "ep 2649: ep_len:530 episode reward: total was -5.960000. running mean: -11.062215\n",
      "ep 2649: ep_len:560 episode reward: total was -27.030000. running mean: -11.221893\n",
      "ep 2649: ep_len:775 episode reward: total was -73.520000. running mean: -11.844874\n",
      "ep 2649: ep_len:3 episode reward: total was 0.000000. running mean: -11.726426\n",
      "ep 2649: ep_len:232 episode reward: total was 1.100000. running mean: -11.598161\n",
      "ep 2649: ep_len:630 episode reward: total was -36.890000. running mean: -11.851080\n",
      "epsilon:0.038275 episode_count: 18550. steps_count: 8201303.000000\n",
      "ep 2650: ep_len:257 episode reward: total was -28.330000. running mean: -12.015869\n",
      "ep 2650: ep_len:750 episode reward: total was -76.570000. running mean: -12.661410\n",
      "ep 2650: ep_len:361 episode reward: total was -15.890000. running mean: -12.693696\n",
      "ep 2650: ep_len:500 episode reward: total was -8.500000. running mean: -12.651759\n",
      "ep 2650: ep_len:3 episode reward: total was 0.000000. running mean: -12.525242\n",
      "ep 2650: ep_len:500 episode reward: total was -6.080000. running mean: -12.460789\n",
      "ep 2650: ep_len:500 episode reward: total was -18.010000. running mean: -12.516281\n",
      "epsilon:0.038138 episode_count: 18557. steps_count: 8204174.000000\n",
      "ep 2651: ep_len:247 episode reward: total was -1.400000. running mean: -12.405118\n",
      "ep 2651: ep_len:158 episode reward: total was -8.430000. running mean: -12.365367\n",
      "ep 2651: ep_len:418 episode reward: total was 4.240000. running mean: -12.199314\n",
      "ep 2651: ep_len:56 episode reward: total was 2.570000. running mean: -12.051620\n",
      "ep 2651: ep_len:3 episode reward: total was 0.000000. running mean: -11.931104\n",
      "ep 2651: ep_len:515 episode reward: total was -10.230000. running mean: -11.914093\n",
      "ep 2651: ep_len:615 episode reward: total was -28.530000. running mean: -12.080252\n",
      "epsilon:0.038002 episode_count: 18564. steps_count: 8206186.000000\n",
      "ep 2652: ep_len:600 episode reward: total was -13.230000. running mean: -12.091750\n",
      "ep 2652: ep_len:284 episode reward: total was -36.860000. running mean: -12.339432\n",
      "ep 2652: ep_len:520 episode reward: total was -16.480000. running mean: -12.380838\n",
      "ep 2652: ep_len:510 episode reward: total was -16.070000. running mean: -12.417730\n",
      "ep 2652: ep_len:3 episode reward: total was 0.000000. running mean: -12.293552\n",
      "ep 2652: ep_len:570 episode reward: total was -19.380000. running mean: -12.364417\n",
      "ep 2652: ep_len:505 episode reward: total was -2.290000. running mean: -12.263673\n",
      "epsilon:0.037865 episode_count: 18571. steps_count: 8209178.000000\n",
      "ep 2653: ep_len:540 episode reward: total was -0.370000. running mean: -12.144736\n",
      "ep 2653: ep_len:184 episode reward: total was -1.910000. running mean: -12.042388\n",
      "ep 2653: ep_len:600 episode reward: total was -22.730000. running mean: -12.149265\n",
      "ep 2653: ep_len:413 episode reward: total was -2.120000. running mean: -12.048972\n",
      "ep 2653: ep_len:3 episode reward: total was 0.000000. running mean: -11.928482\n",
      "ep 2653: ep_len:590 episode reward: total was -0.580000. running mean: -11.814997\n",
      "ep 2653: ep_len:560 episode reward: total was -27.030000. running mean: -11.967147\n",
      "epsilon:0.037729 episode_count: 18578. steps_count: 8212068.000000\n",
      "ep 2654: ep_len:615 episode reward: total was 8.170000. running mean: -11.765776\n",
      "ep 2654: ep_len:525 episode reward: total was -28.230000. running mean: -11.930418\n",
      "ep 2654: ep_len:575 episode reward: total was -19.730000. running mean: -12.008414\n",
      "ep 2654: ep_len:565 episode reward: total was -0.000000. running mean: -11.888330\n",
      "ep 2654: ep_len:3 episode reward: total was 0.000000. running mean: -11.769447\n",
      "ep 2654: ep_len:640 episode reward: total was -5.510000. running mean: -11.706852\n",
      "ep 2654: ep_len:286 episode reward: total was -28.880000. running mean: -11.878584\n",
      "epsilon:0.037592 episode_count: 18585. steps_count: 8215277.000000\n",
      "ep 2655: ep_len:134 episode reward: total was -16.960000. running mean: -11.929398\n",
      "ep 2655: ep_len:267 episode reward: total was -26.330000. running mean: -12.073404\n",
      "ep 2655: ep_len:650 episode reward: total was -0.640000. running mean: -11.959070\n",
      "ep 2655: ep_len:625 episode reward: total was -5.840000. running mean: -11.897879\n",
      "ep 2655: ep_len:78 episode reward: total was -0.940000. running mean: -11.788300\n",
      "ep 2655: ep_len:535 episode reward: total was -12.130000. running mean: -11.791717\n",
      "ep 2655: ep_len:605 episode reward: total was -10.340000. running mean: -11.777200\n",
      "epsilon:0.037456 episode_count: 18592. steps_count: 8218171.000000\n",
      "ep 2656: ep_len:610 episode reward: total was 4.640000. running mean: -11.613028\n",
      "ep 2656: ep_len:640 episode reward: total was 5.450000. running mean: -11.442398\n",
      "ep 2656: ep_len:750 episode reward: total was -22.650000. running mean: -11.554474\n",
      "ep 2656: ep_len:545 episode reward: total was -6.080000. running mean: -11.499729\n",
      "ep 2656: ep_len:55 episode reward: total was 4.000000. running mean: -11.344732\n",
      "ep 2656: ep_len:690 episode reward: total was -64.520000. running mean: -11.876484\n",
      "ep 2656: ep_len:291 episode reward: total was -5.290000. running mean: -11.810620\n",
      "epsilon:0.037319 episode_count: 18599. steps_count: 8221752.000000\n",
      "ep 2657: ep_len:750 episode reward: total was -33.730000. running mean: -12.029813\n",
      "ep 2657: ep_len:510 episode reward: total was 3.320000. running mean: -11.876315\n",
      "ep 2657: ep_len:555 episode reward: total was -11.610000. running mean: -11.873652\n",
      "ep 2657: ep_len:368 episode reward: total was 4.300000. running mean: -11.711916\n",
      "ep 2657: ep_len:86 episode reward: total was -8.980000. running mean: -11.684596\n",
      "ep 2657: ep_len:630 episode reward: total was -11.690000. running mean: -11.684650\n",
      "ep 2657: ep_len:520 episode reward: total was -25.100000. running mean: -11.818804\n",
      "epsilon:0.037183 episode_count: 18606. steps_count: 8225171.000000\n",
      "ep 2658: ep_len:560 episode reward: total was 1.570000. running mean: -11.684916\n",
      "ep 2658: ep_len:640 episode reward: total was 11.010000. running mean: -11.457967\n",
      "ep 2658: ep_len:500 episode reward: total was 2.500000. running mean: -11.318387\n",
      "ep 2658: ep_len:530 episode reward: total was -15.130000. running mean: -11.356503\n",
      "ep 2658: ep_len:3 episode reward: total was 0.000000. running mean: -11.242938\n",
      "ep 2658: ep_len:620 episode reward: total was 5.040000. running mean: -11.080109\n",
      "ep 2658: ep_len:505 episode reward: total was -5.810000. running mean: -11.027408\n",
      "epsilon:0.037046 episode_count: 18613. steps_count: 8228529.000000\n",
      "ep 2659: ep_len:192 episode reward: total was 1.050000. running mean: -10.906634\n",
      "ep 2659: ep_len:505 episode reward: total was -13.920000. running mean: -10.936767\n",
      "ep 2659: ep_len:660 episode reward: total was -0.820000. running mean: -10.835600\n",
      "ep 2659: ep_len:510 episode reward: total was 4.880000. running mean: -10.678444\n",
      "ep 2659: ep_len:3 episode reward: total was 0.000000. running mean: -10.571659\n",
      "ep 2659: ep_len:525 episode reward: total was -5.080000. running mean: -10.516743\n",
      "ep 2659: ep_len:565 episode reward: total was -19.640000. running mean: -10.607975\n",
      "epsilon:0.036910 episode_count: 18620. steps_count: 8231489.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2660: ep_len:510 episode reward: total was -24.900000. running mean: -10.750895\n",
      "ep 2660: ep_len:540 episode reward: total was -17.210000. running mean: -10.815486\n",
      "ep 2660: ep_len:505 episode reward: total was -9.990000. running mean: -10.807232\n",
      "ep 2660: ep_len:500 episode reward: total was -10.720000. running mean: -10.806359\n",
      "ep 2660: ep_len:3 episode reward: total was 0.000000. running mean: -10.698296\n",
      "ep 2660: ep_len:585 episode reward: total was -17.630000. running mean: -10.767613\n",
      "ep 2660: ep_len:580 episode reward: total was -2.010000. running mean: -10.680037\n",
      "epsilon:0.036773 episode_count: 18627. steps_count: 8234712.000000\n",
      "ep 2661: ep_len:660 episode reward: total was -15.130000. running mean: -10.724536\n",
      "ep 2661: ep_len:580 episode reward: total was -32.750000. running mean: -10.944791\n",
      "ep 2661: ep_len:670 episode reward: total was -9.650000. running mean: -10.931843\n",
      "ep 2661: ep_len:500 episode reward: total was -3.070000. running mean: -10.853225\n",
      "ep 2661: ep_len:3 episode reward: total was 0.000000. running mean: -10.744692\n",
      "ep 2661: ep_len:161 episode reward: total was -9.940000. running mean: -10.736645\n",
      "ep 2661: ep_len:615 episode reward: total was -17.420000. running mean: -10.803479\n",
      "epsilon:0.036637 episode_count: 18634. steps_count: 8237901.000000\n",
      "ep 2662: ep_len:249 episode reward: total was 4.130000. running mean: -10.654144\n",
      "ep 2662: ep_len:500 episode reward: total was 15.680000. running mean: -10.390803\n",
      "ep 2662: ep_len:417 episode reward: total was -0.320000. running mean: -10.290095\n",
      "ep 2662: ep_len:545 episode reward: total was 11.550000. running mean: -10.071694\n",
      "ep 2662: ep_len:102 episode reward: total was -11.460000. running mean: -10.085577\n",
      "ep 2662: ep_len:565 episode reward: total was -15.930000. running mean: -10.144021\n",
      "ep 2662: ep_len:277 episode reward: total was -27.420000. running mean: -10.316781\n",
      "epsilon:0.036500 episode_count: 18641. steps_count: 8240556.000000\n",
      "ep 2663: ep_len:708 episode reward: total was -47.220000. running mean: -10.685813\n",
      "ep 2663: ep_len:505 episode reward: total was -17.930000. running mean: -10.758255\n",
      "ep 2663: ep_len:625 episode reward: total was -8.530000. running mean: -10.735972\n",
      "ep 2663: ep_len:500 episode reward: total was -18.110000. running mean: -10.809713\n",
      "ep 2663: ep_len:3 episode reward: total was 0.000000. running mean: -10.701615\n",
      "ep 2663: ep_len:610 episode reward: total was -36.610000. running mean: -10.960699\n",
      "ep 2663: ep_len:178 episode reward: total was -2.870000. running mean: -10.879792\n",
      "epsilon:0.036364 episode_count: 18648. steps_count: 8243685.000000\n",
      "ep 2664: ep_len:500 episode reward: total was -19.420000. running mean: -10.965194\n",
      "ep 2664: ep_len:535 episode reward: total was 1.900000. running mean: -10.836542\n",
      "ep 2664: ep_len:545 episode reward: total was -19.790000. running mean: -10.926077\n",
      "ep 2664: ep_len:500 episode reward: total was 4.400000. running mean: -10.772816\n",
      "ep 2664: ep_len:2 episode reward: total was 0.000000. running mean: -10.665088\n",
      "ep 2664: ep_len:219 episode reward: total was 4.130000. running mean: -10.517137\n",
      "ep 2664: ep_len:600 episode reward: total was -7.840000. running mean: -10.490366\n",
      "epsilon:0.036227 episode_count: 18655. steps_count: 8246586.000000\n",
      "ep 2665: ep_len:500 episode reward: total was -22.400000. running mean: -10.609462\n",
      "ep 2665: ep_len:540 episode reward: total was -15.300000. running mean: -10.656368\n",
      "ep 2665: ep_len:530 episode reward: total was -11.790000. running mean: -10.667704\n",
      "ep 2665: ep_len:545 episode reward: total was 6.430000. running mean: -10.496727\n",
      "ep 2665: ep_len:3 episode reward: total was 0.000000. running mean: -10.391760\n",
      "ep 2665: ep_len:605 episode reward: total was 4.350000. running mean: -10.244342\n",
      "ep 2665: ep_len:298 episode reward: total was -4.270000. running mean: -10.184599\n",
      "epsilon:0.036091 episode_count: 18662. steps_count: 8249607.000000\n",
      "ep 2666: ep_len:116 episode reward: total was -8.910000. running mean: -10.171853\n",
      "ep 2666: ep_len:535 episode reward: total was 15.890000. running mean: -9.911234\n",
      "ep 2666: ep_len:625 episode reward: total was -72.660000. running mean: -10.538722\n",
      "ep 2666: ep_len:625 episode reward: total was 7.650000. running mean: -10.356835\n",
      "ep 2666: ep_len:100 episode reward: total was 5.050000. running mean: -10.202766\n",
      "ep 2666: ep_len:249 episode reward: total was 3.670000. running mean: -10.064038\n",
      "ep 2666: ep_len:570 episode reward: total was -7.000000. running mean: -10.033398\n",
      "epsilon:0.035954 episode_count: 18669. steps_count: 8252427.000000\n",
      "ep 2667: ep_len:535 episode reward: total was -26.440000. running mean: -10.197464\n",
      "ep 2667: ep_len:530 episode reward: total was 12.830000. running mean: -9.967189\n",
      "ep 2667: ep_len:655 episode reward: total was -4.660000. running mean: -9.914118\n",
      "ep 2667: ep_len:535 episode reward: total was -68.690000. running mean: -10.501876\n",
      "ep 2667: ep_len:115 episode reward: total was 0.550000. running mean: -10.391358\n",
      "ep 2667: ep_len:500 episode reward: total was -27.750000. running mean: -10.564944\n",
      "ep 2667: ep_len:620 episode reward: total was -18.870000. running mean: -10.647995\n",
      "epsilon:0.035818 episode_count: 18676. steps_count: 8255917.000000\n",
      "ep 2668: ep_len:525 episode reward: total was -0.180000. running mean: -10.543315\n",
      "ep 2668: ep_len:500 episode reward: total was -21.540000. running mean: -10.653282\n",
      "ep 2668: ep_len:620 episode reward: total was -5.900000. running mean: -10.605749\n",
      "ep 2668: ep_len:500 episode reward: total was -8.990000. running mean: -10.589591\n",
      "ep 2668: ep_len:97 episode reward: total was -10.950000. running mean: -10.593195\n",
      "ep 2668: ep_len:605 episode reward: total was -26.750000. running mean: -10.754763\n",
      "ep 2668: ep_len:332 episode reward: total was -14.820000. running mean: -10.795416\n",
      "epsilon:0.035681 episode_count: 18683. steps_count: 8259096.000000\n",
      "ep 2669: ep_len:545 episode reward: total was -20.760000. running mean: -10.895062\n",
      "ep 2669: ep_len:505 episode reward: total was -5.000000. running mean: -10.836111\n",
      "ep 2669: ep_len:590 episode reward: total was -26.750000. running mean: -10.995250\n",
      "ep 2669: ep_len:500 episode reward: total was -1.120000. running mean: -10.896497\n",
      "ep 2669: ep_len:3 episode reward: total was 0.000000. running mean: -10.787532\n",
      "ep 2669: ep_len:595 episode reward: total was -9.140000. running mean: -10.771057\n",
      "ep 2669: ep_len:610 episode reward: total was -8.040000. running mean: -10.743746\n",
      "epsilon:0.035545 episode_count: 18690. steps_count: 8262444.000000\n",
      "ep 2670: ep_len:505 episode reward: total was 9.740000. running mean: -10.538909\n",
      "ep 2670: ep_len:595 episode reward: total was 1.610000. running mean: -10.417420\n",
      "ep 2670: ep_len:585 episode reward: total was -10.520000. running mean: -10.418446\n",
      "ep 2670: ep_len:505 episode reward: total was -20.090000. running mean: -10.515161\n",
      "ep 2670: ep_len:3 episode reward: total was 0.000000. running mean: -10.410010\n",
      "ep 2670: ep_len:545 episode reward: total was -7.780000. running mean: -10.383710\n",
      "ep 2670: ep_len:580 episode reward: total was -19.500000. running mean: -10.474872\n",
      "epsilon:0.035408 episode_count: 18697. steps_count: 8265762.000000\n",
      "ep 2671: ep_len:500 episode reward: total was -3.220000. running mean: -10.402324\n",
      "ep 2671: ep_len:520 episode reward: total was 7.380000. running mean: -10.224501\n",
      "ep 2671: ep_len:765 episode reward: total was -51.740000. running mean: -10.639656\n",
      "ep 2671: ep_len:609 episode reward: total was -40.470000. running mean: -10.937959\n",
      "ep 2671: ep_len:3 episode reward: total was 0.000000. running mean: -10.828579\n",
      "ep 2671: ep_len:500 episode reward: total was -1.180000. running mean: -10.732094\n",
      "ep 2671: ep_len:515 episode reward: total was -17.930000. running mean: -10.804073\n",
      "epsilon:0.035272 episode_count: 18704. steps_count: 8269174.000000\n",
      "ep 2672: ep_len:500 episode reward: total was 6.760000. running mean: -10.628432\n",
      "ep 2672: ep_len:555 episode reward: total was -7.490000. running mean: -10.597048\n",
      "ep 2672: ep_len:65 episode reward: total was 0.020000. running mean: -10.490877\n",
      "ep 2672: ep_len:505 episode reward: total was -8.520000. running mean: -10.471168\n",
      "ep 2672: ep_len:2 episode reward: total was 0.000000. running mean: -10.366457\n",
      "ep 2672: ep_len:530 episode reward: total was -13.780000. running mean: -10.400592\n",
      "ep 2672: ep_len:605 episode reward: total was -10.770000. running mean: -10.404286\n",
      "epsilon:0.035135 episode_count: 18711. steps_count: 8271936.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2673: ep_len:265 episode reward: total was -8.900000. running mean: -10.389243\n",
      "ep 2673: ep_len:257 episode reward: total was -23.320000. running mean: -10.518551\n",
      "ep 2673: ep_len:500 episode reward: total was -8.960000. running mean: -10.502965\n",
      "ep 2673: ep_len:565 episode reward: total was 6.060000. running mean: -10.337336\n",
      "ep 2673: ep_len:3 episode reward: total was 0.000000. running mean: -10.233962\n",
      "ep 2673: ep_len:224 episode reward: total was 3.160000. running mean: -10.100023\n",
      "ep 2673: ep_len:560 episode reward: total was -14.590000. running mean: -10.144923\n",
      "epsilon:0.034999 episode_count: 18718. steps_count: 8274310.000000\n",
      "ep 2674: ep_len:595 episode reward: total was 7.090000. running mean: -9.972573\n",
      "ep 2674: ep_len:500 episode reward: total was -3.510000. running mean: -9.907948\n",
      "ep 2674: ep_len:500 episode reward: total was -4.580000. running mean: -9.854668\n",
      "ep 2674: ep_len:500 episode reward: total was -5.990000. running mean: -9.816021\n",
      "ep 2674: ep_len:109 episode reward: total was 4.050000. running mean: -9.677361\n",
      "ep 2674: ep_len:685 episode reward: total was -21.290000. running mean: -9.793488\n",
      "ep 2674: ep_len:505 episode reward: total was -4.060000. running mean: -9.736153\n",
      "epsilon:0.034862 episode_count: 18725. steps_count: 8277704.000000\n",
      "ep 2675: ep_len:500 episode reward: total was -12.270000. running mean: -9.761491\n",
      "ep 2675: ep_len:670 episode reward: total was -29.370000. running mean: -9.957576\n",
      "ep 2675: ep_len:575 episode reward: total was -1.200000. running mean: -9.870000\n",
      "ep 2675: ep_len:515 episode reward: total was -2.690000. running mean: -9.798200\n",
      "ep 2675: ep_len:84 episode reward: total was 6.540000. running mean: -9.634818\n",
      "ep 2675: ep_len:500 episode reward: total was -5.310000. running mean: -9.591570\n",
      "ep 2675: ep_len:500 episode reward: total was -5.790000. running mean: -9.553555\n",
      "epsilon:0.034726 episode_count: 18732. steps_count: 8281048.000000\n",
      "ep 2676: ep_len:620 episode reward: total was -0.000000. running mean: -9.458019\n",
      "ep 2676: ep_len:379 episode reward: total was -12.880000. running mean: -9.492239\n",
      "ep 2676: ep_len:500 episode reward: total was -3.630000. running mean: -9.433616\n",
      "ep 2676: ep_len:132 episode reward: total was 3.590000. running mean: -9.303380\n",
      "ep 2676: ep_len:3 episode reward: total was 0.000000. running mean: -9.210346\n",
      "ep 2676: ep_len:615 episode reward: total was -1.390000. running mean: -9.132143\n",
      "ep 2676: ep_len:595 episode reward: total was -11.030000. running mean: -9.151122\n",
      "epsilon:0.034589 episode_count: 18739. steps_count: 8283892.000000\n",
      "ep 2677: ep_len:182 episode reward: total was -7.410000. running mean: -9.133710\n",
      "ep 2677: ep_len:505 episode reward: total was 10.160000. running mean: -8.940773\n",
      "ep 2677: ep_len:846 episode reward: total was -54.650000. running mean: -9.397866\n",
      "ep 2677: ep_len:505 episode reward: total was -9.010000. running mean: -9.393987\n",
      "ep 2677: ep_len:3 episode reward: total was 0.000000. running mean: -9.300047\n",
      "ep 2677: ep_len:500 episode reward: total was 5.930000. running mean: -9.147747\n",
      "ep 2677: ep_len:530 episode reward: total was -5.570000. running mean: -9.111969\n",
      "epsilon:0.034453 episode_count: 18746. steps_count: 8286963.000000\n",
      "ep 2678: ep_len:505 episode reward: total was 4.890000. running mean: -8.971949\n",
      "ep 2678: ep_len:565 episode reward: total was -34.460000. running mean: -9.226830\n",
      "ep 2678: ep_len:550 episode reward: total was -2.460000. running mean: -9.159162\n",
      "ep 2678: ep_len:386 episode reward: total was -32.180000. running mean: -9.389370\n",
      "ep 2678: ep_len:3 episode reward: total was 0.000000. running mean: -9.295476\n",
      "ep 2678: ep_len:316 episode reward: total was 1.650000. running mean: -9.186022\n",
      "ep 2678: ep_len:500 episode reward: total was 3.970000. running mean: -9.054461\n",
      "epsilon:0.034316 episode_count: 18753. steps_count: 8289788.000000\n",
      "ep 2679: ep_len:241 episode reward: total was 5.620000. running mean: -8.907717\n",
      "ep 2679: ep_len:640 episode reward: total was 14.090000. running mean: -8.677740\n",
      "ep 2679: ep_len:610 episode reward: total was -4.890000. running mean: -8.639862\n",
      "ep 2679: ep_len:505 episode reward: total was 11.450000. running mean: -8.438964\n",
      "ep 2679: ep_len:3 episode reward: total was 0.000000. running mean: -8.354574\n",
      "ep 2679: ep_len:555 episode reward: total was -85.400000. running mean: -9.125028\n",
      "ep 2679: ep_len:590 episode reward: total was -16.550000. running mean: -9.199278\n",
      "epsilon:0.034180 episode_count: 18760. steps_count: 8292932.000000\n",
      "ep 2680: ep_len:500 episode reward: total was 1.710000. running mean: -9.090185\n",
      "ep 2680: ep_len:500 episode reward: total was 3.810000. running mean: -8.961183\n",
      "ep 2680: ep_len:70 episode reward: total was -0.950000. running mean: -8.881071\n",
      "ep 2680: ep_len:585 episode reward: total was 11.440000. running mean: -8.677861\n",
      "ep 2680: ep_len:102 episode reward: total was 7.560000. running mean: -8.515482\n",
      "ep 2680: ep_len:615 episode reward: total was 3.480000. running mean: -8.395527\n",
      "ep 2680: ep_len:605 episode reward: total was -1.030000. running mean: -8.321872\n",
      "epsilon:0.034043 episode_count: 18767. steps_count: 8295909.000000\n",
      "ep 2681: ep_len:660 episode reward: total was -21.800000. running mean: -8.456653\n",
      "ep 2681: ep_len:725 episode reward: total was -50.360000. running mean: -8.875687\n",
      "ep 2681: ep_len:500 episode reward: total was 2.410000. running mean: -8.762830\n",
      "ep 2681: ep_len:510 episode reward: total was -15.090000. running mean: -8.826102\n",
      "ep 2681: ep_len:3 episode reward: total was 0.000000. running mean: -8.737841\n",
      "ep 2681: ep_len:530 episode reward: total was -11.440000. running mean: -8.764862\n",
      "ep 2681: ep_len:595 episode reward: total was -9.840000. running mean: -8.775614\n",
      "epsilon:0.033907 episode_count: 18774. steps_count: 8299432.000000\n",
      "ep 2682: ep_len:218 episode reward: total was 2.690000. running mean: -8.660957\n",
      "ep 2682: ep_len:265 episode reward: total was -16.360000. running mean: -8.737948\n",
      "ep 2682: ep_len:555 episode reward: total was -11.820000. running mean: -8.768768\n",
      "ep 2682: ep_len:137 episode reward: total was 4.080000. running mean: -8.640281\n",
      "ep 2682: ep_len:3 episode reward: total was 0.000000. running mean: -8.553878\n",
      "ep 2682: ep_len:535 episode reward: total was -23.550000. running mean: -8.703839\n",
      "ep 2682: ep_len:520 episode reward: total was -0.510000. running mean: -8.621901\n",
      "epsilon:0.033770 episode_count: 18781. steps_count: 8301665.000000\n",
      "ep 2683: ep_len:500 episode reward: total was -37.130000. running mean: -8.906982\n",
      "ep 2683: ep_len:655 episode reward: total was 15.100000. running mean: -8.666912\n",
      "ep 2683: ep_len:565 episode reward: total was -3.640000. running mean: -8.616643\n",
      "ep 2683: ep_len:56 episode reward: total was -1.940000. running mean: -8.549876\n",
      "ep 2683: ep_len:3 episode reward: total was 0.000000. running mean: -8.464378\n",
      "ep 2683: ep_len:500 episode reward: total was -22.780000. running mean: -8.607534\n",
      "ep 2683: ep_len:535 episode reward: total was -6.560000. running mean: -8.587058\n",
      "epsilon:0.033634 episode_count: 18788. steps_count: 8304479.000000\n",
      "ep 2684: ep_len:590 episode reward: total was 0.430000. running mean: -8.496888\n",
      "ep 2684: ep_len:500 episode reward: total was -58.860000. running mean: -9.000519\n",
      "ep 2684: ep_len:500 episode reward: total was -3.690000. running mean: -8.947414\n",
      "ep 2684: ep_len:416 episode reward: total was -28.600000. running mean: -9.143940\n",
      "ep 2684: ep_len:89 episode reward: total was -7.450000. running mean: -9.127000\n",
      "ep 2684: ep_len:650 episode reward: total was -8.170000. running mean: -9.117430\n",
      "ep 2684: ep_len:735 episode reward: total was -90.160000. running mean: -9.927856\n",
      "epsilon:0.033497 episode_count: 18795. steps_count: 8307959.000000\n",
      "ep 2685: ep_len:500 episode reward: total was 2.720000. running mean: -9.801377\n",
      "ep 2685: ep_len:500 episode reward: total was 10.670000. running mean: -9.596664\n",
      "ep 2685: ep_len:500 episode reward: total was 1.970000. running mean: -9.480997\n",
      "ep 2685: ep_len:155 episode reward: total was 3.640000. running mean: -9.349787\n",
      "ep 2685: ep_len:3 episode reward: total was 0.000000. running mean: -9.256289\n",
      "ep 2685: ep_len:515 episode reward: total was -9.770000. running mean: -9.261426\n",
      "ep 2685: ep_len:565 episode reward: total was -2.000000. running mean: -9.188812\n",
      "epsilon:0.033361 episode_count: 18802. steps_count: 8310697.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2686: ep_len:585 episode reward: total was 2.920000. running mean: -9.067724\n",
      "ep 2686: ep_len:515 episode reward: total was -39.530000. running mean: -9.372347\n",
      "ep 2686: ep_len:500 episode reward: total was 0.530000. running mean: -9.273323\n",
      "ep 2686: ep_len:126 episode reward: total was -0.900000. running mean: -9.189590\n",
      "ep 2686: ep_len:101 episode reward: total was 5.540000. running mean: -9.042294\n",
      "ep 2686: ep_len:680 episode reward: total was -21.730000. running mean: -9.169171\n",
      "ep 2686: ep_len:645 episode reward: total was -37.510000. running mean: -9.452579\n",
      "epsilon:0.033224 episode_count: 18809. steps_count: 8313849.000000\n",
      "ep 2687: ep_len:500 episode reward: total was 3.870000. running mean: -9.319354\n",
      "ep 2687: ep_len:620 episode reward: total was -48.610000. running mean: -9.712260\n",
      "ep 2687: ep_len:605 episode reward: total was -8.020000. running mean: -9.695337\n",
      "ep 2687: ep_len:710 episode reward: total was -58.960000. running mean: -10.187984\n",
      "ep 2687: ep_len:3 episode reward: total was 0.000000. running mean: -10.086104\n",
      "ep 2687: ep_len:500 episode reward: total was -3.830000. running mean: -10.023543\n",
      "ep 2687: ep_len:510 episode reward: total was -15.440000. running mean: -10.077708\n",
      "epsilon:0.033088 episode_count: 18816. steps_count: 8317297.000000\n",
      "ep 2688: ep_len:545 episode reward: total was -31.440000. running mean: -10.291331\n",
      "ep 2688: ep_len:715 episode reward: total was -31.250000. running mean: -10.500917\n",
      "ep 2688: ep_len:680 episode reward: total was 5.360000. running mean: -10.342308\n",
      "ep 2688: ep_len:590 episode reward: total was 1.350000. running mean: -10.225385\n",
      "ep 2688: ep_len:3 episode reward: total was 0.000000. running mean: -10.123131\n",
      "ep 2688: ep_len:580 episode reward: total was -21.300000. running mean: -10.234900\n",
      "ep 2688: ep_len:500 episode reward: total was -5.690000. running mean: -10.189451\n",
      "epsilon:0.032951 episode_count: 18823. steps_count: 8320910.000000\n",
      "ep 2689: ep_len:510 episode reward: total was 2.660000. running mean: -10.060956\n",
      "ep 2689: ep_len:500 episode reward: total was -1.470000. running mean: -9.975047\n",
      "ep 2689: ep_len:585 episode reward: total was -9.040000. running mean: -9.965696\n",
      "ep 2689: ep_len:620 episode reward: total was -4.910000. running mean: -9.915139\n",
      "ep 2689: ep_len:3 episode reward: total was 0.000000. running mean: -9.815988\n",
      "ep 2689: ep_len:515 episode reward: total was -8.240000. running mean: -9.800228\n",
      "ep 2689: ep_len:500 episode reward: total was -20.650000. running mean: -9.908726\n",
      "epsilon:0.032815 episode_count: 18830. steps_count: 8324143.000000\n",
      "ep 2690: ep_len:595 episode reward: total was -8.420000. running mean: -9.893839\n",
      "ep 2690: ep_len:630 episode reward: total was -7.590000. running mean: -9.870800\n",
      "ep 2690: ep_len:378 episode reward: total was 7.240000. running mean: -9.699692\n",
      "ep 2690: ep_len:500 episode reward: total was -25.580000. running mean: -9.858495\n",
      "ep 2690: ep_len:97 episode reward: total was 6.050000. running mean: -9.699410\n",
      "ep 2690: ep_len:620 episode reward: total was -27.550000. running mean: -9.877916\n",
      "ep 2690: ep_len:301 episode reward: total was -0.790000. running mean: -9.787037\n",
      "epsilon:0.032678 episode_count: 18837. steps_count: 8327264.000000\n",
      "ep 2691: ep_len:990 episode reward: total was -157.340000. running mean: -11.262567\n",
      "ep 2691: ep_len:575 episode reward: total was -28.200000. running mean: -11.431941\n",
      "ep 2691: ep_len:469 episode reward: total was 6.780000. running mean: -11.249822\n",
      "ep 2691: ep_len:56 episode reward: total was -4.940000. running mean: -11.186723\n",
      "ep 2691: ep_len:126 episode reward: total was 6.050000. running mean: -11.014356\n",
      "ep 2691: ep_len:535 episode reward: total was -23.410000. running mean: -11.138313\n",
      "ep 2691: ep_len:505 episode reward: total was -10.070000. running mean: -11.127630\n",
      "epsilon:0.032542 episode_count: 18844. steps_count: 8330520.000000\n",
      "ep 2692: ep_len:223 episode reward: total was 9.650000. running mean: -10.919853\n",
      "ep 2692: ep_len:575 episode reward: total was -10.080000. running mean: -10.911455\n",
      "ep 2692: ep_len:560 episode reward: total was -1.600000. running mean: -10.818340\n",
      "ep 2692: ep_len:165 episode reward: total was 4.140000. running mean: -10.668757\n",
      "ep 2692: ep_len:95 episode reward: total was 6.050000. running mean: -10.501569\n",
      "ep 2692: ep_len:500 episode reward: total was -47.150000. running mean: -10.868053\n",
      "ep 2692: ep_len:600 episode reward: total was 0.530000. running mean: -10.754073\n",
      "epsilon:0.032405 episode_count: 18851. steps_count: 8333238.000000\n",
      "ep 2693: ep_len:745 episode reward: total was -57.350000. running mean: -11.220032\n",
      "ep 2693: ep_len:500 episode reward: total was -15.550000. running mean: -11.263332\n",
      "ep 2693: ep_len:590 episode reward: total was -13.980000. running mean: -11.290499\n",
      "ep 2693: ep_len:520 episode reward: total was -21.100000. running mean: -11.388594\n",
      "ep 2693: ep_len:3 episode reward: total was 0.000000. running mean: -11.274708\n",
      "ep 2693: ep_len:515 episode reward: total was -13.950000. running mean: -11.301461\n",
      "ep 2693: ep_len:164 episode reward: total was -1.360000. running mean: -11.202046\n",
      "epsilon:0.032269 episode_count: 18858. steps_count: 8336275.000000\n",
      "ep 2694: ep_len:750 episode reward: total was -36.220000. running mean: -11.452226\n",
      "ep 2694: ep_len:185 episode reward: total was -3.370000. running mean: -11.371403\n",
      "ep 2694: ep_len:600 episode reward: total was -14.450000. running mean: -11.402189\n",
      "ep 2694: ep_len:500 episode reward: total was 6.330000. running mean: -11.224867\n",
      "ep 2694: ep_len:3 episode reward: total was 0.000000. running mean: -11.112619\n",
      "ep 2694: ep_len:500 episode reward: total was -39.640000. running mean: -11.397892\n",
      "ep 2694: ep_len:680 episode reward: total was -64.130000. running mean: -11.925214\n",
      "epsilon:0.032132 episode_count: 18865. steps_count: 8339493.000000\n",
      "ep 2695: ep_len:515 episode reward: total was -10.490000. running mean: -11.910861\n",
      "ep 2695: ep_len:500 episode reward: total was 16.900000. running mean: -11.622753\n",
      "ep 2695: ep_len:710 episode reward: total was -16.190000. running mean: -11.668425\n",
      "ep 2695: ep_len:560 episode reward: total was -2.100000. running mean: -11.572741\n",
      "ep 2695: ep_len:3 episode reward: total was 0.000000. running mean: -11.457014\n",
      "ep 2695: ep_len:500 episode reward: total was -8.510000. running mean: -11.427543\n",
      "ep 2695: ep_len:1165 episode reward: total was -133.660000. running mean: -12.649868\n",
      "epsilon:0.031996 episode_count: 18872. steps_count: 8343446.000000\n",
      "ep 2696: ep_len:535 episode reward: total was 7.470000. running mean: -12.448669\n",
      "ep 2696: ep_len:500 episode reward: total was -20.210000. running mean: -12.526283\n",
      "ep 2696: ep_len:655 episode reward: total was -4.690000. running mean: -12.447920\n",
      "ep 2696: ep_len:515 episode reward: total was 0.890000. running mean: -12.314541\n",
      "ep 2696: ep_len:103 episode reward: total was 6.550000. running mean: -12.125895\n",
      "ep 2696: ep_len:505 episode reward: total was 3.910000. running mean: -11.965536\n",
      "ep 2696: ep_len:500 episode reward: total was -38.140000. running mean: -12.227281\n",
      "epsilon:0.031859 episode_count: 18879. steps_count: 8346759.000000\n",
      "ep 2697: ep_len:510 episode reward: total was -3.950000. running mean: -12.144508\n",
      "ep 2697: ep_len:550 episode reward: total was -31.490000. running mean: -12.337963\n",
      "ep 2697: ep_len:730 episode reward: total was -16.180000. running mean: -12.376383\n",
      "ep 2697: ep_len:515 episode reward: total was -24.630000. running mean: -12.498920\n",
      "ep 2697: ep_len:3 episode reward: total was 0.000000. running mean: -12.373930\n",
      "ep 2697: ep_len:585 episode reward: total was -15.750000. running mean: -12.407691\n",
      "ep 2697: ep_len:645 episode reward: total was -12.860000. running mean: -12.412214\n",
      "epsilon:0.031723 episode_count: 18886. steps_count: 8350297.000000\n",
      "ep 2698: ep_len:500 episode reward: total was 9.820000. running mean: -12.189892\n",
      "ep 2698: ep_len:515 episode reward: total was -13.520000. running mean: -12.203193\n",
      "ep 2698: ep_len:600 episode reward: total was -2.700000. running mean: -12.108161\n",
      "ep 2698: ep_len:505 episode reward: total was -10.420000. running mean: -12.091280\n",
      "ep 2698: ep_len:3 episode reward: total was 0.000000. running mean: -11.970367\n",
      "ep 2698: ep_len:585 episode reward: total was -22.240000. running mean: -12.073063\n",
      "ep 2698: ep_len:505 episode reward: total was -20.700000. running mean: -12.159332\n",
      "epsilon:0.031586 episode_count: 18893. steps_count: 8353510.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2699: ep_len:645 episode reward: total was -43.790000. running mean: -12.475639\n",
      "ep 2699: ep_len:500 episode reward: total was -2.160000. running mean: -12.372483\n",
      "ep 2699: ep_len:650 episode reward: total was -6.180000. running mean: -12.310558\n",
      "ep 2699: ep_len:500 episode reward: total was 2.410000. running mean: -12.163352\n",
      "ep 2699: ep_len:87 episode reward: total was 5.540000. running mean: -11.986319\n",
      "ep 2699: ep_len:500 episode reward: total was -6.770000. running mean: -11.934156\n",
      "ep 2699: ep_len:500 episode reward: total was -6.860000. running mean: -11.883414\n",
      "epsilon:0.031450 episode_count: 18900. steps_count: 8356892.000000\n",
      "ep 2700: ep_len:500 episode reward: total was 3.760000. running mean: -11.726980\n",
      "ep 2700: ep_len:640 episode reward: total was -3.960000. running mean: -11.649310\n",
      "ep 2700: ep_len:343 episode reward: total was -18.850000. running mean: -11.721317\n",
      "ep 2700: ep_len:500 episode reward: total was -17.650000. running mean: -11.780604\n",
      "ep 2700: ep_len:3 episode reward: total was 0.000000. running mean: -11.662798\n",
      "ep 2700: ep_len:500 episode reward: total was -7.290000. running mean: -11.619070\n",
      "ep 2700: ep_len:585 episode reward: total was -11.850000. running mean: -11.621379\n",
      "epsilon:0.031313 episode_count: 18907. steps_count: 8359963.000000\n",
      "ep 2701: ep_len:685 episode reward: total was -19.180000. running mean: -11.696965\n",
      "ep 2701: ep_len:525 episode reward: total was 12.860000. running mean: -11.451396\n",
      "ep 2701: ep_len:372 episode reward: total was 2.700000. running mean: -11.309882\n",
      "ep 2701: ep_len:121 episode reward: total was -0.930000. running mean: -11.206083\n",
      "ep 2701: ep_len:48 episode reward: total was 3.000000. running mean: -11.064022\n",
      "ep 2701: ep_len:510 episode reward: total was -19.230000. running mean: -11.145682\n",
      "ep 2701: ep_len:530 episode reward: total was -28.990000. running mean: -11.324125\n",
      "epsilon:0.031177 episode_count: 18914. steps_count: 8362754.000000\n",
      "ep 2702: ep_len:116 episode reward: total was -7.900000. running mean: -11.289884\n",
      "ep 2702: ep_len:635 episode reward: total was -43.640000. running mean: -11.613385\n",
      "ep 2702: ep_len:500 episode reward: total was -8.020000. running mean: -11.577451\n",
      "ep 2702: ep_len:520 episode reward: total was 9.400000. running mean: -11.367677\n",
      "ep 2702: ep_len:91 episode reward: total was 0.040000. running mean: -11.253600\n",
      "ep 2702: ep_len:505 episode reward: total was -18.620000. running mean: -11.327264\n",
      "ep 2702: ep_len:346 episode reward: total was -16.310000. running mean: -11.377091\n",
      "epsilon:0.031040 episode_count: 18921. steps_count: 8365467.000000\n",
      "ep 2703: ep_len:600 episode reward: total was 1.340000. running mean: -11.249920\n",
      "ep 2703: ep_len:745 episode reward: total was -58.430000. running mean: -11.721721\n",
      "ep 2703: ep_len:665 episode reward: total was -14.690000. running mean: -11.751404\n",
      "ep 2703: ep_len:505 episode reward: total was -21.190000. running mean: -11.845790\n",
      "ep 2703: ep_len:3 episode reward: total was 0.000000. running mean: -11.727332\n",
      "ep 2703: ep_len:510 episode reward: total was -2.370000. running mean: -11.633759\n",
      "ep 2703: ep_len:211 episode reward: total was -7.880000. running mean: -11.596221\n",
      "epsilon:0.030904 episode_count: 18928. steps_count: 8368706.000000\n",
      "ep 2704: ep_len:560 episode reward: total was -18.250000. running mean: -11.662759\n",
      "ep 2704: ep_len:500 episode reward: total was -13.070000. running mean: -11.676831\n",
      "ep 2704: ep_len:580 episode reward: total was -7.990000. running mean: -11.639963\n",
      "ep 2704: ep_len:530 episode reward: total was -4.370000. running mean: -11.567263\n",
      "ep 2704: ep_len:94 episode reward: total was -3.950000. running mean: -11.491091\n",
      "ep 2704: ep_len:660 episode reward: total was -5.690000. running mean: -11.433080\n",
      "ep 2704: ep_len:605 episode reward: total was -26.930000. running mean: -11.588049\n",
      "epsilon:0.030767 episode_count: 18935. steps_count: 8372235.000000\n",
      "ep 2705: ep_len:565 episode reward: total was 5.910000. running mean: -11.413068\n",
      "ep 2705: ep_len:500 episode reward: total was -5.500000. running mean: -11.353938\n",
      "ep 2705: ep_len:600 episode reward: total was -5.980000. running mean: -11.300198\n",
      "ep 2705: ep_len:655 episode reward: total was -47.030000. running mean: -11.657496\n",
      "ep 2705: ep_len:70 episode reward: total was -6.950000. running mean: -11.610421\n",
      "ep 2705: ep_len:605 episode reward: total was -67.540000. running mean: -12.169717\n",
      "ep 2705: ep_len:500 episode reward: total was -4.190000. running mean: -12.089920\n",
      "epsilon:0.030631 episode_count: 18942. steps_count: 8375730.000000\n",
      "ep 2706: ep_len:570 episode reward: total was 2.560000. running mean: -11.943421\n",
      "ep 2706: ep_len:720 episode reward: total was -66.900000. running mean: -12.492987\n",
      "ep 2706: ep_len:570 episode reward: total was -2.220000. running mean: -12.390257\n",
      "ep 2706: ep_len:500 episode reward: total was 9.880000. running mean: -12.167554\n",
      "ep 2706: ep_len:3 episode reward: total was 0.000000. running mean: -12.045879\n",
      "ep 2706: ep_len:500 episode reward: total was 1.890000. running mean: -11.906520\n",
      "ep 2706: ep_len:545 episode reward: total was -17.510000. running mean: -11.962555\n",
      "epsilon:0.030494 episode_count: 18949. steps_count: 8379138.000000\n",
      "ep 2707: ep_len:655 episode reward: total was -40.880000. running mean: -12.251729\n",
      "ep 2707: ep_len:500 episode reward: total was -4.700000. running mean: -12.176212\n",
      "ep 2707: ep_len:500 episode reward: total was -2.060000. running mean: -12.075050\n",
      "ep 2707: ep_len:402 episode reward: total was -9.110000. running mean: -12.045399\n",
      "ep 2707: ep_len:79 episode reward: total was 1.020000. running mean: -11.914745\n",
      "ep 2707: ep_len:610 episode reward: total was -47.620000. running mean: -12.271798\n",
      "ep 2707: ep_len:279 episode reward: total was -7.360000. running mean: -12.222680\n",
      "epsilon:0.030358 episode_count: 18956. steps_count: 8382163.000000\n",
      "ep 2708: ep_len:500 episode reward: total was -36.040000. running mean: -12.460853\n",
      "ep 2708: ep_len:515 episode reward: total was -1.860000. running mean: -12.354844\n",
      "ep 2708: ep_len:565 episode reward: total was -14.850000. running mean: -12.379796\n",
      "ep 2708: ep_len:500 episode reward: total was -2.880000. running mean: -12.284798\n",
      "ep 2708: ep_len:3 episode reward: total was 0.000000. running mean: -12.161950\n",
      "ep 2708: ep_len:505 episode reward: total was -20.870000. running mean: -12.249031\n",
      "ep 2708: ep_len:555 episode reward: total was -8.120000. running mean: -12.207740\n",
      "epsilon:0.030221 episode_count: 18963. steps_count: 8385306.000000\n",
      "ep 2709: ep_len:565 episode reward: total was -40.920000. running mean: -12.494863\n",
      "ep 2709: ep_len:500 episode reward: total was 6.220000. running mean: -12.307714\n",
      "ep 2709: ep_len:500 episode reward: total was -2.070000. running mean: -12.205337\n",
      "ep 2709: ep_len:505 episode reward: total was -4.960000. running mean: -12.132884\n",
      "ep 2709: ep_len:3 episode reward: total was 0.000000. running mean: -12.011555\n",
      "ep 2709: ep_len:525 episode reward: total was -8.830000. running mean: -11.979739\n",
      "ep 2709: ep_len:198 episode reward: total was -6.900000. running mean: -11.928942\n",
      "epsilon:0.030085 episode_count: 18970. steps_count: 8388102.000000\n",
      "ep 2710: ep_len:565 episode reward: total was -2.140000. running mean: -11.831053\n",
      "ep 2710: ep_len:595 episode reward: total was -14.600000. running mean: -11.858742\n",
      "ep 2710: ep_len:640 episode reward: total was -6.920000. running mean: -11.809355\n",
      "ep 2710: ep_len:500 episode reward: total was -0.910000. running mean: -11.700361\n",
      "ep 2710: ep_len:3 episode reward: total was 0.000000. running mean: -11.583357\n",
      "ep 2710: ep_len:675 episode reward: total was 0.300000. running mean: -11.464524\n",
      "ep 2710: ep_len:610 episode reward: total was -8.830000. running mean: -11.438179\n",
      "epsilon:0.029948 episode_count: 18977. steps_count: 8391690.000000\n",
      "ep 2711: ep_len:590 episode reward: total was -5.600000. running mean: -11.379797\n",
      "ep 2711: ep_len:640 episode reward: total was -1.580000. running mean: -11.281799\n",
      "ep 2711: ep_len:447 episode reward: total was -6.270000. running mean: -11.231681\n",
      "ep 2711: ep_len:535 episode reward: total was -7.940000. running mean: -11.198764\n",
      "ep 2711: ep_len:87 episode reward: total was 0.550000. running mean: -11.081276\n",
      "ep 2711: ep_len:505 episode reward: total was -14.120000. running mean: -11.111664\n",
      "ep 2711: ep_len:281 episode reward: total was -6.290000. running mean: -11.063447\n",
      "epsilon:0.029812 episode_count: 18984. steps_count: 8394775.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2712: ep_len:133 episode reward: total was -8.970000. running mean: -11.042513\n",
      "ep 2712: ep_len:500 episode reward: total was 14.820000. running mean: -10.783887\n",
      "ep 2712: ep_len:500 episode reward: total was 0.760000. running mean: -10.668449\n",
      "ep 2712: ep_len:540 episode reward: total was -6.180000. running mean: -10.623564\n",
      "ep 2712: ep_len:3 episode reward: total was 0.000000. running mean: -10.517328\n",
      "ep 2712: ep_len:620 episode reward: total was -6.050000. running mean: -10.472655\n",
      "ep 2712: ep_len:202 episode reward: total was -7.900000. running mean: -10.446929\n",
      "epsilon:0.029675 episode_count: 18991. steps_count: 8397273.000000\n",
      "ep 2713: ep_len:265 episode reward: total was -0.910000. running mean: -10.351559\n",
      "ep 2713: ep_len:605 episode reward: total was -42.090000. running mean: -10.668944\n",
      "ep 2713: ep_len:402 episode reward: total was -20.880000. running mean: -10.771054\n",
      "ep 2713: ep_len:500 episode reward: total was -12.550000. running mean: -10.788844\n",
      "ep 2713: ep_len:94 episode reward: total was -9.460000. running mean: -10.775555\n",
      "ep 2713: ep_len:540 episode reward: total was 8.640000. running mean: -10.581400\n",
      "ep 2713: ep_len:500 episode reward: total was -24.000000. running mean: -10.715586\n",
      "epsilon:0.029539 episode_count: 18998. steps_count: 8400179.000000\n",
      "ep 2714: ep_len:600 episode reward: total was -14.350000. running mean: -10.751930\n",
      "ep 2714: ep_len:625 episode reward: total was -11.560000. running mean: -10.760011\n",
      "ep 2714: ep_len:560 episode reward: total was -10.260000. running mean: -10.755010\n",
      "ep 2714: ep_len:595 episode reward: total was 3.090000. running mean: -10.616560\n",
      "ep 2714: ep_len:3 episode reward: total was 0.000000. running mean: -10.510395\n",
      "ep 2714: ep_len:560 episode reward: total was 7.930000. running mean: -10.325991\n",
      "ep 2714: ep_len:540 episode reward: total was -9.800000. running mean: -10.320731\n",
      "epsilon:0.029402 episode_count: 19005. steps_count: 8403662.000000\n",
      "ep 2715: ep_len:695 episode reward: total was -24.150000. running mean: -10.459024\n",
      "ep 2715: ep_len:550 episode reward: total was -2.180000. running mean: -10.376233\n",
      "ep 2715: ep_len:650 episode reward: total was 1.640000. running mean: -10.256071\n",
      "ep 2715: ep_len:132 episode reward: total was 5.090000. running mean: -10.102610\n",
      "ep 2715: ep_len:3 episode reward: total was 0.000000. running mean: -10.001584\n",
      "ep 2715: ep_len:500 episode reward: total was -24.630000. running mean: -10.147868\n",
      "ep 2715: ep_len:600 episode reward: total was -13.570000. running mean: -10.182090\n",
      "epsilon:0.029266 episode_count: 19012. steps_count: 8406792.000000\n",
      "ep 2716: ep_len:530 episode reward: total was -26.640000. running mean: -10.346669\n",
      "ep 2716: ep_len:540 episode reward: total was -12.340000. running mean: -10.366602\n",
      "ep 2716: ep_len:595 episode reward: total was 5.080000. running mean: -10.212136\n",
      "ep 2716: ep_len:520 episode reward: total was 11.480000. running mean: -9.995215\n",
      "ep 2716: ep_len:70 episode reward: total was -7.450000. running mean: -9.969763\n",
      "ep 2716: ep_len:585 episode reward: total was 7.950000. running mean: -9.790565\n",
      "ep 2716: ep_len:313 episode reward: total was -9.840000. running mean: -9.791059\n",
      "epsilon:0.029129 episode_count: 19019. steps_count: 8409945.000000\n",
      "ep 2717: ep_len:540 episode reward: total was -10.500000. running mean: -9.798149\n",
      "ep 2717: ep_len:505 episode reward: total was -16.030000. running mean: -9.860467\n",
      "ep 2717: ep_len:520 episode reward: total was -0.960000. running mean: -9.771463\n",
      "ep 2717: ep_len:500 episode reward: total was -34.810000. running mean: -10.021848\n",
      "ep 2717: ep_len:50 episode reward: total was 3.500000. running mean: -9.886629\n",
      "ep 2717: ep_len:500 episode reward: total was 4.420000. running mean: -9.743563\n",
      "ep 2717: ep_len:610 episode reward: total was -16.140000. running mean: -9.807528\n",
      "epsilon:0.028993 episode_count: 19026. steps_count: 8413170.000000\n",
      "ep 2718: ep_len:585 episode reward: total was -12.280000. running mean: -9.832252\n",
      "ep 2718: ep_len:525 episode reward: total was -12.380000. running mean: -9.857730\n",
      "ep 2718: ep_len:650 episode reward: total was -10.220000. running mean: -9.861352\n",
      "ep 2718: ep_len:500 episode reward: total was -15.550000. running mean: -9.918239\n",
      "ep 2718: ep_len:54 episode reward: total was 5.000000. running mean: -9.769057\n",
      "ep 2718: ep_len:500 episode reward: total was -0.720000. running mean: -9.678566\n",
      "ep 2718: ep_len:310 episode reward: total was -7.820000. running mean: -9.659980\n",
      "epsilon:0.028856 episode_count: 19033. steps_count: 8416294.000000\n",
      "ep 2719: ep_len:239 episode reward: total was 1.600000. running mean: -9.547380\n",
      "ep 2719: ep_len:615 episode reward: total was -10.620000. running mean: -9.558107\n",
      "ep 2719: ep_len:570 episode reward: total was -6.690000. running mean: -9.529426\n",
      "ep 2719: ep_len:56 episode reward: total was 1.560000. running mean: -9.418531\n",
      "ep 2719: ep_len:3 episode reward: total was 0.000000. running mean: -9.324346\n",
      "ep 2719: ep_len:545 episode reward: total was -13.150000. running mean: -9.362603\n",
      "ep 2719: ep_len:530 episode reward: total was -14.390000. running mean: -9.412877\n",
      "epsilon:0.028720 episode_count: 19040. steps_count: 8418852.000000\n",
      "ep 2720: ep_len:580 episode reward: total was -6.040000. running mean: -9.379148\n",
      "ep 2720: ep_len:500 episode reward: total was -16.530000. running mean: -9.450656\n",
      "ep 2720: ep_len:505 episode reward: total was -11.490000. running mean: -9.471050\n",
      "ep 2720: ep_len:133 episode reward: total was -2.900000. running mean: -9.405339\n",
      "ep 2720: ep_len:3 episode reward: total was 0.000000. running mean: -9.311286\n",
      "ep 2720: ep_len:505 episode reward: total was 5.590000. running mean: -9.162273\n",
      "ep 2720: ep_len:560 episode reward: total was -18.940000. running mean: -9.260050\n",
      "epsilon:0.028583 episode_count: 19047. steps_count: 8421638.000000\n",
      "ep 2721: ep_len:510 episode reward: total was -1.620000. running mean: -9.183650\n",
      "ep 2721: ep_len:500 episode reward: total was -3.290000. running mean: -9.124713\n",
      "ep 2721: ep_len:565 episode reward: total was -13.330000. running mean: -9.166766\n",
      "ep 2721: ep_len:500 episode reward: total was -8.990000. running mean: -9.164998\n",
      "ep 2721: ep_len:2 episode reward: total was 0.000000. running mean: -9.073349\n",
      "ep 2721: ep_len:233 episode reward: total was 1.080000. running mean: -8.971815\n",
      "ep 2721: ep_len:535 episode reward: total was -16.130000. running mean: -9.043397\n",
      "epsilon:0.028447 episode_count: 19054. steps_count: 8424483.000000\n",
      "ep 2722: ep_len:560 episode reward: total was 1.920000. running mean: -8.933763\n",
      "ep 2722: ep_len:500 episode reward: total was -35.240000. running mean: -9.196825\n",
      "ep 2722: ep_len:413 episode reward: total was -7.280000. running mean: -9.177657\n",
      "ep 2722: ep_len:610 episode reward: total was 15.410000. running mean: -8.931780\n",
      "ep 2722: ep_len:3 episode reward: total was 0.000000. running mean: -8.842463\n",
      "ep 2722: ep_len:540 episode reward: total was -5.360000. running mean: -8.807638\n",
      "ep 2722: ep_len:337 episode reward: total was -7.790000. running mean: -8.797462\n",
      "epsilon:0.028310 episode_count: 19061. steps_count: 8427446.000000\n",
      "ep 2723: ep_len:575 episode reward: total was -27.280000. running mean: -8.982287\n",
      "ep 2723: ep_len:640 episode reward: total was -53.780000. running mean: -9.430264\n",
      "ep 2723: ep_len:645 episode reward: total was -20.270000. running mean: -9.538662\n",
      "ep 2723: ep_len:630 episode reward: total was -39.750000. running mean: -9.840775\n",
      "ep 2723: ep_len:3 episode reward: total was 0.000000. running mean: -9.742367\n",
      "ep 2723: ep_len:290 episode reward: total was -38.430000. running mean: -10.029243\n",
      "ep 2723: ep_len:184 episode reward: total was -23.950000. running mean: -10.168451\n",
      "epsilon:0.028174 episode_count: 19068. steps_count: 8430413.000000\n",
      "ep 2724: ep_len:510 episode reward: total was -4.130000. running mean: -10.108067\n",
      "ep 2724: ep_len:500 episode reward: total was -0.030000. running mean: -10.007286\n",
      "ep 2724: ep_len:540 episode reward: total was -4.760000. running mean: -9.954813\n",
      "ep 2724: ep_len:575 episode reward: total was -1.940000. running mean: -9.874665\n",
      "ep 2724: ep_len:85 episode reward: total was 3.550000. running mean: -9.740418\n",
      "ep 2724: ep_len:505 episode reward: total was -7.740000. running mean: -9.720414\n",
      "ep 2724: ep_len:530 episode reward: total was -27.240000. running mean: -9.895610\n",
      "epsilon:0.028037 episode_count: 19075. steps_count: 8433658.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2725: ep_len:250 episode reward: total was 0.630000. running mean: -9.790354\n",
      "ep 2725: ep_len:605 episode reward: total was 11.140000. running mean: -9.581050\n",
      "ep 2725: ep_len:400 episode reward: total was 2.720000. running mean: -9.458040\n",
      "ep 2725: ep_len:56 episode reward: total was 1.560000. running mean: -9.347859\n",
      "ep 2725: ep_len:97 episode reward: total was -11.440000. running mean: -9.368781\n",
      "ep 2725: ep_len:500 episode reward: total was -0.740000. running mean: -9.282493\n",
      "ep 2725: ep_len:190 episode reward: total was 1.200000. running mean: -9.177668\n",
      "epsilon:0.027901 episode_count: 19082. steps_count: 8435756.000000\n",
      "ep 2726: ep_len:500 episode reward: total was -20.950000. running mean: -9.295391\n",
      "ep 2726: ep_len:605 episode reward: total was 2.920000. running mean: -9.173237\n",
      "ep 2726: ep_len:500 episode reward: total was -1.600000. running mean: -9.097505\n",
      "ep 2726: ep_len:500 episode reward: total was -9.050000. running mean: -9.097030\n",
      "ep 2726: ep_len:3 episode reward: total was 0.000000. running mean: -9.006060\n",
      "ep 2726: ep_len:505 episode reward: total was -2.540000. running mean: -8.941399\n",
      "ep 2726: ep_len:152 episode reward: total was -9.430000. running mean: -8.946285\n",
      "epsilon:0.027764 episode_count: 19089. steps_count: 8438521.000000\n",
      "ep 2727: ep_len:750 episode reward: total was -71.370000. running mean: -9.570522\n",
      "ep 2727: ep_len:525 episode reward: total was 8.570000. running mean: -9.389117\n",
      "ep 2727: ep_len:575 episode reward: total was 3.130000. running mean: -9.263926\n",
      "ep 2727: ep_len:525 episode reward: total was 5.540000. running mean: -9.115887\n",
      "ep 2727: ep_len:71 episode reward: total was -1.960000. running mean: -9.044328\n",
      "ep 2727: ep_len:520 episode reward: total was -6.210000. running mean: -9.015984\n",
      "ep 2727: ep_len:510 episode reward: total was -24.030000. running mean: -9.166125\n",
      "epsilon:0.027628 episode_count: 19096. steps_count: 8441997.000000\n",
      "ep 2728: ep_len:885 episode reward: total was -63.220000. running mean: -9.706663\n",
      "ep 2728: ep_len:590 episode reward: total was 12.650000. running mean: -9.483097\n",
      "ep 2728: ep_len:570 episode reward: total was 0.940000. running mean: -9.378866\n",
      "ep 2728: ep_len:500 episode reward: total was -13.560000. running mean: -9.420677\n",
      "ep 2728: ep_len:31 episode reward: total was 3.000000. running mean: -9.296470\n",
      "ep 2728: ep_len:520 episode reward: total was -24.880000. running mean: -9.452306\n",
      "ep 2728: ep_len:500 episode reward: total was -13.340000. running mean: -9.491183\n",
      "epsilon:0.027491 episode_count: 19103. steps_count: 8445593.000000\n",
      "ep 2729: ep_len:570 episode reward: total was -16.730000. running mean: -9.563571\n",
      "ep 2729: ep_len:520 episode reward: total was 9.400000. running mean: -9.373935\n",
      "ep 2729: ep_len:53 episode reward: total was -0.980000. running mean: -9.289996\n",
      "ep 2729: ep_len:389 episode reward: total was -22.160000. running mean: -9.418696\n",
      "ep 2729: ep_len:104 episode reward: total was -10.940000. running mean: -9.433909\n",
      "ep 2729: ep_len:515 episode reward: total was 3.960000. running mean: -9.299970\n",
      "ep 2729: ep_len:570 episode reward: total was -43.150000. running mean: -9.638470\n",
      "epsilon:0.027355 episode_count: 19110. steps_count: 8448314.000000\n",
      "ep 2730: ep_len:665 episode reward: total was -8.690000. running mean: -9.628985\n",
      "ep 2730: ep_len:555 episode reward: total was -4.930000. running mean: -9.581995\n",
      "ep 2730: ep_len:515 episode reward: total was -3.270000. running mean: -9.518876\n",
      "ep 2730: ep_len:590 episode reward: total was 1.590000. running mean: -9.407787\n",
      "ep 2730: ep_len:99 episode reward: total was 6.050000. running mean: -9.253209\n",
      "ep 2730: ep_len:500 episode reward: total was 1.540000. running mean: -9.145277\n",
      "ep 2730: ep_len:500 episode reward: total was -11.780000. running mean: -9.171624\n",
      "epsilon:0.027218 episode_count: 19117. steps_count: 8451738.000000\n",
      "ep 2731: ep_len:520 episode reward: total was 0.110000. running mean: -9.078808\n",
      "ep 2731: ep_len:530 episode reward: total was -1.780000. running mean: -9.005820\n",
      "ep 2731: ep_len:565 episode reward: total was -5.200000. running mean: -8.967762\n",
      "ep 2731: ep_len:143 episode reward: total was 7.100000. running mean: -8.807084\n",
      "ep 2731: ep_len:3 episode reward: total was 0.000000. running mean: -8.719013\n",
      "ep 2731: ep_len:765 episode reward: total was -55.590000. running mean: -9.187723\n",
      "ep 2731: ep_len:510 episode reward: total was -9.540000. running mean: -9.191246\n",
      "epsilon:0.027082 episode_count: 19124. steps_count: 8454774.000000\n",
      "ep 2732: ep_len:600 episode reward: total was -5.140000. running mean: -9.150733\n",
      "ep 2732: ep_len:500 episode reward: total was 10.690000. running mean: -8.952326\n",
      "ep 2732: ep_len:505 episode reward: total was -18.100000. running mean: -9.043803\n",
      "ep 2732: ep_len:660 episode reward: total was -27.510000. running mean: -9.228465\n",
      "ep 2732: ep_len:50 episode reward: total was 3.500000. running mean: -9.101180\n",
      "ep 2732: ep_len:500 episode reward: total was -10.270000. running mean: -9.112868\n",
      "ep 2732: ep_len:545 episode reward: total was -6.580000. running mean: -9.087539\n",
      "epsilon:0.026945 episode_count: 19131. steps_count: 8458134.000000\n",
      "ep 2733: ep_len:545 episode reward: total was -16.290000. running mean: -9.159564\n",
      "ep 2733: ep_len:201 episode reward: total was -3.320000. running mean: -9.101168\n",
      "ep 2733: ep_len:585 episode reward: total was -4.350000. running mean: -9.053657\n",
      "ep 2733: ep_len:570 episode reward: total was 8.120000. running mean: -8.881920\n",
      "ep 2733: ep_len:88 episode reward: total was 4.040000. running mean: -8.752701\n",
      "ep 2733: ep_len:505 episode reward: total was -10.250000. running mean: -8.767674\n",
      "ep 2733: ep_len:635 episode reward: total was -25.610000. running mean: -8.936097\n",
      "epsilon:0.026809 episode_count: 19138. steps_count: 8461263.000000\n",
      "ep 2734: ep_len:530 episode reward: total was 4.430000. running mean: -8.802436\n",
      "ep 2734: ep_len:600 episode reward: total was 16.410000. running mean: -8.550312\n",
      "ep 2734: ep_len:565 episode reward: total was 0.950000. running mean: -8.455309\n",
      "ep 2734: ep_len:600 episode reward: total was 1.600000. running mean: -8.354756\n",
      "ep 2734: ep_len:3 episode reward: total was 0.000000. running mean: -8.271208\n",
      "ep 2734: ep_len:530 episode reward: total was -1.570000. running mean: -8.204196\n",
      "ep 2734: ep_len:580 episode reward: total was -2.040000. running mean: -8.142554\n",
      "epsilon:0.026672 episode_count: 19145. steps_count: 8464671.000000\n",
      "ep 2735: ep_len:615 episode reward: total was 0.080000. running mean: -8.060329\n",
      "ep 2735: ep_len:515 episode reward: total was -2.320000. running mean: -8.002925\n",
      "ep 2735: ep_len:545 episode reward: total was -13.800000. running mean: -8.060896\n",
      "ep 2735: ep_len:695 episode reward: total was -47.320000. running mean: -8.453487\n",
      "ep 2735: ep_len:3 episode reward: total was 0.000000. running mean: -8.368952\n",
      "ep 2735: ep_len:235 episode reward: total was 5.160000. running mean: -8.233663\n",
      "ep 2735: ep_len:500 episode reward: total was -11.090000. running mean: -8.262226\n",
      "epsilon:0.026536 episode_count: 19152. steps_count: 8467779.000000\n",
      "ep 2736: ep_len:760 episode reward: total was -25.230000. running mean: -8.431904\n",
      "ep 2736: ep_len:305 episode reward: total was -41.860000. running mean: -8.766185\n",
      "ep 2736: ep_len:445 episode reward: total was 1.790000. running mean: -8.660623\n",
      "ep 2736: ep_len:392 episode reward: total was -38.220000. running mean: -8.956217\n",
      "ep 2736: ep_len:3 episode reward: total was 0.000000. running mean: -8.866655\n",
      "ep 2736: ep_len:560 episode reward: total was -20.360000. running mean: -8.981588\n",
      "ep 2736: ep_len:349 episode reward: total was -3.180000. running mean: -8.923572\n",
      "epsilon:0.026399 episode_count: 19159. steps_count: 8470593.000000\n",
      "ep 2737: ep_len:212 episode reward: total was -25.370000. running mean: -9.088036\n",
      "ep 2737: ep_len:196 episode reward: total was -6.330000. running mean: -9.060456\n",
      "ep 2737: ep_len:510 episode reward: total was -7.090000. running mean: -9.040751\n",
      "ep 2737: ep_len:505 episode reward: total was -13.510000. running mean: -9.085444\n",
      "ep 2737: ep_len:3 episode reward: total was 0.000000. running mean: -8.994589\n",
      "ep 2737: ep_len:615 episode reward: total was -3.540000. running mean: -8.940044\n",
      "ep 2737: ep_len:595 episode reward: total was 4.300000. running mean: -8.807643\n",
      "epsilon:0.026263 episode_count: 19166. steps_count: 8473229.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2738: ep_len:500 episode reward: total was 1.420000. running mean: -8.705367\n",
      "ep 2738: ep_len:560 episode reward: total was -18.830000. running mean: -8.806613\n",
      "ep 2738: ep_len:585 episode reward: total was -11.040000. running mean: -8.828947\n",
      "ep 2738: ep_len:510 episode reward: total was 4.420000. running mean: -8.696457\n",
      "ep 2738: ep_len:119 episode reward: total was 3.070000. running mean: -8.578793\n",
      "ep 2738: ep_len:570 episode reward: total was -13.240000. running mean: -8.625405\n",
      "ep 2738: ep_len:550 episode reward: total was -5.560000. running mean: -8.594751\n",
      "epsilon:0.026126 episode_count: 19173. steps_count: 8476623.000000\n",
      "ep 2739: ep_len:580 episode reward: total was 0.990000. running mean: -8.498903\n",
      "ep 2739: ep_len:182 episode reward: total was -11.900000. running mean: -8.532914\n",
      "ep 2739: ep_len:555 episode reward: total was -12.750000. running mean: -8.575085\n",
      "ep 2739: ep_len:402 episode reward: total was -0.680000. running mean: -8.496134\n",
      "ep 2739: ep_len:126 episode reward: total was 8.070000. running mean: -8.330473\n",
      "ep 2739: ep_len:620 episode reward: total was -12.100000. running mean: -8.368168\n",
      "ep 2739: ep_len:500 episode reward: total was -18.050000. running mean: -8.464987\n",
      "epsilon:0.025990 episode_count: 19180. steps_count: 8479588.000000\n",
      "ep 2740: ep_len:625 episode reward: total was -14.740000. running mean: -8.527737\n",
      "ep 2740: ep_len:349 episode reward: total was -18.340000. running mean: -8.625859\n",
      "ep 2740: ep_len:570 episode reward: total was -20.930000. running mean: -8.748901\n",
      "ep 2740: ep_len:500 episode reward: total was -5.440000. running mean: -8.715812\n",
      "ep 2740: ep_len:99 episode reward: total was 5.040000. running mean: -8.578254\n",
      "ep 2740: ep_len:585 episode reward: total was -15.610000. running mean: -8.648571\n",
      "ep 2740: ep_len:535 episode reward: total was -8.350000. running mean: -8.645585\n",
      "epsilon:0.025853 episode_count: 19187. steps_count: 8482851.000000\n",
      "ep 2741: ep_len:515 episode reward: total was 12.440000. running mean: -8.434730\n",
      "ep 2741: ep_len:610 episode reward: total was 3.020000. running mean: -8.320182\n",
      "ep 2741: ep_len:540 episode reward: total was 0.940000. running mean: -8.227580\n",
      "ep 2741: ep_len:500 episode reward: total was 10.030000. running mean: -8.045005\n",
      "ep 2741: ep_len:3 episode reward: total was 0.000000. running mean: -7.964555\n",
      "ep 2741: ep_len:500 episode reward: total was 3.890000. running mean: -7.846009\n",
      "ep 2741: ep_len:635 episode reward: total was -5.260000. running mean: -7.820149\n",
      "epsilon:0.025717 episode_count: 19194. steps_count: 8486154.000000\n",
      "ep 2742: ep_len:610 episode reward: total was -7.570000. running mean: -7.817647\n",
      "ep 2742: ep_len:590 episode reward: total was 4.470000. running mean: -7.694771\n",
      "ep 2742: ep_len:610 episode reward: total was -21.990000. running mean: -7.837723\n",
      "ep 2742: ep_len:500 episode reward: total was 4.510000. running mean: -7.714246\n",
      "ep 2742: ep_len:3 episode reward: total was 0.000000. running mean: -7.637104\n",
      "ep 2742: ep_len:530 episode reward: total was -25.870000. running mean: -7.819433\n",
      "ep 2742: ep_len:312 episode reward: total was -3.280000. running mean: -7.774038\n",
      "epsilon:0.025580 episode_count: 19201. steps_count: 8489309.000000\n",
      "ep 2743: ep_len:134 episode reward: total was 1.560000. running mean: -7.680698\n",
      "ep 2743: ep_len:362 episode reward: total was -8.810000. running mean: -7.691991\n",
      "ep 2743: ep_len:885 episode reward: total was -57.740000. running mean: -8.192471\n",
      "ep 2743: ep_len:600 episode reward: total was 13.920000. running mean: -7.971346\n",
      "ep 2743: ep_len:56 episode reward: total was 5.500000. running mean: -7.836633\n",
      "ep 2743: ep_len:690 episode reward: total was -39.370000. running mean: -8.151966\n",
      "ep 2743: ep_len:750 episode reward: total was -59.400000. running mean: -8.664447\n",
      "epsilon:0.025444 episode_count: 19208. steps_count: 8492786.000000\n",
      "ep 2744: ep_len:525 episode reward: total was -17.400000. running mean: -8.751802\n",
      "ep 2744: ep_len:505 episode reward: total was -2.440000. running mean: -8.688684\n",
      "ep 2744: ep_len:447 episode reward: total was 0.740000. running mean: -8.594397\n",
      "ep 2744: ep_len:56 episode reward: total was 2.570000. running mean: -8.482753\n",
      "ep 2744: ep_len:48 episode reward: total was 3.000000. running mean: -8.367926\n",
      "ep 2744: ep_len:505 episode reward: total was -5.780000. running mean: -8.342047\n",
      "ep 2744: ep_len:555 episode reward: total was -4.810000. running mean: -8.306726\n",
      "epsilon:0.025307 episode_count: 19215. steps_count: 8495427.000000\n",
      "ep 2745: ep_len:575 episode reward: total was -0.650000. running mean: -8.230159\n",
      "ep 2745: ep_len:500 episode reward: total was 14.280000. running mean: -8.005057\n",
      "ep 2745: ep_len:650 episode reward: total was -20.260000. running mean: -8.127607\n",
      "ep 2745: ep_len:560 episode reward: total was -27.270000. running mean: -8.319031\n",
      "ep 2745: ep_len:77 episode reward: total was 0.040000. running mean: -8.235440\n",
      "ep 2745: ep_len:525 episode reward: total was 9.010000. running mean: -8.062986\n",
      "ep 2745: ep_len:565 episode reward: total was -11.920000. running mean: -8.101556\n",
      "epsilon:0.025171 episode_count: 19222. steps_count: 8498879.000000\n",
      "ep 2746: ep_len:253 episode reward: total was 3.100000. running mean: -7.989541\n",
      "ep 2746: ep_len:500 episode reward: total was -18.400000. running mean: -8.093645\n",
      "ep 2746: ep_len:695 episode reward: total was -53.850000. running mean: -8.551209\n",
      "ep 2746: ep_len:530 episode reward: total was -10.710000. running mean: -8.572797\n",
      "ep 2746: ep_len:105 episode reward: total was 6.040000. running mean: -8.426669\n",
      "ep 2746: ep_len:565 episode reward: total was -23.750000. running mean: -8.579902\n",
      "ep 2746: ep_len:515 episode reward: total was -1.540000. running mean: -8.509503\n",
      "epsilon:0.025034 episode_count: 19229. steps_count: 8502042.000000\n",
      "ep 2747: ep_len:595 episode reward: total was 0.430000. running mean: -8.420108\n",
      "ep 2747: ep_len:500 episode reward: total was 4.140000. running mean: -8.294507\n",
      "ep 2747: ep_len:675 episode reward: total was -4.170000. running mean: -8.253262\n",
      "ep 2747: ep_len:550 episode reward: total was -43.230000. running mean: -8.603029\n",
      "ep 2747: ep_len:95 episode reward: total was 2.040000. running mean: -8.496599\n",
      "ep 2747: ep_len:500 episode reward: total was 8.050000. running mean: -8.331133\n",
      "ep 2747: ep_len:500 episode reward: total was -15.020000. running mean: -8.398022\n",
      "epsilon:0.024898 episode_count: 19236. steps_count: 8505457.000000\n",
      "ep 2748: ep_len:630 episode reward: total was -20.730000. running mean: -8.521341\n",
      "ep 2748: ep_len:530 episode reward: total was 7.710000. running mean: -8.359028\n",
      "ep 2748: ep_len:550 episode reward: total was -35.510000. running mean: -8.630538\n",
      "ep 2748: ep_len:615 episode reward: total was 12.660000. running mean: -8.417632\n",
      "ep 2748: ep_len:3 episode reward: total was 0.000000. running mean: -8.333456\n",
      "ep 2748: ep_len:178 episode reward: total was 4.100000. running mean: -8.209121\n",
      "ep 2748: ep_len:590 episode reward: total was -6.050000. running mean: -8.187530\n",
      "epsilon:0.024761 episode_count: 19243. steps_count: 8508553.000000\n",
      "ep 2749: ep_len:540 episode reward: total was -32.050000. running mean: -8.426155\n",
      "ep 2749: ep_len:605 episode reward: total was -22.460000. running mean: -8.566493\n",
      "ep 2749: ep_len:540 episode reward: total was -8.280000. running mean: -8.563628\n",
      "ep 2749: ep_len:500 episode reward: total was -0.940000. running mean: -8.487392\n",
      "ep 2749: ep_len:3 episode reward: total was 0.000000. running mean: -8.402518\n",
      "ep 2749: ep_len:505 episode reward: total was -10.220000. running mean: -8.420693\n",
      "ep 2749: ep_len:530 episode reward: total was -2.520000. running mean: -8.361686\n",
      "epsilon:0.024625 episode_count: 19250. steps_count: 8511776.000000\n",
      "ep 2750: ep_len:620 episode reward: total was -6.090000. running mean: -8.338969\n",
      "ep 2750: ep_len:585 episode reward: total was -14.910000. running mean: -8.404680\n",
      "ep 2750: ep_len:595 episode reward: total was -5.610000. running mean: -8.376733\n",
      "ep 2750: ep_len:500 episode reward: total was 2.010000. running mean: -8.272865\n",
      "ep 2750: ep_len:105 episode reward: total was -3.940000. running mean: -8.229537\n",
      "ep 2750: ep_len:615 episode reward: total was -1.080000. running mean: -8.158041\n",
      "ep 2750: ep_len:520 episode reward: total was -19.520000. running mean: -8.271661\n",
      "epsilon:0.024488 episode_count: 19257. steps_count: 8515316.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2751: ep_len:565 episode reward: total was -0.380000. running mean: -8.192744\n",
      "ep 2751: ep_len:515 episode reward: total was -6.310000. running mean: -8.173917\n",
      "ep 2751: ep_len:585 episode reward: total was -13.780000. running mean: -8.229978\n",
      "ep 2751: ep_len:500 episode reward: total was -14.540000. running mean: -8.293078\n",
      "ep 2751: ep_len:83 episode reward: total was 6.540000. running mean: -8.144747\n",
      "ep 2751: ep_len:510 episode reward: total was 1.900000. running mean: -8.044300\n",
      "ep 2751: ep_len:545 episode reward: total was 0.950000. running mean: -7.954357\n",
      "epsilon:0.024352 episode_count: 19264. steps_count: 8518619.000000\n",
      "ep 2752: ep_len:500 episode reward: total was 0.760000. running mean: -7.867213\n",
      "ep 2752: ep_len:500 episode reward: total was -27.870000. running mean: -8.067241\n",
      "ep 2752: ep_len:625 episode reward: total was -3.990000. running mean: -8.026469\n",
      "ep 2752: ep_len:535 episode reward: total was 6.030000. running mean: -7.885904\n",
      "ep 2752: ep_len:3 episode reward: total was 0.000000. running mean: -7.807045\n",
      "ep 2752: ep_len:605 episode reward: total was 8.990000. running mean: -7.639074\n",
      "ep 2752: ep_len:500 episode reward: total was -11.560000. running mean: -7.678284\n",
      "epsilon:0.024215 episode_count: 19271. steps_count: 8521887.000000\n",
      "ep 2753: ep_len:515 episode reward: total was 7.890000. running mean: -7.522601\n",
      "ep 2753: ep_len:620 episode reward: total was -27.860000. running mean: -7.725975\n",
      "ep 2753: ep_len:595 episode reward: total was -7.650000. running mean: -7.725215\n",
      "ep 2753: ep_len:620 episode reward: total was 15.690000. running mean: -7.491063\n",
      "ep 2753: ep_len:3 episode reward: total was 0.000000. running mean: -7.416152\n",
      "ep 2753: ep_len:540 episode reward: total was 0.060000. running mean: -7.341391\n",
      "ep 2753: ep_len:600 episode reward: total was 0.730000. running mean: -7.260677\n",
      "epsilon:0.024079 episode_count: 19278. steps_count: 8525380.000000\n",
      "ep 2754: ep_len:500 episode reward: total was 2.230000. running mean: -7.165770\n",
      "ep 2754: ep_len:500 episode reward: total was -22.690000. running mean: -7.321012\n",
      "ep 2754: ep_len:505 episode reward: total was 1.340000. running mean: -7.234402\n",
      "ep 2754: ep_len:500 episode reward: total was -22.670000. running mean: -7.388758\n",
      "ep 2754: ep_len:121 episode reward: total was 6.560000. running mean: -7.249271\n",
      "ep 2754: ep_len:525 episode reward: total was -4.580000. running mean: -7.222578\n",
      "ep 2754: ep_len:203 episode reward: total was -11.370000. running mean: -7.264052\n",
      "epsilon:0.023942 episode_count: 19285. steps_count: 8528234.000000\n",
      "ep 2755: ep_len:560 episode reward: total was 6.390000. running mean: -7.127512\n",
      "ep 2755: ep_len:630 episode reward: total was -13.170000. running mean: -7.187937\n",
      "ep 2755: ep_len:565 episode reward: total was -31.410000. running mean: -7.430157\n",
      "ep 2755: ep_len:590 episode reward: total was -61.650000. running mean: -7.972356\n",
      "ep 2755: ep_len:110 episode reward: total was 3.020000. running mean: -7.862432\n",
      "ep 2755: ep_len:500 episode reward: total was -0.770000. running mean: -7.791508\n",
      "ep 2755: ep_len:625 episode reward: total was -2.250000. running mean: -7.736093\n",
      "epsilon:0.023806 episode_count: 19292. steps_count: 8531814.000000\n",
      "ep 2756: ep_len:500 episode reward: total was 7.450000. running mean: -7.584232\n",
      "ep 2756: ep_len:510 episode reward: total was -26.980000. running mean: -7.778189\n",
      "ep 2756: ep_len:705 episode reward: total was -49.700000. running mean: -8.197408\n",
      "ep 2756: ep_len:500 episode reward: total was -19.120000. running mean: -8.306633\n",
      "ep 2756: ep_len:29 episode reward: total was -0.500000. running mean: -8.228567\n",
      "ep 2756: ep_len:500 episode reward: total was -23.660000. running mean: -8.382881\n",
      "ep 2756: ep_len:189 episode reward: total was -11.910000. running mean: -8.418153\n",
      "epsilon:0.023669 episode_count: 19299. steps_count: 8534747.000000\n",
      "ep 2757: ep_len:155 episode reward: total was 1.570000. running mean: -8.318271\n",
      "ep 2757: ep_len:590 episode reward: total was -14.960000. running mean: -8.384688\n",
      "ep 2757: ep_len:70 episode reward: total was -1.960000. running mean: -8.320442\n",
      "ep 2757: ep_len:525 episode reward: total was 13.980000. running mean: -8.097437\n",
      "ep 2757: ep_len:3 episode reward: total was 0.000000. running mean: -8.016463\n",
      "ep 2757: ep_len:535 episode reward: total was -5.580000. running mean: -7.992098\n",
      "ep 2757: ep_len:515 episode reward: total was -7.080000. running mean: -7.982977\n",
      "epsilon:0.023533 episode_count: 19306. steps_count: 8537140.000000\n",
      "ep 2758: ep_len:500 episode reward: total was 13.770000. running mean: -7.765447\n",
      "ep 2758: ep_len:600 episode reward: total was -13.810000. running mean: -7.825893\n",
      "ep 2758: ep_len:570 episode reward: total was 1.970000. running mean: -7.727934\n",
      "ep 2758: ep_len:422 episode reward: total was 4.890000. running mean: -7.601755\n",
      "ep 2758: ep_len:123 episode reward: total was 6.070000. running mean: -7.465037\n",
      "ep 2758: ep_len:510 episode reward: total was -11.510000. running mean: -7.505487\n",
      "ep 2758: ep_len:635 episode reward: total was -17.960000. running mean: -7.610032\n",
      "epsilon:0.023396 episode_count: 19313. steps_count: 8540500.000000\n",
      "ep 2759: ep_len:218 episode reward: total was 5.600000. running mean: -7.477932\n",
      "ep 2759: ep_len:500 episode reward: total was -28.800000. running mean: -7.691152\n",
      "ep 2759: ep_len:680 episode reward: total was -9.640000. running mean: -7.710641\n",
      "ep 2759: ep_len:500 episode reward: total was -15.550000. running mean: -7.789034\n",
      "ep 2759: ep_len:3 episode reward: total was 0.000000. running mean: -7.711144\n",
      "ep 2759: ep_len:555 episode reward: total was -7.070000. running mean: -7.704732\n",
      "ep 2759: ep_len:500 episode reward: total was -5.160000. running mean: -7.679285\n",
      "epsilon:0.023260 episode_count: 19320. steps_count: 8543456.000000\n",
      "ep 2760: ep_len:575 episode reward: total was 10.180000. running mean: -7.500692\n",
      "ep 2760: ep_len:500 episode reward: total was -0.400000. running mean: -7.429685\n",
      "ep 2760: ep_len:700 episode reward: total was -23.680000. running mean: -7.592189\n",
      "ep 2760: ep_len:500 episode reward: total was -14.500000. running mean: -7.661267\n",
      "ep 2760: ep_len:3 episode reward: total was 0.000000. running mean: -7.584654\n",
      "ep 2760: ep_len:500 episode reward: total was -10.520000. running mean: -7.614007\n",
      "ep 2760: ep_len:595 episode reward: total was -10.300000. running mean: -7.640867\n",
      "epsilon:0.023123 episode_count: 19327. steps_count: 8546829.000000\n",
      "ep 2761: ep_len:134 episode reward: total was 2.570000. running mean: -7.538759\n",
      "ep 2761: ep_len:530 episode reward: total was 6.120000. running mean: -7.402171\n",
      "ep 2761: ep_len:620 episode reward: total was -3.030000. running mean: -7.358449\n",
      "ep 2761: ep_len:520 episode reward: total was 7.590000. running mean: -7.208965\n",
      "ep 2761: ep_len:3 episode reward: total was 0.000000. running mean: -7.136875\n",
      "ep 2761: ep_len:500 episode reward: total was -1.290000. running mean: -7.078406\n",
      "ep 2761: ep_len:208 episode reward: total was -4.340000. running mean: -7.051022\n",
      "epsilon:0.022987 episode_count: 19334. steps_count: 8549344.000000\n",
      "ep 2762: ep_len:500 episode reward: total was -18.920000. running mean: -7.169712\n",
      "ep 2762: ep_len:640 episode reward: total was -16.580000. running mean: -7.263815\n",
      "ep 2762: ep_len:580 episode reward: total was -3.640000. running mean: -7.227577\n",
      "ep 2762: ep_len:500 episode reward: total was 1.920000. running mean: -7.136101\n",
      "ep 2762: ep_len:3 episode reward: total was 0.000000. running mean: -7.064740\n",
      "ep 2762: ep_len:540 episode reward: total was -4.870000. running mean: -7.042793\n",
      "ep 2762: ep_len:321 episode reward: total was -7.310000. running mean: -7.045465\n",
      "epsilon:0.022850 episode_count: 19341. steps_count: 8552428.000000\n",
      "ep 2763: ep_len:525 episode reward: total was 7.070000. running mean: -6.904310\n",
      "ep 2763: ep_len:545 episode reward: total was 1.630000. running mean: -6.818967\n",
      "ep 2763: ep_len:500 episode reward: total was -5.360000. running mean: -6.804377\n",
      "ep 2763: ep_len:565 episode reward: total was -13.440000. running mean: -6.870734\n",
      "ep 2763: ep_len:88 episode reward: total was 3.030000. running mean: -6.771726\n",
      "ep 2763: ep_len:515 episode reward: total was -24.430000. running mean: -6.948309\n",
      "ep 2763: ep_len:625 episode reward: total was -21.920000. running mean: -7.098026\n",
      "epsilon:0.022714 episode_count: 19348. steps_count: 8555791.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2764: ep_len:185 episode reward: total was 4.110000. running mean: -6.985946\n",
      "ep 2764: ep_len:165 episode reward: total was -9.960000. running mean: -7.015686\n",
      "ep 2764: ep_len:590 episode reward: total was -8.350000. running mean: -7.029029\n",
      "ep 2764: ep_len:402 episode reward: total was 10.380000. running mean: -6.854939\n",
      "ep 2764: ep_len:3 episode reward: total was 0.000000. running mean: -6.786390\n",
      "ep 2764: ep_len:500 episode reward: total was -0.700000. running mean: -6.725526\n",
      "ep 2764: ep_len:500 episode reward: total was 4.460000. running mean: -6.613671\n",
      "epsilon:0.022577 episode_count: 19355. steps_count: 8558136.000000\n",
      "ep 2765: ep_len:580 episode reward: total was -19.130000. running mean: -6.738834\n",
      "ep 2765: ep_len:565 episode reward: total was -3.380000. running mean: -6.705245\n",
      "ep 2765: ep_len:525 episode reward: total was 1.430000. running mean: -6.623893\n",
      "ep 2765: ep_len:515 episode reward: total was 6.420000. running mean: -6.493454\n",
      "ep 2765: ep_len:3 episode reward: total was 0.000000. running mean: -6.428520\n",
      "ep 2765: ep_len:650 episode reward: total was -1.680000. running mean: -6.381034\n",
      "ep 2765: ep_len:610 episode reward: total was -14.960000. running mean: -6.466824\n",
      "epsilon:0.022441 episode_count: 19362. steps_count: 8561584.000000\n",
      "ep 2766: ep_len:113 episode reward: total was 2.070000. running mean: -6.381456\n",
      "ep 2766: ep_len:535 episode reward: total was 8.600000. running mean: -6.231641\n",
      "ep 2766: ep_len:595 episode reward: total was -19.530000. running mean: -6.364625\n",
      "ep 2766: ep_len:520 episode reward: total was -22.140000. running mean: -6.522379\n",
      "ep 2766: ep_len:3 episode reward: total was 0.000000. running mean: -6.457155\n",
      "ep 2766: ep_len:595 episode reward: total was -12.590000. running mean: -6.518483\n",
      "ep 2766: ep_len:333 episode reward: total was -1.770000. running mean: -6.470998\n",
      "epsilon:0.022304 episode_count: 19369. steps_count: 8564278.000000\n",
      "ep 2767: ep_len:550 episode reward: total was -72.680000. running mean: -7.133088\n",
      "ep 2767: ep_len:535 episode reward: total was 10.510000. running mean: -6.956658\n",
      "ep 2767: ep_len:550 episode reward: total was -4.040000. running mean: -6.927491\n",
      "ep 2767: ep_len:505 episode reward: total was -10.510000. running mean: -6.963316\n",
      "ep 2767: ep_len:91 episode reward: total was 6.040000. running mean: -6.833283\n",
      "ep 2767: ep_len:520 episode reward: total was -12.470000. running mean: -6.889650\n",
      "ep 2767: ep_len:575 episode reward: total was -9.050000. running mean: -6.911254\n",
      "epsilon:0.022168 episode_count: 19376. steps_count: 8567604.000000\n",
      "ep 2768: ep_len:212 episode reward: total was 5.620000. running mean: -6.785941\n",
      "ep 2768: ep_len:287 episode reward: total was -31.830000. running mean: -7.036382\n",
      "ep 2768: ep_len:580 episode reward: total was -20.200000. running mean: -7.168018\n",
      "ep 2768: ep_len:500 episode reward: total was 3.540000. running mean: -7.060938\n",
      "ep 2768: ep_len:89 episode reward: total was -10.960000. running mean: -7.099928\n",
      "ep 2768: ep_len:660 episode reward: total was 5.900000. running mean: -6.969929\n",
      "ep 2768: ep_len:595 episode reward: total was -13.390000. running mean: -7.034130\n",
      "epsilon:0.022031 episode_count: 19383. steps_count: 8570527.000000\n",
      "ep 2769: ep_len:670 episode reward: total was -3.110000. running mean: -6.994888\n",
      "ep 2769: ep_len:167 episode reward: total was -3.900000. running mean: -6.963939\n",
      "ep 2769: ep_len:500 episode reward: total was 4.420000. running mean: -6.850100\n",
      "ep 2769: ep_len:555 episode reward: total was 10.070000. running mean: -6.680899\n",
      "ep 2769: ep_len:3 episode reward: total was 0.000000. running mean: -6.614090\n",
      "ep 2769: ep_len:630 episode reward: total was 1.510000. running mean: -6.532849\n",
      "ep 2769: ep_len:272 episode reward: total was -2.840000. running mean: -6.495921\n",
      "epsilon:0.021895 episode_count: 19390. steps_count: 8573324.000000\n",
      "ep 2770: ep_len:560 episode reward: total was -14.670000. running mean: -6.577661\n",
      "ep 2770: ep_len:500 episode reward: total was -13.990000. running mean: -6.651785\n",
      "ep 2770: ep_len:550 episode reward: total was -23.780000. running mean: -6.823067\n",
      "ep 2770: ep_len:515 episode reward: total was -11.500000. running mean: -6.869836\n",
      "ep 2770: ep_len:49 episode reward: total was 4.500000. running mean: -6.756138\n",
      "ep 2770: ep_len:640 episode reward: total was -9.000000. running mean: -6.778577\n",
      "ep 2770: ep_len:570 episode reward: total was -2.870000. running mean: -6.739491\n",
      "epsilon:0.021758 episode_count: 19397. steps_count: 8576708.000000\n",
      "ep 2771: ep_len:510 episode reward: total was -11.860000. running mean: -6.790696\n",
      "ep 2771: ep_len:635 episode reward: total was 10.530000. running mean: -6.617489\n",
      "ep 2771: ep_len:580 episode reward: total was -21.290000. running mean: -6.764214\n",
      "ep 2771: ep_len:510 episode reward: total was -19.500000. running mean: -6.891572\n",
      "ep 2771: ep_len:77 episode reward: total was 0.500000. running mean: -6.817656\n",
      "ep 2771: ep_len:545 episode reward: total was -15.050000. running mean: -6.899980\n",
      "ep 2771: ep_len:284 episode reward: total was -11.890000. running mean: -6.949880\n",
      "epsilon:0.021622 episode_count: 19404. steps_count: 8579849.000000\n",
      "ep 2772: ep_len:520 episode reward: total was -2.110000. running mean: -6.901481\n",
      "ep 2772: ep_len:620 episode reward: total was -16.610000. running mean: -6.998566\n",
      "ep 2772: ep_len:690 episode reward: total was -53.830000. running mean: -7.466881\n",
      "ep 2772: ep_len:600 episode reward: total was 5.580000. running mean: -7.336412\n",
      "ep 2772: ep_len:54 episode reward: total was 5.000000. running mean: -7.213048\n",
      "ep 2772: ep_len:550 episode reward: total was 5.650000. running mean: -7.084417\n",
      "ep 2772: ep_len:560 episode reward: total was -16.290000. running mean: -7.176473\n",
      "epsilon:0.021485 episode_count: 19411. steps_count: 8583443.000000\n",
      "ep 2773: ep_len:655 episode reward: total was -27.780000. running mean: -7.382508\n",
      "ep 2773: ep_len:201 episode reward: total was -8.340000. running mean: -7.392083\n",
      "ep 2773: ep_len:500 episode reward: total was -7.980000. running mean: -7.397962\n",
      "ep 2773: ep_len:530 episode reward: total was 2.390000. running mean: -7.300083\n",
      "ep 2773: ep_len:79 episode reward: total was -11.970000. running mean: -7.346782\n",
      "ep 2773: ep_len:505 episode reward: total was 0.250000. running mean: -7.270814\n",
      "ep 2773: ep_len:520 episode reward: total was -4.560000. running mean: -7.243706\n",
      "epsilon:0.021349 episode_count: 19418. steps_count: 8586433.000000\n",
      "ep 2774: ep_len:565 episode reward: total was 6.200000. running mean: -7.109269\n",
      "ep 2774: ep_len:186 episode reward: total was 5.170000. running mean: -6.986476\n",
      "ep 2774: ep_len:500 episode reward: total was -21.470000. running mean: -7.131311\n",
      "ep 2774: ep_len:615 episode reward: total was 3.070000. running mean: -7.029298\n",
      "ep 2774: ep_len:3 episode reward: total was 0.000000. running mean: -6.959005\n",
      "ep 2774: ep_len:590 episode reward: total was -14.240000. running mean: -7.031815\n",
      "ep 2774: ep_len:525 episode reward: total was -6.050000. running mean: -7.021997\n",
      "epsilon:0.021212 episode_count: 19425. steps_count: 8589417.000000\n",
      "ep 2775: ep_len:510 episode reward: total was 8.940000. running mean: -6.862377\n",
      "ep 2775: ep_len:500 episode reward: total was -12.980000. running mean: -6.923553\n",
      "ep 2775: ep_len:64 episode reward: total was -3.480000. running mean: -6.889118\n",
      "ep 2775: ep_len:510 episode reward: total was -26.510000. running mean: -7.085327\n",
      "ep 2775: ep_len:3 episode reward: total was 0.000000. running mean: -7.014473\n",
      "ep 2775: ep_len:286 episode reward: total was 2.140000. running mean: -6.922929\n",
      "ep 2775: ep_len:520 episode reward: total was -5.020000. running mean: -6.903899\n",
      "epsilon:0.021076 episode_count: 19432. steps_count: 8591810.000000\n",
      "ep 2776: ep_len:255 episode reward: total was 4.640000. running mean: -6.788460\n",
      "ep 2776: ep_len:585 episode reward: total was -3.830000. running mean: -6.758876\n",
      "ep 2776: ep_len:645 episode reward: total was -8.370000. running mean: -6.774987\n",
      "ep 2776: ep_len:510 episode reward: total was -12.980000. running mean: -6.837037\n",
      "ep 2776: ep_len:46 episode reward: total was -9.000000. running mean: -6.858667\n",
      "ep 2776: ep_len:500 episode reward: total was -10.550000. running mean: -6.895580\n",
      "ep 2776: ep_len:283 episode reward: total was -4.820000. running mean: -6.874824\n",
      "epsilon:0.020939 episode_count: 19439. steps_count: 8594634.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2777: ep_len:565 episode reward: total was 7.610000. running mean: -6.729976\n",
      "ep 2777: ep_len:500 episode reward: total was -58.810000. running mean: -7.250776\n",
      "ep 2777: ep_len:500 episode reward: total was 5.400000. running mean: -7.124269\n",
      "ep 2777: ep_len:500 episode reward: total was -11.530000. running mean: -7.168326\n",
      "ep 2777: ep_len:106 episode reward: total was 7.540000. running mean: -7.021243\n",
      "ep 2777: ep_len:500 episode reward: total was -28.120000. running mean: -7.232230\n",
      "ep 2777: ep_len:605 episode reward: total was -0.480000. running mean: -7.164708\n",
      "epsilon:0.020803 episode_count: 19446. steps_count: 8597910.000000\n",
      "ep 2778: ep_len:134 episode reward: total was 1.560000. running mean: -7.077461\n",
      "ep 2778: ep_len:545 episode reward: total was -2.270000. running mean: -7.029386\n",
      "ep 2778: ep_len:620 episode reward: total was -17.910000. running mean: -7.138192\n",
      "ep 2778: ep_len:600 episode reward: total was 10.110000. running mean: -6.965710\n",
      "ep 2778: ep_len:102 episode reward: total was 8.050000. running mean: -6.815553\n",
      "ep 2778: ep_len:1195 episode reward: total was -153.280000. running mean: -8.280198\n",
      "ep 2778: ep_len:653 episode reward: total was -59.430000. running mean: -8.791696\n",
      "epsilon:0.020666 episode_count: 19453. steps_count: 8601759.000000\n",
      "ep 2779: ep_len:500 episode reward: total was 5.690000. running mean: -8.646879\n",
      "ep 2779: ep_len:535 episode reward: total was -48.680000. running mean: -9.047210\n",
      "ep 2779: ep_len:635 episode reward: total was -20.920000. running mean: -9.165938\n",
      "ep 2779: ep_len:150 episode reward: total was 2.610000. running mean: -9.048179\n",
      "ep 2779: ep_len:3 episode reward: total was 0.000000. running mean: -8.957697\n",
      "ep 2779: ep_len:500 episode reward: total was 5.920000. running mean: -8.808920\n",
      "ep 2779: ep_len:297 episode reward: total was -5.800000. running mean: -8.778831\n",
      "epsilon:0.020530 episode_count: 19460. steps_count: 8604379.000000\n",
      "ep 2780: ep_len:655 episode reward: total was -22.270000. running mean: -8.913742\n",
      "ep 2780: ep_len:545 episode reward: total was -14.820000. running mean: -8.972805\n",
      "ep 2780: ep_len:640 episode reward: total was -15.270000. running mean: -9.035777\n",
      "ep 2780: ep_len:545 episode reward: total was 2.910000. running mean: -8.916319\n",
      "ep 2780: ep_len:3 episode reward: total was 0.000000. running mean: -8.827156\n",
      "ep 2780: ep_len:510 episode reward: total was 3.920000. running mean: -8.699684\n",
      "ep 2780: ep_len:201 episode reward: total was 0.670000. running mean: -8.605987\n",
      "epsilon:0.020393 episode_count: 19467. steps_count: 8607478.000000\n",
      "ep 2781: ep_len:555 episode reward: total was 0.410000. running mean: -8.515828\n",
      "ep 2781: ep_len:745 episode reward: total was -54.910000. running mean: -8.979769\n",
      "ep 2781: ep_len:570 episode reward: total was -18.230000. running mean: -9.072272\n",
      "ep 2781: ep_len:575 episode reward: total was -36.830000. running mean: -9.349849\n",
      "ep 2781: ep_len:3 episode reward: total was 0.000000. running mean: -9.256350\n",
      "ep 2781: ep_len:304 episode reward: total was 2.170000. running mean: -9.142087\n",
      "ep 2781: ep_len:500 episode reward: total was -3.100000. running mean: -9.081666\n",
      "epsilon:0.020257 episode_count: 19474. steps_count: 8610730.000000\n",
      "ep 2782: ep_len:645 episode reward: total was -1.950000. running mean: -9.010349\n",
      "ep 2782: ep_len:500 episode reward: total was 13.620000. running mean: -8.784046\n",
      "ep 2782: ep_len:510 episode reward: total was -15.000000. running mean: -8.846205\n",
      "ep 2782: ep_len:353 episode reward: total was 5.800000. running mean: -8.699743\n",
      "ep 2782: ep_len:3 episode reward: total was 0.000000. running mean: -8.612746\n",
      "ep 2782: ep_len:680 episode reward: total was -4.620000. running mean: -8.572819\n",
      "ep 2782: ep_len:515 episode reward: total was -9.330000. running mean: -8.580390\n",
      "epsilon:0.020120 episode_count: 19481. steps_count: 8613936.000000\n",
      "ep 2783: ep_len:500 episode reward: total was 3.240000. running mean: -8.462186\n",
      "ep 2783: ep_len:348 episode reward: total was -37.300000. running mean: -8.750565\n",
      "ep 2783: ep_len:520 episode reward: total was -13.620000. running mean: -8.799259\n",
      "ep 2783: ep_len:500 episode reward: total was 3.010000. running mean: -8.681166\n",
      "ep 2783: ep_len:3 episode reward: total was 0.000000. running mean: -8.594355\n",
      "ep 2783: ep_len:570 episode reward: total was -1.110000. running mean: -8.519511\n",
      "ep 2783: ep_len:580 episode reward: total was -15.820000. running mean: -8.592516\n",
      "epsilon:0.019984 episode_count: 19488. steps_count: 8616957.000000\n",
      "ep 2784: ep_len:640 episode reward: total was -20.710000. running mean: -8.713691\n",
      "ep 2784: ep_len:565 episode reward: total was 7.080000. running mean: -8.555754\n",
      "ep 2784: ep_len:570 episode reward: total was -3.080000. running mean: -8.500996\n",
      "ep 2784: ep_len:840 episode reward: total was -95.770000. running mean: -9.373686\n",
      "ep 2784: ep_len:55 episode reward: total was 4.000000. running mean: -9.239950\n",
      "ep 2784: ep_len:300 episode reward: total was 0.640000. running mean: -9.141150\n",
      "ep 2784: ep_len:575 episode reward: total was -10.590000. running mean: -9.155639\n",
      "epsilon:0.019847 episode_count: 19495. steps_count: 8620502.000000\n",
      "ep 2785: ep_len:620 episode reward: total was -13.190000. running mean: -9.195982\n",
      "ep 2785: ep_len:545 episode reward: total was -39.480000. running mean: -9.498822\n",
      "ep 2785: ep_len:535 episode reward: total was -15.890000. running mean: -9.562734\n",
      "ep 2785: ep_len:505 episode reward: total was 4.440000. running mean: -9.422707\n",
      "ep 2785: ep_len:3 episode reward: total was 0.000000. running mean: -9.328480\n",
      "ep 2785: ep_len:1195 episode reward: total was -140.580000. running mean: -10.640995\n",
      "ep 2785: ep_len:510 episode reward: total was 1.240000. running mean: -10.522185\n",
      "epsilon:0.019711 episode_count: 19502. steps_count: 8624415.000000\n",
      "ep 2786: ep_len:530 episode reward: total was 4.910000. running mean: -10.367863\n",
      "ep 2786: ep_len:201 episode reward: total was -1.350000. running mean: -10.277684\n",
      "ep 2786: ep_len:655 episode reward: total was -15.750000. running mean: -10.332408\n",
      "ep 2786: ep_len:580 episode reward: total was 7.600000. running mean: -10.153084\n",
      "ep 2786: ep_len:125 episode reward: total was 3.050000. running mean: -10.021053\n",
      "ep 2786: ep_len:525 episode reward: total was -4.100000. running mean: -9.961842\n",
      "ep 2786: ep_len:535 episode reward: total was -19.010000. running mean: -10.052324\n",
      "epsilon:0.019574 episode_count: 19509. steps_count: 8627566.000000\n",
      "ep 2787: ep_len:129 episode reward: total was 2.070000. running mean: -9.931101\n",
      "ep 2787: ep_len:620 episode reward: total was -25.290000. running mean: -10.084690\n",
      "ep 2787: ep_len:67 episode reward: total was -3.470000. running mean: -10.018543\n",
      "ep 2787: ep_len:56 episode reward: total was 2.570000. running mean: -9.892657\n",
      "ep 2787: ep_len:129 episode reward: total was 4.560000. running mean: -9.748131\n",
      "ep 2787: ep_len:715 episode reward: total was -20.740000. running mean: -9.858049\n",
      "ep 2787: ep_len:344 episode reward: total was -8.330000. running mean: -9.842769\n",
      "epsilon:0.019438 episode_count: 19516. steps_count: 8629626.000000\n",
      "ep 2788: ep_len:560 episode reward: total was -25.400000. running mean: -9.998341\n",
      "ep 2788: ep_len:630 episode reward: total was 11.540000. running mean: -9.782958\n",
      "ep 2788: ep_len:394 episode reward: total was -7.840000. running mean: -9.763528\n",
      "ep 2788: ep_len:500 episode reward: total was 5.390000. running mean: -9.611993\n",
      "ep 2788: ep_len:53 episode reward: total was 5.000000. running mean: -9.465873\n",
      "ep 2788: ep_len:630 episode reward: total was -0.370000. running mean: -9.374914\n",
      "ep 2788: ep_len:585 episode reward: total was -1.590000. running mean: -9.297065\n",
      "epsilon:0.019301 episode_count: 19523. steps_count: 8632978.000000\n",
      "ep 2789: ep_len:505 episode reward: total was -26.410000. running mean: -9.468194\n",
      "ep 2789: ep_len:550 episode reward: total was 16.880000. running mean: -9.204712\n",
      "ep 2789: ep_len:550 episode reward: total was -1.620000. running mean: -9.128865\n",
      "ep 2789: ep_len:500 episode reward: total was 4.920000. running mean: -8.988377\n",
      "ep 2789: ep_len:89 episode reward: total was 5.050000. running mean: -8.847993\n",
      "ep 2789: ep_len:615 episode reward: total was -40.030000. running mean: -9.159813\n",
      "ep 2789: ep_len:297 episode reward: total was -5.820000. running mean: -9.126415\n",
      "epsilon:0.019165 episode_count: 19530. steps_count: 8636084.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2790: ep_len:197 episode reward: total was 3.570000. running mean: -8.999451\n",
      "ep 2790: ep_len:590 episode reward: total was -1.830000. running mean: -8.927756\n",
      "ep 2790: ep_len:560 episode reward: total was -28.260000. running mean: -9.121079\n",
      "ep 2790: ep_len:540 episode reward: total was 12.970000. running mean: -8.900168\n",
      "ep 2790: ep_len:2 episode reward: total was 0.000000. running mean: -8.811166\n",
      "ep 2790: ep_len:560 episode reward: total was -25.290000. running mean: -8.975955\n",
      "ep 2790: ep_len:281 episode reward: total was -1.330000. running mean: -8.899495\n",
      "epsilon:0.019028 episode_count: 19537. steps_count: 8638814.000000\n",
      "ep 2791: ep_len:525 episode reward: total was -20.470000. running mean: -9.015200\n",
      "ep 2791: ep_len:575 episode reward: total was 0.130000. running mean: -8.923748\n",
      "ep 2791: ep_len:550 episode reward: total was -13.820000. running mean: -8.972711\n",
      "ep 2791: ep_len:575 episode reward: total was -2.460000. running mean: -8.907583\n",
      "ep 2791: ep_len:3 episode reward: total was 0.000000. running mean: -8.818508\n",
      "ep 2791: ep_len:605 episode reward: total was -31.090000. running mean: -9.041223\n",
      "ep 2791: ep_len:328 episode reward: total was -13.870000. running mean: -9.089510\n",
      "epsilon:0.018892 episode_count: 19544. steps_count: 8641975.000000\n",
      "ep 2792: ep_len:500 episode reward: total was 1.710000. running mean: -8.981515\n",
      "ep 2792: ep_len:645 episode reward: total was -6.940000. running mean: -8.961100\n",
      "ep 2792: ep_len:585 episode reward: total was -13.840000. running mean: -9.009889\n",
      "ep 2792: ep_len:505 episode reward: total was -2.660000. running mean: -8.946390\n",
      "ep 2792: ep_len:129 episode reward: total was 7.060000. running mean: -8.786326\n",
      "ep 2792: ep_len:259 episode reward: total was -0.860000. running mean: -8.707063\n",
      "ep 2792: ep_len:515 episode reward: total was -5.360000. running mean: -8.673592\n",
      "epsilon:0.018755 episode_count: 19551. steps_count: 8645113.000000\n",
      "ep 2793: ep_len:695 episode reward: total was -18.210000. running mean: -8.768956\n",
      "ep 2793: ep_len:525 episode reward: total was -4.800000. running mean: -8.729267\n",
      "ep 2793: ep_len:366 episode reward: total was 1.150000. running mean: -8.630474\n",
      "ep 2793: ep_len:565 episode reward: total was -3.390000. running mean: -8.578069\n",
      "ep 2793: ep_len:3 episode reward: total was 0.000000. running mean: -8.492289\n",
      "ep 2793: ep_len:585 episode reward: total was -0.660000. running mean: -8.413966\n",
      "ep 2793: ep_len:645 episode reward: total was -39.980000. running mean: -8.729626\n",
      "epsilon:0.018619 episode_count: 19558. steps_count: 8648497.000000\n",
      "ep 2794: ep_len:500 episode reward: total was 2.780000. running mean: -8.614530\n",
      "ep 2794: ep_len:515 episode reward: total was 1.750000. running mean: -8.510885\n",
      "ep 2794: ep_len:530 episode reward: total was -6.650000. running mean: -8.492276\n",
      "ep 2794: ep_len:161 episode reward: total was 0.590000. running mean: -8.401453\n",
      "ep 2794: ep_len:3 episode reward: total was 0.000000. running mean: -8.317439\n",
      "ep 2794: ep_len:625 episode reward: total was -41.600000. running mean: -8.650264\n",
      "ep 2794: ep_len:500 episode reward: total was -3.620000. running mean: -8.599962\n",
      "epsilon:0.018482 episode_count: 19565. steps_count: 8651331.000000\n",
      "ep 2795: ep_len:840 episode reward: total was -55.230000. running mean: -9.066262\n",
      "ep 2795: ep_len:295 episode reward: total was -7.840000. running mean: -9.053999\n",
      "ep 2795: ep_len:356 episode reward: total was -0.860000. running mean: -8.972059\n",
      "ep 2795: ep_len:500 episode reward: total was -35.150000. running mean: -9.233839\n",
      "ep 2795: ep_len:102 episode reward: total was 5.050000. running mean: -9.091000\n",
      "ep 2795: ep_len:500 episode reward: total was -4.990000. running mean: -9.049990\n",
      "ep 2795: ep_len:625 episode reward: total was -16.080000. running mean: -9.120290\n",
      "epsilon:0.018346 episode_count: 19572. steps_count: 8654549.000000\n",
      "ep 2796: ep_len:615 episode reward: total was 1.400000. running mean: -9.015088\n",
      "ep 2796: ep_len:525 episode reward: total was -11.890000. running mean: -9.043837\n",
      "ep 2796: ep_len:560 episode reward: total was -17.030000. running mean: -9.123698\n",
      "ep 2796: ep_len:505 episode reward: total was -8.980000. running mean: -9.122261\n",
      "ep 2796: ep_len:86 episode reward: total was -11.950000. running mean: -9.150539\n",
      "ep 2796: ep_len:560 episode reward: total was -36.110000. running mean: -9.420133\n",
      "ep 2796: ep_len:550 episode reward: total was -8.780000. running mean: -9.413732\n",
      "epsilon:0.018209 episode_count: 19579. steps_count: 8657950.000000\n",
      "ep 2797: ep_len:570 episode reward: total was 2.860000. running mean: -9.290995\n",
      "ep 2797: ep_len:540 episode reward: total was -12.410000. running mean: -9.322185\n",
      "ep 2797: ep_len:406 episode reward: total was 5.210000. running mean: -9.176863\n",
      "ep 2797: ep_len:545 episode reward: total was -18.530000. running mean: -9.270394\n",
      "ep 2797: ep_len:3 episode reward: total was 0.000000. running mean: -9.177690\n",
      "ep 2797: ep_len:169 episode reward: total was 7.110000. running mean: -9.014813\n",
      "ep 2797: ep_len:660 episode reward: total was -35.880000. running mean: -9.283465\n",
      "epsilon:0.018073 episode_count: 19586. steps_count: 8660843.000000\n",
      "ep 2798: ep_len:660 episode reward: total was -8.920000. running mean: -9.279831\n",
      "ep 2798: ep_len:600 episode reward: total was -12.920000. running mean: -9.316232\n",
      "ep 2798: ep_len:555 episode reward: total was -24.580000. running mean: -9.468870\n",
      "ep 2798: ep_len:500 episode reward: total was -13.010000. running mean: -9.504281\n",
      "ep 2798: ep_len:3 episode reward: total was 0.000000. running mean: -9.409238\n",
      "ep 2798: ep_len:540 episode reward: total was 10.130000. running mean: -9.213846\n",
      "ep 2798: ep_len:735 episode reward: total was -87.530000. running mean: -9.997008\n",
      "epsilon:0.017936 episode_count: 19593. steps_count: 8664436.000000\n",
      "ep 2799: ep_len:500 episode reward: total was -55.250000. running mean: -10.449538\n",
      "ep 2799: ep_len:625 episode reward: total was 16.070000. running mean: -10.184342\n",
      "ep 2799: ep_len:545 episode reward: total was -11.840000. running mean: -10.200899\n",
      "ep 2799: ep_len:525 episode reward: total was -14.570000. running mean: -10.244590\n",
      "ep 2799: ep_len:3 episode reward: total was 0.000000. running mean: -10.142144\n",
      "ep 2799: ep_len:178 episode reward: total was 5.110000. running mean: -9.989622\n",
      "ep 2799: ep_len:197 episode reward: total was -8.890000. running mean: -9.978626\n",
      "epsilon:0.017800 episode_count: 19600. steps_count: 8667009.000000\n",
      "ep 2800: ep_len:610 episode reward: total was 4.450000. running mean: -9.834340\n",
      "ep 2800: ep_len:302 episode reward: total was -15.360000. running mean: -9.889597\n",
      "ep 2800: ep_len:655 episode reward: total was -3.140000. running mean: -9.822101\n",
      "ep 2800: ep_len:505 episode reward: total was 3.820000. running mean: -9.685680\n",
      "ep 2800: ep_len:3 episode reward: total was 0.000000. running mean: -9.588823\n",
      "ep 2800: ep_len:565 episode reward: total was 3.370000. running mean: -9.459235\n",
      "ep 2800: ep_len:575 episode reward: total was -5.290000. running mean: -9.417542\n",
      "epsilon:0.017663 episode_count: 19607. steps_count: 8670224.000000\n",
      "ep 2801: ep_len:500 episode reward: total was 13.220000. running mean: -9.191167\n",
      "ep 2801: ep_len:500 episode reward: total was -30.560000. running mean: -9.404855\n",
      "ep 2801: ep_len:525 episode reward: total was -22.070000. running mean: -9.531507\n",
      "ep 2801: ep_len:595 episode reward: total was 0.580000. running mean: -9.430391\n",
      "ep 2801: ep_len:3 episode reward: total was 0.000000. running mean: -9.336088\n",
      "ep 2801: ep_len:545 episode reward: total was -31.490000. running mean: -9.557627\n",
      "ep 2801: ep_len:500 episode reward: total was -8.150000. running mean: -9.543550\n",
      "epsilon:0.017527 episode_count: 19614. steps_count: 8673392.000000\n",
      "ep 2802: ep_len:530 episode reward: total was -21.950000. running mean: -9.667615\n",
      "ep 2802: ep_len:565 episode reward: total was 10.830000. running mean: -9.462639\n",
      "ep 2802: ep_len:645 episode reward: total was 1.370000. running mean: -9.354312\n",
      "ep 2802: ep_len:510 episode reward: total was -14.050000. running mean: -9.401269\n",
      "ep 2802: ep_len:3 episode reward: total was 0.000000. running mean: -9.307257\n",
      "ep 2802: ep_len:500 episode reward: total was 7.910000. running mean: -9.135084\n",
      "ep 2802: ep_len:288 episode reward: total was -6.770000. running mean: -9.111433\n",
      "epsilon:0.017390 episode_count: 19621. steps_count: 8676433.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2803: ep_len:590 episode reward: total was -34.440000. running mean: -9.364719\n",
      "ep 2803: ep_len:1500 episode reward: total was -205.910000. running mean: -11.330172\n",
      "ep 2803: ep_len:575 episode reward: total was -5.230000. running mean: -11.269170\n",
      "ep 2803: ep_len:615 episode reward: total was -19.890000. running mean: -11.355378\n",
      "ep 2803: ep_len:3 episode reward: total was 0.000000. running mean: -11.241824\n",
      "ep 2803: ep_len:610 episode reward: total was -1.590000. running mean: -11.145306\n",
      "ep 2803: ep_len:500 episode reward: total was -5.500000. running mean: -11.088853\n",
      "epsilon:0.017254 episode_count: 19628. steps_count: 8680826.000000\n",
      "ep 2804: ep_len:515 episode reward: total was -13.810000. running mean: -11.116065\n",
      "ep 2804: ep_len:359 episode reward: total was -39.840000. running mean: -11.403304\n",
      "ep 2804: ep_len:625 episode reward: total was -2.560000. running mean: -11.314871\n",
      "ep 2804: ep_len:393 episode reward: total was -16.640000. running mean: -11.368122\n",
      "ep 2804: ep_len:3 episode reward: total was 0.000000. running mean: -11.254441\n",
      "ep 2804: ep_len:535 episode reward: total was -5.550000. running mean: -11.197397\n",
      "ep 2804: ep_len:500 episode reward: total was -17.930000. running mean: -11.264723\n",
      "epsilon:0.017117 episode_count: 19635. steps_count: 8683756.000000\n",
      "ep 2805: ep_len:665 episode reward: total was -7.100000. running mean: -11.223075\n",
      "ep 2805: ep_len:580 episode reward: total was 0.690000. running mean: -11.103945\n",
      "ep 2805: ep_len:560 episode reward: total was -25.270000. running mean: -11.245605\n",
      "ep 2805: ep_len:500 episode reward: total was 0.820000. running mean: -11.124949\n",
      "ep 2805: ep_len:55 episode reward: total was 4.000000. running mean: -10.973700\n",
      "ep 2805: ep_len:575 episode reward: total was -1.660000. running mean: -10.880563\n",
      "ep 2805: ep_len:505 episode reward: total was -6.090000. running mean: -10.832657\n",
      "epsilon:0.016981 episode_count: 19642. steps_count: 8687196.000000\n",
      "ep 2806: ep_len:605 episode reward: total was 5.180000. running mean: -10.672530\n",
      "ep 2806: ep_len:193 episode reward: total was -8.930000. running mean: -10.655105\n",
      "ep 2806: ep_len:500 episode reward: total was -16.920000. running mean: -10.717754\n",
      "ep 2806: ep_len:132 episode reward: total was 5.090000. running mean: -10.559677\n",
      "ep 2806: ep_len:3 episode reward: total was 0.000000. running mean: -10.454080\n",
      "ep 2806: ep_len:510 episode reward: total was 2.400000. running mean: -10.325539\n",
      "ep 2806: ep_len:510 episode reward: total was -17.470000. running mean: -10.396984\n",
      "epsilon:0.016844 episode_count: 19649. steps_count: 8689649.000000\n",
      "ep 2807: ep_len:121 episode reward: total was 0.040000. running mean: -10.292614\n",
      "ep 2807: ep_len:515 episode reward: total was -15.420000. running mean: -10.343888\n",
      "ep 2807: ep_len:79 episode reward: total was 0.040000. running mean: -10.240049\n",
      "ep 2807: ep_len:520 episode reward: total was 1.530000. running mean: -10.122348\n",
      "ep 2807: ep_len:3 episode reward: total was 0.000000. running mean: -10.021125\n",
      "ep 2807: ep_len:565 episode reward: total was -4.410000. running mean: -9.965014\n",
      "ep 2807: ep_len:500 episode reward: total was -17.300000. running mean: -10.038363\n",
      "epsilon:0.016708 episode_count: 19656. steps_count: 8691952.000000\n",
      "ep 2808: ep_len:500 episode reward: total was -49.330000. running mean: -10.431280\n",
      "ep 2808: ep_len:690 episode reward: total was -42.350000. running mean: -10.750467\n",
      "ep 2808: ep_len:655 episode reward: total was -3.830000. running mean: -10.681262\n",
      "ep 2808: ep_len:600 episode reward: total was 2.590000. running mean: -10.548550\n",
      "ep 2808: ep_len:3 episode reward: total was 0.000000. running mean: -10.443064\n",
      "ep 2808: ep_len:510 episode reward: total was -4.210000. running mean: -10.380734\n",
      "ep 2808: ep_len:176 episode reward: total was -7.430000. running mean: -10.351226\n",
      "epsilon:0.016571 episode_count: 19663. steps_count: 8695086.000000\n",
      "ep 2809: ep_len:520 episode reward: total was -5.980000. running mean: -10.307514\n",
      "ep 2809: ep_len:595 episode reward: total was -11.600000. running mean: -10.320439\n",
      "ep 2809: ep_len:500 episode reward: total was -2.440000. running mean: -10.241634\n",
      "ep 2809: ep_len:56 episode reward: total was -2.430000. running mean: -10.163518\n",
      "ep 2809: ep_len:99 episode reward: total was -8.440000. running mean: -10.146283\n",
      "ep 2809: ep_len:545 episode reward: total was -36.470000. running mean: -10.409520\n",
      "ep 2809: ep_len:570 episode reward: total was 0.480000. running mean: -10.300625\n",
      "epsilon:0.016435 episode_count: 19670. steps_count: 8697971.000000\n",
      "ep 2810: ep_len:630 episode reward: total was -8.180000. running mean: -10.279419\n",
      "ep 2810: ep_len:610 episode reward: total was -26.490000. running mean: -10.441524\n",
      "ep 2810: ep_len:525 episode reward: total was -16.300000. running mean: -10.500109\n",
      "ep 2810: ep_len:555 episode reward: total was 5.900000. running mean: -10.336108\n",
      "ep 2810: ep_len:126 episode reward: total was 6.570000. running mean: -10.167047\n",
      "ep 2810: ep_len:500 episode reward: total was -38.600000. running mean: -10.451377\n",
      "ep 2810: ep_len:500 episode reward: total was -20.750000. running mean: -10.554363\n",
      "epsilon:0.016298 episode_count: 19677. steps_count: 8701417.000000\n",
      "ep 2811: ep_len:550 episode reward: total was 0.830000. running mean: -10.440519\n",
      "ep 2811: ep_len:575 episode reward: total was -1.600000. running mean: -10.352114\n",
      "ep 2811: ep_len:535 episode reward: total was -9.380000. running mean: -10.342393\n",
      "ep 2811: ep_len:107 episode reward: total was 1.600000. running mean: -10.222969\n",
      "ep 2811: ep_len:3 episode reward: total was 0.000000. running mean: -10.120739\n",
      "ep 2811: ep_len:610 episode reward: total was -1.530000. running mean: -10.034832\n",
      "ep 2811: ep_len:329 episode reward: total was -18.400000. running mean: -10.118483\n",
      "epsilon:0.016162 episode_count: 19684. steps_count: 8704126.000000\n",
      "ep 2812: ep_len:194 episode reward: total was 7.080000. running mean: -9.946499\n",
      "ep 2812: ep_len:500 episode reward: total was -9.510000. running mean: -9.942134\n",
      "ep 2812: ep_len:520 episode reward: total was -8.180000. running mean: -9.924512\n",
      "ep 2812: ep_len:500 episode reward: total was -37.710000. running mean: -10.202367\n",
      "ep 2812: ep_len:3 episode reward: total was 0.000000. running mean: -10.100344\n",
      "ep 2812: ep_len:680 episode reward: total was -132.990000. running mean: -11.329240\n",
      "ep 2812: ep_len:336 episode reward: total was -6.840000. running mean: -11.284348\n",
      "epsilon:0.016025 episode_count: 19691. steps_count: 8706859.000000\n",
      "ep 2813: ep_len:630 episode reward: total was 5.160000. running mean: -11.119904\n",
      "ep 2813: ep_len:625 episode reward: total was 16.610000. running mean: -10.842605\n",
      "ep 2813: ep_len:364 episode reward: total was 5.700000. running mean: -10.677179\n",
      "ep 2813: ep_len:600 episode reward: total was 3.590000. running mean: -10.534507\n",
      "ep 2813: ep_len:3 episode reward: total was 0.000000. running mean: -10.429162\n",
      "ep 2813: ep_len:670 episode reward: total was -6.660000. running mean: -10.391471\n",
      "ep 2813: ep_len:324 episode reward: total was 0.720000. running mean: -10.280356\n",
      "epsilon:0.015889 episode_count: 19698. steps_count: 8710075.000000\n",
      "ep 2814: ep_len:500 episode reward: total was -1.740000. running mean: -10.194952\n",
      "ep 2814: ep_len:377 episode reward: total was -8.790000. running mean: -10.180903\n",
      "ep 2814: ep_len:346 episode reward: total was 4.700000. running mean: -10.032094\n",
      "ep 2814: ep_len:515 episode reward: total was 5.040000. running mean: -9.881373\n",
      "ep 2814: ep_len:3 episode reward: total was 0.000000. running mean: -9.782559\n",
      "ep 2814: ep_len:309 episode reward: total was -1.310000. running mean: -9.697834\n",
      "ep 2814: ep_len:500 episode reward: total was -14.330000. running mean: -9.744155\n",
      "epsilon:0.015752 episode_count: 19705. steps_count: 8712625.000000\n",
      "ep 2815: ep_len:575 episode reward: total was -28.750000. running mean: -9.934214\n",
      "ep 2815: ep_len:500 episode reward: total was -28.010000. running mean: -10.114972\n",
      "ep 2815: ep_len:500 episode reward: total was -7.030000. running mean: -10.084122\n",
      "ep 2815: ep_len:500 episode reward: total was 8.970000. running mean: -9.893581\n",
      "ep 2815: ep_len:3 episode reward: total was 0.000000. running mean: -9.794645\n",
      "ep 2815: ep_len:500 episode reward: total was -21.190000. running mean: -9.908598\n",
      "ep 2815: ep_len:545 episode reward: total was -11.930000. running mean: -9.928812\n",
      "epsilon:0.015616 episode_count: 19712. steps_count: 8715748.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2816: ep_len:505 episode reward: total was -19.860000. running mean: -10.028124\n",
      "ep 2816: ep_len:374 episode reward: total was -11.360000. running mean: -10.041443\n",
      "ep 2816: ep_len:605 episode reward: total was 0.440000. running mean: -9.936629\n",
      "ep 2816: ep_len:585 episode reward: total was 2.030000. running mean: -9.816962\n",
      "ep 2816: ep_len:3 episode reward: total was 0.000000. running mean: -9.718793\n",
      "ep 2816: ep_len:237 episode reward: total was -17.900000. running mean: -9.800605\n",
      "ep 2816: ep_len:540 episode reward: total was -16.600000. running mean: -9.868599\n",
      "epsilon:0.015479 episode_count: 19719. steps_count: 8718597.000000\n",
      "ep 2817: ep_len:119 episode reward: total was 2.570000. running mean: -9.744213\n",
      "ep 2817: ep_len:500 episode reward: total was 11.670000. running mean: -9.530071\n",
      "ep 2817: ep_len:829 episode reward: total was -112.070000. running mean: -10.555470\n",
      "ep 2817: ep_len:565 episode reward: total was -7.470000. running mean: -10.524615\n",
      "ep 2817: ep_len:3 episode reward: total was 0.000000. running mean: -10.419369\n",
      "ep 2817: ep_len:500 episode reward: total was 0.260000. running mean: -10.312575\n",
      "ep 2817: ep_len:580 episode reward: total was -29.320000. running mean: -10.502650\n",
      "epsilon:0.015343 episode_count: 19726. steps_count: 8721693.000000\n",
      "ep 2818: ep_len:259 episode reward: total was 3.110000. running mean: -10.366523\n",
      "ep 2818: ep_len:565 episode reward: total was 4.840000. running mean: -10.214458\n",
      "ep 2818: ep_len:540 episode reward: total was -5.070000. running mean: -10.163013\n",
      "ep 2818: ep_len:520 episode reward: total was -25.630000. running mean: -10.317683\n",
      "ep 2818: ep_len:104 episode reward: total was 4.040000. running mean: -10.174106\n",
      "ep 2818: ep_len:510 episode reward: total was -7.760000. running mean: -10.149965\n",
      "ep 2818: ep_len:500 episode reward: total was 0.800000. running mean: -10.040466\n",
      "epsilon:0.015206 episode_count: 19733. steps_count: 8724691.000000\n",
      "ep 2819: ep_len:500 episode reward: total was -18.630000. running mean: -10.126361\n",
      "ep 2819: ep_len:580 episode reward: total was -1.980000. running mean: -10.044897\n",
      "ep 2819: ep_len:540 episode reward: total was -0.600000. running mean: -9.950448\n",
      "ep 2819: ep_len:550 episode reward: total was -5.550000. running mean: -9.906444\n",
      "ep 2819: ep_len:3 episode reward: total was 0.000000. running mean: -9.807379\n",
      "ep 2819: ep_len:246 episode reward: total was 9.180000. running mean: -9.617506\n",
      "ep 2819: ep_len:580 episode reward: total was 0.170000. running mean: -9.519631\n",
      "epsilon:0.015070 episode_count: 19740. steps_count: 8727690.000000\n",
      "ep 2820: ep_len:212 episode reward: total was -0.400000. running mean: -9.428434\n",
      "ep 2820: ep_len:680 episode reward: total was -38.790000. running mean: -9.722050\n",
      "ep 2820: ep_len:565 episode reward: total was -18.870000. running mean: -9.813529\n",
      "ep 2820: ep_len:162 episode reward: total was 0.130000. running mean: -9.714094\n",
      "ep 2820: ep_len:3 episode reward: total was 0.000000. running mean: -9.616953\n",
      "ep 2820: ep_len:248 episode reward: total was 4.650000. running mean: -9.474284\n",
      "ep 2820: ep_len:530 episode reward: total was -25.380000. running mean: -9.633341\n",
      "epsilon:0.014933 episode_count: 19747. steps_count: 8730090.000000\n",
      "ep 2821: ep_len:710 episode reward: total was -8.110000. running mean: -9.618107\n",
      "ep 2821: ep_len:500 episode reward: total was -28.570000. running mean: -9.807626\n",
      "ep 2821: ep_len:615 episode reward: total was -1.980000. running mean: -9.729350\n",
      "ep 2821: ep_len:165 episode reward: total was -1.940000. running mean: -9.651457\n",
      "ep 2821: ep_len:80 episode reward: total was 3.540000. running mean: -9.519542\n",
      "ep 2821: ep_len:565 episode reward: total was -8.050000. running mean: -9.504847\n",
      "ep 2821: ep_len:500 episode reward: total was -8.610000. running mean: -9.495898\n",
      "epsilon:0.014797 episode_count: 19754. steps_count: 8733225.000000\n",
      "ep 2822: ep_len:500 episode reward: total was 4.220000. running mean: -9.358739\n",
      "ep 2822: ep_len:500 episode reward: total was 9.640000. running mean: -9.168752\n",
      "ep 2822: ep_len:500 episode reward: total was -10.350000. running mean: -9.180564\n",
      "ep 2822: ep_len:128 episode reward: total was 0.080000. running mean: -9.087959\n",
      "ep 2822: ep_len:3 episode reward: total was 0.000000. running mean: -8.997079\n",
      "ep 2822: ep_len:655 episode reward: total was -50.170000. running mean: -9.408808\n",
      "ep 2822: ep_len:505 episode reward: total was -5.020000. running mean: -9.364920\n",
      "epsilon:0.014660 episode_count: 19761. steps_count: 8736016.000000\n",
      "ep 2823: ep_len:620 episode reward: total was 8.970000. running mean: -9.181571\n",
      "ep 2823: ep_len:500 episode reward: total was -18.190000. running mean: -9.271655\n",
      "ep 2823: ep_len:372 episode reward: total was 4.220000. running mean: -9.136739\n",
      "ep 2823: ep_len:560 episode reward: total was 14.940000. running mean: -8.895971\n",
      "ep 2823: ep_len:3 episode reward: total was 0.000000. running mean: -8.807012\n",
      "ep 2823: ep_len:500 episode reward: total was -19.440000. running mean: -8.913341\n",
      "ep 2823: ep_len:565 episode reward: total was -9.270000. running mean: -8.916908\n",
      "epsilon:0.014524 episode_count: 19768. steps_count: 8739136.000000\n",
      "ep 2824: ep_len:500 episode reward: total was -11.920000. running mean: -8.946939\n",
      "ep 2824: ep_len:505 episode reward: total was -3.630000. running mean: -8.893770\n",
      "ep 2824: ep_len:625 episode reward: total was -6.230000. running mean: -8.867132\n",
      "ep 2824: ep_len:500 episode reward: total was -5.500000. running mean: -8.833461\n",
      "ep 2824: ep_len:3 episode reward: total was 0.000000. running mean: -8.745126\n",
      "ep 2824: ep_len:630 episode reward: total was 6.990000. running mean: -8.587775\n",
      "ep 2824: ep_len:620 episode reward: total was -6.350000. running mean: -8.565397\n",
      "epsilon:0.014387 episode_count: 19775. steps_count: 8742519.000000\n",
      "ep 2825: ep_len:630 episode reward: total was 6.690000. running mean: -8.412843\n",
      "ep 2825: ep_len:510 episode reward: total was -0.290000. running mean: -8.331615\n",
      "ep 2825: ep_len:820 episode reward: total was -56.220000. running mean: -8.810498\n",
      "ep 2825: ep_len:570 episode reward: total was 4.580000. running mean: -8.676593\n",
      "ep 2825: ep_len:129 episode reward: total was 5.560000. running mean: -8.534227\n",
      "ep 2825: ep_len:239 episode reward: total was 7.640000. running mean: -8.372485\n",
      "ep 2825: ep_len:600 episode reward: total was -4.340000. running mean: -8.332160\n",
      "epsilon:0.014251 episode_count: 19782. steps_count: 8746017.000000\n",
      "ep 2826: ep_len:500 episode reward: total was 1.390000. running mean: -8.234939\n",
      "ep 2826: ep_len:500 episode reward: total was -2.010000. running mean: -8.172689\n",
      "ep 2826: ep_len:500 episode reward: total was -1.570000. running mean: -8.106662\n",
      "ep 2826: ep_len:98 episode reward: total was 5.070000. running mean: -7.974896\n",
      "ep 2826: ep_len:3 episode reward: total was 0.000000. running mean: -7.895147\n",
      "ep 2826: ep_len:640 episode reward: total was 4.140000. running mean: -7.774795\n",
      "ep 2826: ep_len:500 episode reward: total was -33.740000. running mean: -8.034447\n",
      "epsilon:0.014114 episode_count: 19789. steps_count: 8748758.000000\n",
      "ep 2827: ep_len:620 episode reward: total was -7.220000. running mean: -8.026303\n",
      "ep 2827: ep_len:570 episode reward: total was 1.930000. running mean: -7.926740\n",
      "ep 2827: ep_len:64 episode reward: total was 1.540000. running mean: -7.832073\n",
      "ep 2827: ep_len:500 episode reward: total was 9.390000. running mean: -7.659852\n",
      "ep 2827: ep_len:127 episode reward: total was 7.060000. running mean: -7.512653\n",
      "ep 2827: ep_len:670 episode reward: total was -5.190000. running mean: -7.489427\n",
      "ep 2827: ep_len:335 episode reward: total was -6.810000. running mean: -7.482633\n",
      "epsilon:0.013978 episode_count: 19796. steps_count: 8751644.000000\n",
      "ep 2828: ep_len:530 episode reward: total was -4.980000. running mean: -7.457606\n",
      "ep 2828: ep_len:346 episode reward: total was -7.780000. running mean: -7.460830\n",
      "ep 2828: ep_len:500 episode reward: total was -30.490000. running mean: -7.691122\n",
      "ep 2828: ep_len:580 episode reward: total was -0.440000. running mean: -7.618611\n",
      "ep 2828: ep_len:88 episode reward: total was 5.050000. running mean: -7.491925\n",
      "ep 2828: ep_len:535 episode reward: total was -42.550000. running mean: -7.842505\n",
      "ep 2828: ep_len:630 episode reward: total was -30.460000. running mean: -8.068680\n",
      "epsilon:0.013841 episode_count: 19803. steps_count: 8754853.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2829: ep_len:237 episode reward: total was -0.930000. running mean: -7.997293\n",
      "ep 2829: ep_len:610 episode reward: total was -19.340000. running mean: -8.110720\n",
      "ep 2829: ep_len:535 episode reward: total was -11.400000. running mean: -8.143613\n",
      "ep 2829: ep_len:550 episode reward: total was 9.530000. running mean: -7.966877\n",
      "ep 2829: ep_len:91 episode reward: total was 4.540000. running mean: -7.841808\n",
      "ep 2829: ep_len:570 episode reward: total was -5.570000. running mean: -7.819090\n",
      "ep 2829: ep_len:545 episode reward: total was -11.760000. running mean: -7.858499\n",
      "epsilon:0.013705 episode_count: 19810. steps_count: 8757991.000000\n",
      "ep 2830: ep_len:500 episode reward: total was 9.960000. running mean: -7.680314\n",
      "ep 2830: ep_len:530 episode reward: total was -7.320000. running mean: -7.676711\n",
      "ep 2830: ep_len:500 episode reward: total was -21.520000. running mean: -7.815144\n",
      "ep 2830: ep_len:555 episode reward: total was -8.070000. running mean: -7.817693\n",
      "ep 2830: ep_len:3 episode reward: total was 0.000000. running mean: -7.739516\n",
      "ep 2830: ep_len:650 episode reward: total was -3.840000. running mean: -7.700521\n",
      "ep 2830: ep_len:600 episode reward: total was -4.970000. running mean: -7.673215\n",
      "epsilon:0.013568 episode_count: 19817. steps_count: 8761329.000000\n",
      "ep 2831: ep_len:535 episode reward: total was 5.900000. running mean: -7.537483\n",
      "ep 2831: ep_len:720 episode reward: total was -24.800000. running mean: -7.710108\n",
      "ep 2831: ep_len:550 episode reward: total was -27.440000. running mean: -7.907407\n",
      "ep 2831: ep_len:500 episode reward: total was -1.630000. running mean: -7.844633\n",
      "ep 2831: ep_len:3 episode reward: total was 0.000000. running mean: -7.766187\n",
      "ep 2831: ep_len:530 episode reward: total was -6.080000. running mean: -7.749325\n",
      "ep 2831: ep_len:620 episode reward: total was -35.410000. running mean: -8.025932\n",
      "epsilon:0.013432 episode_count: 19824. steps_count: 8764787.000000\n",
      "ep 2832: ep_len:650 episode reward: total was -0.870000. running mean: -7.954372\n",
      "ep 2832: ep_len:525 episode reward: total was 0.630000. running mean: -7.868529\n",
      "ep 2832: ep_len:575 episode reward: total was 4.080000. running mean: -7.749043\n",
      "ep 2832: ep_len:56 episode reward: total was 2.570000. running mean: -7.645853\n",
      "ep 2832: ep_len:3 episode reward: total was 0.000000. running mean: -7.569395\n",
      "ep 2832: ep_len:258 episode reward: total was 3.140000. running mean: -7.462301\n",
      "ep 2832: ep_len:535 episode reward: total was -8.520000. running mean: -7.472878\n",
      "epsilon:0.013295 episode_count: 19831. steps_count: 8767389.000000\n",
      "ep 2833: ep_len:238 episode reward: total was 1.630000. running mean: -7.381849\n",
      "ep 2833: ep_len:289 episode reward: total was -6.890000. running mean: -7.376930\n",
      "ep 2833: ep_len:555 episode reward: total was -0.260000. running mean: -7.305761\n",
      "ep 2833: ep_len:56 episode reward: total was -0.930000. running mean: -7.242003\n",
      "ep 2833: ep_len:130 episode reward: total was 5.050000. running mean: -7.119083\n",
      "ep 2833: ep_len:510 episode reward: total was -17.280000. running mean: -7.220693\n",
      "ep 2833: ep_len:343 episode reward: total was -13.810000. running mean: -7.286586\n",
      "epsilon:0.013159 episode_count: 19838. steps_count: 8769510.000000\n",
      "ep 2834: ep_len:590 episode reward: total was -20.180000. running mean: -7.415520\n",
      "ep 2834: ep_len:550 episode reward: total was -4.800000. running mean: -7.389365\n",
      "ep 2834: ep_len:368 episode reward: total was 7.260000. running mean: -7.242871\n",
      "ep 2834: ep_len:500 episode reward: total was 8.060000. running mean: -7.089842\n",
      "ep 2834: ep_len:3 episode reward: total was 0.000000. running mean: -7.018944\n",
      "ep 2834: ep_len:625 episode reward: total was 2.970000. running mean: -6.919054\n",
      "ep 2834: ep_len:515 episode reward: total was -15.160000. running mean: -7.001464\n",
      "epsilon:0.013022 episode_count: 19845. steps_count: 8772661.000000\n",
      "ep 2835: ep_len:580 episode reward: total was 9.030000. running mean: -6.841149\n",
      "ep 2835: ep_len:530 episode reward: total was 14.370000. running mean: -6.629038\n",
      "ep 2835: ep_len:600 episode reward: total was -8.580000. running mean: -6.648547\n",
      "ep 2835: ep_len:520 episode reward: total was -29.730000. running mean: -6.879362\n",
      "ep 2835: ep_len:3 episode reward: total was 0.000000. running mean: -6.810568\n",
      "ep 2835: ep_len:277 episode reward: total was -23.390000. running mean: -6.976362\n",
      "ep 2835: ep_len:505 episode reward: total was -4.360000. running mean: -6.950199\n",
      "epsilon:0.012886 episode_count: 19852. steps_count: 8775676.000000\n",
      "ep 2836: ep_len:620 episode reward: total was -14.500000. running mean: -7.025697\n",
      "ep 2836: ep_len:500 episode reward: total was 8.110000. running mean: -6.874340\n",
      "ep 2836: ep_len:590 episode reward: total was -6.610000. running mean: -6.871697\n",
      "ep 2836: ep_len:530 episode reward: total was -14.720000. running mean: -6.950180\n",
      "ep 2836: ep_len:115 episode reward: total was 2.020000. running mean: -6.860478\n",
      "ep 2836: ep_len:565 episode reward: total was 12.080000. running mean: -6.671073\n",
      "ep 2836: ep_len:615 episode reward: total was -22.430000. running mean: -6.828662\n",
      "epsilon:0.012749 episode_count: 19859. steps_count: 8779211.000000\n",
      "ep 2837: ep_len:500 episode reward: total was 1.190000. running mean: -6.748476\n",
      "ep 2837: ep_len:615 episode reward: total was 11.070000. running mean: -6.570291\n",
      "ep 2837: ep_len:50 episode reward: total was 1.520000. running mean: -6.489388\n",
      "ep 2837: ep_len:505 episode reward: total was -13.530000. running mean: -6.559794\n",
      "ep 2837: ep_len:3 episode reward: total was 0.000000. running mean: -6.494196\n",
      "ep 2837: ep_len:565 episode reward: total was -4.120000. running mean: -6.470454\n",
      "ep 2837: ep_len:505 episode reward: total was -34.120000. running mean: -6.746950\n",
      "epsilon:0.012613 episode_count: 19866. steps_count: 8781954.000000\n",
      "ep 2838: ep_len:510 episode reward: total was -4.670000. running mean: -6.726180\n",
      "ep 2838: ep_len:182 episode reward: total was -7.910000. running mean: -6.738018\n",
      "ep 2838: ep_len:600 episode reward: total was 9.510000. running mean: -6.575538\n",
      "ep 2838: ep_len:520 episode reward: total was -30.190000. running mean: -6.811683\n",
      "ep 2838: ep_len:54 episode reward: total was 5.000000. running mean: -6.693566\n",
      "ep 2838: ep_len:159 episode reward: total was 3.600000. running mean: -6.590630\n",
      "ep 2838: ep_len:590 episode reward: total was -12.360000. running mean: -6.648324\n",
      "epsilon:0.012476 episode_count: 19873. steps_count: 8784569.000000\n",
      "ep 2839: ep_len:690 episode reward: total was -40.780000. running mean: -6.989641\n",
      "ep 2839: ep_len:500 episode reward: total was -31.020000. running mean: -7.229944\n",
      "ep 2839: ep_len:371 episode reward: total was 5.230000. running mean: -7.105345\n",
      "ep 2839: ep_len:500 episode reward: total was -28.120000. running mean: -7.315491\n",
      "ep 2839: ep_len:3 episode reward: total was 0.000000. running mean: -7.242337\n",
      "ep 2839: ep_len:770 episode reward: total was -37.280000. running mean: -7.542713\n",
      "ep 2839: ep_len:500 episode reward: total was -1.190000. running mean: -7.479186\n",
      "epsilon:0.012340 episode_count: 19880. steps_count: 8787903.000000\n",
      "ep 2840: ep_len:236 episode reward: total was 1.640000. running mean: -7.387994\n",
      "ep 2840: ep_len:605 episode reward: total was -4.110000. running mean: -7.355214\n",
      "ep 2840: ep_len:500 episode reward: total was 1.390000. running mean: -7.267762\n",
      "ep 2840: ep_len:500 episode reward: total was 1.350000. running mean: -7.181584\n",
      "ep 2840: ep_len:122 episode reward: total was 2.550000. running mean: -7.084269\n",
      "ep 2840: ep_len:695 episode reward: total was 5.880000. running mean: -6.954626\n",
      "ep 2840: ep_len:500 episode reward: total was -23.620000. running mean: -7.121280\n",
      "epsilon:0.012203 episode_count: 19887. steps_count: 8791061.000000\n",
      "ep 2841: ep_len:860 episode reward: total was -46.160000. running mean: -7.511667\n",
      "ep 2841: ep_len:665 episode reward: total was 19.680000. running mean: -7.239750\n",
      "ep 2841: ep_len:620 episode reward: total was -35.530000. running mean: -7.522653\n",
      "ep 2841: ep_len:500 episode reward: total was 13.430000. running mean: -7.313126\n",
      "ep 2841: ep_len:3 episode reward: total was 0.000000. running mean: -7.239995\n",
      "ep 2841: ep_len:505 episode reward: total was -26.240000. running mean: -7.429995\n",
      "ep 2841: ep_len:805 episode reward: total was -38.210000. running mean: -7.737795\n",
      "epsilon:0.012067 episode_count: 19894. steps_count: 8795019.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2842: ep_len:630 episode reward: total was 9.660000. running mean: -7.563817\n",
      "ep 2842: ep_len:825 episode reward: total was -53.330000. running mean: -8.021479\n",
      "ep 2842: ep_len:555 episode reward: total was -9.280000. running mean: -8.034064\n",
      "ep 2842: ep_len:585 episode reward: total was -2.540000. running mean: -7.979123\n",
      "ep 2842: ep_len:3 episode reward: total was 0.000000. running mean: -7.899332\n",
      "ep 2842: ep_len:235 episode reward: total was 9.650000. running mean: -7.723839\n",
      "ep 2842: ep_len:326 episode reward: total was -6.830000. running mean: -7.714901\n",
      "epsilon:0.011930 episode_count: 19901. steps_count: 8798178.000000\n",
      "ep 2843: ep_len:660 episode reward: total was -12.160000. running mean: -7.759352\n",
      "ep 2843: ep_len:500 episode reward: total was -31.930000. running mean: -8.001058\n",
      "ep 2843: ep_len:62 episode reward: total was 1.050000. running mean: -7.910547\n",
      "ep 2843: ep_len:500 episode reward: total was 9.590000. running mean: -7.735542\n",
      "ep 2843: ep_len:3 episode reward: total was 0.000000. running mean: -7.658187\n",
      "ep 2843: ep_len:500 episode reward: total was 4.480000. running mean: -7.536805\n",
      "ep 2843: ep_len:500 episode reward: total was -24.630000. running mean: -7.707737\n",
      "epsilon:0.011794 episode_count: 19908. steps_count: 8800903.000000\n",
      "ep 2844: ep_len:500 episode reward: total was 4.390000. running mean: -7.586759\n",
      "ep 2844: ep_len:500 episode reward: total was -33.400000. running mean: -7.844892\n",
      "ep 2844: ep_len:550 episode reward: total was -16.420000. running mean: -7.930643\n",
      "ep 2844: ep_len:500 episode reward: total was 5.060000. running mean: -7.800736\n",
      "ep 2844: ep_len:97 episode reward: total was -9.960000. running mean: -7.822329\n",
      "ep 2844: ep_len:500 episode reward: total was -25.730000. running mean: -8.001406\n",
      "ep 2844: ep_len:600 episode reward: total was -9.880000. running mean: -8.020192\n",
      "epsilon:0.011657 episode_count: 19915. steps_count: 8804150.000000\n",
      "ep 2845: ep_len:555 episode reward: total was -13.700000. running mean: -8.076990\n",
      "ep 2845: ep_len:590 episode reward: total was -18.830000. running mean: -8.184520\n",
      "ep 2845: ep_len:590 episode reward: total was 7.040000. running mean: -8.032275\n",
      "ep 2845: ep_len:131 episode reward: total was 4.110000. running mean: -7.910852\n",
      "ep 2845: ep_len:48 episode reward: total was 4.500000. running mean: -7.786743\n",
      "ep 2845: ep_len:535 episode reward: total was -29.980000. running mean: -8.008676\n",
      "ep 2845: ep_len:600 episode reward: total was -29.670000. running mean: -8.225289\n",
      "epsilon:0.011521 episode_count: 19922. steps_count: 8807199.000000\n",
      "ep 2846: ep_len:610 episode reward: total was -11.460000. running mean: -8.257636\n",
      "ep 2846: ep_len:525 episode reward: total was -2.370000. running mean: -8.198760\n",
      "ep 2846: ep_len:595 episode reward: total was -10.520000. running mean: -8.221972\n",
      "ep 2846: ep_len:500 episode reward: total was -19.090000. running mean: -8.330653\n",
      "ep 2846: ep_len:3 episode reward: total was 0.000000. running mean: -8.247346\n",
      "ep 2846: ep_len:1105 episode reward: total was -134.060000. running mean: -9.505473\n",
      "ep 2846: ep_len:545 episode reward: total was -4.420000. running mean: -9.454618\n",
      "epsilon:0.011384 episode_count: 19929. steps_count: 8811082.000000\n",
      "ep 2847: ep_len:530 episode reward: total was -25.060000. running mean: -9.610672\n",
      "ep 2847: ep_len:500 episode reward: total was 8.050000. running mean: -9.434065\n",
      "ep 2847: ep_len:500 episode reward: total was 3.270000. running mean: -9.307024\n",
      "ep 2847: ep_len:111 episode reward: total was 4.070000. running mean: -9.173254\n",
      "ep 2847: ep_len:3 episode reward: total was 0.000000. running mean: -9.081522\n",
      "ep 2847: ep_len:585 episode reward: total was -13.640000. running mean: -9.127106\n",
      "ep 2847: ep_len:545 episode reward: total was -17.850000. running mean: -9.214335\n",
      "epsilon:0.011248 episode_count: 19936. steps_count: 8813856.000000\n",
      "ep 2848: ep_len:500 episode reward: total was -11.710000. running mean: -9.239292\n",
      "ep 2848: ep_len:545 episode reward: total was -11.330000. running mean: -9.260199\n",
      "ep 2848: ep_len:670 episode reward: total was 1.910000. running mean: -9.148497\n",
      "ep 2848: ep_len:600 episode reward: total was 14.960000. running mean: -8.907412\n",
      "ep 2848: ep_len:78 episode reward: total was -9.460000. running mean: -8.912938\n",
      "ep 2848: ep_len:540 episode reward: total was 2.970000. running mean: -8.794109\n",
      "ep 2848: ep_len:535 episode reward: total was -32.610000. running mean: -9.032267\n",
      "epsilon:0.011111 episode_count: 19943. steps_count: 8817324.000000\n",
      "ep 2849: ep_len:505 episode reward: total was 0.910000. running mean: -8.932845\n",
      "ep 2849: ep_len:535 episode reward: total was -10.030000. running mean: -8.943816\n",
      "ep 2849: ep_len:500 episode reward: total was -13.610000. running mean: -8.990478\n",
      "ep 2849: ep_len:505 episode reward: total was -8.430000. running mean: -8.984873\n",
      "ep 2849: ep_len:3 episode reward: total was 0.000000. running mean: -8.895025\n",
      "ep 2849: ep_len:590 episode reward: total was -3.520000. running mean: -8.841274\n",
      "ep 2849: ep_len:178 episode reward: total was -5.880000. running mean: -8.811662\n",
      "epsilon:0.010975 episode_count: 19950. steps_count: 8820140.000000\n",
      "ep 2850: ep_len:500 episode reward: total was 13.450000. running mean: -8.589045\n",
      "ep 2850: ep_len:600 episode reward: total was -1.320000. running mean: -8.516355\n",
      "ep 2850: ep_len:570 episode reward: total was -32.920000. running mean: -8.760391\n",
      "ep 2850: ep_len:500 episode reward: total was -16.100000. running mean: -8.833787\n",
      "ep 2850: ep_len:47 episode reward: total was 4.500000. running mean: -8.700449\n",
      "ep 2850: ep_len:500 episode reward: total was 8.440000. running mean: -8.529045\n",
      "ep 2850: ep_len:600 episode reward: total was -16.490000. running mean: -8.608654\n",
      "epsilon:0.010838 episode_count: 19957. steps_count: 8823457.000000\n",
      "ep 2851: ep_len:219 episode reward: total was -0.920000. running mean: -8.531768\n",
      "ep 2851: ep_len:500 episode reward: total was -22.710000. running mean: -8.673550\n",
      "ep 2851: ep_len:500 episode reward: total was -1.080000. running mean: -8.597615\n",
      "ep 2851: ep_len:535 episode reward: total was -7.590000. running mean: -8.587538\n",
      "ep 2851: ep_len:104 episode reward: total was -7.940000. running mean: -8.581063\n",
      "ep 2851: ep_len:560 episode reward: total was -6.160000. running mean: -8.556852\n",
      "ep 2851: ep_len:201 episode reward: total was -8.370000. running mean: -8.554984\n",
      "epsilon:0.010702 episode_count: 19964. steps_count: 8826076.000000\n",
      "ep 2852: ep_len:500 episode reward: total was -14.410000. running mean: -8.613534\n",
      "ep 2852: ep_len:500 episode reward: total was -30.310000. running mean: -8.830499\n",
      "ep 2852: ep_len:605 episode reward: total was 4.610000. running mean: -8.696094\n",
      "ep 2852: ep_len:500 episode reward: total was 7.430000. running mean: -8.534833\n",
      "ep 2852: ep_len:3 episode reward: total was 0.000000. running mean: -8.449484\n",
      "ep 2852: ep_len:500 episode reward: total was -25.560000. running mean: -8.620590\n",
      "ep 2852: ep_len:710 episode reward: total was -91.660000. running mean: -9.450984\n",
      "epsilon:0.010565 episode_count: 19971. steps_count: 8829394.000000\n",
      "ep 2853: ep_len:247 episode reward: total was -6.940000. running mean: -9.425874\n",
      "ep 2853: ep_len:540 episode reward: total was 20.390000. running mean: -9.127715\n",
      "ep 2853: ep_len:70 episode reward: total was 2.570000. running mean: -9.010738\n",
      "ep 2853: ep_len:56 episode reward: total was -3.490000. running mean: -8.955531\n",
      "ep 2853: ep_len:3 episode reward: total was 0.000000. running mean: -8.865975\n",
      "ep 2853: ep_len:765 episode reward: total was -54.150000. running mean: -9.318816\n",
      "ep 2853: ep_len:595 episode reward: total was -8.800000. running mean: -9.313627\n",
      "epsilon:0.010429 episode_count: 19978. steps_count: 8831670.000000\n",
      "ep 2854: ep_len:570 episode reward: total was 13.480000. running mean: -9.085691\n",
      "ep 2854: ep_len:510 episode reward: total was 15.140000. running mean: -8.843434\n",
      "ep 2854: ep_len:615 episode reward: total was -2.000000. running mean: -8.775000\n",
      "ep 2854: ep_len:500 episode reward: total was -27.160000. running mean: -8.958850\n",
      "ep 2854: ep_len:3 episode reward: total was 0.000000. running mean: -8.869261\n",
      "ep 2854: ep_len:635 episode reward: total was -23.270000. running mean: -9.013269\n",
      "ep 2854: ep_len:605 episode reward: total was -17.460000. running mean: -9.097736\n",
      "epsilon:0.010292 episode_count: 19985. steps_count: 8835108.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2855: ep_len:620 episode reward: total was -0.400000. running mean: -9.010759\n",
      "ep 2855: ep_len:560 episode reward: total was 19.840000. running mean: -8.722251\n",
      "ep 2855: ep_len:376 episode reward: total was 4.750000. running mean: -8.587529\n",
      "ep 2855: ep_len:525 episode reward: total was -53.200000. running mean: -9.033653\n",
      "ep 2855: ep_len:3 episode reward: total was 0.000000. running mean: -8.943317\n",
      "ep 2855: ep_len:515 episode reward: total was -12.540000. running mean: -8.979284\n",
      "ep 2855: ep_len:191 episode reward: total was -7.920000. running mean: -8.968691\n",
      "epsilon:0.010156 episode_count: 19992. steps_count: 8837898.000000\n",
      "ep 2856: ep_len:645 episode reward: total was -15.940000. running mean: -9.038404\n",
      "ep 2856: ep_len:525 episode reward: total was -7.330000. running mean: -9.021320\n",
      "ep 2856: ep_len:72 episode reward: total was 2.540000. running mean: -8.905707\n",
      "ep 2856: ep_len:56 episode reward: total was 0.550000. running mean: -8.811150\n",
      "ep 2856: ep_len:3 episode reward: total was 0.000000. running mean: -8.723038\n",
      "ep 2856: ep_len:180 episode reward: total was 2.090000. running mean: -8.614908\n",
      "ep 2856: ep_len:500 episode reward: total was -13.380000. running mean: -8.662559\n",
      "epsilon:0.010019 episode_count: 19999. steps_count: 8839879.000000\n",
      "ep 2857: ep_len:500 episode reward: total was 8.400000. running mean: -8.491933\n",
      "ep 2857: ep_len:525 episode reward: total was -19.840000. running mean: -8.605414\n",
      "ep 2857: ep_len:555 episode reward: total was 2.020000. running mean: -8.499160\n",
      "ep 2857: ep_len:505 episode reward: total was -5.630000. running mean: -8.470468\n",
      "ep 2857: ep_len:92 episode reward: total was 7.050000. running mean: -8.315263\n",
      "ep 2857: ep_len:257 episode reward: total was 3.630000. running mean: -8.195811\n",
      "ep 2857: ep_len:500 episode reward: total was -9.130000. running mean: -8.205153\n",
      "epsilon:0.010000 episode_count: 20006. steps_count: 8842813.000000\n",
      "ep 2858: ep_len:500 episode reward: total was 6.210000. running mean: -8.061001\n",
      "ep 2858: ep_len:540 episode reward: total was 18.390000. running mean: -7.796491\n",
      "ep 2858: ep_len:575 episode reward: total was -11.730000. running mean: -7.835826\n",
      "ep 2858: ep_len:510 episode reward: total was -27.940000. running mean: -8.036868\n",
      "ep 2858: ep_len:3 episode reward: total was 0.000000. running mean: -7.956499\n",
      "ep 2858: ep_len:173 episode reward: total was 5.590000. running mean: -7.821034\n",
      "ep 2858: ep_len:530 episode reward: total was -22.400000. running mean: -7.966824\n",
      "epsilon:0.010000 episode_count: 20013. steps_count: 8845644.000000\n",
      "ep 2859: ep_len:620 episode reward: total was -23.520000. running mean: -8.122356\n",
      "ep 2859: ep_len:500 episode reward: total was 10.680000. running mean: -7.934332\n",
      "ep 2859: ep_len:500 episode reward: total was -25.580000. running mean: -8.110789\n",
      "ep 2859: ep_len:654 episode reward: total was -22.320000. running mean: -8.252881\n",
      "ep 2859: ep_len:130 episode reward: total was -10.420000. running mean: -8.274552\n",
      "ep 2859: ep_len:565 episode reward: total was -10.710000. running mean: -8.298907\n",
      "ep 2859: ep_len:282 episode reward: total was -4.330000. running mean: -8.259217\n",
      "epsilon:0.010000 episode_count: 20020. steps_count: 8848895.000000\n",
      "ep 2860: ep_len:525 episode reward: total was -16.440000. running mean: -8.341025\n",
      "ep 2860: ep_len:565 episode reward: total was -4.500000. running mean: -8.302615\n",
      "ep 2860: ep_len:695 episode reward: total was -12.670000. running mean: -8.346289\n",
      "ep 2860: ep_len:500 episode reward: total was -6.540000. running mean: -8.328226\n",
      "ep 2860: ep_len:3 episode reward: total was 0.000000. running mean: -8.244944\n",
      "ep 2860: ep_len:500 episode reward: total was -15.200000. running mean: -8.314494\n",
      "ep 2860: ep_len:595 episode reward: total was -21.300000. running mean: -8.444349\n",
      "epsilon:0.010000 episode_count: 20027. steps_count: 8852278.000000\n",
      "ep 2861: ep_len:545 episode reward: total was -0.500000. running mean: -8.364906\n",
      "ep 2861: ep_len:525 episode reward: total was -18.410000. running mean: -8.465357\n",
      "ep 2861: ep_len:625 episode reward: total was 2.830000. running mean: -8.352403\n",
      "ep 2861: ep_len:385 episode reward: total was -0.660000. running mean: -8.275479\n",
      "ep 2861: ep_len:92 episode reward: total was -8.940000. running mean: -8.282124\n",
      "ep 2861: ep_len:610 episode reward: total was -0.610000. running mean: -8.205403\n",
      "ep 2861: ep_len:610 episode reward: total was -9.350000. running mean: -8.216849\n",
      "epsilon:0.010000 episode_count: 20034. steps_count: 8855670.000000\n",
      "ep 2862: ep_len:500 episode reward: total was 16.340000. running mean: -7.971281\n",
      "ep 2862: ep_len:500 episode reward: total was 13.210000. running mean: -7.759468\n",
      "ep 2862: ep_len:530 episode reward: total was -24.860000. running mean: -7.930473\n",
      "ep 2862: ep_len:500 episode reward: total was 6.570000. running mean: -7.785468\n",
      "ep 2862: ep_len:3 episode reward: total was 0.000000. running mean: -7.707614\n",
      "ep 2862: ep_len:525 episode reward: total was -36.500000. running mean: -7.995538\n",
      "ep 2862: ep_len:328 episode reward: total was -27.300000. running mean: -8.188582\n",
      "epsilon:0.010000 episode_count: 20041. steps_count: 8858556.000000\n",
      "ep 2863: ep_len:248 episode reward: total was -17.830000. running mean: -8.284996\n",
      "ep 2863: ep_len:500 episode reward: total was -24.570000. running mean: -8.447846\n",
      "ep 2863: ep_len:660 episode reward: total was -0.160000. running mean: -8.364968\n",
      "ep 2863: ep_len:500 episode reward: total was -12.050000. running mean: -8.401818\n",
      "ep 2863: ep_len:3 episode reward: total was 0.000000. running mean: -8.317800\n",
      "ep 2863: ep_len:227 episode reward: total was 5.160000. running mean: -8.183022\n",
      "ep 2863: ep_len:590 episode reward: total was -3.760000. running mean: -8.138792\n",
      "epsilon:0.010000 episode_count: 20048. steps_count: 8861284.000000\n",
      "ep 2864: ep_len:620 episode reward: total was -5.810000. running mean: -8.115504\n",
      "ep 2864: ep_len:585 episode reward: total was -0.530000. running mean: -8.039649\n",
      "ep 2864: ep_len:610 episode reward: total was -0.720000. running mean: -7.966452\n",
      "ep 2864: ep_len:555 episode reward: total was 16.080000. running mean: -7.725988\n",
      "ep 2864: ep_len:3 episode reward: total was 0.000000. running mean: -7.648728\n",
      "ep 2864: ep_len:500 episode reward: total was -3.910000. running mean: -7.611341\n",
      "ep 2864: ep_len:207 episode reward: total was -2.840000. running mean: -7.563627\n",
      "epsilon:0.010000 episode_count: 20055. steps_count: 8864364.000000\n",
      "ep 2865: ep_len:610 episode reward: total was 9.010000. running mean: -7.397891\n",
      "ep 2865: ep_len:500 episode reward: total was -14.850000. running mean: -7.472412\n",
      "ep 2865: ep_len:386 episode reward: total was 1.720000. running mean: -7.380488\n",
      "ep 2865: ep_len:575 episode reward: total was -0.470000. running mean: -7.311383\n",
      "ep 2865: ep_len:3 episode reward: total was 0.000000. running mean: -7.238269\n",
      "ep 2865: ep_len:680 episode reward: total was -52.730000. running mean: -7.693187\n",
      "ep 2865: ep_len:289 episode reward: total was -4.320000. running mean: -7.659455\n",
      "epsilon:0.010000 episode_count: 20062. steps_count: 8867407.000000\n",
      "ep 2866: ep_len:251 episode reward: total was 4.110000. running mean: -7.541760\n",
      "ep 2866: ep_len:565 episode reward: total was -13.940000. running mean: -7.605743\n",
      "ep 2866: ep_len:600 episode reward: total was -14.900000. running mean: -7.678685\n",
      "ep 2866: ep_len:132 episode reward: total was 6.100000. running mean: -7.540898\n",
      "ep 2866: ep_len:85 episode reward: total was -10.940000. running mean: -7.574889\n",
      "ep 2866: ep_len:540 episode reward: total was -4.350000. running mean: -7.542640\n",
      "ep 2866: ep_len:550 episode reward: total was -1.300000. running mean: -7.480214\n",
      "epsilon:0.010000 episode_count: 20069. steps_count: 8870130.000000\n",
      "ep 2867: ep_len:520 episode reward: total was -26.930000. running mean: -7.674712\n",
      "ep 2867: ep_len:745 episode reward: total was -67.320000. running mean: -8.271165\n",
      "ep 2867: ep_len:500 episode reward: total was 6.900000. running mean: -8.119453\n",
      "ep 2867: ep_len:505 episode reward: total was -5.520000. running mean: -8.093459\n",
      "ep 2867: ep_len:3 episode reward: total was 0.000000. running mean: -8.012524\n",
      "ep 2867: ep_len:261 episode reward: total was 6.640000. running mean: -7.865999\n",
      "ep 2867: ep_len:530 episode reward: total was -4.860000. running mean: -7.835939\n",
      "epsilon:0.010000 episode_count: 20076. steps_count: 8873194.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2868: ep_len:215 episode reward: total was 1.100000. running mean: -7.746579\n",
      "ep 2868: ep_len:560 episode reward: total was 19.900000. running mean: -7.470114\n",
      "ep 2868: ep_len:358 episode reward: total was -21.840000. running mean: -7.613812\n",
      "ep 2868: ep_len:500 episode reward: total was 14.900000. running mean: -7.388674\n",
      "ep 2868: ep_len:3 episode reward: total was 0.000000. running mean: -7.314788\n",
      "ep 2868: ep_len:695 episode reward: total was -45.300000. running mean: -7.694640\n",
      "ep 2868: ep_len:605 episode reward: total was -5.490000. running mean: -7.672593\n",
      "epsilon:0.010000 episode_count: 20083. steps_count: 8876130.000000\n",
      "ep 2869: ep_len:540 episode reward: total was 6.910000. running mean: -7.526767\n",
      "ep 2869: ep_len:372 episode reward: total was -7.320000. running mean: -7.524700\n",
      "ep 2869: ep_len:625 episode reward: total was 1.020000. running mean: -7.439253\n",
      "ep 2869: ep_len:133 episode reward: total was 1.080000. running mean: -7.354060\n",
      "ep 2869: ep_len:3 episode reward: total was 0.000000. running mean: -7.280520\n",
      "ep 2869: ep_len:550 episode reward: total was -4.570000. running mean: -7.253414\n",
      "ep 2869: ep_len:610 episode reward: total was -18.990000. running mean: -7.370780\n",
      "epsilon:0.010000 episode_count: 20090. steps_count: 8878963.000000\n",
      "ep 2870: ep_len:505 episode reward: total was -24.940000. running mean: -7.546472\n",
      "ep 2870: ep_len:520 episode reward: total was -12.970000. running mean: -7.600708\n",
      "ep 2870: ep_len:625 episode reward: total was -5.890000. running mean: -7.583601\n",
      "ep 2870: ep_len:500 episode reward: total was -4.490000. running mean: -7.552665\n",
      "ep 2870: ep_len:95 episode reward: total was 3.510000. running mean: -7.442038\n",
      "ep 2870: ep_len:545 episode reward: total was 5.070000. running mean: -7.316918\n",
      "ep 2870: ep_len:515 episode reward: total was -23.060000. running mean: -7.474348\n",
      "epsilon:0.010000 episode_count: 20097. steps_count: 8882268.000000\n",
      "ep 2871: ep_len:715 episode reward: total was -37.240000. running mean: -7.772005\n",
      "ep 2871: ep_len:500 episode reward: total was 4.590000. running mean: -7.648385\n",
      "ep 2871: ep_len:555 episode reward: total was -1.080000. running mean: -7.582701\n",
      "ep 2871: ep_len:545 episode reward: total was -0.090000. running mean: -7.507774\n",
      "ep 2871: ep_len:3 episode reward: total was 0.000000. running mean: -7.432696\n",
      "ep 2871: ep_len:500 episode reward: total was -7.720000. running mean: -7.435569\n",
      "ep 2871: ep_len:525 episode reward: total was 0.070000. running mean: -7.360514\n",
      "epsilon:0.010000 episode_count: 20104. steps_count: 8885611.000000\n",
      "ep 2872: ep_len:580 episode reward: total was 10.470000. running mean: -7.182209\n",
      "ep 2872: ep_len:500 episode reward: total was 9.920000. running mean: -7.011186\n",
      "ep 2872: ep_len:1075 episode reward: total was -84.830000. running mean: -7.789375\n",
      "ep 2872: ep_len:565 episode reward: total was 4.010000. running mean: -7.671381\n",
      "ep 2872: ep_len:3 episode reward: total was 0.000000. running mean: -7.594667\n",
      "ep 2872: ep_len:223 episode reward: total was 6.620000. running mean: -7.452520\n",
      "ep 2872: ep_len:510 episode reward: total was -19.500000. running mean: -7.572995\n",
      "epsilon:0.010000 episode_count: 20111. steps_count: 8889067.000000\n",
      "ep 2873: ep_len:590 episode reward: total was 7.110000. running mean: -7.426165\n",
      "ep 2873: ep_len:680 episode reward: total was -14.780000. running mean: -7.499704\n",
      "ep 2873: ep_len:590 episode reward: total was 1.460000. running mean: -7.410107\n",
      "ep 2873: ep_len:377 episode reward: total was 9.330000. running mean: -7.242705\n",
      "ep 2873: ep_len:127 episode reward: total was 4.550000. running mean: -7.124778\n",
      "ep 2873: ep_len:862 episode reward: total was -55.780000. running mean: -7.611331\n",
      "ep 2873: ep_len:595 episode reward: total was -1.250000. running mean: -7.547717\n",
      "epsilon:0.010000 episode_count: 20118. steps_count: 8892888.000000\n",
      "ep 2874: ep_len:525 episode reward: total was -2.970000. running mean: -7.501940\n",
      "ep 2874: ep_len:500 episode reward: total was -3.360000. running mean: -7.460521\n",
      "ep 2874: ep_len:560 episode reward: total was -17.720000. running mean: -7.563116\n",
      "ep 2874: ep_len:620 episode reward: total was 19.960000. running mean: -7.287884\n",
      "ep 2874: ep_len:3 episode reward: total was 0.000000. running mean: -7.215006\n",
      "ep 2874: ep_len:176 episode reward: total was -23.920000. running mean: -7.382055\n",
      "ep 2874: ep_len:610 episode reward: total was 3.290000. running mean: -7.275335\n",
      "epsilon:0.010000 episode_count: 20125. steps_count: 8895882.000000\n",
      "ep 2875: ep_len:570 episode reward: total was 6.090000. running mean: -7.141682\n",
      "ep 2875: ep_len:570 episode reward: total was -14.830000. running mean: -7.218565\n",
      "ep 2875: ep_len:406 episode reward: total was 3.750000. running mean: -7.108879\n",
      "ep 2875: ep_len:520 episode reward: total was 12.460000. running mean: -6.913190\n",
      "ep 2875: ep_len:122 episode reward: total was 2.580000. running mean: -6.818258\n",
      "ep 2875: ep_len:595 episode reward: total was 0.430000. running mean: -6.745776\n",
      "ep 2875: ep_len:535 episode reward: total was -10.460000. running mean: -6.782918\n",
      "epsilon:0.010000 episode_count: 20132. steps_count: 8899200.000000\n",
      "ep 2876: ep_len:605 episode reward: total was -11.260000. running mean: -6.827689\n",
      "ep 2876: ep_len:500 episode reward: total was -45.290000. running mean: -7.212312\n",
      "ep 2876: ep_len:585 episode reward: total was -19.380000. running mean: -7.333989\n",
      "ep 2876: ep_len:575 episode reward: total was 1.410000. running mean: -7.246549\n",
      "ep 2876: ep_len:93 episode reward: total was -10.960000. running mean: -7.283683\n",
      "ep 2876: ep_len:560 episode reward: total was -16.910000. running mean: -7.379947\n",
      "ep 2876: ep_len:540 episode reward: total was -10.120000. running mean: -7.407347\n",
      "epsilon:0.010000 episode_count: 20139. steps_count: 8902658.000000\n",
      "ep 2877: ep_len:250 episode reward: total was 4.120000. running mean: -7.292074\n",
      "ep 2877: ep_len:505 episode reward: total was -19.490000. running mean: -7.414053\n",
      "ep 2877: ep_len:535 episode reward: total was -8.370000. running mean: -7.423612\n",
      "ep 2877: ep_len:395 episode reward: total was -27.140000. running mean: -7.620776\n",
      "ep 2877: ep_len:117 episode reward: total was -10.940000. running mean: -7.653969\n",
      "ep 2877: ep_len:515 episode reward: total was 5.460000. running mean: -7.522829\n",
      "ep 2877: ep_len:665 episode reward: total was -14.290000. running mean: -7.590501\n",
      "epsilon:0.010000 episode_count: 20146. steps_count: 8905640.000000\n",
      "ep 2878: ep_len:515 episode reward: total was -28.440000. running mean: -7.798996\n",
      "ep 2878: ep_len:570 episode reward: total was 24.410000. running mean: -7.476906\n",
      "ep 2878: ep_len:580 episode reward: total was -15.840000. running mean: -7.560537\n",
      "ep 2878: ep_len:520 episode reward: total was 7.030000. running mean: -7.414631\n",
      "ep 2878: ep_len:113 episode reward: total was 4.060000. running mean: -7.299885\n",
      "ep 2878: ep_len:630 episode reward: total was -28.850000. running mean: -7.515386\n",
      "ep 2878: ep_len:595 episode reward: total was -42.590000. running mean: -7.866132\n",
      "epsilon:0.010000 episode_count: 20153. steps_count: 8909163.000000\n",
      "ep 2879: ep_len:128 episode reward: total was 2.590000. running mean: -7.761571\n",
      "ep 2879: ep_len:550 episode reward: total was -9.920000. running mean: -7.783155\n",
      "ep 2879: ep_len:500 episode reward: total was -27.980000. running mean: -7.985124\n",
      "ep 2879: ep_len:412 episode reward: total was -33.170000. running mean: -8.236972\n",
      "ep 2879: ep_len:94 episode reward: total was -0.480000. running mean: -8.159403\n",
      "ep 2879: ep_len:520 episode reward: total was -17.860000. running mean: -8.256409\n",
      "ep 2879: ep_len:195 episode reward: total was -2.830000. running mean: -8.202145\n",
      "epsilon:0.010000 episode_count: 20160. steps_count: 8911562.000000\n",
      "ep 2880: ep_len:635 episode reward: total was 2.220000. running mean: -8.097923\n",
      "ep 2880: ep_len:500 episode reward: total was -45.740000. running mean: -8.474344\n",
      "ep 2880: ep_len:630 episode reward: total was -15.250000. running mean: -8.542100\n",
      "ep 2880: ep_len:615 episode reward: total was 8.090000. running mean: -8.375779\n",
      "ep 2880: ep_len:3 episode reward: total was 0.000000. running mean: -8.292022\n",
      "ep 2880: ep_len:306 episode reward: total was -0.850000. running mean: -8.217601\n",
      "ep 2880: ep_len:311 episode reward: total was -5.820000. running mean: -8.193625\n",
      "epsilon:0.010000 episode_count: 20167. steps_count: 8914562.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2881: ep_len:715 episode reward: total was -30.750000. running mean: -8.419189\n",
      "ep 2881: ep_len:570 episode reward: total was 25.430000. running mean: -8.080697\n",
      "ep 2881: ep_len:570 episode reward: total was 12.040000. running mean: -7.879490\n",
      "ep 2881: ep_len:500 episode reward: total was -15.080000. running mean: -7.951495\n",
      "ep 2881: ep_len:117 episode reward: total was 5.050000. running mean: -7.821480\n",
      "ep 2881: ep_len:765 episode reward: total was -22.660000. running mean: -7.969866\n",
      "ep 2881: ep_len:600 episode reward: total was -4.840000. running mean: -7.938567\n",
      "epsilon:0.010000 episode_count: 20174. steps_count: 8918399.000000\n",
      "ep 2882: ep_len:540 episode reward: total was -12.600000. running mean: -7.985181\n",
      "ep 2882: ep_len:540 episode reward: total was 14.390000. running mean: -7.761429\n",
      "ep 2882: ep_len:444 episode reward: total was 9.290000. running mean: -7.590915\n",
      "ep 2882: ep_len:515 episode reward: total was -4.980000. running mean: -7.564806\n",
      "ep 2882: ep_len:53 episode reward: total was 5.000000. running mean: -7.439158\n",
      "ep 2882: ep_len:605 episode reward: total was 7.490000. running mean: -7.289866\n",
      "ep 2882: ep_len:525 episode reward: total was -27.990000. running mean: -7.496868\n",
      "epsilon:0.010000 episode_count: 20181. steps_count: 8921621.000000\n",
      "ep 2883: ep_len:630 episode reward: total was -17.760000. running mean: -7.599499\n",
      "ep 2883: ep_len:500 episode reward: total was -32.330000. running mean: -7.846804\n",
      "ep 2883: ep_len:1135 episode reward: total was -156.760000. running mean: -9.335936\n",
      "ep 2883: ep_len:500 episode reward: total was -11.590000. running mean: -9.358477\n",
      "ep 2883: ep_len:118 episode reward: total was 8.540000. running mean: -9.179492\n",
      "ep 2883: ep_len:645 episode reward: total was -43.470000. running mean: -9.522397\n",
      "ep 2883: ep_len:565 episode reward: total was -28.100000. running mean: -9.708173\n",
      "epsilon:0.010000 episode_count: 20188. steps_count: 8925714.000000\n",
      "ep 2884: ep_len:675 episode reward: total was -17.240000. running mean: -9.783491\n",
      "ep 2884: ep_len:645 episode reward: total was -15.040000. running mean: -9.836056\n",
      "ep 2884: ep_len:615 episode reward: total was -5.730000. running mean: -9.794996\n",
      "ep 2884: ep_len:45 episode reward: total was 1.050000. running mean: -9.686546\n",
      "ep 2884: ep_len:3 episode reward: total was 0.000000. running mean: -9.589680\n",
      "ep 2884: ep_len:151 episode reward: total was 7.610000. running mean: -9.417684\n",
      "ep 2884: ep_len:195 episode reward: total was -2.830000. running mean: -9.351807\n",
      "epsilon:0.010000 episode_count: 20195. steps_count: 8928043.000000\n",
      "ep 2885: ep_len:565 episode reward: total was -2.550000. running mean: -9.283789\n",
      "ep 2885: ep_len:505 episode reward: total was -11.040000. running mean: -9.301351\n",
      "ep 2885: ep_len:640 episode reward: total was -49.890000. running mean: -9.707237\n",
      "ep 2885: ep_len:570 episode reward: total was 10.000000. running mean: -9.510165\n",
      "ep 2885: ep_len:3 episode reward: total was 0.000000. running mean: -9.415063\n",
      "ep 2885: ep_len:660 episode reward: total was 11.960000. running mean: -9.201313\n",
      "ep 2885: ep_len:630 episode reward: total was -1.500000. running mean: -9.124299\n",
      "epsilon:0.010000 episode_count: 20202. steps_count: 8931616.000000\n",
      "ep 2886: ep_len:625 episode reward: total was -4.120000. running mean: -9.074256\n",
      "ep 2886: ep_len:500 episode reward: total was -30.160000. running mean: -9.285114\n",
      "ep 2886: ep_len:500 episode reward: total was 3.590000. running mean: -9.156363\n",
      "ep 2886: ep_len:525 episode reward: total was 5.950000. running mean: -9.005299\n",
      "ep 2886: ep_len:77 episode reward: total was 0.010000. running mean: -8.915146\n",
      "ep 2886: ep_len:500 episode reward: total was -22.000000. running mean: -9.045995\n",
      "ep 2886: ep_len:525 episode reward: total was -5.540000. running mean: -9.010935\n",
      "epsilon:0.010000 episode_count: 20209. steps_count: 8934868.000000\n",
      "ep 2887: ep_len:500 episode reward: total was -4.570000. running mean: -8.966525\n",
      "ep 2887: ep_len:590 episode reward: total was 12.000000. running mean: -8.756860\n",
      "ep 2887: ep_len:505 episode reward: total was -10.760000. running mean: -8.776892\n",
      "ep 2887: ep_len:505 episode reward: total was -7.510000. running mean: -8.764223\n",
      "ep 2887: ep_len:3 episode reward: total was 0.000000. running mean: -8.676580\n",
      "ep 2887: ep_len:520 episode reward: total was -0.560000. running mean: -8.595415\n",
      "ep 2887: ep_len:610 episode reward: total was -0.780000. running mean: -8.517260\n",
      "epsilon:0.010000 episode_count: 20216. steps_count: 8938101.000000\n",
      "ep 2888: ep_len:615 episode reward: total was -18.280000. running mean: -8.614888\n",
      "ep 2888: ep_len:500 episode reward: total was -1.770000. running mean: -8.546439\n",
      "ep 2888: ep_len:500 episode reward: total was 5.890000. running mean: -8.402075\n",
      "ep 2888: ep_len:500 episode reward: total was 7.080000. running mean: -8.247254\n",
      "ep 2888: ep_len:3 episode reward: total was 0.000000. running mean: -8.164781\n",
      "ep 2888: ep_len:615 episode reward: total was 10.630000. running mean: -7.976833\n",
      "ep 2888: ep_len:500 episode reward: total was -18.920000. running mean: -8.086265\n",
      "epsilon:0.010000 episode_count: 20223. steps_count: 8941334.000000\n",
      "ep 2889: ep_len:116 episode reward: total was 3.580000. running mean: -7.969603\n",
      "ep 2889: ep_len:610 episode reward: total was -7.420000. running mean: -7.964106\n",
      "ep 2889: ep_len:565 episode reward: total was 5.420000. running mean: -7.830265\n",
      "ep 2889: ep_len:500 episode reward: total was -19.520000. running mean: -7.947163\n",
      "ep 2889: ep_len:40 episode reward: total was 1.500000. running mean: -7.852691\n",
      "ep 2889: ep_len:610 episode reward: total was -0.860000. running mean: -7.782764\n",
      "ep 2889: ep_len:340 episode reward: total was -14.880000. running mean: -7.853737\n",
      "epsilon:0.010000 episode_count: 20230. steps_count: 8944115.000000\n",
      "ep 2890: ep_len:121 episode reward: total was 4.110000. running mean: -7.734099\n",
      "ep 2890: ep_len:595 episode reward: total was 4.060000. running mean: -7.616158\n",
      "ep 2890: ep_len:69 episode reward: total was 1.030000. running mean: -7.529697\n",
      "ep 2890: ep_len:500 episode reward: total was 14.960000. running mean: -7.304800\n",
      "ep 2890: ep_len:3 episode reward: total was 0.000000. running mean: -7.231752\n",
      "ep 2890: ep_len:500 episode reward: total was -27.630000. running mean: -7.435734\n",
      "ep 2890: ep_len:625 episode reward: total was -7.330000. running mean: -7.434677\n",
      "epsilon:0.010000 episode_count: 20237. steps_count: 8946528.000000\n",
      "ep 2891: ep_len:500 episode reward: total was -23.460000. running mean: -7.594930\n",
      "ep 2891: ep_len:500 episode reward: total was -11.270000. running mean: -7.631681\n",
      "ep 2891: ep_len:500 episode reward: total was 2.920000. running mean: -7.526164\n",
      "ep 2891: ep_len:585 episode reward: total was 12.650000. running mean: -7.324402\n",
      "ep 2891: ep_len:53 episode reward: total was 5.000000. running mean: -7.201158\n",
      "ep 2891: ep_len:510 episode reward: total was 4.900000. running mean: -7.080147\n",
      "ep 2891: ep_len:505 episode reward: total was -6.760000. running mean: -7.076945\n",
      "epsilon:0.010000 episode_count: 20244. steps_count: 8949681.000000\n",
      "ep 2892: ep_len:500 episode reward: total was 6.210000. running mean: -6.944076\n",
      "ep 2892: ep_len:545 episode reward: total was -11.360000. running mean: -6.988235\n",
      "ep 2892: ep_len:575 episode reward: total was -2.260000. running mean: -6.940953\n",
      "ep 2892: ep_len:620 episode reward: total was 18.200000. running mean: -6.689543\n",
      "ep 2892: ep_len:109 episode reward: total was 7.050000. running mean: -6.552148\n",
      "ep 2892: ep_len:515 episode reward: total was 1.970000. running mean: -6.466926\n",
      "ep 2892: ep_len:605 episode reward: total was -33.110000. running mean: -6.733357\n",
      "epsilon:0.010000 episode_count: 20251. steps_count: 8953150.000000\n",
      "ep 2893: ep_len:660 episode reward: total was -20.210000. running mean: -6.868123\n",
      "ep 2893: ep_len:500 episode reward: total was -16.710000. running mean: -6.966542\n",
      "ep 2893: ep_len:402 episode reward: total was -7.250000. running mean: -6.969377\n",
      "ep 2893: ep_len:56 episode reward: total was 1.560000. running mean: -6.884083\n",
      "ep 2893: ep_len:3 episode reward: total was 0.000000. running mean: -6.815242\n",
      "ep 2893: ep_len:555 episode reward: total was 3.090000. running mean: -6.716190\n",
      "ep 2893: ep_len:590 episode reward: total was -19.510000. running mean: -6.844128\n",
      "epsilon:0.010000 episode_count: 20258. steps_count: 8955916.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2894: ep_len:640 episode reward: total was -27.290000. running mean: -7.048587\n",
      "ep 2894: ep_len:500 episode reward: total was -13.010000. running mean: -7.108201\n",
      "ep 2894: ep_len:595 episode reward: total was -10.740000. running mean: -7.144519\n",
      "ep 2894: ep_len:540 episode reward: total was 17.000000. running mean: -6.903073\n",
      "ep 2894: ep_len:3 episode reward: total was 0.000000. running mean: -6.834043\n",
      "ep 2894: ep_len:710 episode reward: total was 3.430000. running mean: -6.731402\n",
      "ep 2894: ep_len:575 episode reward: total was -8.600000. running mean: -6.750088\n",
      "epsilon:0.010000 episode_count: 20265. steps_count: 8959479.000000\n",
      "ep 2895: ep_len:259 episode reward: total was -19.890000. running mean: -6.881487\n",
      "ep 2895: ep_len:565 episode reward: total was -6.530000. running mean: -6.877973\n",
      "ep 2895: ep_len:635 episode reward: total was -11.260000. running mean: -6.921793\n",
      "ep 2895: ep_len:500 episode reward: total was 8.030000. running mean: -6.772275\n",
      "ep 2895: ep_len:108 episode reward: total was 6.040000. running mean: -6.644152\n",
      "ep 2895: ep_len:530 episode reward: total was 5.400000. running mean: -6.523711\n",
      "ep 2895: ep_len:325 episode reward: total was -11.390000. running mean: -6.572374\n",
      "epsilon:0.010000 episode_count: 20272. steps_count: 8962401.000000\n",
      "ep 2896: ep_len:525 episode reward: total was -15.930000. running mean: -6.665950\n",
      "ep 2896: ep_len:505 episode reward: total was -17.930000. running mean: -6.778590\n",
      "ep 2896: ep_len:535 episode reward: total was -5.710000. running mean: -6.767904\n",
      "ep 2896: ep_len:525 episode reward: total was 12.470000. running mean: -6.575525\n",
      "ep 2896: ep_len:125 episode reward: total was 4.060000. running mean: -6.469170\n",
      "ep 2896: ep_len:540 episode reward: total was -17.100000. running mean: -6.575478\n",
      "ep 2896: ep_len:505 episode reward: total was -12.090000. running mean: -6.630624\n",
      "epsilon:0.010000 episode_count: 20279. steps_count: 8965661.000000\n",
      "ep 2897: ep_len:575 episode reward: total was -21.280000. running mean: -6.777117\n",
      "ep 2897: ep_len:560 episode reward: total was 15.620000. running mean: -6.553146\n",
      "ep 2897: ep_len:402 episode reward: total was -8.290000. running mean: -6.570515\n",
      "ep 2897: ep_len:56 episode reward: total was 2.570000. running mean: -6.479110\n",
      "ep 2897: ep_len:74 episode reward: total was -11.980000. running mean: -6.534118\n",
      "ep 2897: ep_len:575 episode reward: total was -18.560000. running mean: -6.654377\n",
      "ep 2897: ep_len:315 episode reward: total was 3.280000. running mean: -6.555034\n",
      "epsilon:0.010000 episode_count: 20286. steps_count: 8968218.000000\n",
      "ep 2898: ep_len:229 episode reward: total was 5.130000. running mean: -6.438183\n",
      "ep 2898: ep_len:500 episode reward: total was 0.060000. running mean: -6.373201\n",
      "ep 2898: ep_len:610 episode reward: total was -7.090000. running mean: -6.380369\n",
      "ep 2898: ep_len:500 episode reward: total was 2.870000. running mean: -6.287866\n",
      "ep 2898: ep_len:113 episode reward: total was 7.550000. running mean: -6.149487\n",
      "ep 2898: ep_len:500 episode reward: total was -4.470000. running mean: -6.132692\n",
      "ep 2898: ep_len:525 episode reward: total was -6.340000. running mean: -6.134765\n",
      "epsilon:0.010000 episode_count: 20293. steps_count: 8971195.000000\n",
      "ep 2899: ep_len:545 episode reward: total was 4.050000. running mean: -6.032918\n",
      "ep 2899: ep_len:505 episode reward: total was 4.250000. running mean: -5.930088\n",
      "ep 2899: ep_len:356 episode reward: total was 11.200000. running mean: -5.758787\n",
      "ep 2899: ep_len:605 episode reward: total was 19.500000. running mean: -5.506200\n",
      "ep 2899: ep_len:3 episode reward: total was 0.000000. running mean: -5.451138\n",
      "ep 2899: ep_len:500 episode reward: total was -5.390000. running mean: -5.450526\n",
      "ep 2899: ep_len:515 episode reward: total was -3.280000. running mean: -5.428821\n",
      "epsilon:0.010000 episode_count: 20300. steps_count: 8974224.000000\n",
      "ep 2900: ep_len:525 episode reward: total was -0.400000. running mean: -5.378533\n",
      "ep 2900: ep_len:585 episode reward: total was -2.100000. running mean: -5.345747\n",
      "ep 2900: ep_len:465 episode reward: total was 6.260000. running mean: -5.229690\n",
      "ep 2900: ep_len:505 episode reward: total was 5.360000. running mean: -5.123793\n",
      "ep 2900: ep_len:3 episode reward: total was 0.000000. running mean: -5.072555\n",
      "ep 2900: ep_len:510 episode reward: total was 5.600000. running mean: -4.965830\n",
      "ep 2900: ep_len:555 episode reward: total was -4.550000. running mean: -4.961671\n",
      "epsilon:0.010000 episode_count: 20307. steps_count: 8977372.000000\n",
      "ep 2901: ep_len:575 episode reward: total was 3.430000. running mean: -4.877755\n",
      "ep 2901: ep_len:525 episode reward: total was 20.380000. running mean: -4.625177\n",
      "ep 2901: ep_len:565 episode reward: total was -5.750000. running mean: -4.636425\n",
      "ep 2901: ep_len:540 episode reward: total was 10.440000. running mean: -4.485661\n",
      "ep 2901: ep_len:3 episode reward: total was 0.000000. running mean: -4.440804\n",
      "ep 2901: ep_len:175 episode reward: total was 4.620000. running mean: -4.350196\n",
      "ep 2901: ep_len:294 episode reward: total was -11.380000. running mean: -4.420494\n",
      "epsilon:0.010000 episode_count: 20314. steps_count: 8980049.000000\n",
      "ep 2902: ep_len:645 episode reward: total was -0.350000. running mean: -4.379789\n",
      "ep 2902: ep_len:605 episode reward: total was -22.950000. running mean: -4.565492\n",
      "ep 2902: ep_len:590 episode reward: total was -13.530000. running mean: -4.655137\n",
      "ep 2902: ep_len:505 episode reward: total was 10.440000. running mean: -4.504185\n",
      "ep 2902: ep_len:104 episode reward: total was 5.540000. running mean: -4.403743\n",
      "ep 2902: ep_len:600 episode reward: total was 8.980000. running mean: -4.269906\n",
      "ep 2902: ep_len:600 episode reward: total was -6.530000. running mean: -4.292507\n",
      "epsilon:0.010000 episode_count: 20321. steps_count: 8983698.000000\n",
      "ep 2903: ep_len:237 episode reward: total was 4.630000. running mean: -4.203282\n",
      "ep 2903: ep_len:565 episode reward: total was 7.560000. running mean: -4.085649\n",
      "ep 2903: ep_len:635 episode reward: total was -7.940000. running mean: -4.124193\n",
      "ep 2903: ep_len:570 episode reward: total was 8.480000. running mean: -3.998151\n",
      "ep 2903: ep_len:3 episode reward: total was 0.000000. running mean: -3.958169\n",
      "ep 2903: ep_len:500 episode reward: total was 7.420000. running mean: -3.844387\n",
      "ep 2903: ep_len:500 episode reward: total was -16.660000. running mean: -3.972544\n",
      "epsilon:0.010000 episode_count: 20328. steps_count: 8986708.000000\n",
      "ep 2904: ep_len:570 episode reward: total was 3.410000. running mean: -3.898718\n",
      "ep 2904: ep_len:570 episode reward: total was -4.410000. running mean: -3.903831\n",
      "ep 2904: ep_len:555 episode reward: total was -10.410000. running mean: -3.968893\n",
      "ep 2904: ep_len:420 episode reward: total was 5.410000. running mean: -3.875104\n",
      "ep 2904: ep_len:3 episode reward: total was 0.000000. running mean: -3.836353\n",
      "ep 2904: ep_len:500 episode reward: total was 1.880000. running mean: -3.779189\n",
      "ep 2904: ep_len:560 episode reward: total was -1.070000. running mean: -3.752097\n",
      "epsilon:0.010000 episode_count: 20335. steps_count: 8989886.000000\n",
      "ep 2905: ep_len:229 episode reward: total was 5.620000. running mean: -3.658376\n",
      "ep 2905: ep_len:570 episode reward: total was -7.980000. running mean: -3.701593\n",
      "ep 2905: ep_len:500 episode reward: total was 3.380000. running mean: -3.630777\n",
      "ep 2905: ep_len:500 episode reward: total was -5.530000. running mean: -3.649769\n",
      "ep 2905: ep_len:88 episode reward: total was 5.050000. running mean: -3.562771\n",
      "ep 2905: ep_len:525 episode reward: total was -6.780000. running mean: -3.594943\n",
      "ep 2905: ep_len:595 episode reward: total was 0.750000. running mean: -3.551494\n",
      "epsilon:0.010000 episode_count: 20342. steps_count: 8992893.000000\n",
      "ep 2906: ep_len:515 episode reward: total was -34.950000. running mean: -3.865479\n",
      "ep 2906: ep_len:615 episode reward: total was 17.940000. running mean: -3.647424\n",
      "ep 2906: ep_len:655 episode reward: total was -11.740000. running mean: -3.728350\n",
      "ep 2906: ep_len:132 episode reward: total was 6.100000. running mean: -3.630067\n",
      "ep 2906: ep_len:3 episode reward: total was 0.000000. running mean: -3.593766\n",
      "ep 2906: ep_len:234 episode reward: total was 5.140000. running mean: -3.506428\n",
      "ep 2906: ep_len:540 episode reward: total was -37.010000. running mean: -3.841464\n",
      "epsilon:0.010000 episode_count: 20349. steps_count: 8995587.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2907: ep_len:500 episode reward: total was 6.410000. running mean: -3.738949\n",
      "ep 2907: ep_len:560 episode reward: total was -37.000000. running mean: -4.071560\n",
      "ep 2907: ep_len:655 episode reward: total was -15.780000. running mean: -4.188644\n",
      "ep 2907: ep_len:505 episode reward: total was -1.070000. running mean: -4.157458\n",
      "ep 2907: ep_len:3 episode reward: total was 0.000000. running mean: -4.115883\n",
      "ep 2907: ep_len:675 episode reward: total was -16.690000. running mean: -4.241624\n",
      "ep 2907: ep_len:520 episode reward: total was -31.730000. running mean: -4.516508\n",
      "epsilon:0.010000 episode_count: 20356. steps_count: 8999005.000000\n",
      "ep 2908: ep_len:515 episode reward: total was -9.910000. running mean: -4.570443\n",
      "ep 2908: ep_len:500 episode reward: total was 0.120000. running mean: -4.523539\n",
      "ep 2908: ep_len:406 episode reward: total was -3.270000. running mean: -4.511003\n",
      "ep 2908: ep_len:555 episode reward: total was -7.070000. running mean: -4.536593\n",
      "ep 2908: ep_len:3 episode reward: total was 0.000000. running mean: -4.491227\n",
      "ep 2908: ep_len:500 episode reward: total was -9.300000. running mean: -4.539315\n",
      "ep 2908: ep_len:525 episode reward: total was -1.570000. running mean: -4.509622\n",
      "epsilon:0.010000 episode_count: 20363. steps_count: 9002009.000000\n",
      "ep 2909: ep_len:560 episode reward: total was -2.410000. running mean: -4.488626\n",
      "ep 2909: ep_len:615 episode reward: total was 7.560000. running mean: -4.368139\n",
      "ep 2909: ep_len:555 episode reward: total was -53.010000. running mean: -4.854558\n",
      "ep 2909: ep_len:169 episode reward: total was 5.620000. running mean: -4.749812\n",
      "ep 2909: ep_len:3 episode reward: total was 0.000000. running mean: -4.702314\n",
      "ep 2909: ep_len:500 episode reward: total was -6.620000. running mean: -4.721491\n",
      "ep 2909: ep_len:211 episode reward: total was -2.340000. running mean: -4.697676\n",
      "epsilon:0.010000 episode_count: 20370. steps_count: 9004622.000000\n",
      "ep 2910: ep_len:550 episode reward: total was -20.260000. running mean: -4.853299\n",
      "ep 2910: ep_len:600 episode reward: total was -42.680000. running mean: -5.231566\n",
      "ep 2910: ep_len:500 episode reward: total was -3.770000. running mean: -5.216951\n",
      "ep 2910: ep_len:170 episode reward: total was -0.890000. running mean: -5.173681\n",
      "ep 2910: ep_len:3 episode reward: total was 0.000000. running mean: -5.121944\n",
      "ep 2910: ep_len:252 episode reward: total was 8.640000. running mean: -4.984325\n",
      "ep 2910: ep_len:500 episode reward: total was -17.260000. running mean: -5.107082\n",
      "epsilon:0.010000 episode_count: 20377. steps_count: 9007197.000000\n",
      "ep 2911: ep_len:625 episode reward: total was -18.800000. running mean: -5.244011\n",
      "ep 2911: ep_len:565 episode reward: total was -3.650000. running mean: -5.228071\n",
      "ep 2911: ep_len:79 episode reward: total was 0.040000. running mean: -5.175390\n",
      "ep 2911: ep_len:590 episode reward: total was -1.930000. running mean: -5.142936\n",
      "ep 2911: ep_len:86 episode reward: total was 6.030000. running mean: -5.031207\n",
      "ep 2911: ep_len:500 episode reward: total was 2.720000. running mean: -4.953695\n",
      "ep 2911: ep_len:580 episode reward: total was -4.550000. running mean: -4.949658\n",
      "epsilon:0.010000 episode_count: 20384. steps_count: 9010222.000000\n",
      "ep 2912: ep_len:545 episode reward: total was -2.120000. running mean: -4.921361\n",
      "ep 2912: ep_len:595 episode reward: total was 0.170000. running mean: -4.870448\n",
      "ep 2912: ep_len:555 episode reward: total was 4.620000. running mean: -4.775543\n",
      "ep 2912: ep_len:500 episode reward: total was -5.010000. running mean: -4.777888\n",
      "ep 2912: ep_len:3 episode reward: total was 0.000000. running mean: -4.730109\n",
      "ep 2912: ep_len:168 episode reward: total was 5.120000. running mean: -4.631608\n",
      "ep 2912: ep_len:580 episode reward: total was -34.460000. running mean: -4.929892\n",
      "epsilon:0.010000 episode_count: 20391. steps_count: 9013168.000000\n",
      "ep 2913: ep_len:670 episode reward: total was -3.830000. running mean: -4.918893\n",
      "ep 2913: ep_len:535 episode reward: total was -15.980000. running mean: -5.029504\n",
      "ep 2913: ep_len:575 episode reward: total was 3.660000. running mean: -4.942609\n",
      "ep 2913: ep_len:620 episode reward: total was 10.590000. running mean: -4.787283\n",
      "ep 2913: ep_len:3 episode reward: total was 0.000000. running mean: -4.739410\n",
      "ep 2913: ep_len:256 episode reward: total was 8.650000. running mean: -4.605516\n",
      "ep 2913: ep_len:590 episode reward: total was -18.010000. running mean: -4.739561\n",
      "epsilon:0.010000 episode_count: 20398. steps_count: 9016417.000000\n",
      "ep 2914: ep_len:215 episode reward: total was 0.580000. running mean: -4.686365\n",
      "ep 2914: ep_len:500 episode reward: total was -10.830000. running mean: -4.747801\n",
      "ep 2914: ep_len:79 episode reward: total was 0.040000. running mean: -4.699923\n",
      "ep 2914: ep_len:500 episode reward: total was -5.570000. running mean: -4.708624\n",
      "ep 2914: ep_len:3 episode reward: total was 0.000000. running mean: -4.661538\n",
      "ep 2914: ep_len:680 episode reward: total was 2.280000. running mean: -4.592123\n",
      "ep 2914: ep_len:535 episode reward: total was -0.490000. running mean: -4.551101\n",
      "epsilon:0.010000 episode_count: 20405. steps_count: 9018929.000000\n",
      "ep 2915: ep_len:610 episode reward: total was -17.770000. running mean: -4.683290\n",
      "ep 2915: ep_len:570 episode reward: total was 19.410000. running mean: -4.442357\n",
      "ep 2915: ep_len:409 episode reward: total was 5.760000. running mean: -4.340334\n",
      "ep 2915: ep_len:600 episode reward: total was 8.350000. running mean: -4.213430\n",
      "ep 2915: ep_len:84 episode reward: total was 0.540000. running mean: -4.165896\n",
      "ep 2915: ep_len:505 episode reward: total was -6.650000. running mean: -4.190737\n",
      "ep 2915: ep_len:570 episode reward: total was -30.440000. running mean: -4.453230\n",
      "epsilon:0.010000 episode_count: 20412. steps_count: 9022277.000000\n",
      "ep 2916: ep_len:545 episode reward: total was 7.600000. running mean: -4.332698\n",
      "ep 2916: ep_len:520 episode reward: total was 19.910000. running mean: -4.090271\n",
      "ep 2916: ep_len:500 episode reward: total was -22.700000. running mean: -4.276368\n",
      "ep 2916: ep_len:520 episode reward: total was 7.930000. running mean: -4.154304\n",
      "ep 2916: ep_len:3 episode reward: total was 0.000000. running mean: -4.112761\n",
      "ep 2916: ep_len:535 episode reward: total was 11.190000. running mean: -3.959734\n",
      "ep 2916: ep_len:625 episode reward: total was 1.710000. running mean: -3.903036\n",
      "epsilon:0.010000 episode_count: 20419. steps_count: 9025525.000000\n",
      "ep 2917: ep_len:670 episode reward: total was -17.680000. running mean: -4.040806\n",
      "ep 2917: ep_len:515 episode reward: total was 14.930000. running mean: -3.851098\n",
      "ep 2917: ep_len:382 episode reward: total was 8.190000. running mean: -3.730687\n",
      "ep 2917: ep_len:515 episode reward: total was 1.860000. running mean: -3.674780\n",
      "ep 2917: ep_len:3 episode reward: total was 0.000000. running mean: -3.638032\n",
      "ep 2917: ep_len:600 episode reward: total was -21.670000. running mean: -3.818352\n",
      "ep 2917: ep_len:319 episode reward: total was -12.880000. running mean: -3.908968\n",
      "epsilon:0.010000 episode_count: 20426. steps_count: 9028529.000000\n",
      "ep 2918: ep_len:745 episode reward: total was -58.310000. running mean: -4.452979\n",
      "ep 2918: ep_len:565 episode reward: total was -18.930000. running mean: -4.597749\n",
      "ep 2918: ep_len:640 episode reward: total was -33.520000. running mean: -4.886971\n",
      "ep 2918: ep_len:515 episode reward: total was -7.980000. running mean: -4.917902\n",
      "ep 2918: ep_len:3 episode reward: total was 0.000000. running mean: -4.868723\n",
      "ep 2918: ep_len:695 episode reward: total was -1.560000. running mean: -4.835635\n",
      "ep 2918: ep_len:500 episode reward: total was 3.830000. running mean: -4.748979\n",
      "epsilon:0.010000 episode_count: 20433. steps_count: 9032192.000000\n",
      "ep 2919: ep_len:500 episode reward: total was -49.790000. running mean: -5.199389\n",
      "ep 2919: ep_len:555 episode reward: total was 8.850000. running mean: -5.058895\n",
      "ep 2919: ep_len:79 episode reward: total was 1.050000. running mean: -4.997806\n",
      "ep 2919: ep_len:500 episode reward: total was -0.870000. running mean: -4.956528\n",
      "ep 2919: ep_len:3 episode reward: total was 0.000000. running mean: -4.906963\n",
      "ep 2919: ep_len:550 episode reward: total was -4.210000. running mean: -4.899993\n",
      "ep 2919: ep_len:287 episode reward: total was -3.770000. running mean: -4.888693\n",
      "epsilon:0.010000 episode_count: 20440. steps_count: 9034666.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2920: ep_len:585 episode reward: total was 7.680000. running mean: -4.763007\n",
      "ep 2920: ep_len:595 episode reward: total was -15.900000. running mean: -4.874376\n",
      "ep 2920: ep_len:500 episode reward: total was -29.530000. running mean: -5.120933\n",
      "ep 2920: ep_len:389 episode reward: total was -2.720000. running mean: -5.096923\n",
      "ep 2920: ep_len:87 episode reward: total was -10.940000. running mean: -5.155354\n",
      "ep 2920: ep_len:540 episode reward: total was -1.470000. running mean: -5.118501\n",
      "ep 2920: ep_len:585 episode reward: total was -5.980000. running mean: -5.127116\n",
      "epsilon:0.010000 episode_count: 20447. steps_count: 9037947.000000\n",
      "ep 2921: ep_len:590 episode reward: total was -1.470000. running mean: -5.090544\n",
      "ep 2921: ep_len:545 episode reward: total was 2.420000. running mean: -5.015439\n",
      "ep 2921: ep_len:650 episode reward: total was -48.340000. running mean: -5.448685\n",
      "ep 2921: ep_len:102 episode reward: total was 5.060000. running mean: -5.343598\n",
      "ep 2921: ep_len:3 episode reward: total was 0.000000. running mean: -5.290162\n",
      "ep 2921: ep_len:550 episode reward: total was 2.270000. running mean: -5.214560\n",
      "ep 2921: ep_len:595 episode reward: total was 2.660000. running mean: -5.135815\n",
      "epsilon:0.010000 episode_count: 20454. steps_count: 9040982.000000\n",
      "ep 2922: ep_len:715 episode reward: total was -34.270000. running mean: -5.427156\n",
      "ep 2922: ep_len:255 episode reward: total was -12.860000. running mean: -5.501485\n",
      "ep 2922: ep_len:456 episode reward: total was 7.740000. running mean: -5.369070\n",
      "ep 2922: ep_len:505 episode reward: total was -4.540000. running mean: -5.360779\n",
      "ep 2922: ep_len:54 episode reward: total was 5.000000. running mean: -5.257172\n",
      "ep 2922: ep_len:530 episode reward: total was -0.570000. running mean: -5.210300\n",
      "ep 2922: ep_len:575 episode reward: total was -0.910000. running mean: -5.167297\n",
      "epsilon:0.010000 episode_count: 20461. steps_count: 9044072.000000\n",
      "ep 2923: ep_len:575 episode reward: total was 5.470000. running mean: -5.060924\n",
      "ep 2923: ep_len:500 episode reward: total was -9.370000. running mean: -5.104015\n",
      "ep 2923: ep_len:500 episode reward: total was 2.950000. running mean: -5.023474\n",
      "ep 2923: ep_len:500 episode reward: total was -8.970000. running mean: -5.062940\n",
      "ep 2923: ep_len:124 episode reward: total was 7.050000. running mean: -4.941810\n",
      "ep 2923: ep_len:555 episode reward: total was -7.040000. running mean: -4.962792\n",
      "ep 2923: ep_len:580 episode reward: total was -12.860000. running mean: -5.041764\n",
      "epsilon:0.010000 episode_count: 20468. steps_count: 9047406.000000\n",
      "ep 2924: ep_len:600 episode reward: total was 17.550000. running mean: -4.815847\n",
      "ep 2924: ep_len:510 episode reward: total was 14.910000. running mean: -4.618588\n",
      "ep 2924: ep_len:620 episode reward: total was -2.660000. running mean: -4.599002\n",
      "ep 2924: ep_len:500 episode reward: total was -17.510000. running mean: -4.728112\n",
      "ep 2924: ep_len:3 episode reward: total was 0.000000. running mean: -4.680831\n",
      "ep 2924: ep_len:500 episode reward: total was 1.730000. running mean: -4.616723\n",
      "ep 2924: ep_len:500 episode reward: total was -7.410000. running mean: -4.644656\n",
      "epsilon:0.010000 episode_count: 20475. steps_count: 9050639.000000\n",
      "ep 2925: ep_len:264 episode reward: total was 5.140000. running mean: -4.546809\n",
      "ep 2925: ep_len:600 episode reward: total was 0.460000. running mean: -4.496741\n",
      "ep 2925: ep_len:615 episode reward: total was -12.720000. running mean: -4.578974\n",
      "ep 2925: ep_len:500 episode reward: total was -2.940000. running mean: -4.562584\n",
      "ep 2925: ep_len:3 episode reward: total was 0.000000. running mean: -4.516958\n",
      "ep 2925: ep_len:1525 episode reward: total was -94.280000. running mean: -5.414588\n",
      "ep 2925: ep_len:560 episode reward: total was 3.030000. running mean: -5.330143\n",
      "epsilon:0.010000 episode_count: 20482. steps_count: 9054706.000000\n",
      "ep 2926: ep_len:535 episode reward: total was -11.290000. running mean: -5.389741\n",
      "ep 2926: ep_len:500 episode reward: total was -39.330000. running mean: -5.729144\n",
      "ep 2926: ep_len:700 episode reward: total was -8.560000. running mean: -5.757452\n",
      "ep 2926: ep_len:590 episode reward: total was 1.600000. running mean: -5.683878\n",
      "ep 2926: ep_len:103 episode reward: total was 5.540000. running mean: -5.571639\n",
      "ep 2926: ep_len:2326 episode reward: total was -193.760000. running mean: -7.453523\n",
      "ep 2926: ep_len:520 episode reward: total was -1.530000. running mean: -7.394287\n",
      "epsilon:0.010000 episode_count: 20489. steps_count: 9059980.000000\n",
      "ep 2927: ep_len:197 episode reward: total was -9.840000. running mean: -7.418744\n",
      "ep 2927: ep_len:635 episode reward: total was -26.520000. running mean: -7.609757\n",
      "ep 2927: ep_len:530 episode reward: total was -3.360000. running mean: -7.567259\n",
      "ep 2927: ep_len:500 episode reward: total was 11.930000. running mean: -7.372287\n",
      "ep 2927: ep_len:3 episode reward: total was 0.000000. running mean: -7.298564\n",
      "ep 2927: ep_len:500 episode reward: total was -4.750000. running mean: -7.273078\n",
      "ep 2927: ep_len:525 episode reward: total was -2.990000. running mean: -7.230248\n",
      "epsilon:0.010000 episode_count: 20496. steps_count: 9062870.000000\n",
      "ep 2928: ep_len:520 episode reward: total was -17.860000. running mean: -7.336545\n",
      "ep 2928: ep_len:640 episode reward: total was 22.150000. running mean: -7.041680\n",
      "ep 2928: ep_len:1245 episode reward: total was -116.820000. running mean: -8.139463\n",
      "ep 2928: ep_len:500 episode reward: total was 5.880000. running mean: -7.999268\n",
      "ep 2928: ep_len:3 episode reward: total was 0.000000. running mean: -7.919276\n",
      "ep 2928: ep_len:520 episode reward: total was -32.970000. running mean: -8.169783\n",
      "ep 2928: ep_len:555 episode reward: total was 7.520000. running mean: -8.012885\n",
      "epsilon:0.010000 episode_count: 20503. steps_count: 9066853.000000\n",
      "ep 2929: ep_len:185 episode reward: total was -28.920000. running mean: -8.221956\n",
      "ep 2929: ep_len:620 episode reward: total was -7.130000. running mean: -8.211037\n",
      "ep 2929: ep_len:580 episode reward: total was -13.220000. running mean: -8.261126\n",
      "ep 2929: ep_len:500 episode reward: total was 0.030000. running mean: -8.178215\n",
      "ep 2929: ep_len:3 episode reward: total was 0.000000. running mean: -8.096433\n",
      "ep 2929: ep_len:530 episode reward: total was -44.690000. running mean: -8.462368\n",
      "ep 2929: ep_len:500 episode reward: total was -1.130000. running mean: -8.389045\n",
      "epsilon:0.010000 episode_count: 20510. steps_count: 9069771.000000\n",
      "ep 2930: ep_len:121 episode reward: total was 5.090000. running mean: -8.254254\n",
      "ep 2930: ep_len:500 episode reward: total was -21.330000. running mean: -8.385012\n",
      "ep 2930: ep_len:725 episode reward: total was -18.090000. running mean: -8.482062\n",
      "ep 2930: ep_len:580 episode reward: total was 10.450000. running mean: -8.292741\n",
      "ep 2930: ep_len:3 episode reward: total was 0.000000. running mean: -8.209814\n",
      "ep 2930: ep_len:500 episode reward: total was -2.760000. running mean: -8.155315\n",
      "ep 2930: ep_len:600 episode reward: total was -43.600000. running mean: -8.509762\n",
      "epsilon:0.010000 episode_count: 20517. steps_count: 9072800.000000\n",
      "ep 2931: ep_len:625 episode reward: total was -1.850000. running mean: -8.443165\n",
      "ep 2931: ep_len:545 episode reward: total was -81.280000. running mean: -9.171533\n",
      "ep 2931: ep_len:660 episode reward: total was -10.550000. running mean: -9.185318\n",
      "ep 2931: ep_len:500 episode reward: total was 8.580000. running mean: -9.007665\n",
      "ep 2931: ep_len:48 episode reward: total was 4.500000. running mean: -8.872588\n",
      "ep 2931: ep_len:500 episode reward: total was -37.800000. running mean: -9.161862\n",
      "ep 2931: ep_len:530 episode reward: total was -16.410000. running mean: -9.234343\n",
      "epsilon:0.010000 episode_count: 20524. steps_count: 9076208.000000\n",
      "ep 2932: ep_len:560 episode reward: total was -27.510000. running mean: -9.417100\n",
      "ep 2932: ep_len:520 episode reward: total was -5.870000. running mean: -9.381629\n",
      "ep 2932: ep_len:565 episode reward: total was -5.850000. running mean: -9.346313\n",
      "ep 2932: ep_len:520 episode reward: total was 4.380000. running mean: -9.209050\n",
      "ep 2932: ep_len:86 episode reward: total was 3.550000. running mean: -9.081459\n",
      "ep 2932: ep_len:500 episode reward: total was -13.240000. running mean: -9.123044\n",
      "ep 2932: ep_len:570 episode reward: total was -7.440000. running mean: -9.106214\n",
      "epsilon:0.010000 episode_count: 20531. steps_count: 9079529.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2933: ep_len:885 episode reward: total was -33.190000. running mean: -9.347052\n",
      "ep 2933: ep_len:725 episode reward: total was -27.700000. running mean: -9.530581\n",
      "ep 2933: ep_len:75 episode reward: total was -1.460000. running mean: -9.449876\n",
      "ep 2933: ep_len:525 episode reward: total was -37.250000. running mean: -9.727877\n",
      "ep 2933: ep_len:3 episode reward: total was 0.000000. running mean: -9.630598\n",
      "ep 2933: ep_len:500 episode reward: total was -0.080000. running mean: -9.535092\n",
      "ep 2933: ep_len:520 episode reward: total was -11.950000. running mean: -9.559241\n",
      "epsilon:0.010000 episode_count: 20538. steps_count: 9082762.000000\n",
      "ep 2934: ep_len:106 episode reward: total was 4.580000. running mean: -9.417849\n",
      "ep 2934: ep_len:500 episode reward: total was 7.190000. running mean: -9.251770\n",
      "ep 2934: ep_len:565 episode reward: total was -2.430000. running mean: -9.183553\n",
      "ep 2934: ep_len:510 episode reward: total was -12.000000. running mean: -9.211717\n",
      "ep 2934: ep_len:76 episode reward: total was 4.510000. running mean: -9.074500\n",
      "ep 2934: ep_len:580 episode reward: total was 14.100000. running mean: -8.842755\n",
      "ep 2934: ep_len:505 episode reward: total was -32.220000. running mean: -9.076527\n",
      "epsilon:0.010000 episode_count: 20545. steps_count: 9085604.000000\n",
      "ep 2935: ep_len:240 episode reward: total was -17.810000. running mean: -9.163862\n",
      "ep 2935: ep_len:620 episode reward: total was -14.620000. running mean: -9.218423\n",
      "ep 2935: ep_len:740 episode reward: total was -21.120000. running mean: -9.337439\n",
      "ep 2935: ep_len:411 episode reward: total was -11.310000. running mean: -9.357165\n",
      "ep 2935: ep_len:114 episode reward: total was 5.040000. running mean: -9.213193\n",
      "ep 2935: ep_len:535 episode reward: total was -17.090000. running mean: -9.291961\n",
      "ep 2935: ep_len:510 episode reward: total was -20.550000. running mean: -9.404542\n",
      "epsilon:0.010000 episode_count: 20552. steps_count: 9088774.000000\n",
      "ep 2936: ep_len:540 episode reward: total was -46.540000. running mean: -9.775896\n",
      "ep 2936: ep_len:500 episode reward: total was -13.200000. running mean: -9.810137\n",
      "ep 2936: ep_len:555 episode reward: total was -3.560000. running mean: -9.747636\n",
      "ep 2936: ep_len:500 episode reward: total was -13.030000. running mean: -9.780459\n",
      "ep 2936: ep_len:3 episode reward: total was 0.000000. running mean: -9.682655\n",
      "ep 2936: ep_len:540 episode reward: total was 14.630000. running mean: -9.439528\n",
      "ep 2936: ep_len:500 episode reward: total was -13.960000. running mean: -9.484733\n",
      "epsilon:0.010000 episode_count: 20559. steps_count: 9091912.000000\n",
      "ep 2937: ep_len:515 episode reward: total was 5.920000. running mean: -9.330686\n",
      "ep 2937: ep_len:525 episode reward: total was -39.650000. running mean: -9.633879\n",
      "ep 2937: ep_len:500 episode reward: total was -22.620000. running mean: -9.763740\n",
      "ep 2937: ep_len:555 episode reward: total was 2.900000. running mean: -9.637103\n",
      "ep 2937: ep_len:3 episode reward: total was 0.000000. running mean: -9.540732\n",
      "ep 2937: ep_len:510 episode reward: total was -10.450000. running mean: -9.549824\n",
      "ep 2937: ep_len:575 episode reward: total was -7.910000. running mean: -9.533426\n",
      "epsilon:0.010000 episode_count: 20566. steps_count: 9095095.000000\n",
      "ep 2938: ep_len:570 episode reward: total was 6.450000. running mean: -9.373592\n",
      "ep 2938: ep_len:510 episode reward: total was -11.020000. running mean: -9.390056\n",
      "ep 2938: ep_len:610 episode reward: total was 6.470000. running mean: -9.231455\n",
      "ep 2938: ep_len:605 episode reward: total was 14.480000. running mean: -8.994341\n",
      "ep 2938: ep_len:89 episode reward: total was 4.040000. running mean: -8.863997\n",
      "ep 2938: ep_len:515 episode reward: total was 0.330000. running mean: -8.772057\n",
      "ep 2938: ep_len:331 episode reward: total was 1.720000. running mean: -8.667137\n",
      "epsilon:0.010000 episode_count: 20573. steps_count: 9098325.000000\n",
      "ep 2939: ep_len:228 episode reward: total was 4.120000. running mean: -8.539265\n",
      "ep 2939: ep_len:525 episode reward: total was -6.440000. running mean: -8.518273\n",
      "ep 2939: ep_len:670 episode reward: total was -11.110000. running mean: -8.544190\n",
      "ep 2939: ep_len:500 episode reward: total was 2.490000. running mean: -8.433848\n",
      "ep 2939: ep_len:87 episode reward: total was 5.510000. running mean: -8.294410\n",
      "ep 2939: ep_len:715 episode reward: total was -68.930000. running mean: -8.900766\n",
      "ep 2939: ep_len:293 episode reward: total was 2.250000. running mean: -8.789258\n",
      "epsilon:0.010000 episode_count: 20580. steps_count: 9101343.000000\n",
      "ep 2940: ep_len:625 episode reward: total was 1.160000. running mean: -8.689765\n",
      "ep 2940: ep_len:525 episode reward: total was -13.940000. running mean: -8.742268\n",
      "ep 2940: ep_len:735 episode reward: total was -19.110000. running mean: -8.845945\n",
      "ep 2940: ep_len:560 episode reward: total was 10.460000. running mean: -8.652886\n",
      "ep 2940: ep_len:3 episode reward: total was 0.000000. running mean: -8.566357\n",
      "ep 2940: ep_len:500 episode reward: total was 6.660000. running mean: -8.414093\n",
      "ep 2940: ep_len:725 episode reward: total was -34.860000. running mean: -8.678552\n",
      "epsilon:0.010000 episode_count: 20587. steps_count: 9105016.000000\n",
      "ep 2941: ep_len:530 episode reward: total was 7.480000. running mean: -8.516967\n",
      "ep 2941: ep_len:650 episode reward: total was -17.810000. running mean: -8.609897\n",
      "ep 2941: ep_len:500 episode reward: total was -4.630000. running mean: -8.570098\n",
      "ep 2941: ep_len:570 episode reward: total was 2.060000. running mean: -8.463797\n",
      "ep 2941: ep_len:3 episode reward: total was 0.000000. running mean: -8.379159\n",
      "ep 2941: ep_len:198 episode reward: total was -23.930000. running mean: -8.534668\n",
      "ep 2941: ep_len:635 episode reward: total was -39.370000. running mean: -8.843021\n",
      "epsilon:0.010000 episode_count: 20594. steps_count: 9108102.000000\n",
      "ep 2942: ep_len:219 episode reward: total was -22.850000. running mean: -8.983091\n",
      "ep 2942: ep_len:292 episode reward: total was -8.350000. running mean: -8.976760\n",
      "ep 2942: ep_len:560 episode reward: total was -6.530000. running mean: -8.952292\n",
      "ep 2942: ep_len:382 episode reward: total was 0.770000. running mean: -8.855069\n",
      "ep 2942: ep_len:3 episode reward: total was 0.000000. running mean: -8.766519\n",
      "ep 2942: ep_len:545 episode reward: total was -0.570000. running mean: -8.684553\n",
      "ep 2942: ep_len:292 episode reward: total was -5.280000. running mean: -8.650508\n",
      "epsilon:0.010000 episode_count: 20601. steps_count: 9110395.000000\n",
      "ep 2943: ep_len:565 episode reward: total was -6.440000. running mean: -8.628403\n",
      "ep 2943: ep_len:540 episode reward: total was -20.020000. running mean: -8.742319\n",
      "ep 2943: ep_len:605 episode reward: total was -14.730000. running mean: -8.802196\n",
      "ep 2943: ep_len:500 episode reward: total was -23.810000. running mean: -8.952274\n",
      "ep 2943: ep_len:95 episode reward: total was -8.960000. running mean: -8.952351\n",
      "ep 2943: ep_len:520 episode reward: total was -1.080000. running mean: -8.873627\n",
      "ep 2943: ep_len:525 episode reward: total was 4.020000. running mean: -8.744691\n",
      "epsilon:0.010000 episode_count: 20608. steps_count: 9113745.000000\n",
      "ep 2944: ep_len:123 episode reward: total was 6.590000. running mean: -8.591344\n",
      "ep 2944: ep_len:540 episode reward: total was -12.750000. running mean: -8.632931\n",
      "ep 2944: ep_len:560 episode reward: total was -6.450000. running mean: -8.611101\n",
      "ep 2944: ep_len:560 episode reward: total was -6.560000. running mean: -8.590590\n",
      "ep 2944: ep_len:3 episode reward: total was 0.000000. running mean: -8.504684\n",
      "ep 2944: ep_len:660 episode reward: total was -10.580000. running mean: -8.525438\n",
      "ep 2944: ep_len:515 episode reward: total was -15.070000. running mean: -8.590883\n",
      "epsilon:0.010000 episode_count: 20615. steps_count: 9116706.000000\n",
      "ep 2945: ep_len:660 episode reward: total was -32.300000. running mean: -8.827974\n",
      "ep 2945: ep_len:545 episode reward: total was 3.560000. running mean: -8.704095\n",
      "ep 2945: ep_len:441 episode reward: total was -17.690000. running mean: -8.793954\n",
      "ep 2945: ep_len:510 episode reward: total was -1.670000. running mean: -8.722714\n",
      "ep 2945: ep_len:3 episode reward: total was 0.000000. running mean: -8.635487\n",
      "ep 2945: ep_len:560 episode reward: total was -8.590000. running mean: -8.635032\n",
      "ep 2945: ep_len:505 episode reward: total was -39.580000. running mean: -8.944482\n",
      "epsilon:0.010000 episode_count: 20622. steps_count: 9119930.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2946: ep_len:134 episode reward: total was 4.590000. running mean: -8.809137\n",
      "ep 2946: ep_len:670 episode reward: total was -21.290000. running mean: -8.933946\n",
      "ep 2946: ep_len:500 episode reward: total was -18.450000. running mean: -9.029106\n",
      "ep 2946: ep_len:419 episode reward: total was 1.880000. running mean: -8.920015\n",
      "ep 2946: ep_len:133 episode reward: total was 7.560000. running mean: -8.755215\n",
      "ep 2946: ep_len:520 episode reward: total was -8.120000. running mean: -8.748863\n",
      "ep 2946: ep_len:585 episode reward: total was -17.860000. running mean: -8.839974\n",
      "epsilon:0.010000 episode_count: 20629. steps_count: 9122891.000000\n",
      "ep 2947: ep_len:565 episode reward: total was -2.100000. running mean: -8.772574\n",
      "ep 2947: ep_len:535 episode reward: total was 1.920000. running mean: -8.665649\n",
      "ep 2947: ep_len:615 episode reward: total was -20.190000. running mean: -8.780892\n",
      "ep 2947: ep_len:595 episode reward: total was -10.460000. running mean: -8.797683\n",
      "ep 2947: ep_len:3 episode reward: total was 0.000000. running mean: -8.709706\n",
      "ep 2947: ep_len:520 episode reward: total was -8.720000. running mean: -8.709809\n",
      "ep 2947: ep_len:203 episode reward: total was -5.850000. running mean: -8.681211\n",
      "epsilon:0.010000 episode_count: 20636. steps_count: 9125927.000000\n",
      "ep 2948: ep_len:640 episode reward: total was 8.200000. running mean: -8.512399\n",
      "ep 2948: ep_len:565 episode reward: total was -0.820000. running mean: -8.435475\n",
      "ep 2948: ep_len:645 episode reward: total was 4.660000. running mean: -8.304520\n",
      "ep 2948: ep_len:520 episode reward: total was 4.580000. running mean: -8.175675\n",
      "ep 2948: ep_len:101 episode reward: total was 6.030000. running mean: -8.033619\n",
      "ep 2948: ep_len:580 episode reward: total was -9.730000. running mean: -8.050582\n",
      "ep 2948: ep_len:705 episode reward: total was -31.290000. running mean: -8.282976\n",
      "epsilon:0.010000 episode_count: 20643. steps_count: 9129683.000000\n",
      "ep 2949: ep_len:595 episode reward: total was -1.850000. running mean: -8.218647\n",
      "ep 2949: ep_len:595 episode reward: total was -14.050000. running mean: -8.276960\n",
      "ep 2949: ep_len:570 episode reward: total was -20.800000. running mean: -8.402191\n",
      "ep 2949: ep_len:500 episode reward: total was -4.000000. running mean: -8.358169\n",
      "ep 2949: ep_len:3 episode reward: total was 0.000000. running mean: -8.274587\n",
      "ep 2949: ep_len:535 episode reward: total was -20.810000. running mean: -8.399941\n",
      "ep 2949: ep_len:575 episode reward: total was -8.570000. running mean: -8.401642\n",
      "epsilon:0.010000 episode_count: 20650. steps_count: 9133056.000000\n",
      "ep 2950: ep_len:510 episode reward: total was 1.380000. running mean: -8.303825\n",
      "ep 2950: ep_len:605 episode reward: total was 8.070000. running mean: -8.140087\n",
      "ep 2950: ep_len:610 episode reward: total was -27.980000. running mean: -8.338486\n",
      "ep 2950: ep_len:500 episode reward: total was 6.010000. running mean: -8.195001\n",
      "ep 2950: ep_len:3 episode reward: total was 0.000000. running mean: -8.113051\n",
      "ep 2950: ep_len:500 episode reward: total was -3.290000. running mean: -8.064821\n",
      "ep 2950: ep_len:615 episode reward: total was -46.050000. running mean: -8.444673\n",
      "epsilon:0.010000 episode_count: 20657. steps_count: 9136399.000000\n",
      "ep 2951: ep_len:565 episode reward: total was -5.110000. running mean: -8.411326\n",
      "ep 2951: ep_len:545 episode reward: total was 11.400000. running mean: -8.213213\n",
      "ep 2951: ep_len:540 episode reward: total was -5.940000. running mean: -8.190481\n",
      "ep 2951: ep_len:464 episode reward: total was 0.980000. running mean: -8.098776\n",
      "ep 2951: ep_len:3 episode reward: total was 0.000000. running mean: -8.017788\n",
      "ep 2951: ep_len:580 episode reward: total was -15.610000. running mean: -8.093710\n",
      "ep 2951: ep_len:575 episode reward: total was -3.550000. running mean: -8.048273\n",
      "epsilon:0.010000 episode_count: 20664. steps_count: 9139671.000000\n",
      "ep 2952: ep_len:525 episode reward: total was -7.730000. running mean: -8.045090\n",
      "ep 2952: ep_len:550 episode reward: total was -0.080000. running mean: -7.965439\n",
      "ep 2952: ep_len:740 episode reward: total was -51.280000. running mean: -8.398585\n",
      "ep 2952: ep_len:595 episode reward: total was 11.600000. running mean: -8.198599\n",
      "ep 2952: ep_len:3 episode reward: total was 0.000000. running mean: -8.116613\n",
      "ep 2952: ep_len:565 episode reward: total was -11.120000. running mean: -8.146647\n",
      "ep 2952: ep_len:675 episode reward: total was -24.250000. running mean: -8.307681\n",
      "epsilon:0.010000 episode_count: 20671. steps_count: 9143324.000000\n",
      "ep 2953: ep_len:228 episode reward: total was 6.150000. running mean: -8.163104\n",
      "ep 2953: ep_len:500 episode reward: total was 21.690000. running mean: -7.864573\n",
      "ep 2953: ep_len:705 episode reward: total was -8.090000. running mean: -7.866827\n",
      "ep 2953: ep_len:515 episode reward: total was 4.350000. running mean: -7.744659\n",
      "ep 2953: ep_len:3 episode reward: total was 0.000000. running mean: -7.667212\n",
      "ep 2953: ep_len:540 episode reward: total was -6.750000. running mean: -7.658040\n",
      "ep 2953: ep_len:314 episode reward: total was -5.820000. running mean: -7.639660\n",
      "epsilon:0.010000 episode_count: 20678. steps_count: 9146129.000000\n",
      "ep 2954: ep_len:500 episode reward: total was 6.270000. running mean: -7.500563\n",
      "ep 2954: ep_len:375 episode reward: total was -39.810000. running mean: -7.823657\n",
      "ep 2954: ep_len:515 episode reward: total was -12.340000. running mean: -7.868821\n",
      "ep 2954: ep_len:505 episode reward: total was -8.030000. running mean: -7.870433\n",
      "ep 2954: ep_len:3 episode reward: total was 0.000000. running mean: -7.791728\n",
      "ep 2954: ep_len:565 episode reward: total was -3.180000. running mean: -7.745611\n",
      "ep 2954: ep_len:555 episode reward: total was 2.990000. running mean: -7.638255\n",
      "epsilon:0.010000 episode_count: 20685. steps_count: 9149147.000000\n",
      "ep 2955: ep_len:730 episode reward: total was -32.680000. running mean: -7.888672\n",
      "ep 2955: ep_len:525 episode reward: total was -12.640000. running mean: -7.936186\n",
      "ep 2955: ep_len:76 episode reward: total was -5.960000. running mean: -7.916424\n",
      "ep 2955: ep_len:127 episode reward: total was -2.400000. running mean: -7.861259\n",
      "ep 2955: ep_len:3 episode reward: total was 0.000000. running mean: -7.782647\n",
      "ep 2955: ep_len:242 episode reward: total was -14.810000. running mean: -7.852920\n",
      "ep 2955: ep_len:261 episode reward: total was -6.330000. running mean: -7.837691\n",
      "epsilon:0.010000 episode_count: 20692. steps_count: 9151111.000000\n",
      "ep 2956: ep_len:230 episode reward: total was 8.140000. running mean: -7.677914\n",
      "ep 2956: ep_len:500 episode reward: total was -17.050000. running mean: -7.771635\n",
      "ep 2956: ep_len:77 episode reward: total was 1.540000. running mean: -7.678519\n",
      "ep 2956: ep_len:515 episode reward: total was 4.550000. running mean: -7.556234\n",
      "ep 2956: ep_len:3 episode reward: total was 0.000000. running mean: -7.480671\n",
      "ep 2956: ep_len:510 episode reward: total was -2.890000. running mean: -7.434765\n",
      "ep 2956: ep_len:505 episode reward: total was -10.310000. running mean: -7.463517\n",
      "epsilon:0.010000 episode_count: 20699. steps_count: 9153451.000000\n",
      "ep 2957: ep_len:134 episode reward: total was 4.590000. running mean: -7.342982\n",
      "ep 2957: ep_len:500 episode reward: total was -8.970000. running mean: -7.359252\n",
      "ep 2957: ep_len:453 episode reward: total was 9.780000. running mean: -7.187859\n",
      "ep 2957: ep_len:530 episode reward: total was -23.130000. running mean: -7.347281\n",
      "ep 2957: ep_len:3 episode reward: total was 0.000000. running mean: -7.273808\n",
      "ep 2957: ep_len:289 episode reward: total was 1.620000. running mean: -7.184870\n",
      "ep 2957: ep_len:500 episode reward: total was 4.320000. running mean: -7.069821\n",
      "epsilon:0.010000 episode_count: 20706. steps_count: 9155860.000000\n",
      "ep 2958: ep_len:500 episode reward: total was 3.250000. running mean: -6.966623\n",
      "ep 2958: ep_len:295 episode reward: total was -8.860000. running mean: -6.985557\n",
      "ep 2958: ep_len:500 episode reward: total was 2.260000. running mean: -6.893101\n",
      "ep 2958: ep_len:500 episode reward: total was 12.940000. running mean: -6.694770\n",
      "ep 2958: ep_len:3 episode reward: total was 0.000000. running mean: -6.627823\n",
      "ep 2958: ep_len:500 episode reward: total was -3.430000. running mean: -6.595844\n",
      "ep 2958: ep_len:520 episode reward: total was -10.010000. running mean: -6.629986\n",
      "epsilon:0.010000 episode_count: 20713. steps_count: 9158678.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2959: ep_len:500 episode reward: total was 2.750000. running mean: -6.536186\n",
      "ep 2959: ep_len:510 episode reward: total was -13.850000. running mean: -6.609324\n",
      "ep 2959: ep_len:535 episode reward: total was -4.090000. running mean: -6.584131\n",
      "ep 2959: ep_len:546 episode reward: total was 8.120000. running mean: -6.437090\n",
      "ep 2959: ep_len:3 episode reward: total was 0.000000. running mean: -6.372719\n",
      "ep 2959: ep_len:500 episode reward: total was 3.760000. running mean: -6.271391\n",
      "ep 2959: ep_len:198 episode reward: total was -4.880000. running mean: -6.257478\n",
      "epsilon:0.010000 episode_count: 20720. steps_count: 9161470.000000\n",
      "ep 2960: ep_len:109 episode reward: total was 1.080000. running mean: -6.184103\n",
      "ep 2960: ep_len:500 episode reward: total was 10.650000. running mean: -6.015762\n",
      "ep 2960: ep_len:710 episode reward: total was -54.310000. running mean: -6.498704\n",
      "ep 2960: ep_len:520 episode reward: total was 1.820000. running mean: -6.415517\n",
      "ep 2960: ep_len:105 episode reward: total was -9.950000. running mean: -6.450862\n",
      "ep 2960: ep_len:500 episode reward: total was -18.400000. running mean: -6.570353\n",
      "ep 2960: ep_len:580 episode reward: total was -2.560000. running mean: -6.530250\n",
      "epsilon:0.010000 episode_count: 20727. steps_count: 9164494.000000\n",
      "ep 2961: ep_len:505 episode reward: total was 4.440000. running mean: -6.420547\n",
      "ep 2961: ep_len:191 episode reward: total was -0.850000. running mean: -6.364842\n",
      "ep 2961: ep_len:570 episode reward: total was -2.450000. running mean: -6.325693\n",
      "ep 2961: ep_len:575 episode reward: total was 7.960000. running mean: -6.182836\n",
      "ep 2961: ep_len:3 episode reward: total was 0.000000. running mean: -6.121008\n",
      "ep 2961: ep_len:252 episode reward: total was 7.660000. running mean: -5.983198\n",
      "ep 2961: ep_len:188 episode reward: total was -6.340000. running mean: -5.986766\n",
      "epsilon:0.010000 episode_count: 20734. steps_count: 9166778.000000\n",
      "ep 2962: ep_len:505 episode reward: total was -8.070000. running mean: -6.007598\n",
      "ep 2962: ep_len:590 episode reward: total was 1.330000. running mean: -5.934222\n",
      "ep 2962: ep_len:580 episode reward: total was -28.370000. running mean: -6.158580\n",
      "ep 2962: ep_len:515 episode reward: total was -28.520000. running mean: -6.382194\n",
      "ep 2962: ep_len:129 episode reward: total was 8.070000. running mean: -6.237672\n",
      "ep 2962: ep_len:500 episode reward: total was -2.420000. running mean: -6.199496\n",
      "ep 2962: ep_len:500 episode reward: total was -18.050000. running mean: -6.318001\n",
      "epsilon:0.010000 episode_count: 20741. steps_count: 9170097.000000\n",
      "ep 2963: ep_len:590 episode reward: total was -22.690000. running mean: -6.481721\n",
      "ep 2963: ep_len:500 episode reward: total was -47.270000. running mean: -6.889604\n",
      "ep 2963: ep_len:590 episode reward: total was -28.100000. running mean: -7.101707\n",
      "ep 2963: ep_len:406 episode reward: total was 6.890000. running mean: -6.961790\n",
      "ep 2963: ep_len:129 episode reward: total was -7.920000. running mean: -6.971373\n",
      "ep 2963: ep_len:510 episode reward: total was -2.370000. running mean: -6.925359\n",
      "ep 2963: ep_len:515 episode reward: total was -14.160000. running mean: -6.997705\n",
      "epsilon:0.010000 episode_count: 20748. steps_count: 9173337.000000\n",
      "ep 2964: ep_len:550 episode reward: total was -2.630000. running mean: -6.954028\n",
      "ep 2964: ep_len:500 episode reward: total was -8.990000. running mean: -6.974388\n",
      "ep 2964: ep_len:545 episode reward: total was -20.330000. running mean: -7.107944\n",
      "ep 2964: ep_len:600 episode reward: total was -2.920000. running mean: -7.066065\n",
      "ep 2964: ep_len:103 episode reward: total was 6.550000. running mean: -6.929904\n",
      "ep 2964: ep_len:505 episode reward: total was -24.500000. running mean: -7.105605\n",
      "ep 2964: ep_len:575 episode reward: total was -26.060000. running mean: -7.295149\n",
      "epsilon:0.010000 episode_count: 20755. steps_count: 9176715.000000\n",
      "ep 2965: ep_len:555 episode reward: total was 11.500000. running mean: -7.107197\n",
      "ep 2965: ep_len:540 episode reward: total was -13.930000. running mean: -7.175425\n",
      "ep 2965: ep_len:530 episode reward: total was -2.440000. running mean: -7.128071\n",
      "ep 2965: ep_len:500 episode reward: total was -21.530000. running mean: -7.272090\n",
      "ep 2965: ep_len:125 episode reward: total was 1.550000. running mean: -7.183869\n",
      "ep 2965: ep_len:555 episode reward: total was -0.140000. running mean: -7.113431\n",
      "ep 2965: ep_len:600 episode reward: total was -0.470000. running mean: -7.046996\n",
      "epsilon:0.010000 episode_count: 20762. steps_count: 9180120.000000\n",
      "ep 2966: ep_len:134 episode reward: total was -3.430000. running mean: -7.010827\n",
      "ep 2966: ep_len:595 episode reward: total was -5.610000. running mean: -6.996818\n",
      "ep 2966: ep_len:530 episode reward: total was -7.780000. running mean: -7.004650\n",
      "ep 2966: ep_len:520 episode reward: total was -15.960000. running mean: -7.094204\n",
      "ep 2966: ep_len:34 episode reward: total was 1.500000. running mean: -7.008262\n",
      "ep 2966: ep_len:565 episode reward: total was 0.340000. running mean: -6.934779\n",
      "ep 2966: ep_len:210 episode reward: total was -2.310000. running mean: -6.888531\n",
      "epsilon:0.010000 episode_count: 20769. steps_count: 9182708.000000\n",
      "ep 2967: ep_len:500 episode reward: total was 7.310000. running mean: -6.746546\n",
      "ep 2967: ep_len:610 episode reward: total was -8.340000. running mean: -6.762480\n",
      "ep 2967: ep_len:500 episode reward: total was -7.600000. running mean: -6.770856\n",
      "ep 2967: ep_len:49 episode reward: total was 1.050000. running mean: -6.692647\n",
      "ep 2967: ep_len:3 episode reward: total was 0.000000. running mean: -6.625721\n",
      "ep 2967: ep_len:640 episode reward: total was 4.300000. running mean: -6.516463\n",
      "ep 2967: ep_len:288 episode reward: total was -0.770000. running mean: -6.458999\n",
      "epsilon:0.010000 episode_count: 20776. steps_count: 9185298.000000\n",
      "ep 2968: ep_len:500 episode reward: total was -11.740000. running mean: -6.511809\n",
      "ep 2968: ep_len:500 episode reward: total was -34.600000. running mean: -6.792691\n",
      "ep 2968: ep_len:585 episode reward: total was -2.240000. running mean: -6.747164\n",
      "ep 2968: ep_len:500 episode reward: total was 10.370000. running mean: -6.575992\n",
      "ep 2968: ep_len:3 episode reward: total was 0.000000. running mean: -6.510232\n",
      "ep 2968: ep_len:500 episode reward: total was 13.560000. running mean: -6.309530\n",
      "ep 2968: ep_len:344 episode reward: total was -6.800000. running mean: -6.314435\n",
      "epsilon:0.010000 episode_count: 20783. steps_count: 9188230.000000\n",
      "ep 2969: ep_len:227 episode reward: total was 3.630000. running mean: -6.214990\n",
      "ep 2969: ep_len:620 episode reward: total was -8.070000. running mean: -6.233540\n",
      "ep 2969: ep_len:535 episode reward: total was -9.500000. running mean: -6.266205\n",
      "ep 2969: ep_len:500 episode reward: total was -12.130000. running mean: -6.324843\n",
      "ep 2969: ep_len:53 episode reward: total was 5.000000. running mean: -6.211594\n",
      "ep 2969: ep_len:605 episode reward: total was -13.110000. running mean: -6.280578\n",
      "ep 2969: ep_len:283 episode reward: total was -4.850000. running mean: -6.266273\n",
      "epsilon:0.010000 episode_count: 20790. steps_count: 9191053.000000\n",
      "ep 2970: ep_len:500 episode reward: total was 5.360000. running mean: -6.150010\n",
      "ep 2970: ep_len:364 episode reward: total was -7.340000. running mean: -6.161910\n",
      "ep 2970: ep_len:500 episode reward: total was -2.330000. running mean: -6.123591\n",
      "ep 2970: ep_len:500 episode reward: total was -9.430000. running mean: -6.156655\n",
      "ep 2970: ep_len:3 episode reward: total was 0.000000. running mean: -6.095088\n",
      "ep 2970: ep_len:580 episode reward: total was 5.480000. running mean: -5.979337\n",
      "ep 2970: ep_len:500 episode reward: total was 0.310000. running mean: -5.916444\n",
      "epsilon:0.010000 episode_count: 20797. steps_count: 9194000.000000\n",
      "ep 2971: ep_len:500 episode reward: total was 11.780000. running mean: -5.739480\n",
      "ep 2971: ep_len:505 episode reward: total was -9.930000. running mean: -5.781385\n",
      "ep 2971: ep_len:430 episode reward: total was -12.770000. running mean: -5.851271\n",
      "ep 2971: ep_len:500 episode reward: total was 16.430000. running mean: -5.628458\n",
      "ep 2971: ep_len:81 episode reward: total was 4.030000. running mean: -5.531874\n",
      "ep 2971: ep_len:590 episode reward: total was 3.490000. running mean: -5.441655\n",
      "ep 2971: ep_len:500 episode reward: total was -16.510000. running mean: -5.552338\n",
      "epsilon:0.010000 episode_count: 20804. steps_count: 9197106.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2972: ep_len:660 episode reward: total was -18.680000. running mean: -5.683615\n",
      "ep 2972: ep_len:535 episode reward: total was -10.290000. running mean: -5.729679\n",
      "ep 2972: ep_len:685 episode reward: total was -61.830000. running mean: -6.290682\n",
      "ep 2972: ep_len:570 episode reward: total was -1.040000. running mean: -6.238175\n",
      "ep 2972: ep_len:98 episode reward: total was 5.040000. running mean: -6.125393\n",
      "ep 2972: ep_len:505 episode reward: total was -15.900000. running mean: -6.223140\n",
      "ep 2972: ep_len:500 episode reward: total was -29.640000. running mean: -6.457308\n",
      "epsilon:0.010000 episode_count: 20811. steps_count: 9200659.000000\n",
      "ep 2973: ep_len:610 episode reward: total was 4.150000. running mean: -6.351235\n",
      "ep 2973: ep_len:530 episode reward: total was 17.330000. running mean: -6.114423\n",
      "ep 2973: ep_len:615 episode reward: total was -27.540000. running mean: -6.328678\n",
      "ep 2973: ep_len:600 episode reward: total was 14.470000. running mean: -6.120692\n",
      "ep 2973: ep_len:120 episode reward: total was 2.520000. running mean: -6.034285\n",
      "ep 2973: ep_len:530 episode reward: total was -19.490000. running mean: -6.168842\n",
      "ep 2973: ep_len:575 episode reward: total was -9.540000. running mean: -6.202554\n",
      "epsilon:0.010000 episode_count: 20818. steps_count: 9204239.000000\n",
      "ep 2974: ep_len:675 episode reward: total was -57.360000. running mean: -6.714128\n",
      "ep 2974: ep_len:975 episode reward: total was -75.380000. running mean: -7.400787\n",
      "ep 2974: ep_len:500 episode reward: total was 5.410000. running mean: -7.272679\n",
      "ep 2974: ep_len:510 episode reward: total was -5.910000. running mean: -7.259052\n",
      "ep 2974: ep_len:3 episode reward: total was 0.000000. running mean: -7.186462\n",
      "ep 2974: ep_len:690 episode reward: total was 4.430000. running mean: -7.070297\n",
      "ep 2974: ep_len:500 episode reward: total was -5.060000. running mean: -7.050194\n",
      "epsilon:0.010000 episode_count: 20825. steps_count: 9208092.000000\n",
      "ep 2975: ep_len:565 episode reward: total was 12.460000. running mean: -6.855092\n",
      "ep 2975: ep_len:530 episode reward: total was 13.320000. running mean: -6.653341\n",
      "ep 2975: ep_len:535 episode reward: total was -19.640000. running mean: -6.783208\n",
      "ep 2975: ep_len:515 episode reward: total was -10.490000. running mean: -6.820276\n",
      "ep 2975: ep_len:3 episode reward: total was 0.000000. running mean: -6.752073\n",
      "ep 2975: ep_len:500 episode reward: total was -3.190000. running mean: -6.716452\n",
      "ep 2975: ep_len:515 episode reward: total was 3.240000. running mean: -6.616888\n",
      "epsilon:0.010000 episode_count: 20832. steps_count: 9211255.000000\n",
      "ep 2976: ep_len:630 episode reward: total was -10.140000. running mean: -6.652119\n",
      "ep 2976: ep_len:655 episode reward: total was -44.320000. running mean: -7.028798\n",
      "ep 2976: ep_len:800 episode reward: total was -43.100000. running mean: -7.389510\n",
      "ep 2976: ep_len:535 episode reward: total was -8.610000. running mean: -7.401714\n",
      "ep 2976: ep_len:3 episode reward: total was 0.000000. running mean: -7.327697\n",
      "ep 2976: ep_len:530 episode reward: total was 2.120000. running mean: -7.233220\n",
      "ep 2976: ep_len:500 episode reward: total was 0.730000. running mean: -7.153588\n",
      "epsilon:0.010000 episode_count: 20839. steps_count: 9214908.000000\n",
      "ep 2977: ep_len:500 episode reward: total was -25.480000. running mean: -7.336852\n",
      "ep 2977: ep_len:343 episode reward: total was -38.810000. running mean: -7.651584\n",
      "ep 2977: ep_len:570 episode reward: total was 6.990000. running mean: -7.505168\n",
      "ep 2977: ep_len:550 episode reward: total was -11.510000. running mean: -7.545216\n",
      "ep 2977: ep_len:1 episode reward: total was 0.000000. running mean: -7.469764\n",
      "ep 2977: ep_len:705 episode reward: total was -15.770000. running mean: -7.552766\n",
      "ep 2977: ep_len:570 episode reward: total was -9.530000. running mean: -7.572539\n",
      "epsilon:0.010000 episode_count: 20846. steps_count: 9218147.000000\n",
      "ep 2978: ep_len:640 episode reward: total was -12.200000. running mean: -7.618813\n",
      "ep 2978: ep_len:620 episode reward: total was 14.930000. running mean: -7.393325\n",
      "ep 2978: ep_len:500 episode reward: total was -21.370000. running mean: -7.533092\n",
      "ep 2978: ep_len:500 episode reward: total was -14.000000. running mean: -7.597761\n",
      "ep 2978: ep_len:49 episode reward: total was 4.500000. running mean: -7.476783\n",
      "ep 2978: ep_len:167 episode reward: total was 7.600000. running mean: -7.326016\n",
      "ep 2978: ep_len:530 episode reward: total was -0.500000. running mean: -7.257755\n",
      "epsilon:0.010000 episode_count: 20853. steps_count: 9221153.000000\n",
      "ep 2979: ep_len:585 episode reward: total was 6.640000. running mean: -7.118778\n",
      "ep 2979: ep_len:655 episode reward: total was 9.560000. running mean: -6.951990\n",
      "ep 2979: ep_len:830 episode reward: total was -48.150000. running mean: -7.363970\n",
      "ep 2979: ep_len:500 episode reward: total was -6.950000. running mean: -7.359831\n",
      "ep 2979: ep_len:3 episode reward: total was 0.000000. running mean: -7.286232\n",
      "ep 2979: ep_len:500 episode reward: total was 7.280000. running mean: -7.140570\n",
      "ep 2979: ep_len:565 episode reward: total was -1.730000. running mean: -7.086464\n",
      "epsilon:0.010000 episode_count: 20860. steps_count: 9224791.000000\n",
      "ep 2980: ep_len:1207 episode reward: total was -93.540000. running mean: -7.951000\n",
      "ep 2980: ep_len:545 episode reward: total was 14.310000. running mean: -7.728390\n",
      "ep 2980: ep_len:417 episode reward: total was 5.270000. running mean: -7.598406\n",
      "ep 2980: ep_len:575 episode reward: total was 9.540000. running mean: -7.427022\n",
      "ep 2980: ep_len:109 episode reward: total was 6.040000. running mean: -7.292351\n",
      "ep 2980: ep_len:500 episode reward: total was -19.730000. running mean: -7.416728\n",
      "ep 2980: ep_len:520 episode reward: total was -24.420000. running mean: -7.586761\n",
      "epsilon:0.010000 episode_count: 20867. steps_count: 9228664.000000\n",
      "ep 2981: ep_len:625 episode reward: total was -16.240000. running mean: -7.673293\n",
      "ep 2981: ep_len:640 episode reward: total was 14.590000. running mean: -7.450660\n",
      "ep 2981: ep_len:500 episode reward: total was 2.890000. running mean: -7.347253\n",
      "ep 2981: ep_len:625 episode reward: total was 11.110000. running mean: -7.162681\n",
      "ep 2981: ep_len:96 episode reward: total was 5.050000. running mean: -7.040554\n",
      "ep 2981: ep_len:500 episode reward: total was -3.230000. running mean: -7.002449\n",
      "ep 2981: ep_len:595 episode reward: total was -25.040000. running mean: -7.182824\n",
      "epsilon:0.010000 episode_count: 20874. steps_count: 9232245.000000\n",
      "ep 2982: ep_len:515 episode reward: total was 1.110000. running mean: -7.099896\n",
      "ep 2982: ep_len:675 episode reward: total was -6.000000. running mean: -7.088897\n",
      "ep 2982: ep_len:635 episode reward: total was -7.320000. running mean: -7.091208\n",
      "ep 2982: ep_len:500 episode reward: total was -19.610000. running mean: -7.216396\n",
      "ep 2982: ep_len:124 episode reward: total was 4.540000. running mean: -7.098832\n",
      "ep 2982: ep_len:500 episode reward: total was 9.590000. running mean: -6.931944\n",
      "ep 2982: ep_len:565 episode reward: total was -15.970000. running mean: -7.022324\n",
      "epsilon:0.010000 episode_count: 20881. steps_count: 9235759.000000\n",
      "ep 2983: ep_len:555 episode reward: total was -23.310000. running mean: -7.185201\n",
      "ep 2983: ep_len:600 episode reward: total was -5.330000. running mean: -7.166649\n",
      "ep 2983: ep_len:540 episode reward: total was -0.630000. running mean: -7.101282\n",
      "ep 2983: ep_len:645 episode reward: total was -24.810000. running mean: -7.278370\n",
      "ep 2983: ep_len:3 episode reward: total was 0.000000. running mean: -7.205586\n",
      "ep 2983: ep_len:520 episode reward: total was -6.300000. running mean: -7.196530\n",
      "ep 2983: ep_len:605 episode reward: total was -8.900000. running mean: -7.213565\n",
      "epsilon:0.010000 episode_count: 20888. steps_count: 9239227.000000\n",
      "ep 2984: ep_len:550 episode reward: total was -16.220000. running mean: -7.303629\n",
      "ep 2984: ep_len:570 episode reward: total was -7.100000. running mean: -7.301593\n",
      "ep 2984: ep_len:500 episode reward: total was -2.900000. running mean: -7.257577\n",
      "ep 2984: ep_len:565 episode reward: total was -36.620000. running mean: -7.551201\n",
      "ep 2984: ep_len:89 episode reward: total was -13.970000. running mean: -7.615389\n",
      "ep 2984: ep_len:585 episode reward: total was -79.850000. running mean: -8.337735\n",
      "ep 2984: ep_len:206 episode reward: total was -3.330000. running mean: -8.287658\n",
      "epsilon:0.010000 episode_count: 20895. steps_count: 9242292.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2985: ep_len:500 episode reward: total was -19.420000. running mean: -8.398981\n",
      "ep 2985: ep_len:580 episode reward: total was 5.590000. running mean: -8.259091\n",
      "ep 2985: ep_len:505 episode reward: total was -12.510000. running mean: -8.301601\n",
      "ep 2985: ep_len:515 episode reward: total was 5.570000. running mean: -8.162885\n",
      "ep 2985: ep_len:3 episode reward: total was 0.000000. running mean: -8.081256\n",
      "ep 2985: ep_len:525 episode reward: total was 1.560000. running mean: -7.984843\n",
      "ep 2985: ep_len:535 episode reward: total was -6.000000. running mean: -7.964995\n",
      "epsilon:0.010000 episode_count: 20902. steps_count: 9245455.000000\n",
      "ep 2986: ep_len:226 episode reward: total was 9.640000. running mean: -7.788945\n",
      "ep 2986: ep_len:580 episode reward: total was 8.100000. running mean: -7.630055\n",
      "ep 2986: ep_len:61 episode reward: total was 0.530000. running mean: -7.548455\n",
      "ep 2986: ep_len:51 episode reward: total was 0.050000. running mean: -7.472470\n",
      "ep 2986: ep_len:3 episode reward: total was 0.000000. running mean: -7.397745\n",
      "ep 2986: ep_len:655 episode reward: total was -63.110000. running mean: -7.954868\n",
      "ep 2986: ep_len:650 episode reward: total was 3.280000. running mean: -7.842519\n",
      "epsilon:0.010000 episode_count: 20909. steps_count: 9247681.000000\n",
      "ep 2987: ep_len:585 episode reward: total was 1.620000. running mean: -7.747894\n",
      "ep 2987: ep_len:530 episode reward: total was -21.110000. running mean: -7.881515\n",
      "ep 2987: ep_len:500 episode reward: total was 0.470000. running mean: -7.798000\n",
      "ep 2987: ep_len:520 episode reward: total was 13.520000. running mean: -7.584820\n",
      "ep 2987: ep_len:3 episode reward: total was 0.000000. running mean: -7.508972\n",
      "ep 2987: ep_len:685 episode reward: total was -17.160000. running mean: -7.605482\n",
      "ep 2987: ep_len:500 episode reward: total was -24.160000. running mean: -7.771027\n",
      "epsilon:0.010000 episode_count: 20916. steps_count: 9251004.000000\n",
      "ep 2988: ep_len:510 episode reward: total was -27.440000. running mean: -7.967717\n",
      "ep 2988: ep_len:630 episode reward: total was -14.190000. running mean: -8.029940\n",
      "ep 2988: ep_len:710 episode reward: total was -45.740000. running mean: -8.407040\n",
      "ep 2988: ep_len:500 episode reward: total was 5.580000. running mean: -8.267170\n",
      "ep 2988: ep_len:119 episode reward: total was -8.920000. running mean: -8.273698\n",
      "ep 2988: ep_len:790 episode reward: total was -38.740000. running mean: -8.578361\n",
      "ep 2988: ep_len:585 episode reward: total was -7.030000. running mean: -8.562878\n",
      "epsilon:0.010000 episode_count: 20923. steps_count: 9254848.000000\n",
      "ep 2989: ep_len:500 episode reward: total was -22.150000. running mean: -8.698749\n",
      "ep 2989: ep_len:500 episode reward: total was 7.770000. running mean: -8.534062\n",
      "ep 2989: ep_len:625 episode reward: total was 2.150000. running mean: -8.427221\n",
      "ep 2989: ep_len:500 episode reward: total was -0.910000. running mean: -8.352049\n",
      "ep 2989: ep_len:116 episode reward: total was -2.940000. running mean: -8.297928\n",
      "ep 2989: ep_len:565 episode reward: total was 2.650000. running mean: -8.188449\n",
      "ep 2989: ep_len:358 episode reward: total was -10.320000. running mean: -8.209764\n",
      "epsilon:0.010000 episode_count: 20930. steps_count: 9258012.000000\n",
      "ep 2990: ep_len:193 episode reward: total was 2.610000. running mean: -8.101567\n",
      "ep 2990: ep_len:660 episode reward: total was -30.060000. running mean: -8.321151\n",
      "ep 2990: ep_len:575 episode reward: total was -7.300000. running mean: -8.310940\n",
      "ep 2990: ep_len:500 episode reward: total was -23.040000. running mean: -8.458230\n",
      "ep 2990: ep_len:3 episode reward: total was 0.000000. running mean: -8.373648\n",
      "ep 2990: ep_len:585 episode reward: total was -13.350000. running mean: -8.423411\n",
      "ep 2990: ep_len:192 episode reward: total was -6.910000. running mean: -8.408277\n",
      "epsilon:0.010000 episode_count: 20937. steps_count: 9260720.000000\n",
      "ep 2991: ep_len:635 episode reward: total was -4.450000. running mean: -8.368695\n",
      "ep 2991: ep_len:515 episode reward: total was 1.630000. running mean: -8.268708\n",
      "ep 2991: ep_len:725 episode reward: total was -32.260000. running mean: -8.508621\n",
      "ep 2991: ep_len:37 episode reward: total was 2.530000. running mean: -8.398234\n",
      "ep 2991: ep_len:93 episode reward: total was 5.000000. running mean: -8.264252\n",
      "ep 2991: ep_len:505 episode reward: total was -34.520000. running mean: -8.526809\n",
      "ep 2991: ep_len:510 episode reward: total was -19.180000. running mean: -8.633341\n",
      "epsilon:0.010000 episode_count: 20944. steps_count: 9263740.000000\n",
      "ep 2992: ep_len:650 episode reward: total was 11.190000. running mean: -8.435108\n",
      "ep 2992: ep_len:500 episode reward: total was -19.830000. running mean: -8.549057\n",
      "ep 2992: ep_len:79 episode reward: total was 1.050000. running mean: -8.453066\n",
      "ep 2992: ep_len:605 episode reward: total was -5.470000. running mean: -8.423236\n",
      "ep 2992: ep_len:47 episode reward: total was 3.000000. running mean: -8.309003\n",
      "ep 2992: ep_len:183 episode reward: total was 1.600000. running mean: -8.209913\n",
      "ep 2992: ep_len:590 episode reward: total was -6.760000. running mean: -8.195414\n",
      "epsilon:0.010000 episode_count: 20951. steps_count: 9266394.000000\n",
      "ep 2993: ep_len:116 episode reward: total was 3.070000. running mean: -8.082760\n",
      "ep 2993: ep_len:605 episode reward: total was -7.390000. running mean: -8.075832\n",
      "ep 2993: ep_len:825 episode reward: total was -37.080000. running mean: -8.365874\n",
      "ep 2993: ep_len:500 episode reward: total was 12.100000. running mean: -8.161215\n",
      "ep 2993: ep_len:3 episode reward: total was 0.000000. running mean: -8.079603\n",
      "ep 2993: ep_len:500 episode reward: total was 1.220000. running mean: -7.986607\n",
      "ep 2993: ep_len:510 episode reward: total was -12.550000. running mean: -8.032241\n",
      "epsilon:0.010000 episode_count: 20958. steps_count: 9269453.000000\n",
      "ep 2994: ep_len:625 episode reward: total was -17.710000. running mean: -8.129019\n",
      "ep 2994: ep_len:585 episode reward: total was 22.430000. running mean: -7.823428\n",
      "ep 2994: ep_len:515 episode reward: total was -14.590000. running mean: -7.891094\n",
      "ep 2994: ep_len:500 episode reward: total was 13.420000. running mean: -7.677983\n",
      "ep 2994: ep_len:3 episode reward: total was 0.000000. running mean: -7.601203\n",
      "ep 2994: ep_len:550 episode reward: total was -3.240000. running mean: -7.557591\n",
      "ep 2994: ep_len:605 episode reward: total was -24.100000. running mean: -7.723015\n",
      "epsilon:0.010000 episode_count: 20965. steps_count: 9272836.000000\n",
      "ep 2995: ep_len:550 episode reward: total was -6.470000. running mean: -7.710485\n",
      "ep 2995: ep_len:500 episode reward: total was -30.040000. running mean: -7.933780\n",
      "ep 2995: ep_len:520 episode reward: total was -12.960000. running mean: -7.984043\n",
      "ep 2995: ep_len:535 episode reward: total was 8.420000. running mean: -7.820002\n",
      "ep 2995: ep_len:89 episode reward: total was -4.980000. running mean: -7.791602\n",
      "ep 2995: ep_len:650 episode reward: total was -24.700000. running mean: -7.960686\n",
      "ep 2995: ep_len:590 episode reward: total was -20.060000. running mean: -8.081679\n",
      "epsilon:0.010000 episode_count: 20972. steps_count: 9276270.000000\n",
      "ep 2996: ep_len:249 episode reward: total was 5.140000. running mean: -7.949463\n",
      "ep 2996: ep_len:545 episode reward: total was 21.910000. running mean: -7.650868\n",
      "ep 2996: ep_len:645 episode reward: total was -3.390000. running mean: -7.608259\n",
      "ep 2996: ep_len:535 episode reward: total was 6.900000. running mean: -7.463177\n",
      "ep 2996: ep_len:3 episode reward: total was 0.000000. running mean: -7.388545\n",
      "ep 2996: ep_len:665 episode reward: total was -22.740000. running mean: -7.542059\n",
      "ep 2996: ep_len:500 episode reward: total was -25.530000. running mean: -7.721939\n",
      "epsilon:0.010000 episode_count: 20979. steps_count: 9279412.000000\n",
      "ep 2997: ep_len:570 episode reward: total was -3.920000. running mean: -7.683919\n",
      "ep 2997: ep_len:565 episode reward: total was -5.340000. running mean: -7.660480\n",
      "ep 2997: ep_len:600 episode reward: total was -5.550000. running mean: -7.639375\n",
      "ep 2997: ep_len:614 episode reward: total was -18.820000. running mean: -7.751182\n",
      "ep 2997: ep_len:3 episode reward: total was 0.000000. running mean: -7.673670\n",
      "ep 2997: ep_len:665 episode reward: total was -0.150000. running mean: -7.598433\n",
      "ep 2997: ep_len:350 episode reward: total was -12.810000. running mean: -7.650549\n",
      "epsilon:0.010000 episode_count: 20986. steps_count: 9282779.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2998: ep_len:755 episode reward: total was -54.220000. running mean: -8.116243\n",
      "ep 2998: ep_len:530 episode reward: total was -3.730000. running mean: -8.072381\n",
      "ep 2998: ep_len:500 episode reward: total was -30.470000. running mean: -8.296357\n",
      "ep 2998: ep_len:595 episode reward: total was -2.030000. running mean: -8.233694\n",
      "ep 2998: ep_len:3 episode reward: total was 0.000000. running mean: -8.151357\n",
      "ep 2998: ep_len:500 episode reward: total was 11.600000. running mean: -7.953843\n",
      "ep 2998: ep_len:575 episode reward: total was -8.010000. running mean: -7.954405\n",
      "epsilon:0.010000 episode_count: 20993. steps_count: 9286237.000000\n",
      "ep 2999: ep_len:500 episode reward: total was -2.610000. running mean: -7.900961\n",
      "ep 2999: ep_len:500 episode reward: total was 5.860000. running mean: -7.763351\n",
      "ep 2999: ep_len:565 episode reward: total was -16.480000. running mean: -7.850517\n",
      "ep 2999: ep_len:500 episode reward: total was 5.000000. running mean: -7.722012\n",
      "ep 2999: ep_len:3 episode reward: total was 0.000000. running mean: -7.644792\n",
      "ep 2999: ep_len:520 episode reward: total was 0.280000. running mean: -7.565544\n",
      "ep 2999: ep_len:550 episode reward: total was -18.660000. running mean: -7.676489\n",
      "epsilon:0.010000 episode_count: 21000. steps_count: 9289375.000000\n",
      "ep 3000: ep_len:500 episode reward: total was 7.280000. running mean: -7.526924\n",
      "ep 3000: ep_len:500 episode reward: total was -6.190000. running mean: -7.513555\n",
      "ep 3000: ep_len:500 episode reward: total was -10.460000. running mean: -7.543019\n",
      "ep 3000: ep_len:56 episode reward: total was 0.550000. running mean: -7.462089\n",
      "ep 3000: ep_len:97 episode reward: total was 5.040000. running mean: -7.337068\n",
      "ep 3000: ep_len:530 episode reward: total was 3.680000. running mean: -7.226897\n",
      "ep 3000: ep_len:201 episode reward: total was -7.870000. running mean: -7.233328\n",
      "epsilon:0.010000 episode_count: 21007. steps_count: 9291759.000000\n",
      "ep 3001: ep_len:116 episode reward: total was -5.960000. running mean: -7.220595\n",
      "ep 3001: ep_len:383 episode reward: total was -5.310000. running mean: -7.201489\n",
      "ep 3001: ep_len:610 episode reward: total was -2.400000. running mean: -7.153474\n",
      "ep 3001: ep_len:115 episode reward: total was 1.630000. running mean: -7.065639\n",
      "ep 3001: ep_len:99 episode reward: total was 6.540000. running mean: -6.929583\n",
      "ep 3001: ep_len:500 episode reward: total was -9.590000. running mean: -6.956187\n",
      "ep 3001: ep_len:565 episode reward: total was -8.810000. running mean: -6.974725\n",
      "epsilon:0.010000 episode_count: 21014. steps_count: 9294147.000000\n",
      "ep 3002: ep_len:116 episode reward: total was 0.550000. running mean: -6.899478\n",
      "ep 3002: ep_len:500 episode reward: total was -1.810000. running mean: -6.848583\n",
      "ep 3002: ep_len:635 episode reward: total was -11.090000. running mean: -6.890998\n",
      "ep 3002: ep_len:56 episode reward: total was -1.940000. running mean: -6.841488\n",
      "ep 3002: ep_len:3 episode reward: total was 0.000000. running mean: -6.773073\n",
      "ep 3002: ep_len:520 episode reward: total was -40.530000. running mean: -7.110642\n",
      "ep 3002: ep_len:505 episode reward: total was -4.910000. running mean: -7.088636\n",
      "epsilon:0.010000 episode_count: 21021. steps_count: 9296482.000000\n",
      "ep 3003: ep_len:500 episode reward: total was 0.160000. running mean: -7.016149\n",
      "ep 3003: ep_len:500 episode reward: total was -18.060000. running mean: -7.126588\n",
      "ep 3003: ep_len:720 episode reward: total was -33.140000. running mean: -7.386722\n",
      "ep 3003: ep_len:570 episode reward: total was 3.420000. running mean: -7.278655\n",
      "ep 3003: ep_len:3 episode reward: total was 0.000000. running mean: -7.205868\n",
      "ep 3003: ep_len:242 episode reward: total was 7.150000. running mean: -7.062309\n",
      "ep 3003: ep_len:600 episode reward: total was 4.140000. running mean: -6.950286\n",
      "epsilon:0.010000 episode_count: 21028. steps_count: 9299617.000000\n",
      "ep 3004: ep_len:255 episode reward: total was 5.140000. running mean: -6.829383\n",
      "ep 3004: ep_len:590 episode reward: total was -10.890000. running mean: -6.869990\n",
      "ep 3004: ep_len:750 episode reward: total was -57.260000. running mean: -7.373890\n",
      "ep 3004: ep_len:555 episode reward: total was -11.590000. running mean: -7.416051\n",
      "ep 3004: ep_len:3 episode reward: total was 0.000000. running mean: -7.341890\n",
      "ep 3004: ep_len:520 episode reward: total was -24.590000. running mean: -7.514371\n",
      "ep 3004: ep_len:595 episode reward: total was -16.530000. running mean: -7.604528\n",
      "epsilon:0.010000 episode_count: 21035. steps_count: 9302885.000000\n",
      "ep 3005: ep_len:211 episode reward: total was -3.400000. running mean: -7.562482\n",
      "ep 3005: ep_len:500 episode reward: total was -0.140000. running mean: -7.488258\n",
      "ep 3005: ep_len:630 episode reward: total was -25.290000. running mean: -7.666275\n",
      "ep 3005: ep_len:540 episode reward: total was -5.600000. running mean: -7.645612\n",
      "ep 3005: ep_len:3 episode reward: total was 0.000000. running mean: -7.569156\n",
      "ep 3005: ep_len:500 episode reward: total was -2.680000. running mean: -7.520265\n",
      "ep 3005: ep_len:358 episode reward: total was -12.800000. running mean: -7.573062\n",
      "epsilon:0.010000 episode_count: 21042. steps_count: 9305627.000000\n",
      "ep 3006: ep_len:690 episode reward: total was -10.080000. running mean: -7.598131\n",
      "ep 3006: ep_len:580 episode reward: total was -31.490000. running mean: -7.837050\n",
      "ep 3006: ep_len:595 episode reward: total was -3.180000. running mean: -7.790479\n",
      "ep 3006: ep_len:520 episode reward: total was -26.120000. running mean: -7.973775\n",
      "ep 3006: ep_len:3 episode reward: total was 0.000000. running mean: -7.894037\n",
      "ep 3006: ep_len:525 episode reward: total was 8.200000. running mean: -7.733097\n",
      "ep 3006: ep_len:520 episode reward: total was -48.700000. running mean: -8.142766\n",
      "epsilon:0.010000 episode_count: 21049. steps_count: 9309060.000000\n",
      "ep 3007: ep_len:565 episode reward: total was -13.040000. running mean: -8.191738\n",
      "ep 3007: ep_len:685 episode reward: total was -17.800000. running mean: -8.287821\n",
      "ep 3007: ep_len:500 episode reward: total was -0.620000. running mean: -8.211142\n",
      "ep 3007: ep_len:500 episode reward: total was -18.060000. running mean: -8.309631\n",
      "ep 3007: ep_len:3 episode reward: total was 0.000000. running mean: -8.226535\n",
      "ep 3007: ep_len:500 episode reward: total was -2.760000. running mean: -8.171869\n",
      "ep 3007: ep_len:560 episode reward: total was -3.300000. running mean: -8.123151\n",
      "epsilon:0.010000 episode_count: 21056. steps_count: 9312373.000000\n",
      "ep 3008: ep_len:605 episode reward: total was 5.100000. running mean: -7.990919\n",
      "ep 3008: ep_len:500 episode reward: total was -17.830000. running mean: -8.089310\n",
      "ep 3008: ep_len:500 episode reward: total was -2.470000. running mean: -8.033117\n",
      "ep 3008: ep_len:530 episode reward: total was 6.880000. running mean: -7.883986\n",
      "ep 3008: ep_len:3 episode reward: total was 0.000000. running mean: -7.805146\n",
      "ep 3008: ep_len:500 episode reward: total was -2.360000. running mean: -7.750694\n",
      "ep 3008: ep_len:500 episode reward: total was -13.250000. running mean: -7.805687\n",
      "epsilon:0.010000 episode_count: 21063. steps_count: 9315511.000000\n",
      "ep 3009: ep_len:660 episode reward: total was -20.270000. running mean: -7.930330\n",
      "ep 3009: ep_len:575 episode reward: total was -12.520000. running mean: -7.976227\n",
      "ep 3009: ep_len:370 episode reward: total was 2.140000. running mean: -7.875065\n",
      "ep 3009: ep_len:570 episode reward: total was 4.890000. running mean: -7.747414\n",
      "ep 3009: ep_len:3 episode reward: total was 0.000000. running mean: -7.669940\n",
      "ep 3009: ep_len:500 episode reward: total was 2.900000. running mean: -7.564241\n",
      "ep 3009: ep_len:505 episode reward: total was -7.880000. running mean: -7.567398\n",
      "epsilon:0.010000 episode_count: 21070. steps_count: 9318694.000000\n",
      "ep 3010: ep_len:500 episode reward: total was -20.250000. running mean: -7.694224\n",
      "ep 3010: ep_len:585 episode reward: total was -36.520000. running mean: -7.982482\n",
      "ep 3010: ep_len:590 episode reward: total was -43.660000. running mean: -8.339257\n",
      "ep 3010: ep_len:575 episode reward: total was 12.090000. running mean: -8.134965\n",
      "ep 3010: ep_len:89 episode reward: total was 4.040000. running mean: -8.013215\n",
      "ep 3010: ep_len:505 episode reward: total was -12.260000. running mean: -8.055683\n",
      "ep 3010: ep_len:300 episode reward: total was -6.830000. running mean: -8.043426\n",
      "epsilon:0.010000 episode_count: 21077. steps_count: 9321838.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3011: ep_len:620 episode reward: total was -28.050000. running mean: -8.243492\n",
      "ep 3011: ep_len:500 episode reward: total was -0.210000. running mean: -8.163157\n",
      "ep 3011: ep_len:530 episode reward: total was 1.860000. running mean: -8.062925\n",
      "ep 3011: ep_len:432 episode reward: total was 10.920000. running mean: -7.873096\n",
      "ep 3011: ep_len:127 episode reward: total was 7.060000. running mean: -7.723765\n",
      "ep 3011: ep_len:510 episode reward: total was -10.320000. running mean: -7.749727\n",
      "ep 3011: ep_len:575 episode reward: total was -8.240000. running mean: -7.754630\n",
      "epsilon:0.010000 episode_count: 21084. steps_count: 9325132.000000\n",
      "ep 3012: ep_len:560 episode reward: total was 8.970000. running mean: -7.587384\n",
      "ep 3012: ep_len:505 episode reward: total was 6.160000. running mean: -7.449910\n",
      "ep 3012: ep_len:565 episode reward: total was -6.470000. running mean: -7.440111\n",
      "ep 3012: ep_len:610 episode reward: total was 1.560000. running mean: -7.350110\n",
      "ep 3012: ep_len:3 episode reward: total was 0.000000. running mean: -7.276609\n",
      "ep 3012: ep_len:635 episode reward: total was 13.010000. running mean: -7.073743\n",
      "ep 3012: ep_len:540 episode reward: total was -2.520000. running mean: -7.028205\n",
      "epsilon:0.010000 episode_count: 21091. steps_count: 9328550.000000\n",
      "ep 3013: ep_len:590 episode reward: total was 7.440000. running mean: -6.883523\n",
      "ep 3013: ep_len:500 episode reward: total was -2.390000. running mean: -6.838588\n",
      "ep 3013: ep_len:79 episode reward: total was -0.970000. running mean: -6.779902\n",
      "ep 3013: ep_len:500 episode reward: total was -11.040000. running mean: -6.822503\n",
      "ep 3013: ep_len:3 episode reward: total was 0.000000. running mean: -6.754278\n",
      "ep 3013: ep_len:550 episode reward: total was -13.480000. running mean: -6.821535\n",
      "ep 3013: ep_len:515 episode reward: total was -15.590000. running mean: -6.909220\n",
      "epsilon:0.010000 episode_count: 21098. steps_count: 9331287.000000\n",
      "ep 3014: ep_len:640 episode reward: total was 3.930000. running mean: -6.800828\n",
      "ep 3014: ep_len:500 episode reward: total was -28.570000. running mean: -7.018519\n",
      "ep 3014: ep_len:515 episode reward: total was 2.930000. running mean: -6.919034\n",
      "ep 3014: ep_len:132 episode reward: total was 6.100000. running mean: -6.788844\n",
      "ep 3014: ep_len:115 episode reward: total was 6.060000. running mean: -6.660355\n",
      "ep 3014: ep_len:790 episode reward: total was -34.180000. running mean: -6.935552\n",
      "ep 3014: ep_len:500 episode reward: total was -9.150000. running mean: -6.957696\n",
      "epsilon:0.010000 episode_count: 21105. steps_count: 9334479.000000\n",
      "ep 3015: ep_len:680 episode reward: total was 6.460000. running mean: -6.823519\n",
      "ep 3015: ep_len:500 episode reward: total was 1.700000. running mean: -6.738284\n",
      "ep 3015: ep_len:50 episode reward: total was -1.480000. running mean: -6.685701\n",
      "ep 3015: ep_len:575 episode reward: total was 12.430000. running mean: -6.494544\n",
      "ep 3015: ep_len:3 episode reward: total was 0.000000. running mean: -6.429599\n",
      "ep 3015: ep_len:500 episode reward: total was 0.420000. running mean: -6.361103\n",
      "ep 3015: ep_len:565 episode reward: total was -11.590000. running mean: -6.413392\n",
      "epsilon:0.010000 episode_count: 21112. steps_count: 9337352.000000\n",
      "ep 3016: ep_len:500 episode reward: total was 3.270000. running mean: -6.316558\n",
      "ep 3016: ep_len:625 episode reward: total was -0.100000. running mean: -6.254392\n",
      "ep 3016: ep_len:419 episode reward: total was -3.830000. running mean: -6.230148\n",
      "ep 3016: ep_len:510 episode reward: total was -43.710000. running mean: -6.604947\n",
      "ep 3016: ep_len:3 episode reward: total was 0.000000. running mean: -6.538897\n",
      "ep 3016: ep_len:243 episode reward: total was 4.640000. running mean: -6.427109\n",
      "ep 3016: ep_len:500 episode reward: total was -16.030000. running mean: -6.523137\n",
      "epsilon:0.010000 episode_count: 21119. steps_count: 9340152.000000\n",
      "ep 3017: ep_len:120 episode reward: total was -0.940000. running mean: -6.467306\n",
      "ep 3017: ep_len:725 episode reward: total was -30.330000. running mean: -6.705933\n",
      "ep 3017: ep_len:500 episode reward: total was 0.380000. running mean: -6.635074\n",
      "ep 3017: ep_len:500 episode reward: total was 4.340000. running mean: -6.525323\n",
      "ep 3017: ep_len:99 episode reward: total was -10.460000. running mean: -6.564670\n",
      "ep 3017: ep_len:560 episode reward: total was 8.150000. running mean: -6.417523\n",
      "ep 3017: ep_len:525 episode reward: total was -9.590000. running mean: -6.449248\n",
      "epsilon:0.010000 episode_count: 21126. steps_count: 9343181.000000\n",
      "ep 3018: ep_len:570 episode reward: total was 1.560000. running mean: -6.369155\n",
      "ep 3018: ep_len:515 episode reward: total was -2.170000. running mean: -6.327164\n",
      "ep 3018: ep_len:605 episode reward: total was -0.920000. running mean: -6.273092\n",
      "ep 3018: ep_len:520 episode reward: total was 8.050000. running mean: -6.129861\n",
      "ep 3018: ep_len:3 episode reward: total was 0.000000. running mean: -6.068563\n",
      "ep 3018: ep_len:535 episode reward: total was -0.550000. running mean: -6.013377\n",
      "ep 3018: ep_len:287 episode reward: total was -7.330000. running mean: -6.026543\n",
      "epsilon:0.010000 episode_count: 21133. steps_count: 9346216.000000\n",
      "ep 3019: ep_len:565 episode reward: total was -2.950000. running mean: -5.995778\n",
      "ep 3019: ep_len:500 episode reward: total was 0.140000. running mean: -5.934420\n",
      "ep 3019: ep_len:381 episode reward: total was 3.170000. running mean: -5.843376\n",
      "ep 3019: ep_len:540 episode reward: total was 5.380000. running mean: -5.731142\n",
      "ep 3019: ep_len:3 episode reward: total was 0.000000. running mean: -5.673831\n",
      "ep 3019: ep_len:525 episode reward: total was -49.520000. running mean: -6.112292\n",
      "ep 3019: ep_len:500 episode reward: total was -26.620000. running mean: -6.317369\n",
      "epsilon:0.010000 episode_count: 21140. steps_count: 9349230.000000\n",
      "ep 3020: ep_len:595 episode reward: total was -16.670000. running mean: -6.420896\n",
      "ep 3020: ep_len:500 episode reward: total was 12.170000. running mean: -6.234987\n",
      "ep 3020: ep_len:600 episode reward: total was -13.200000. running mean: -6.304637\n",
      "ep 3020: ep_len:500 episode reward: total was -3.700000. running mean: -6.278590\n",
      "ep 3020: ep_len:3 episode reward: total was 0.000000. running mean: -6.215805\n",
      "ep 3020: ep_len:525 episode reward: total was -29.510000. running mean: -6.448747\n",
      "ep 3020: ep_len:625 episode reward: total was -2.360000. running mean: -6.407859\n",
      "epsilon:0.010000 episode_count: 21147. steps_count: 9352578.000000\n",
      "ep 3021: ep_len:610 episode reward: total was -18.260000. running mean: -6.526380\n",
      "ep 3021: ep_len:600 episode reward: total was 9.550000. running mean: -6.365617\n",
      "ep 3021: ep_len:555 episode reward: total was -6.460000. running mean: -6.366561\n",
      "ep 3021: ep_len:335 episode reward: total was -11.230000. running mean: -6.415195\n",
      "ep 3021: ep_len:3 episode reward: total was 0.000000. running mean: -6.351043\n",
      "ep 3021: ep_len:620 episode reward: total was -12.590000. running mean: -6.413433\n",
      "ep 3021: ep_len:585 episode reward: total was 0.510000. running mean: -6.344198\n",
      "epsilon:0.010000 episode_count: 21154. steps_count: 9355886.000000\n",
      "ep 3022: ep_len:555 episode reward: total was -0.460000. running mean: -6.285356\n",
      "ep 3022: ep_len:500 episode reward: total was 19.170000. running mean: -6.030803\n",
      "ep 3022: ep_len:585 episode reward: total was 0.980000. running mean: -5.960695\n",
      "ep 3022: ep_len:168 episode reward: total was 6.630000. running mean: -5.834788\n",
      "ep 3022: ep_len:129 episode reward: total was 8.070000. running mean: -5.695740\n",
      "ep 3022: ep_len:635 episode reward: total was -15.070000. running mean: -5.789482\n",
      "ep 3022: ep_len:515 episode reward: total was -4.080000. running mean: -5.772388\n",
      "epsilon:0.010000 episode_count: 21161. steps_count: 9358973.000000\n",
      "ep 3023: ep_len:585 episode reward: total was 5.460000. running mean: -5.660064\n",
      "ep 3023: ep_len:565 episode reward: total was 1.580000. running mean: -5.587663\n",
      "ep 3023: ep_len:565 episode reward: total was -5.100000. running mean: -5.582786\n",
      "ep 3023: ep_len:500 episode reward: total was -4.980000. running mean: -5.576759\n",
      "ep 3023: ep_len:56 episode reward: total was -9.500000. running mean: -5.615991\n",
      "ep 3023: ep_len:500 episode reward: total was 1.250000. running mean: -5.547331\n",
      "ep 3023: ep_len:500 episode reward: total was -25.640000. running mean: -5.748258\n",
      "epsilon:0.010000 episode_count: 21168. steps_count: 9362244.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3024: ep_len:555 episode reward: total was 4.560000. running mean: -5.645175\n",
      "ep 3024: ep_len:575 episode reward: total was -7.070000. running mean: -5.659423\n",
      "ep 3024: ep_len:79 episode reward: total was 0.040000. running mean: -5.602429\n",
      "ep 3024: ep_len:530 episode reward: total was 6.090000. running mean: -5.485505\n",
      "ep 3024: ep_len:128 episode reward: total was 8.070000. running mean: -5.349950\n",
      "ep 3024: ep_len:500 episode reward: total was -3.950000. running mean: -5.335950\n",
      "ep 3024: ep_len:605 episode reward: total was -13.400000. running mean: -5.416591\n",
      "epsilon:0.010000 episode_count: 21175. steps_count: 9365216.000000\n",
      "ep 3025: ep_len:550 episode reward: total was 6.400000. running mean: -5.298425\n",
      "ep 3025: ep_len:590 episode reward: total was -23.440000. running mean: -5.479841\n",
      "ep 3025: ep_len:545 episode reward: total was -3.190000. running mean: -5.456942\n",
      "ep 3025: ep_len:590 episode reward: total was 6.080000. running mean: -5.341573\n",
      "ep 3025: ep_len:55 episode reward: total was 4.000000. running mean: -5.248157\n",
      "ep 3025: ep_len:580 episode reward: total was 0.340000. running mean: -5.192276\n",
      "ep 3025: ep_len:600 episode reward: total was -6.060000. running mean: -5.200953\n",
      "epsilon:0.010000 episode_count: 21182. steps_count: 9368726.000000\n",
      "ep 3026: ep_len:575 episode reward: total was 11.470000. running mean: -5.034243\n",
      "ep 3026: ep_len:500 episode reward: total was -1.040000. running mean: -4.994301\n",
      "ep 3026: ep_len:79 episode reward: total was 0.040000. running mean: -4.943958\n",
      "ep 3026: ep_len:56 episode reward: total was 2.570000. running mean: -4.868818\n",
      "ep 3026: ep_len:96 episode reward: total was -9.970000. running mean: -4.919830\n",
      "ep 3026: ep_len:292 episode reward: total was 5.640000. running mean: -4.814232\n",
      "ep 3026: ep_len:515 episode reward: total was -18.400000. running mean: -4.950089\n",
      "epsilon:0.010000 episode_count: 21189. steps_count: 9370839.000000\n",
      "ep 3027: ep_len:850 episode reward: total was -53.220000. running mean: -5.432789\n",
      "ep 3027: ep_len:192 episode reward: total was 4.170000. running mean: -5.336761\n",
      "ep 3027: ep_len:650 episode reward: total was -6.720000. running mean: -5.350593\n",
      "ep 3027: ep_len:505 episode reward: total was 13.930000. running mean: -5.157787\n",
      "ep 3027: ep_len:3 episode reward: total was 0.000000. running mean: -5.106209\n",
      "ep 3027: ep_len:620 episode reward: total was -21.490000. running mean: -5.270047\n",
      "ep 3027: ep_len:530 episode reward: total was -4.020000. running mean: -5.257547\n",
      "epsilon:0.010000 episode_count: 21196. steps_count: 9374189.000000\n",
      "ep 3028: ep_len:645 episode reward: total was -15.680000. running mean: -5.361771\n",
      "ep 3028: ep_len:500 episode reward: total was 16.720000. running mean: -5.140954\n",
      "ep 3028: ep_len:540 episode reward: total was -23.340000. running mean: -5.322944\n",
      "ep 3028: ep_len:530 episode reward: total was 5.910000. running mean: -5.210615\n",
      "ep 3028: ep_len:51 episode reward: total was 5.000000. running mean: -5.108508\n",
      "ep 3028: ep_len:525 episode reward: total was 7.470000. running mean: -4.982723\n",
      "ep 3028: ep_len:500 episode reward: total was -5.910000. running mean: -4.991996\n",
      "epsilon:0.010000 episode_count: 21203. steps_count: 9377480.000000\n",
      "ep 3029: ep_len:505 episode reward: total was -32.500000. running mean: -5.267076\n",
      "ep 3029: ep_len:505 episode reward: total was -8.840000. running mean: -5.302805\n",
      "ep 3029: ep_len:62 episode reward: total was 3.040000. running mean: -5.219377\n",
      "ep 3029: ep_len:500 episode reward: total was -36.580000. running mean: -5.532984\n",
      "ep 3029: ep_len:3 episode reward: total was 0.000000. running mean: -5.477654\n",
      "ep 3029: ep_len:625 episode reward: total was -1.500000. running mean: -5.437877\n",
      "ep 3029: ep_len:590 episode reward: total was -16.970000. running mean: -5.553198\n",
      "epsilon:0.010000 episode_count: 21210. steps_count: 9380270.000000\n",
      "ep 3030: ep_len:575 episode reward: total was -6.590000. running mean: -5.563566\n",
      "ep 3030: ep_len:500 episode reward: total was -2.470000. running mean: -5.532631\n",
      "ep 3030: ep_len:645 episode reward: total was -52.450000. running mean: -6.001804\n",
      "ep 3030: ep_len:605 episode reward: total was -41.300000. running mean: -6.354786\n",
      "ep 3030: ep_len:3 episode reward: total was 0.000000. running mean: -6.291239\n",
      "ep 3030: ep_len:500 episode reward: total was -6.770000. running mean: -6.296026\n",
      "ep 3030: ep_len:193 episode reward: total was -4.370000. running mean: -6.276766\n",
      "epsilon:0.010000 episode_count: 21217. steps_count: 9383291.000000\n",
      "ep 3031: ep_len:595 episode reward: total was -1.610000. running mean: -6.230098\n",
      "ep 3031: ep_len:500 episode reward: total was 10.840000. running mean: -6.059397\n",
      "ep 3031: ep_len:500 episode reward: total was -26.610000. running mean: -6.264903\n",
      "ep 3031: ep_len:56 episode reward: total was 1.070000. running mean: -6.191554\n",
      "ep 3031: ep_len:3 episode reward: total was 0.000000. running mean: -6.129639\n",
      "ep 3031: ep_len:595 episode reward: total was -0.410000. running mean: -6.072442\n",
      "ep 3031: ep_len:555 episode reward: total was -11.770000. running mean: -6.129418\n",
      "epsilon:0.010000 episode_count: 21224. steps_count: 9386095.000000\n",
      "ep 3032: ep_len:500 episode reward: total was 11.270000. running mean: -5.955424\n",
      "ep 3032: ep_len:201 episode reward: total was -4.350000. running mean: -5.939369\n",
      "ep 3032: ep_len:79 episode reward: total was 0.040000. running mean: -5.879576\n",
      "ep 3032: ep_len:515 episode reward: total was 2.550000. running mean: -5.795280\n",
      "ep 3032: ep_len:3 episode reward: total was 0.000000. running mean: -5.737327\n",
      "ep 3032: ep_len:710 episode reward: total was -32.700000. running mean: -6.006954\n",
      "ep 3032: ep_len:500 episode reward: total was -2.610000. running mean: -5.972984\n",
      "epsilon:0.010000 episode_count: 21231. steps_count: 9388603.000000\n",
      "ep 3033: ep_len:500 episode reward: total was 6.250000. running mean: -5.850755\n",
      "ep 3033: ep_len:500 episode reward: total was 17.160000. running mean: -5.620647\n",
      "ep 3033: ep_len:590 episode reward: total was 2.480000. running mean: -5.539641\n",
      "ep 3033: ep_len:570 episode reward: total was 5.990000. running mean: -5.424344\n",
      "ep 3033: ep_len:3 episode reward: total was 0.000000. running mean: -5.370101\n",
      "ep 3033: ep_len:610 episode reward: total was 5.500000. running mean: -5.261400\n",
      "ep 3033: ep_len:565 episode reward: total was 2.650000. running mean: -5.182286\n",
      "epsilon:0.010000 episode_count: 21238. steps_count: 9391941.000000\n",
      "ep 3034: ep_len:134 episode reward: total was 2.080000. running mean: -5.109663\n",
      "ep 3034: ep_len:500 episode reward: total was -19.340000. running mean: -5.251966\n",
      "ep 3034: ep_len:700 episode reward: total was -34.190000. running mean: -5.541347\n",
      "ep 3034: ep_len:132 episode reward: total was 2.090000. running mean: -5.465033\n",
      "ep 3034: ep_len:52 episode reward: total was 5.000000. running mean: -5.360383\n",
      "ep 3034: ep_len:500 episode reward: total was -16.760000. running mean: -5.474379\n",
      "ep 3034: ep_len:515 episode reward: total was -4.920000. running mean: -5.468835\n",
      "epsilon:0.010000 episode_count: 21245. steps_count: 9394474.000000\n",
      "ep 3035: ep_len:550 episode reward: total was -13.100000. running mean: -5.545147\n",
      "ep 3035: ep_len:655 episode reward: total was -38.320000. running mean: -5.872895\n",
      "ep 3035: ep_len:500 episode reward: total was 0.440000. running mean: -5.809766\n",
      "ep 3035: ep_len:505 episode reward: total was -21.100000. running mean: -5.962669\n",
      "ep 3035: ep_len:3 episode reward: total was 0.000000. running mean: -5.903042\n",
      "ep 3035: ep_len:585 episode reward: total was -3.100000. running mean: -5.875012\n",
      "ep 3035: ep_len:343 episode reward: total was -1.260000. running mean: -5.828861\n",
      "epsilon:0.010000 episode_count: 21252. steps_count: 9397615.000000\n",
      "ep 3036: ep_len:615 episode reward: total was 11.020000. running mean: -5.660373\n",
      "ep 3036: ep_len:500 episode reward: total was 17.180000. running mean: -5.431969\n",
      "ep 3036: ep_len:570 episode reward: total was -5.390000. running mean: -5.431549\n",
      "ep 3036: ep_len:375 episode reward: total was 5.340000. running mean: -5.323834\n",
      "ep 3036: ep_len:3 episode reward: total was 0.000000. running mean: -5.270596\n",
      "ep 3036: ep_len:595 episode reward: total was 2.420000. running mean: -5.193690\n",
      "ep 3036: ep_len:575 episode reward: total was -0.520000. running mean: -5.146953\n",
      "epsilon:0.010000 episode_count: 21259. steps_count: 9400848.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3037: ep_len:520 episode reward: total was 0.740000. running mean: -5.088083\n",
      "ep 3037: ep_len:690 episode reward: total was -14.210000. running mean: -5.179302\n",
      "ep 3037: ep_len:510 episode reward: total was 1.450000. running mean: -5.113009\n",
      "ep 3037: ep_len:595 episode reward: total was -3.420000. running mean: -5.096079\n",
      "ep 3037: ep_len:3 episode reward: total was 0.000000. running mean: -5.045118\n",
      "ep 3037: ep_len:1205 episode reward: total was -150.170000. running mean: -6.496367\n",
      "ep 3037: ep_len:520 episode reward: total was 6.930000. running mean: -6.362104\n",
      "epsilon:0.010000 episode_count: 21266. steps_count: 9404891.000000\n",
      "ep 3038: ep_len:580 episode reward: total was 12.030000. running mean: -6.178183\n",
      "ep 3038: ep_len:307 episode reward: total was -1.840000. running mean: -6.134801\n",
      "ep 3038: ep_len:620 episode reward: total was 2.160000. running mean: -6.051853\n",
      "ep 3038: ep_len:500 episode reward: total was -30.600000. running mean: -6.297334\n",
      "ep 3038: ep_len:3 episode reward: total was 0.000000. running mean: -6.234361\n",
      "ep 3038: ep_len:525 episode reward: total was 12.640000. running mean: -6.045617\n",
      "ep 3038: ep_len:265 episode reward: total was -25.890000. running mean: -6.244061\n",
      "epsilon:0.010000 episode_count: 21273. steps_count: 9407691.000000\n",
      "ep 3039: ep_len:580 episode reward: total was 10.130000. running mean: -6.080320\n",
      "ep 3039: ep_len:510 episode reward: total was 3.080000. running mean: -5.988717\n",
      "ep 3039: ep_len:635 episode reward: total was -4.680000. running mean: -5.975630\n",
      "ep 3039: ep_len:500 episode reward: total was 12.600000. running mean: -5.789874\n",
      "ep 3039: ep_len:108 episode reward: total was 8.060000. running mean: -5.651375\n",
      "ep 3039: ep_len:505 episode reward: total was -28.950000. running mean: -5.884361\n",
      "ep 3039: ep_len:620 episode reward: total was -22.340000. running mean: -6.048918\n",
      "epsilon:0.010000 episode_count: 21280. steps_count: 9411149.000000\n",
      "ep 3040: ep_len:500 episode reward: total was 2.920000. running mean: -5.959229\n",
      "ep 3040: ep_len:381 episode reward: total was -5.310000. running mean: -5.952736\n",
      "ep 3040: ep_len:570 episode reward: total was -4.180000. running mean: -5.935009\n",
      "ep 3040: ep_len:595 episode reward: total was 8.570000. running mean: -5.789959\n",
      "ep 3040: ep_len:89 episode reward: total was 4.040000. running mean: -5.691659\n",
      "ep 3040: ep_len:700 episode reward: total was -2.560000. running mean: -5.660343\n",
      "ep 3040: ep_len:555 episode reward: total was -2.000000. running mean: -5.623739\n",
      "epsilon:0.010000 episode_count: 21287. steps_count: 9414539.000000\n",
      "ep 3041: ep_len:243 episode reward: total was 3.120000. running mean: -5.536302\n",
      "ep 3041: ep_len:500 episode reward: total was -28.340000. running mean: -5.764339\n",
      "ep 3041: ep_len:555 episode reward: total was -5.390000. running mean: -5.760595\n",
      "ep 3041: ep_len:550 episode reward: total was 4.570000. running mean: -5.657289\n",
      "ep 3041: ep_len:3 episode reward: total was 0.000000. running mean: -5.600717\n",
      "ep 3041: ep_len:525 episode reward: total was -4.700000. running mean: -5.591709\n",
      "ep 3041: ep_len:615 episode reward: total was -4.750000. running mean: -5.583292\n",
      "epsilon:0.010000 episode_count: 21294. steps_count: 9417530.000000\n",
      "ep 3042: ep_len:640 episode reward: total was 3.670000. running mean: -5.490759\n",
      "ep 3042: ep_len:530 episode reward: total was -16.140000. running mean: -5.597252\n",
      "ep 3042: ep_len:755 episode reward: total was -31.140000. running mean: -5.852679\n",
      "ep 3042: ep_len:420 episode reward: total was 7.380000. running mean: -5.720352\n",
      "ep 3042: ep_len:3 episode reward: total was 0.000000. running mean: -5.663149\n",
      "ep 3042: ep_len:515 episode reward: total was -9.920000. running mean: -5.705717\n",
      "ep 3042: ep_len:261 episode reward: total was -19.290000. running mean: -5.841560\n",
      "epsilon:0.010000 episode_count: 21301. steps_count: 9420654.000000\n",
      "ep 3043: ep_len:640 episode reward: total was 9.710000. running mean: -5.686045\n",
      "ep 3043: ep_len:580 episode reward: total was 1.670000. running mean: -5.612484\n",
      "ep 3043: ep_len:565 episode reward: total was -25.050000. running mean: -5.806859\n",
      "ep 3043: ep_len:500 episode reward: total was -16.560000. running mean: -5.914391\n",
      "ep 3043: ep_len:3 episode reward: total was 0.000000. running mean: -5.855247\n",
      "ep 3043: ep_len:595 episode reward: total was -6.180000. running mean: -5.858494\n",
      "ep 3043: ep_len:545 episode reward: total was -7.810000. running mean: -5.878009\n",
      "epsilon:0.010000 episode_count: 21308. steps_count: 9424082.000000\n",
      "ep 3044: ep_len:585 episode reward: total was -6.080000. running mean: -5.880029\n",
      "ep 3044: ep_len:540 episode reward: total was 3.910000. running mean: -5.782129\n",
      "ep 3044: ep_len:500 episode reward: total was 8.590000. running mean: -5.638408\n",
      "ep 3044: ep_len:545 episode reward: total was 12.970000. running mean: -5.452324\n",
      "ep 3044: ep_len:3 episode reward: total was 0.000000. running mean: -5.397800\n",
      "ep 3044: ep_len:530 episode reward: total was -14.650000. running mean: -5.490322\n",
      "ep 3044: ep_len:515 episode reward: total was 3.560000. running mean: -5.399819\n",
      "epsilon:0.010000 episode_count: 21315. steps_count: 9427300.000000\n",
      "ep 3045: ep_len:570 episode reward: total was 10.130000. running mean: -5.244521\n",
      "ep 3045: ep_len:355 episode reward: total was -7.810000. running mean: -5.270176\n",
      "ep 3045: ep_len:570 episode reward: total was -6.850000. running mean: -5.285974\n",
      "ep 3045: ep_len:605 episode reward: total was 13.930000. running mean: -5.093814\n",
      "ep 3045: ep_len:3 episode reward: total was 0.000000. running mean: -5.042876\n",
      "ep 3045: ep_len:575 episode reward: total was -10.840000. running mean: -5.100847\n",
      "ep 3045: ep_len:500 episode reward: total was -7.080000. running mean: -5.120639\n",
      "epsilon:0.010000 episode_count: 21322. steps_count: 9430478.000000\n",
      "ep 3046: ep_len:125 episode reward: total was -2.460000. running mean: -5.094033\n",
      "ep 3046: ep_len:500 episode reward: total was -14.360000. running mean: -5.186692\n",
      "ep 3046: ep_len:565 episode reward: total was -7.840000. running mean: -5.213225\n",
      "ep 3046: ep_len:530 episode reward: total was -10.690000. running mean: -5.267993\n",
      "ep 3046: ep_len:3 episode reward: total was 0.000000. running mean: -5.215313\n",
      "ep 3046: ep_len:251 episode reward: total was 7.660000. running mean: -5.086560\n",
      "ep 3046: ep_len:323 episode reward: total was -1.790000. running mean: -5.053594\n",
      "epsilon:0.010000 episode_count: 21329. steps_count: 9432775.000000\n",
      "ep 3047: ep_len:505 episode reward: total was 4.400000. running mean: -4.959058\n",
      "ep 3047: ep_len:590 episode reward: total was -23.760000. running mean: -5.147068\n",
      "ep 3047: ep_len:540 episode reward: total was -22.950000. running mean: -5.325097\n",
      "ep 3047: ep_len:605 episode reward: total was 5.070000. running mean: -5.221146\n",
      "ep 3047: ep_len:98 episode reward: total was 7.550000. running mean: -5.093435\n",
      "ep 3047: ep_len:540 episode reward: total was -15.890000. running mean: -5.201400\n",
      "ep 3047: ep_len:550 episode reward: total was -2.230000. running mean: -5.171686\n",
      "epsilon:0.010000 episode_count: 21336. steps_count: 9436203.000000\n",
      "ep 3048: ep_len:555 episode reward: total was -3.830000. running mean: -5.158270\n",
      "ep 3048: ep_len:545 episode reward: total was 20.920000. running mean: -4.897487\n",
      "ep 3048: ep_len:535 episode reward: total was -49.500000. running mean: -5.343512\n",
      "ep 3048: ep_len:500 episode reward: total was -24.630000. running mean: -5.536377\n",
      "ep 3048: ep_len:3 episode reward: total was 0.000000. running mean: -5.481013\n",
      "ep 3048: ep_len:635 episode reward: total was 7.000000. running mean: -5.356203\n",
      "ep 3048: ep_len:323 episode reward: total was -30.910000. running mean: -5.611741\n",
      "epsilon:0.010000 episode_count: 21343. steps_count: 9439299.000000\n",
      "ep 3049: ep_len:500 episode reward: total was 9.900000. running mean: -5.456624\n",
      "ep 3049: ep_len:500 episode reward: total was -20.730000. running mean: -5.609357\n",
      "ep 3049: ep_len:590 episode reward: total was -25.030000. running mean: -5.803564\n",
      "ep 3049: ep_len:520 episode reward: total was -10.970000. running mean: -5.855228\n",
      "ep 3049: ep_len:3 episode reward: total was 0.000000. running mean: -5.796676\n",
      "ep 3049: ep_len:710 episode reward: total was -9.150000. running mean: -5.830209\n",
      "ep 3049: ep_len:185 episode reward: total was -8.920000. running mean: -5.861107\n",
      "epsilon:0.010000 episode_count: 21350. steps_count: 9442307.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3050: ep_len:565 episode reward: total was -2.740000. running mean: -5.829896\n",
      "ep 3050: ep_len:570 episode reward: total was 17.410000. running mean: -5.597497\n",
      "ep 3050: ep_len:575 episode reward: total was -1.020000. running mean: -5.551722\n",
      "ep 3050: ep_len:500 episode reward: total was -4.490000. running mean: -5.541105\n",
      "ep 3050: ep_len:43 episode reward: total was 4.000000. running mean: -5.445694\n",
      "ep 3050: ep_len:500 episode reward: total was -6.620000. running mean: -5.457437\n",
      "ep 3050: ep_len:570 episode reward: total was -8.420000. running mean: -5.487062\n",
      "epsilon:0.010000 episode_count: 21357. steps_count: 9445630.000000\n",
      "ep 3051: ep_len:645 episode reward: total was -7.130000. running mean: -5.503492\n",
      "ep 3051: ep_len:605 episode reward: total was -2.580000. running mean: -5.474257\n",
      "ep 3051: ep_len:565 episode reward: total was -46.030000. running mean: -5.879814\n",
      "ep 3051: ep_len:530 episode reward: total was 5.040000. running mean: -5.770616\n",
      "ep 3051: ep_len:3 episode reward: total was 0.000000. running mean: -5.712910\n",
      "ep 3051: ep_len:555 episode reward: total was -5.740000. running mean: -5.713181\n",
      "ep 3051: ep_len:298 episode reward: total was -11.340000. running mean: -5.769449\n",
      "epsilon:0.010000 episode_count: 21364. steps_count: 9448831.000000\n",
      "ep 3052: ep_len:530 episode reward: total was 4.630000. running mean: -5.665455\n",
      "ep 3052: ep_len:294 episode reward: total was -8.870000. running mean: -5.697500\n",
      "ep 3052: ep_len:620 episode reward: total was -20.840000. running mean: -5.848925\n",
      "ep 3052: ep_len:600 episode reward: total was 11.610000. running mean: -5.674336\n",
      "ep 3052: ep_len:3 episode reward: total was 0.000000. running mean: -5.617592\n",
      "ep 3052: ep_len:505 episode reward: total was 3.600000. running mean: -5.525416\n",
      "ep 3052: ep_len:525 episode reward: total was 0.990000. running mean: -5.460262\n",
      "epsilon:0.010000 episode_count: 21371. steps_count: 9451908.000000\n",
      "ep 3053: ep_len:570 episode reward: total was 13.950000. running mean: -5.266160\n",
      "ep 3053: ep_len:500 episode reward: total was 21.240000. running mean: -5.001098\n",
      "ep 3053: ep_len:580 episode reward: total was -37.380000. running mean: -5.324887\n",
      "ep 3053: ep_len:129 episode reward: total was 5.600000. running mean: -5.215638\n",
      "ep 3053: ep_len:95 episode reward: total was 5.040000. running mean: -5.113082\n",
      "ep 3053: ep_len:655 episode reward: total was -16.760000. running mean: -5.229551\n",
      "ep 3053: ep_len:211 episode reward: total was -5.370000. running mean: -5.230956\n",
      "epsilon:0.010000 episode_count: 21378. steps_count: 9454648.000000\n",
      "ep 3054: ep_len:214 episode reward: total was 7.610000. running mean: -5.102546\n",
      "ep 3054: ep_len:535 episode reward: total was 19.290000. running mean: -4.858621\n",
      "ep 3054: ep_len:575 episode reward: total was -9.730000. running mean: -4.907334\n",
      "ep 3054: ep_len:505 episode reward: total was 9.930000. running mean: -4.758961\n",
      "ep 3054: ep_len:3 episode reward: total was 0.000000. running mean: -4.711371\n",
      "ep 3054: ep_len:595 episode reward: total was -1.620000. running mean: -4.680458\n",
      "ep 3054: ep_len:500 episode reward: total was -3.700000. running mean: -4.670653\n",
      "epsilon:0.010000 episode_count: 21385. steps_count: 9457575.000000\n",
      "ep 3055: ep_len:525 episode reward: total was -4.860000. running mean: -4.672547\n",
      "ep 3055: ep_len:500 episode reward: total was -12.980000. running mean: -4.755621\n",
      "ep 3055: ep_len:500 episode reward: total was -23.940000. running mean: -4.947465\n",
      "ep 3055: ep_len:500 episode reward: total was 3.020000. running mean: -4.867790\n",
      "ep 3055: ep_len:75 episode reward: total was -0.970000. running mean: -4.828812\n",
      "ep 3055: ep_len:500 episode reward: total was 4.590000. running mean: -4.734624\n",
      "ep 3055: ep_len:500 episode reward: total was -1.570000. running mean: -4.702978\n",
      "epsilon:0.010000 episode_count: 21392. steps_count: 9460675.000000\n",
      "ep 3056: ep_len:580 episode reward: total was -7.730000. running mean: -4.733248\n",
      "ep 3056: ep_len:520 episode reward: total was -24.630000. running mean: -4.932216\n",
      "ep 3056: ep_len:670 episode reward: total was 0.930000. running mean: -4.873594\n",
      "ep 3056: ep_len:132 episode reward: total was 6.100000. running mean: -4.763858\n",
      "ep 3056: ep_len:3 episode reward: total was 0.000000. running mean: -4.716219\n",
      "ep 3056: ep_len:610 episode reward: total was -54.670000. running mean: -5.215757\n",
      "ep 3056: ep_len:600 episode reward: total was -16.310000. running mean: -5.326699\n",
      "epsilon:0.010000 episode_count: 21399. steps_count: 9463790.000000\n",
      "ep 3057: ep_len:555 episode reward: total was -0.980000. running mean: -5.283232\n",
      "ep 3057: ep_len:525 episode reward: total was -9.540000. running mean: -5.325800\n",
      "ep 3057: ep_len:555 episode reward: total was 3.440000. running mean: -5.238142\n",
      "ep 3057: ep_len:147 episode reward: total was 0.090000. running mean: -5.184861\n",
      "ep 3057: ep_len:3 episode reward: total was 0.000000. running mean: -5.133012\n",
      "ep 3057: ep_len:680 episode reward: total was 4.440000. running mean: -5.037282\n",
      "ep 3057: ep_len:193 episode reward: total was -8.900000. running mean: -5.075909\n",
      "epsilon:0.010000 episode_count: 21406. steps_count: 9466448.000000\n",
      "ep 3058: ep_len:560 episode reward: total was -27.800000. running mean: -5.303150\n",
      "ep 3058: ep_len:555 episode reward: total was 18.910000. running mean: -5.061018\n",
      "ep 3058: ep_len:680 episode reward: total was -29.260000. running mean: -5.303008\n",
      "ep 3058: ep_len:56 episode reward: total was 1.560000. running mean: -5.234378\n",
      "ep 3058: ep_len:134 episode reward: total was 4.560000. running mean: -5.136434\n",
      "ep 3058: ep_len:500 episode reward: total was -15.760000. running mean: -5.242670\n",
      "ep 3058: ep_len:590 episode reward: total was -13.450000. running mean: -5.324743\n",
      "epsilon:0.010000 episode_count: 21413. steps_count: 9469523.000000\n",
      "ep 3059: ep_len:525 episode reward: total was -24.980000. running mean: -5.521296\n",
      "ep 3059: ep_len:550 episode reward: total was 11.580000. running mean: -5.350283\n",
      "ep 3059: ep_len:555 episode reward: total was -7.320000. running mean: -5.369980\n",
      "ep 3059: ep_len:505 episode reward: total was 0.890000. running mean: -5.307380\n",
      "ep 3059: ep_len:107 episode reward: total was 6.040000. running mean: -5.193906\n",
      "ep 3059: ep_len:780 episode reward: total was -48.740000. running mean: -5.629367\n",
      "ep 3059: ep_len:500 episode reward: total was -10.000000. running mean: -5.673074\n",
      "epsilon:0.010000 episode_count: 21420. steps_count: 9473045.000000\n",
      "ep 3060: ep_len:600 episode reward: total was -17.700000. running mean: -5.793343\n",
      "ep 3060: ep_len:510 episode reward: total was -42.540000. running mean: -6.160810\n",
      "ep 3060: ep_len:725 episode reward: total was -34.690000. running mean: -6.446101\n",
      "ep 3060: ep_len:500 episode reward: total was 8.380000. running mean: -6.297840\n",
      "ep 3060: ep_len:3 episode reward: total was 0.000000. running mean: -6.234862\n",
      "ep 3060: ep_len:505 episode reward: total was -13.370000. running mean: -6.306213\n",
      "ep 3060: ep_len:306 episode reward: total was -4.310000. running mean: -6.286251\n",
      "epsilon:0.010000 episode_count: 21427. steps_count: 9476194.000000\n",
      "ep 3061: ep_len:615 episode reward: total was 1.080000. running mean: -6.212589\n",
      "ep 3061: ep_len:500 episode reward: total was 21.220000. running mean: -5.938263\n",
      "ep 3061: ep_len:565 episode reward: total was -3.040000. running mean: -5.909280\n",
      "ep 3061: ep_len:500 episode reward: total was -16.090000. running mean: -6.011087\n",
      "ep 3061: ep_len:3 episode reward: total was 0.000000. running mean: -5.950977\n",
      "ep 3061: ep_len:530 episode reward: total was -14.810000. running mean: -6.039567\n",
      "ep 3061: ep_len:595 episode reward: total was -2.510000. running mean: -6.004271\n",
      "epsilon:0.010000 episode_count: 21434. steps_count: 9479502.000000\n",
      "ep 3062: ep_len:510 episode reward: total was -12.350000. running mean: -6.067728\n",
      "ep 3062: ep_len:605 episode reward: total was -21.880000. running mean: -6.225851\n",
      "ep 3062: ep_len:710 episode reward: total was -45.340000. running mean: -6.616993\n",
      "ep 3062: ep_len:500 episode reward: total was -5.600000. running mean: -6.606823\n",
      "ep 3062: ep_len:109 episode reward: total was 6.040000. running mean: -6.480355\n",
      "ep 3062: ep_len:635 episode reward: total was 5.010000. running mean: -6.365451\n",
      "ep 3062: ep_len:540 episode reward: total was -4.930000. running mean: -6.351096\n",
      "epsilon:0.010000 episode_count: 21441. steps_count: 9483111.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3063: ep_len:560 episode reward: total was -2.960000. running mean: -6.317185\n",
      "ep 3063: ep_len:590 episode reward: total was -16.890000. running mean: -6.422914\n",
      "ep 3063: ep_len:500 episode reward: total was 4.970000. running mean: -6.308984\n",
      "ep 3063: ep_len:500 episode reward: total was 8.550000. running mean: -6.160395\n",
      "ep 3063: ep_len:3 episode reward: total was 0.000000. running mean: -6.098791\n",
      "ep 3063: ep_len:610 episode reward: total was -3.640000. running mean: -6.074203\n",
      "ep 3063: ep_len:530 episode reward: total was -11.880000. running mean: -6.132261\n",
      "epsilon:0.010000 episode_count: 21448. steps_count: 9486404.000000\n",
      "ep 3064: ep_len:116 episode reward: total was 3.060000. running mean: -6.040338\n",
      "ep 3064: ep_len:645 episode reward: total was -23.000000. running mean: -6.209935\n",
      "ep 3064: ep_len:570 episode reward: total was 3.990000. running mean: -6.107935\n",
      "ep 3064: ep_len:500 episode reward: total was 0.850000. running mean: -6.038356\n",
      "ep 3064: ep_len:98 episode reward: total was 3.510000. running mean: -5.942873\n",
      "ep 3064: ep_len:221 episode reward: total was 9.130000. running mean: -5.792144\n",
      "ep 3064: ep_len:296 episode reward: total was -16.890000. running mean: -5.903122\n",
      "epsilon:0.010000 episode_count: 21455. steps_count: 9488850.000000\n",
      "ep 3065: ep_len:500 episode reward: total was 10.250000. running mean: -5.741591\n",
      "ep 3065: ep_len:500 episode reward: total was -32.680000. running mean: -6.010975\n",
      "ep 3065: ep_len:500 episode reward: total was 6.900000. running mean: -5.881865\n",
      "ep 3065: ep_len:540 episode reward: total was -11.990000. running mean: -5.942947\n",
      "ep 3065: ep_len:50 episode reward: total was 3.500000. running mean: -5.848517\n",
      "ep 3065: ep_len:500 episode reward: total was -3.260000. running mean: -5.822632\n",
      "ep 3065: ep_len:520 episode reward: total was -6.910000. running mean: -5.833506\n",
      "epsilon:0.010000 episode_count: 21462. steps_count: 9491960.000000\n",
      "ep 3066: ep_len:570 episode reward: total was 14.460000. running mean: -5.630571\n",
      "ep 3066: ep_len:500 episode reward: total was -35.090000. running mean: -5.925165\n",
      "ep 3066: ep_len:645 episode reward: total was -6.190000. running mean: -5.927813\n",
      "ep 3066: ep_len:550 episode reward: total was 5.430000. running mean: -5.814235\n",
      "ep 3066: ep_len:106 episode reward: total was 8.030000. running mean: -5.675793\n",
      "ep 3066: ep_len:279 episode reward: total was -7.830000. running mean: -5.697335\n",
      "ep 3066: ep_len:585 episode reward: total was -12.450000. running mean: -5.764862\n",
      "epsilon:0.010000 episode_count: 21469. steps_count: 9495195.000000\n",
      "ep 3067: ep_len:211 episode reward: total was 5.110000. running mean: -5.656113\n",
      "ep 3067: ep_len:550 episode reward: total was -7.370000. running mean: -5.673252\n",
      "ep 3067: ep_len:515 episode reward: total was 3.420000. running mean: -5.582319\n",
      "ep 3067: ep_len:545 episode reward: total was -26.100000. running mean: -5.787496\n",
      "ep 3067: ep_len:3 episode reward: total was 0.000000. running mean: -5.729621\n",
      "ep 3067: ep_len:510 episode reward: total was -10.240000. running mean: -5.774725\n",
      "ep 3067: ep_len:520 episode reward: total was -7.560000. running mean: -5.792578\n",
      "epsilon:0.010000 episode_count: 21476. steps_count: 9498049.000000\n",
      "ep 3068: ep_len:640 episode reward: total was -13.210000. running mean: -5.866752\n",
      "ep 3068: ep_len:525 episode reward: total was 6.240000. running mean: -5.745684\n",
      "ep 3068: ep_len:535 episode reward: total was -4.360000. running mean: -5.731828\n",
      "ep 3068: ep_len:505 episode reward: total was -19.110000. running mean: -5.865609\n",
      "ep 3068: ep_len:3 episode reward: total was 0.000000. running mean: -5.806953\n",
      "ep 3068: ep_len:500 episode reward: total was -6.950000. running mean: -5.818384\n",
      "ep 3068: ep_len:520 episode reward: total was -27.970000. running mean: -6.039900\n",
      "epsilon:0.010000 episode_count: 21483. steps_count: 9501277.000000\n",
      "ep 3069: ep_len:560 episode reward: total was 13.980000. running mean: -5.839701\n",
      "ep 3069: ep_len:535 episode reward: total was 8.090000. running mean: -5.700404\n",
      "ep 3069: ep_len:570 episode reward: total was -18.100000. running mean: -5.824400\n",
      "ep 3069: ep_len:605 episode reward: total was 16.010000. running mean: -5.606056\n",
      "ep 3069: ep_len:86 episode reward: total was 5.020000. running mean: -5.499795\n",
      "ep 3069: ep_len:620 episode reward: total was -1.970000. running mean: -5.464497\n",
      "ep 3069: ep_len:500 episode reward: total was -5.930000. running mean: -5.469152\n",
      "epsilon:0.010000 episode_count: 21490. steps_count: 9504753.000000\n",
      "ep 3070: ep_len:600 episode reward: total was 22.000000. running mean: -5.194461\n",
      "ep 3070: ep_len:555 episode reward: total was -6.840000. running mean: -5.210916\n",
      "ep 3070: ep_len:560 episode reward: total was -15.850000. running mean: -5.317307\n",
      "ep 3070: ep_len:105 episode reward: total was 1.580000. running mean: -5.248334\n",
      "ep 3070: ep_len:95 episode reward: total was 6.050000. running mean: -5.135351\n",
      "ep 3070: ep_len:540 episode reward: total was 4.650000. running mean: -5.037497\n",
      "ep 3070: ep_len:580 episode reward: total was 2.440000. running mean: -4.962722\n",
      "epsilon:0.010000 episode_count: 21497. steps_count: 9507788.000000\n",
      "ep 3071: ep_len:620 episode reward: total was -14.720000. running mean: -5.060295\n",
      "ep 3071: ep_len:550 episode reward: total was -13.970000. running mean: -5.149392\n",
      "ep 3071: ep_len:448 episode reward: total was 6.290000. running mean: -5.034998\n",
      "ep 3071: ep_len:520 episode reward: total was 3.060000. running mean: -4.954048\n",
      "ep 3071: ep_len:113 episode reward: total was 5.530000. running mean: -4.849208\n",
      "ep 3071: ep_len:500 episode reward: total was -8.240000. running mean: -4.883116\n",
      "ep 3071: ep_len:535 episode reward: total was -16.010000. running mean: -4.994384\n",
      "epsilon:0.010000 episode_count: 21504. steps_count: 9511074.000000\n",
      "ep 3072: ep_len:630 episode reward: total was -15.250000. running mean: -5.096941\n",
      "ep 3072: ep_len:248 episode reward: total was -27.350000. running mean: -5.319471\n",
      "ep 3072: ep_len:700 episode reward: total was -7.610000. running mean: -5.342376\n",
      "ep 3072: ep_len:510 episode reward: total was -27.150000. running mean: -5.560453\n",
      "ep 3072: ep_len:45 episode reward: total was 3.000000. running mean: -5.474848\n",
      "ep 3072: ep_len:500 episode reward: total was -6.770000. running mean: -5.487800\n",
      "ep 3072: ep_len:294 episode reward: total was -1.250000. running mean: -5.445422\n",
      "epsilon:0.010000 episode_count: 21511. steps_count: 9514001.000000\n",
      "ep 3073: ep_len:590 episode reward: total was -12.160000. running mean: -5.512567\n",
      "ep 3073: ep_len:645 episode reward: total was -18.050000. running mean: -5.637942\n",
      "ep 3073: ep_len:565 episode reward: total was -20.890000. running mean: -5.790462\n",
      "ep 3073: ep_len:515 episode reward: total was -29.130000. running mean: -6.023858\n",
      "ep 3073: ep_len:3 episode reward: total was 0.000000. running mean: -5.963619\n",
      "ep 3073: ep_len:600 episode reward: total was -17.530000. running mean: -6.079283\n",
      "ep 3073: ep_len:600 episode reward: total was -10.430000. running mean: -6.122790\n",
      "epsilon:0.010000 episode_count: 21518. steps_count: 9517519.000000\n",
      "ep 3074: ep_len:605 episode reward: total was 11.220000. running mean: -5.949362\n",
      "ep 3074: ep_len:555 episode reward: total was -10.880000. running mean: -5.998669\n",
      "ep 3074: ep_len:625 episode reward: total was -9.400000. running mean: -6.032682\n",
      "ep 3074: ep_len:510 episode reward: total was -19.130000. running mean: -6.163655\n",
      "ep 3074: ep_len:3 episode reward: total was 0.000000. running mean: -6.102019\n",
      "ep 3074: ep_len:510 episode reward: total was 7.470000. running mean: -5.966298\n",
      "ep 3074: ep_len:600 episode reward: total was -16.340000. running mean: -6.070035\n",
      "epsilon:0.010000 episode_count: 21525. steps_count: 9520927.000000\n",
      "ep 3075: ep_len:500 episode reward: total was 13.800000. running mean: -5.871335\n",
      "ep 3075: ep_len:500 episode reward: total was -7.990000. running mean: -5.892522\n",
      "ep 3075: ep_len:555 episode reward: total was -0.570000. running mean: -5.839296\n",
      "ep 3075: ep_len:505 episode reward: total was -8.520000. running mean: -5.866104\n",
      "ep 3075: ep_len:98 episode reward: total was 4.030000. running mean: -5.767142\n",
      "ep 3075: ep_len:333 episode reward: total was -3.340000. running mean: -5.742871\n",
      "ep 3075: ep_len:595 episode reward: total was -4.490000. running mean: -5.730342\n",
      "epsilon:0.010000 episode_count: 21532. steps_count: 9524013.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3076: ep_len:545 episode reward: total was 2.410000. running mean: -5.648939\n",
      "ep 3076: ep_len:500 episode reward: total was 11.700000. running mean: -5.475450\n",
      "ep 3076: ep_len:500 episode reward: total was -5.660000. running mean: -5.477295\n",
      "ep 3076: ep_len:429 episode reward: total was -3.140000. running mean: -5.453922\n",
      "ep 3076: ep_len:83 episode reward: total was 2.500000. running mean: -5.374383\n",
      "ep 3076: ep_len:590 episode reward: total was -4.410000. running mean: -5.364739\n",
      "ep 3076: ep_len:530 episode reward: total was -11.110000. running mean: -5.422192\n",
      "epsilon:0.010000 episode_count: 21539. steps_count: 9527190.000000\n",
      "ep 3077: ep_len:229 episode reward: total was -0.410000. running mean: -5.372070\n",
      "ep 3077: ep_len:500 episode reward: total was 13.610000. running mean: -5.182249\n",
      "ep 3077: ep_len:600 episode reward: total was 0.670000. running mean: -5.123727\n",
      "ep 3077: ep_len:500 episode reward: total was 5.950000. running mean: -5.012989\n",
      "ep 3077: ep_len:3 episode reward: total was 0.000000. running mean: -4.962859\n",
      "ep 3077: ep_len:500 episode reward: total was -0.280000. running mean: -4.916031\n",
      "ep 3077: ep_len:309 episode reward: total was -21.800000. running mean: -5.084870\n",
      "epsilon:0.010000 episode_count: 21546. steps_count: 9529831.000000\n",
      "ep 3078: ep_len:610 episode reward: total was -12.750000. running mean: -5.161522\n",
      "ep 3078: ep_len:755 episode reward: total was -57.020000. running mean: -5.680107\n",
      "ep 3078: ep_len:545 episode reward: total was 0.910000. running mean: -5.614205\n",
      "ep 3078: ep_len:530 episode reward: total was -13.010000. running mean: -5.688163\n",
      "ep 3078: ep_len:113 episode reward: total was -13.460000. running mean: -5.765882\n",
      "ep 3078: ep_len:650 episode reward: total was -2.690000. running mean: -5.735123\n",
      "ep 3078: ep_len:575 episode reward: total was -8.640000. running mean: -5.764172\n",
      "epsilon:0.010000 episode_count: 21553. steps_count: 9533609.000000\n",
      "ep 3079: ep_len:505 episode reward: total was -16.400000. running mean: -5.870530\n",
      "ep 3079: ep_len:295 episode reward: total was -34.810000. running mean: -6.159925\n",
      "ep 3079: ep_len:387 episode reward: total was 6.760000. running mean: -6.030725\n",
      "ep 3079: ep_len:510 episode reward: total was -18.660000. running mean: -6.157018\n",
      "ep 3079: ep_len:100 episode reward: total was 4.530000. running mean: -6.050148\n",
      "ep 3079: ep_len:166 episode reward: total was 5.620000. running mean: -5.933447\n",
      "ep 3079: ep_len:575 episode reward: total was -11.570000. running mean: -5.989812\n",
      "epsilon:0.010000 episode_count: 21560. steps_count: 9536147.000000\n",
      "ep 3080: ep_len:540 episode reward: total was 4.920000. running mean: -5.880714\n",
      "ep 3080: ep_len:615 episode reward: total was -1.480000. running mean: -5.836707\n",
      "ep 3080: ep_len:595 episode reward: total was -4.190000. running mean: -5.820240\n",
      "ep 3080: ep_len:365 episode reward: total was 4.310000. running mean: -5.718937\n",
      "ep 3080: ep_len:110 episode reward: total was 4.030000. running mean: -5.621448\n",
      "ep 3080: ep_len:500 episode reward: total was -2.770000. running mean: -5.592934\n",
      "ep 3080: ep_len:520 episode reward: total was -29.130000. running mean: -5.828304\n",
      "epsilon:0.010000 episode_count: 21567. steps_count: 9539392.000000\n",
      "ep 3081: ep_len:550 episode reward: total was 4.160000. running mean: -5.728421\n",
      "ep 3081: ep_len:353 episode reward: total was -14.770000. running mean: -5.818837\n",
      "ep 3081: ep_len:500 episode reward: total was -4.430000. running mean: -5.804949\n",
      "ep 3081: ep_len:565 episode reward: total was 7.560000. running mean: -5.671299\n",
      "ep 3081: ep_len:3 episode reward: total was 0.000000. running mean: -5.614586\n",
      "ep 3081: ep_len:550 episode reward: total was 9.010000. running mean: -5.468340\n",
      "ep 3081: ep_len:530 episode reward: total was -10.560000. running mean: -5.519257\n",
      "epsilon:0.010000 episode_count: 21574. steps_count: 9542443.000000\n",
      "ep 3082: ep_len:530 episode reward: total was 2.460000. running mean: -5.439464\n",
      "ep 3082: ep_len:500 episode reward: total was -36.160000. running mean: -5.746670\n",
      "ep 3082: ep_len:500 episode reward: total was 0.270000. running mean: -5.686503\n",
      "ep 3082: ep_len:530 episode reward: total was 4.930000. running mean: -5.580338\n",
      "ep 3082: ep_len:3 episode reward: total was 0.000000. running mean: -5.524535\n",
      "ep 3082: ep_len:975 episode reward: total was -82.370000. running mean: -6.292989\n",
      "ep 3082: ep_len:585 episode reward: total was -10.570000. running mean: -6.335759\n",
      "epsilon:0.010000 episode_count: 21581. steps_count: 9546066.000000\n",
      "ep 3083: ep_len:249 episode reward: total was 4.140000. running mean: -6.231002\n",
      "ep 3083: ep_len:500 episode reward: total was -22.690000. running mean: -6.395592\n",
      "ep 3083: ep_len:650 episode reward: total was -12.350000. running mean: -6.455136\n",
      "ep 3083: ep_len:396 episode reward: total was 4.320000. running mean: -6.347384\n",
      "ep 3083: ep_len:3 episode reward: total was 0.000000. running mean: -6.283911\n",
      "ep 3083: ep_len:540 episode reward: total was -2.070000. running mean: -6.241771\n",
      "ep 3083: ep_len:207 episode reward: total was -3.330000. running mean: -6.212654\n",
      "epsilon:0.010000 episode_count: 21588. steps_count: 9548611.000000\n",
      "ep 3084: ep_len:120 episode reward: total was 1.600000. running mean: -6.134527\n",
      "ep 3084: ep_len:525 episode reward: total was -6.450000. running mean: -6.137682\n",
      "ep 3084: ep_len:635 episode reward: total was -1.620000. running mean: -6.092505\n",
      "ep 3084: ep_len:500 episode reward: total was 0.500000. running mean: -6.026580\n",
      "ep 3084: ep_len:90 episode reward: total was 4.540000. running mean: -5.920914\n",
      "ep 3084: ep_len:570 episode reward: total was 10.130000. running mean: -5.760405\n",
      "ep 3084: ep_len:585 episode reward: total was -9.950000. running mean: -5.802301\n",
      "epsilon:0.010000 episode_count: 21595. steps_count: 9551636.000000\n",
      "ep 3085: ep_len:505 episode reward: total was 10.120000. running mean: -5.643078\n",
      "ep 3085: ep_len:595 episode reward: total was -9.960000. running mean: -5.686247\n",
      "ep 3085: ep_len:79 episode reward: total was 0.040000. running mean: -5.628985\n",
      "ep 3085: ep_len:550 episode reward: total was -34.220000. running mean: -5.914895\n",
      "ep 3085: ep_len:126 episode reward: total was 5.560000. running mean: -5.800146\n",
      "ep 3085: ep_len:500 episode reward: total was 2.250000. running mean: -5.719645\n",
      "ep 3085: ep_len:640 episode reward: total was -9.290000. running mean: -5.755348\n",
      "epsilon:0.010000 episode_count: 21602. steps_count: 9554631.000000\n",
      "ep 3086: ep_len:505 episode reward: total was -27.420000. running mean: -5.971995\n",
      "ep 3086: ep_len:620 episode reward: total was 11.630000. running mean: -5.795975\n",
      "ep 3086: ep_len:453 episode reward: total was 6.750000. running mean: -5.670515\n",
      "ep 3086: ep_len:550 episode reward: total was 11.450000. running mean: -5.499310\n",
      "ep 3086: ep_len:87 episode reward: total was 5.050000. running mean: -5.393817\n",
      "ep 3086: ep_len:1005 episode reward: total was -75.860000. running mean: -6.098478\n",
      "ep 3086: ep_len:585 episode reward: total was -12.320000. running mean: -6.160694\n",
      "epsilon:0.010000 episode_count: 21609. steps_count: 9558436.000000\n",
      "ep 3087: ep_len:210 episode reward: total was 4.120000. running mean: -6.057887\n",
      "ep 3087: ep_len:500 episode reward: total was 9.580000. running mean: -5.901508\n",
      "ep 3087: ep_len:399 episode reward: total was -2.780000. running mean: -5.870293\n",
      "ep 3087: ep_len:550 episode reward: total was -5.580000. running mean: -5.867390\n",
      "ep 3087: ep_len:3 episode reward: total was 0.000000. running mean: -5.808716\n",
      "ep 3087: ep_len:1110 episode reward: total was -86.760000. running mean: -6.618229\n",
      "ep 3087: ep_len:540 episode reward: total was -12.980000. running mean: -6.681847\n",
      "epsilon:0.010000 episode_count: 21616. steps_count: 9561748.000000\n",
      "ep 3088: ep_len:505 episode reward: total was 1.780000. running mean: -6.597228\n",
      "ep 3088: ep_len:371 episode reward: total was -3.750000. running mean: -6.568756\n",
      "ep 3088: ep_len:640 episode reward: total was -35.780000. running mean: -6.860868\n",
      "ep 3088: ep_len:565 episode reward: total was -5.160000. running mean: -6.843860\n",
      "ep 3088: ep_len:3 episode reward: total was 0.000000. running mean: -6.775421\n",
      "ep 3088: ep_len:505 episode reward: total was -26.020000. running mean: -6.967867\n",
      "ep 3088: ep_len:530 episode reward: total was -7.080000. running mean: -6.968988\n",
      "epsilon:0.010000 episode_count: 21623. steps_count: 9564867.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3089: ep_len:505 episode reward: total was 4.370000. running mean: -6.855598\n",
      "ep 3089: ep_len:605 episode reward: total was 4.720000. running mean: -6.739842\n",
      "ep 3089: ep_len:575 episode reward: total was -1.600000. running mean: -6.688444\n",
      "ep 3089: ep_len:535 episode reward: total was 7.530000. running mean: -6.546259\n",
      "ep 3089: ep_len:3 episode reward: total was 0.000000. running mean: -6.480797\n",
      "ep 3089: ep_len:585 episode reward: total was -31.070000. running mean: -6.726689\n",
      "ep 3089: ep_len:565 episode reward: total was -3.270000. running mean: -6.692122\n",
      "epsilon:0.010000 episode_count: 21630. steps_count: 9568240.000000\n",
      "ep 3090: ep_len:640 episode reward: total was -29.300000. running mean: -6.918201\n",
      "ep 3090: ep_len:635 episode reward: total was -20.350000. running mean: -7.052519\n",
      "ep 3090: ep_len:540 episode reward: total was -9.220000. running mean: -7.074194\n",
      "ep 3090: ep_len:565 episode reward: total was -5.390000. running mean: -7.057352\n",
      "ep 3090: ep_len:88 episode reward: total was -1.460000. running mean: -7.001378\n",
      "ep 3090: ep_len:810 episode reward: total was -50.240000. running mean: -7.433764\n",
      "ep 3090: ep_len:500 episode reward: total was -14.880000. running mean: -7.508227\n",
      "epsilon:0.010000 episode_count: 21637. steps_count: 9572018.000000\n",
      "ep 3091: ep_len:540 episode reward: total was -10.500000. running mean: -7.538144\n",
      "ep 3091: ep_len:500 episode reward: total was -17.260000. running mean: -7.635363\n",
      "ep 3091: ep_len:605 episode reward: total was -10.690000. running mean: -7.665909\n",
      "ep 3091: ep_len:158 episode reward: total was 3.630000. running mean: -7.552950\n",
      "ep 3091: ep_len:3 episode reward: total was 0.000000. running mean: -7.477421\n",
      "ep 3091: ep_len:540 episode reward: total was -8.930000. running mean: -7.491946\n",
      "ep 3091: ep_len:615 episode reward: total was -10.320000. running mean: -7.520227\n",
      "epsilon:0.010000 episode_count: 21644. steps_count: 9574979.000000\n",
      "ep 3092: ep_len:500 episode reward: total was -29.000000. running mean: -7.735025\n",
      "ep 3092: ep_len:500 episode reward: total was 5.740000. running mean: -7.600275\n",
      "ep 3092: ep_len:650 episode reward: total was -3.610000. running mean: -7.560372\n",
      "ep 3092: ep_len:575 episode reward: total was -7.470000. running mean: -7.559468\n",
      "ep 3092: ep_len:3 episode reward: total was 0.000000. running mean: -7.483873\n",
      "ep 3092: ep_len:311 episode reward: total was 3.660000. running mean: -7.372435\n",
      "ep 3092: ep_len:595 episode reward: total was -8.400000. running mean: -7.382710\n",
      "epsilon:0.010000 episode_count: 21651. steps_count: 9578113.000000\n",
      "ep 3093: ep_len:131 episode reward: total was -8.420000. running mean: -7.393083\n",
      "ep 3093: ep_len:500 episode reward: total was 12.190000. running mean: -7.197252\n",
      "ep 3093: ep_len:520 episode reward: total was -9.990000. running mean: -7.225180\n",
      "ep 3093: ep_len:500 episode reward: total was 7.020000. running mean: -7.082728\n",
      "ep 3093: ep_len:3 episode reward: total was 0.000000. running mean: -7.011901\n",
      "ep 3093: ep_len:555 episode reward: total was -27.430000. running mean: -7.216082\n",
      "ep 3093: ep_len:590 episode reward: total was -10.060000. running mean: -7.244521\n",
      "epsilon:0.010000 episode_count: 21658. steps_count: 9580912.000000\n",
      "ep 3094: ep_len:655 episode reward: total was -12.660000. running mean: -7.298676\n",
      "ep 3094: ep_len:500 episode reward: total was 12.720000. running mean: -7.098489\n",
      "ep 3094: ep_len:650 episode reward: total was 2.680000. running mean: -7.000704\n",
      "ep 3094: ep_len:417 episode reward: total was 5.890000. running mean: -6.871797\n",
      "ep 3094: ep_len:3 episode reward: total was 0.000000. running mean: -6.803079\n",
      "ep 3094: ep_len:520 episode reward: total was 9.140000. running mean: -6.643648\n",
      "ep 3094: ep_len:500 episode reward: total was 1.430000. running mean: -6.562912\n",
      "epsilon:0.010000 episode_count: 21665. steps_count: 9584157.000000\n",
      "ep 3095: ep_len:645 episode reward: total was 8.160000. running mean: -6.415683\n",
      "ep 3095: ep_len:500 episode reward: total was 7.570000. running mean: -6.275826\n",
      "ep 3095: ep_len:500 episode reward: total was -3.740000. running mean: -6.250468\n",
      "ep 3095: ep_len:500 episode reward: total was 9.540000. running mean: -6.092563\n",
      "ep 3095: ep_len:3 episode reward: total was 0.000000. running mean: -6.031637\n",
      "ep 3095: ep_len:500 episode reward: total was -1.260000. running mean: -5.983921\n",
      "ep 3095: ep_len:545 episode reward: total was -24.110000. running mean: -6.165182\n",
      "epsilon:0.010000 episode_count: 21672. steps_count: 9587350.000000\n",
      "ep 3096: ep_len:224 episode reward: total was 8.090000. running mean: -6.022630\n",
      "ep 3096: ep_len:565 episode reward: total was 20.370000. running mean: -5.758704\n",
      "ep 3096: ep_len:565 episode reward: total was -14.210000. running mean: -5.843217\n",
      "ep 3096: ep_len:510 episode reward: total was 5.400000. running mean: -5.730784\n",
      "ep 3096: ep_len:3 episode reward: total was 0.000000. running mean: -5.673477\n",
      "ep 3096: ep_len:505 episode reward: total was 4.660000. running mean: -5.570142\n",
      "ep 3096: ep_len:515 episode reward: total was -4.080000. running mean: -5.555240\n",
      "epsilon:0.010000 episode_count: 21679. steps_count: 9590237.000000\n",
      "ep 3097: ep_len:520 episode reward: total was -2.660000. running mean: -5.526288\n",
      "ep 3097: ep_len:705 episode reward: total was -23.260000. running mean: -5.703625\n",
      "ep 3097: ep_len:500 episode reward: total was 0.730000. running mean: -5.639289\n",
      "ep 3097: ep_len:373 episode reward: total was 4.330000. running mean: -5.539596\n",
      "ep 3097: ep_len:88 episode reward: total was 5.540000. running mean: -5.428800\n",
      "ep 3097: ep_len:505 episode reward: total was 3.420000. running mean: -5.340312\n",
      "ep 3097: ep_len:550 episode reward: total was -7.800000. running mean: -5.364909\n",
      "epsilon:0.010000 episode_count: 21686. steps_count: 9593478.000000\n",
      "ep 3098: ep_len:500 episode reward: total was -9.840000. running mean: -5.409660\n",
      "ep 3098: ep_len:580 episode reward: total was -17.080000. running mean: -5.526363\n",
      "ep 3098: ep_len:670 episode reward: total was -24.230000. running mean: -5.713400\n",
      "ep 3098: ep_len:56 episode reward: total was -0.930000. running mean: -5.665566\n",
      "ep 3098: ep_len:3 episode reward: total was 0.000000. running mean: -5.608910\n",
      "ep 3098: ep_len:660 episode reward: total was -35.790000. running mean: -5.910721\n",
      "ep 3098: ep_len:333 episode reward: total was -10.280000. running mean: -5.954414\n",
      "epsilon:0.010000 episode_count: 21693. steps_count: 9596280.000000\n",
      "ep 3099: ep_len:600 episode reward: total was -5.050000. running mean: -5.945369\n",
      "ep 3099: ep_len:500 episode reward: total was -4.750000. running mean: -5.933416\n",
      "ep 3099: ep_len:650 episode reward: total was -25.450000. running mean: -6.128582\n",
      "ep 3099: ep_len:500 episode reward: total was 13.400000. running mean: -5.933296\n",
      "ep 3099: ep_len:68 episode reward: total was 6.020000. running mean: -5.813763\n",
      "ep 3099: ep_len:500 episode reward: total was -15.550000. running mean: -5.911125\n",
      "ep 3099: ep_len:560 episode reward: total was -13.510000. running mean: -5.987114\n",
      "epsilon:0.010000 episode_count: 21700. steps_count: 9599658.000000\n",
      "ep 3100: ep_len:230 episode reward: total was 2.600000. running mean: -5.901243\n",
      "ep 3100: ep_len:575 episode reward: total was 14.350000. running mean: -5.698730\n",
      "ep 3100: ep_len:585 episode reward: total was 2.960000. running mean: -5.612143\n",
      "ep 3100: ep_len:540 episode reward: total was -9.580000. running mean: -5.651822\n",
      "ep 3100: ep_len:100 episode reward: total was 4.040000. running mean: -5.554903\n",
      "ep 3100: ep_len:605 episode reward: total was -22.830000. running mean: -5.727654\n",
      "ep 3100: ep_len:302 episode reward: total was -16.370000. running mean: -5.834078\n",
      "epsilon:0.010000 episode_count: 21707. steps_count: 9602595.000000\n",
      "ep 3101: ep_len:615 episode reward: total was 11.440000. running mean: -5.661337\n",
      "ep 3101: ep_len:201 episode reward: total was 0.640000. running mean: -5.598324\n",
      "ep 3101: ep_len:665 episode reward: total was -30.220000. running mean: -5.844540\n",
      "ep 3101: ep_len:590 episode reward: total was 1.410000. running mean: -5.771995\n",
      "ep 3101: ep_len:105 episode reward: total was 7.050000. running mean: -5.643775\n",
      "ep 3101: ep_len:500 episode reward: total was 3.240000. running mean: -5.554937\n",
      "ep 3101: ep_len:570 episode reward: total was -10.380000. running mean: -5.603188\n",
      "epsilon:0.010000 episode_count: 21714. steps_count: 9605841.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3102: ep_len:580 episode reward: total was -11.710000. running mean: -5.664256\n",
      "ep 3102: ep_len:500 episode reward: total was 1.210000. running mean: -5.595514\n",
      "ep 3102: ep_len:660 episode reward: total was -35.990000. running mean: -5.899458\n",
      "ep 3102: ep_len:515 episode reward: total was -5.000000. running mean: -5.890464\n",
      "ep 3102: ep_len:49 episode reward: total was 4.500000. running mean: -5.786559\n",
      "ep 3102: ep_len:660 episode reward: total was -4.660000. running mean: -5.775294\n",
      "ep 3102: ep_len:595 episode reward: total was -16.090000. running mean: -5.878441\n",
      "epsilon:0.010000 episode_count: 21721. steps_count: 9609400.000000\n",
      "ep 3103: ep_len:555 episode reward: total was 2.920000. running mean: -5.790456\n",
      "ep 3103: ep_len:535 episode reward: total was -4.140000. running mean: -5.773952\n",
      "ep 3103: ep_len:570 episode reward: total was -4.290000. running mean: -5.759112\n",
      "ep 3103: ep_len:520 episode reward: total was 5.910000. running mean: -5.642421\n",
      "ep 3103: ep_len:96 episode reward: total was 6.540000. running mean: -5.520597\n",
      "ep 3103: ep_len:263 episode reward: total was -0.790000. running mean: -5.473291\n",
      "ep 3103: ep_len:500 episode reward: total was -71.310000. running mean: -6.131658\n",
      "epsilon:0.010000 episode_count: 21728. steps_count: 9612439.000000\n",
      "ep 3104: ep_len:500 episode reward: total was 7.910000. running mean: -5.991241\n",
      "ep 3104: ep_len:500 episode reward: total was -6.030000. running mean: -5.991629\n",
      "ep 3104: ep_len:500 episode reward: total was 3.040000. running mean: -5.901313\n",
      "ep 3104: ep_len:575 episode reward: total was -6.600000. running mean: -5.908300\n",
      "ep 3104: ep_len:3 episode reward: total was 0.000000. running mean: -5.849217\n",
      "ep 3104: ep_len:500 episode reward: total was 1.240000. running mean: -5.778324\n",
      "ep 3104: ep_len:358 episode reward: total was -11.300000. running mean: -5.833541\n",
      "epsilon:0.010000 episode_count: 21735. steps_count: 9615375.000000\n",
      "ep 3105: ep_len:685 episode reward: total was -13.640000. running mean: -5.911606\n",
      "ep 3105: ep_len:500 episode reward: total was 5.030000. running mean: -5.802190\n",
      "ep 3105: ep_len:540 episode reward: total was -10.180000. running mean: -5.845968\n",
      "ep 3105: ep_len:605 episode reward: total was 3.050000. running mean: -5.757008\n",
      "ep 3105: ep_len:77 episode reward: total was 6.530000. running mean: -5.634138\n",
      "ep 3105: ep_len:605 episode reward: total was -46.000000. running mean: -6.037797\n",
      "ep 3105: ep_len:575 episode reward: total was -9.110000. running mean: -6.068519\n",
      "epsilon:0.010000 episode_count: 21742. steps_count: 9618962.000000\n",
      "ep 3106: ep_len:113 episode reward: total was -4.940000. running mean: -6.057233\n",
      "ep 3106: ep_len:500 episode reward: total was 7.100000. running mean: -5.925661\n",
      "ep 3106: ep_len:590 episode reward: total was -0.120000. running mean: -5.867605\n",
      "ep 3106: ep_len:505 episode reward: total was -9.010000. running mean: -5.899028\n",
      "ep 3106: ep_len:3 episode reward: total was 0.000000. running mean: -5.840038\n",
      "ep 3106: ep_len:500 episode reward: total was 4.790000. running mean: -5.733738\n",
      "ep 3106: ep_len:575 episode reward: total was -20.560000. running mean: -5.882000\n",
      "epsilon:0.010000 episode_count: 21749. steps_count: 9621748.000000\n",
      "ep 3107: ep_len:620 episode reward: total was 1.090000. running mean: -5.812280\n",
      "ep 3107: ep_len:520 episode reward: total was 20.350000. running mean: -5.550658\n",
      "ep 3107: ep_len:393 episode reward: total was 8.210000. running mean: -5.413051\n",
      "ep 3107: ep_len:505 episode reward: total was -23.560000. running mean: -5.594521\n",
      "ep 3107: ep_len:46 episode reward: total was 1.500000. running mean: -5.523575\n",
      "ep 3107: ep_len:500 episode reward: total was 2.140000. running mean: -5.446940\n",
      "ep 3107: ep_len:580 episode reward: total was -6.240000. running mean: -5.454870\n",
      "epsilon:0.010000 episode_count: 21756. steps_count: 9624912.000000\n",
      "ep 3108: ep_len:515 episode reward: total was 8.890000. running mean: -5.311421\n",
      "ep 3108: ep_len:359 episode reward: total was -6.310000. running mean: -5.321407\n",
      "ep 3108: ep_len:635 episode reward: total was 4.700000. running mean: -5.221193\n",
      "ep 3108: ep_len:500 episode reward: total was 5.550000. running mean: -5.113481\n",
      "ep 3108: ep_len:3 episode reward: total was 0.000000. running mean: -5.062346\n",
      "ep 3108: ep_len:680 episode reward: total was -11.690000. running mean: -5.128623\n",
      "ep 3108: ep_len:610 episode reward: total was -10.600000. running mean: -5.183337\n",
      "epsilon:0.010000 episode_count: 21763. steps_count: 9628214.000000\n",
      "ep 3109: ep_len:500 episode reward: total was 13.770000. running mean: -4.993803\n",
      "ep 3109: ep_len:500 episode reward: total was 0.060000. running mean: -4.943265\n",
      "ep 3109: ep_len:630 episode reward: total was 1.980000. running mean: -4.874033\n",
      "ep 3109: ep_len:170 episode reward: total was 5.650000. running mean: -4.768792\n",
      "ep 3109: ep_len:3 episode reward: total was 0.000000. running mean: -4.721104\n",
      "ep 3109: ep_len:500 episode reward: total was -9.250000. running mean: -4.766393\n",
      "ep 3109: ep_len:296 episode reward: total was -14.340000. running mean: -4.862129\n",
      "epsilon:0.010000 episode_count: 21770. steps_count: 9630813.000000\n",
      "ep 3110: ep_len:540 episode reward: total was 6.860000. running mean: -4.744908\n",
      "ep 3110: ep_len:580 episode reward: total was -19.180000. running mean: -4.889259\n",
      "ep 3110: ep_len:615 episode reward: total was -14.960000. running mean: -4.989967\n",
      "ep 3110: ep_len:500 episode reward: total was 10.410000. running mean: -4.835967\n",
      "ep 3110: ep_len:3 episode reward: total was 0.000000. running mean: -4.787607\n",
      "ep 3110: ep_len:590 episode reward: total was -11.200000. running mean: -4.851731\n",
      "ep 3110: ep_len:284 episode reward: total was -14.310000. running mean: -4.946314\n",
      "epsilon:0.010000 episode_count: 21777. steps_count: 9633925.000000\n",
      "ep 3111: ep_len:223 episode reward: total was 6.620000. running mean: -4.830651\n",
      "ep 3111: ep_len:605 episode reward: total was -5.350000. running mean: -4.835844\n",
      "ep 3111: ep_len:535 episode reward: total was 7.410000. running mean: -4.713386\n",
      "ep 3111: ep_len:550 episode reward: total was -10.010000. running mean: -4.766352\n",
      "ep 3111: ep_len:49 episode reward: total was 4.500000. running mean: -4.673688\n",
      "ep 3111: ep_len:605 episode reward: total was -31.370000. running mean: -4.940651\n",
      "ep 3111: ep_len:296 episode reward: total was -1.790000. running mean: -4.909145\n",
      "epsilon:0.010000 episode_count: 21784. steps_count: 9636788.000000\n",
      "ep 3112: ep_len:615 episode reward: total was -16.230000. running mean: -5.022353\n",
      "ep 3112: ep_len:555 episode reward: total was -0.880000. running mean: -4.980930\n",
      "ep 3112: ep_len:615 episode reward: total was 3.940000. running mean: -4.891721\n",
      "ep 3112: ep_len:500 episode reward: total was -19.130000. running mean: -5.034103\n",
      "ep 3112: ep_len:89 episode reward: total was -11.950000. running mean: -5.103262\n",
      "ep 3112: ep_len:520 episode reward: total was -14.910000. running mean: -5.201330\n",
      "ep 3112: ep_len:500 episode reward: total was -19.990000. running mean: -5.349216\n",
      "epsilon:0.010000 episode_count: 21791. steps_count: 9640182.000000\n",
      "ep 3113: ep_len:625 episode reward: total was -30.020000. running mean: -5.595924\n",
      "ep 3113: ep_len:505 episode reward: total was 6.180000. running mean: -5.478165\n",
      "ep 3113: ep_len:565 episode reward: total was -1.190000. running mean: -5.435283\n",
      "ep 3113: ep_len:500 episode reward: total was -9.490000. running mean: -5.475831\n",
      "ep 3113: ep_len:75 episode reward: total was 3.010000. running mean: -5.390972\n",
      "ep 3113: ep_len:500 episode reward: total was -0.740000. running mean: -5.344463\n",
      "ep 3113: ep_len:545 episode reward: total was -8.820000. running mean: -5.379218\n",
      "epsilon:0.010000 episode_count: 21798. steps_count: 9643497.000000\n",
      "ep 3114: ep_len:655 episode reward: total was -31.390000. running mean: -5.639326\n",
      "ep 3114: ep_len:525 episode reward: total was -35.320000. running mean: -5.936133\n",
      "ep 3114: ep_len:545 episode reward: total was -8.040000. running mean: -5.957171\n",
      "ep 3114: ep_len:525 episode reward: total was 2.490000. running mean: -5.872699\n",
      "ep 3114: ep_len:90 episode reward: total was -12.950000. running mean: -5.943472\n",
      "ep 3114: ep_len:510 episode reward: total was 4.500000. running mean: -5.839038\n",
      "ep 3114: ep_len:510 episode reward: total was -7.650000. running mean: -5.857147\n",
      "epsilon:0.010000 episode_count: 21805. steps_count: 9646857.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3115: ep_len:560 episode reward: total was 6.390000. running mean: -5.734676\n",
      "ep 3115: ep_len:535 episode reward: total was -20.020000. running mean: -5.877529\n",
      "ep 3115: ep_len:560 episode reward: total was -2.670000. running mean: -5.845454\n",
      "ep 3115: ep_len:505 episode reward: total was 10.640000. running mean: -5.680599\n",
      "ep 3115: ep_len:1 episode reward: total was 0.000000. running mean: -5.623793\n",
      "ep 3115: ep_len:635 episode reward: total was -0.470000. running mean: -5.572255\n",
      "ep 3115: ep_len:500 episode reward: total was -13.490000. running mean: -5.651433\n",
      "epsilon:0.010000 episode_count: 21812. steps_count: 9650153.000000\n",
      "ep 3116: ep_len:500 episode reward: total was 16.770000. running mean: -5.427218\n",
      "ep 3116: ep_len:530 episode reward: total was -7.560000. running mean: -5.448546\n",
      "ep 3116: ep_len:625 episode reward: total was 2.990000. running mean: -5.364161\n",
      "ep 3116: ep_len:535 episode reward: total was -0.030000. running mean: -5.310819\n",
      "ep 3116: ep_len:92 episode reward: total was -1.460000. running mean: -5.272311\n",
      "ep 3116: ep_len:565 episode reward: total was 2.510000. running mean: -5.194488\n",
      "ep 3116: ep_len:203 episode reward: total was -9.370000. running mean: -5.236243\n",
      "epsilon:0.010000 episode_count: 21819. steps_count: 9653203.000000\n",
      "ep 3117: ep_len:126 episode reward: total was 5.100000. running mean: -5.132881\n",
      "ep 3117: ep_len:500 episode reward: total was -7.610000. running mean: -5.157652\n",
      "ep 3117: ep_len:620 episode reward: total was -9.050000. running mean: -5.196575\n",
      "ep 3117: ep_len:432 episode reward: total was 6.410000. running mean: -5.080510\n",
      "ep 3117: ep_len:3 episode reward: total was 0.000000. running mean: -5.029704\n",
      "ep 3117: ep_len:640 episode reward: total was -26.830000. running mean: -5.247707\n",
      "ep 3117: ep_len:500 episode reward: total was -2.200000. running mean: -5.217230\n",
      "epsilon:0.010000 episode_count: 21826. steps_count: 9656024.000000\n",
      "ep 3118: ep_len:545 episode reward: total was -6.420000. running mean: -5.229258\n",
      "ep 3118: ep_len:345 episode reward: total was -19.350000. running mean: -5.370465\n",
      "ep 3118: ep_len:615 episode reward: total was -14.220000. running mean: -5.458961\n",
      "ep 3118: ep_len:515 episode reward: total was -11.040000. running mean: -5.514771\n",
      "ep 3118: ep_len:94 episode reward: total was 2.550000. running mean: -5.434123\n",
      "ep 3118: ep_len:730 episode reward: total was -42.760000. running mean: -5.807382\n",
      "ep 3118: ep_len:340 episode reward: total was -13.290000. running mean: -5.882208\n",
      "epsilon:0.010000 episode_count: 21833. steps_count: 9659208.000000\n",
      "ep 3119: ep_len:500 episode reward: total was -16.880000. running mean: -5.992186\n",
      "ep 3119: ep_len:239 episode reward: total was -27.860000. running mean: -6.210864\n",
      "ep 3119: ep_len:505 episode reward: total was 6.450000. running mean: -6.084256\n",
      "ep 3119: ep_len:565 episode reward: total was 6.180000. running mean: -5.961613\n",
      "ep 3119: ep_len:2 episode reward: total was 0.000000. running mean: -5.901997\n",
      "ep 3119: ep_len:595 episode reward: total was 1.990000. running mean: -5.823077\n",
      "ep 3119: ep_len:605 episode reward: total was -4.980000. running mean: -5.814646\n",
      "epsilon:0.010000 episode_count: 21840. steps_count: 9662219.000000\n",
      "ep 3120: ep_len:500 episode reward: total was 13.800000. running mean: -5.618500\n",
      "ep 3120: ep_len:310 episode reward: total was -25.280000. running mean: -5.815115\n",
      "ep 3120: ep_len:394 episode reward: total was -3.800000. running mean: -5.794964\n",
      "ep 3120: ep_len:510 episode reward: total was -15.020000. running mean: -5.887214\n",
      "ep 3120: ep_len:3 episode reward: total was 0.000000. running mean: -5.828342\n",
      "ep 3120: ep_len:650 episode reward: total was -32.770000. running mean: -6.097759\n",
      "ep 3120: ep_len:590 episode reward: total was -14.510000. running mean: -6.181881\n",
      "epsilon:0.010000 episode_count: 21847. steps_count: 9665176.000000\n",
      "ep 3121: ep_len:670 episode reward: total was -10.180000. running mean: -6.221862\n",
      "ep 3121: ep_len:530 episode reward: total was -2.820000. running mean: -6.187844\n",
      "ep 3121: ep_len:357 episode reward: total was -15.310000. running mean: -6.279065\n",
      "ep 3121: ep_len:45 episode reward: total was -0.950000. running mean: -6.225774\n",
      "ep 3121: ep_len:121 episode reward: total was 5.060000. running mean: -6.112917\n",
      "ep 3121: ep_len:545 episode reward: total was 4.020000. running mean: -6.011588\n",
      "ep 3121: ep_len:545 episode reward: total was -6.070000. running mean: -6.012172\n",
      "epsilon:0.010000 episode_count: 21854. steps_count: 9667989.000000\n",
      "ep 3122: ep_len:530 episode reward: total was 8.390000. running mean: -5.868150\n",
      "ep 3122: ep_len:610 episode reward: total was -34.450000. running mean: -6.153968\n",
      "ep 3122: ep_len:500 episode reward: total was -3.140000. running mean: -6.123829\n",
      "ep 3122: ep_len:500 episode reward: total was -5.480000. running mean: -6.117391\n",
      "ep 3122: ep_len:3 episode reward: total was 0.000000. running mean: -6.056217\n",
      "ep 3122: ep_len:1310 episode reward: total was -205.510000. running mean: -8.050754\n",
      "ep 3122: ep_len:500 episode reward: total was -11.750000. running mean: -8.087747\n",
      "epsilon:0.010000 episode_count: 21861. steps_count: 9671942.000000\n",
      "ep 3123: ep_len:575 episode reward: total was -8.960000. running mean: -8.096469\n",
      "ep 3123: ep_len:540 episode reward: total was 18.960000. running mean: -7.825905\n",
      "ep 3123: ep_len:500 episode reward: total was 3.310000. running mean: -7.714546\n",
      "ep 3123: ep_len:500 episode reward: total was -15.080000. running mean: -7.788200\n",
      "ep 3123: ep_len:52 episode reward: total was 5.000000. running mean: -7.660318\n",
      "ep 3123: ep_len:570 episode reward: total was 3.060000. running mean: -7.553115\n",
      "ep 3123: ep_len:268 episode reward: total was -3.840000. running mean: -7.515984\n",
      "epsilon:0.010000 episode_count: 21868. steps_count: 9674947.000000\n",
      "ep 3124: ep_len:640 episode reward: total was -7.380000. running mean: -7.514624\n",
      "ep 3124: ep_len:630 episode reward: total was -8.950000. running mean: -7.528978\n",
      "ep 3124: ep_len:555 episode reward: total was 5.990000. running mean: -7.393788\n",
      "ep 3124: ep_len:540 episode reward: total was -28.520000. running mean: -7.605050\n",
      "ep 3124: ep_len:3 episode reward: total was 0.000000. running mean: -7.529000\n",
      "ep 3124: ep_len:157 episode reward: total was 4.610000. running mean: -7.407610\n",
      "ep 3124: ep_len:605 episode reward: total was -6.300000. running mean: -7.396534\n",
      "epsilon:0.010000 episode_count: 21875. steps_count: 9678077.000000\n",
      "ep 3125: ep_len:525 episode reward: total was 6.860000. running mean: -7.253968\n",
      "ep 3125: ep_len:685 episode reward: total was -52.980000. running mean: -7.711229\n",
      "ep 3125: ep_len:500 episode reward: total was 10.450000. running mean: -7.529616\n",
      "ep 3125: ep_len:510 episode reward: total was 3.440000. running mean: -7.419920\n",
      "ep 3125: ep_len:3 episode reward: total was 0.000000. running mean: -7.345721\n",
      "ep 3125: ep_len:500 episode reward: total was -4.250000. running mean: -7.314764\n",
      "ep 3125: ep_len:500 episode reward: total was -13.080000. running mean: -7.372416\n",
      "epsilon:0.010000 episode_count: 21882. steps_count: 9681300.000000\n",
      "ep 3126: ep_len:1080 episode reward: total was -128.220000. running mean: -8.580892\n",
      "ep 3126: ep_len:280 episode reward: total was -9.290000. running mean: -8.587983\n",
      "ep 3126: ep_len:540 episode reward: total was -1.790000. running mean: -8.520003\n",
      "ep 3126: ep_len:540 episode reward: total was 0.360000. running mean: -8.431203\n",
      "ep 3126: ep_len:98 episode reward: total was 4.550000. running mean: -8.301391\n",
      "ep 3126: ep_len:515 episode reward: total was -20.410000. running mean: -8.422477\n",
      "ep 3126: ep_len:515 episode reward: total was -6.300000. running mean: -8.401252\n",
      "epsilon:0.010000 episode_count: 21889. steps_count: 9684868.000000\n",
      "ep 3127: ep_len:134 episode reward: total was 4.100000. running mean: -8.276240\n",
      "ep 3127: ep_len:535 episode reward: total was -14.410000. running mean: -8.337577\n",
      "ep 3127: ep_len:500 episode reward: total was -6.850000. running mean: -8.322702\n",
      "ep 3127: ep_len:500 episode reward: total was -35.140000. running mean: -8.590875\n",
      "ep 3127: ep_len:3 episode reward: total was 0.000000. running mean: -8.504966\n",
      "ep 3127: ep_len:500 episode reward: total was -5.930000. running mean: -8.479216\n",
      "ep 3127: ep_len:500 episode reward: total was -3.590000. running mean: -8.430324\n",
      "epsilon:0.010000 episode_count: 21896. steps_count: 9687540.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3128: ep_len:685 episode reward: total was -10.090000. running mean: -8.446921\n",
      "ep 3128: ep_len:580 episode reward: total was 20.940000. running mean: -8.153052\n",
      "ep 3128: ep_len:535 episode reward: total was -13.770000. running mean: -8.209221\n",
      "ep 3128: ep_len:500 episode reward: total was 14.750000. running mean: -7.979629\n",
      "ep 3128: ep_len:124 episode reward: total was 8.060000. running mean: -7.819233\n",
      "ep 3128: ep_len:530 episode reward: total was 6.070000. running mean: -7.680340\n",
      "ep 3128: ep_len:605 episode reward: total was -12.500000. running mean: -7.728537\n",
      "epsilon:0.010000 episode_count: 21903. steps_count: 9691099.000000\n",
      "ep 3129: ep_len:565 episode reward: total was 6.430000. running mean: -7.586952\n",
      "ep 3129: ep_len:645 episode reward: total was 9.090000. running mean: -7.420182\n",
      "ep 3129: ep_len:426 episode reward: total was -5.800000. running mean: -7.403980\n",
      "ep 3129: ep_len:510 episode reward: total was -14.080000. running mean: -7.470740\n",
      "ep 3129: ep_len:3 episode reward: total was 0.000000. running mean: -7.396033\n",
      "ep 3129: ep_len:570 episode reward: total was -4.630000. running mean: -7.368373\n",
      "ep 3129: ep_len:211 episode reward: total was -4.820000. running mean: -7.342889\n",
      "epsilon:0.010000 episode_count: 21910. steps_count: 9694029.000000\n",
      "ep 3130: ep_len:116 episode reward: total was 3.580000. running mean: -7.233660\n",
      "ep 3130: ep_len:650 episode reward: total was -13.770000. running mean: -7.299023\n",
      "ep 3130: ep_len:79 episode reward: total was -4.460000. running mean: -7.270633\n",
      "ep 3130: ep_len:560 episode reward: total was 15.000000. running mean: -7.047927\n",
      "ep 3130: ep_len:91 episode reward: total was 5.030000. running mean: -6.927148\n",
      "ep 3130: ep_len:164 episode reward: total was 5.630000. running mean: -6.801576\n",
      "ep 3130: ep_len:560 episode reward: total was -10.370000. running mean: -6.837260\n",
      "epsilon:0.010000 episode_count: 21917. steps_count: 9696249.000000\n",
      "ep 3131: ep_len:208 episode reward: total was 2.640000. running mean: -6.742488\n",
      "ep 3131: ep_len:600 episode reward: total was 0.730000. running mean: -6.667763\n",
      "ep 3131: ep_len:500 episode reward: total was 5.260000. running mean: -6.548485\n",
      "ep 3131: ep_len:505 episode reward: total was 3.360000. running mean: -6.449400\n",
      "ep 3131: ep_len:54 episode reward: total was 5.000000. running mean: -6.334906\n",
      "ep 3131: ep_len:535 episode reward: total was -31.510000. running mean: -6.586657\n",
      "ep 3131: ep_len:555 episode reward: total was -26.620000. running mean: -6.786991\n",
      "epsilon:0.010000 episode_count: 21924. steps_count: 9699206.000000\n",
      "ep 3132: ep_len:630 episode reward: total was -11.760000. running mean: -6.836721\n",
      "ep 3132: ep_len:500 episode reward: total was 8.170000. running mean: -6.686654\n",
      "ep 3132: ep_len:467 episode reward: total was 6.260000. running mean: -6.557187\n",
      "ep 3132: ep_len:555 episode reward: total was 16.980000. running mean: -6.321815\n",
      "ep 3132: ep_len:3 episode reward: total was 0.000000. running mean: -6.258597\n",
      "ep 3132: ep_len:645 episode reward: total was 0.480000. running mean: -6.191211\n",
      "ep 3132: ep_len:635 episode reward: total was -59.550000. running mean: -6.724799\n",
      "epsilon:0.010000 episode_count: 21931. steps_count: 9702641.000000\n",
      "ep 3133: ep_len:500 episode reward: total was -19.940000. running mean: -6.856951\n",
      "ep 3133: ep_len:595 episode reward: total was -12.070000. running mean: -6.909082\n",
      "ep 3133: ep_len:600 episode reward: total was -9.430000. running mean: -6.934291\n",
      "ep 3133: ep_len:121 episode reward: total was 1.110000. running mean: -6.853848\n",
      "ep 3133: ep_len:45 episode reward: total was 3.000000. running mean: -6.755309\n",
      "ep 3133: ep_len:500 episode reward: total was -18.230000. running mean: -6.870056\n",
      "ep 3133: ep_len:600 episode reward: total was -8.300000. running mean: -6.884356\n",
      "epsilon:0.010000 episode_count: 21938. steps_count: 9705602.000000\n",
      "ep 3134: ep_len:580 episode reward: total was -15.600000. running mean: -6.971512\n",
      "ep 3134: ep_len:500 episode reward: total was 18.710000. running mean: -6.714697\n",
      "ep 3134: ep_len:500 episode reward: total was 13.480000. running mean: -6.512750\n",
      "ep 3134: ep_len:500 episode reward: total was -7.520000. running mean: -6.522823\n",
      "ep 3134: ep_len:3 episode reward: total was 0.000000. running mean: -6.457594\n",
      "ep 3134: ep_len:1240 episode reward: total was -152.720000. running mean: -7.920218\n",
      "ep 3134: ep_len:505 episode reward: total was -18.410000. running mean: -8.025116\n",
      "epsilon:0.010000 episode_count: 21945. steps_count: 9709430.000000\n",
      "ep 3135: ep_len:103 episode reward: total was 3.580000. running mean: -7.909065\n",
      "ep 3135: ep_len:520 episode reward: total was 21.250000. running mean: -7.617474\n",
      "ep 3135: ep_len:500 episode reward: total was -0.940000. running mean: -7.550700\n",
      "ep 3135: ep_len:600 episode reward: total was 18.970000. running mean: -7.285493\n",
      "ep 3135: ep_len:3 episode reward: total was 0.000000. running mean: -7.212638\n",
      "ep 3135: ep_len:500 episode reward: total was 1.100000. running mean: -7.129511\n",
      "ep 3135: ep_len:1070 episode reward: total was -105.060000. running mean: -8.108816\n",
      "epsilon:0.010000 episode_count: 21952. steps_count: 9712726.000000\n",
      "ep 3136: ep_len:555 episode reward: total was 9.980000. running mean: -7.927928\n",
      "ep 3136: ep_len:565 episode reward: total was -2.890000. running mean: -7.877549\n",
      "ep 3136: ep_len:500 episode reward: total was -1.370000. running mean: -7.812473\n",
      "ep 3136: ep_len:129 episode reward: total was 2.100000. running mean: -7.713349\n",
      "ep 3136: ep_len:3 episode reward: total was 0.000000. running mean: -7.636215\n",
      "ep 3136: ep_len:237 episode reward: total was -5.830000. running mean: -7.618153\n",
      "ep 3136: ep_len:269 episode reward: total was -3.810000. running mean: -7.580071\n",
      "epsilon:0.010000 episode_count: 21959. steps_count: 9714984.000000\n",
      "ep 3137: ep_len:755 episode reward: total was -12.110000. running mean: -7.625371\n",
      "ep 3137: ep_len:590 episode reward: total was 13.070000. running mean: -7.418417\n",
      "ep 3137: ep_len:580 episode reward: total was -4.680000. running mean: -7.391033\n",
      "ep 3137: ep_len:730 episode reward: total was -70.520000. running mean: -8.022322\n",
      "ep 3137: ep_len:87 episode reward: total was 4.040000. running mean: -7.901699\n",
      "ep 3137: ep_len:324 episode reward: total was 1.660000. running mean: -7.806082\n",
      "ep 3137: ep_len:505 episode reward: total was -16.540000. running mean: -7.893421\n",
      "epsilon:0.010000 episode_count: 21966. steps_count: 9718555.000000\n",
      "ep 3138: ep_len:520 episode reward: total was -5.910000. running mean: -7.873587\n",
      "ep 3138: ep_len:500 episode reward: total was -11.420000. running mean: -7.909051\n",
      "ep 3138: ep_len:615 episode reward: total was 0.620000. running mean: -7.823761\n",
      "ep 3138: ep_len:500 episode reward: total was -1.520000. running mean: -7.760723\n",
      "ep 3138: ep_len:3 episode reward: total was 0.000000. running mean: -7.683116\n",
      "ep 3138: ep_len:500 episode reward: total was -13.770000. running mean: -7.743985\n",
      "ep 3138: ep_len:500 episode reward: total was -4.050000. running mean: -7.707045\n",
      "epsilon:0.010000 episode_count: 21973. steps_count: 9721693.000000\n",
      "ep 3139: ep_len:585 episode reward: total was -23.730000. running mean: -7.867275\n",
      "ep 3139: ep_len:375 episode reward: total was -39.290000. running mean: -8.181502\n",
      "ep 3139: ep_len:500 episode reward: total was -2.010000. running mean: -8.119787\n",
      "ep 3139: ep_len:500 episode reward: total was -25.100000. running mean: -8.289589\n",
      "ep 3139: ep_len:3 episode reward: total was 0.000000. running mean: -8.206693\n",
      "ep 3139: ep_len:600 episode reward: total was -5.050000. running mean: -8.175126\n",
      "ep 3139: ep_len:505 episode reward: total was -1.070000. running mean: -8.104075\n",
      "epsilon:0.010000 episode_count: 21980. steps_count: 9724761.000000\n",
      "ep 3140: ep_len:645 episode reward: total was -18.190000. running mean: -8.204934\n",
      "ep 3140: ep_len:565 episode reward: total was -1.000000. running mean: -8.132885\n",
      "ep 3140: ep_len:595 episode reward: total was 6.470000. running mean: -7.986856\n",
      "ep 3140: ep_len:500 episode reward: total was 6.530000. running mean: -7.841687\n",
      "ep 3140: ep_len:88 episode reward: total was 4.040000. running mean: -7.722870\n",
      "ep 3140: ep_len:525 episode reward: total was 11.630000. running mean: -7.529342\n",
      "ep 3140: ep_len:525 episode reward: total was -7.270000. running mean: -7.526748\n",
      "epsilon:0.010000 episode_count: 21987. steps_count: 9728204.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3141: ep_len:258 episode reward: total was -24.840000. running mean: -7.699881\n",
      "ep 3141: ep_len:500 episode reward: total was 3.530000. running mean: -7.587582\n",
      "ep 3141: ep_len:580 episode reward: total was -3.410000. running mean: -7.545806\n",
      "ep 3141: ep_len:500 episode reward: total was 6.590000. running mean: -7.404448\n",
      "ep 3141: ep_len:3 episode reward: total was 0.000000. running mean: -7.330404\n",
      "ep 3141: ep_len:181 episode reward: total was 4.600000. running mean: -7.211100\n",
      "ep 3141: ep_len:320 episode reward: total was -9.350000. running mean: -7.232489\n",
      "epsilon:0.010000 episode_count: 21994. steps_count: 9730546.000000\n",
      "ep 3142: ep_len:500 episode reward: total was 4.750000. running mean: -7.112664\n",
      "ep 3142: ep_len:600 episode reward: total was -31.930000. running mean: -7.360837\n",
      "ep 3142: ep_len:575 episode reward: total was -33.850000. running mean: -7.625729\n",
      "ep 3142: ep_len:520 episode reward: total was 0.520000. running mean: -7.544271\n",
      "ep 3142: ep_len:3 episode reward: total was 0.000000. running mean: -7.468829\n",
      "ep 3142: ep_len:259 episode reward: total was 4.670000. running mean: -7.347440\n",
      "ep 3142: ep_len:580 episode reward: total was -10.080000. running mean: -7.374766\n",
      "epsilon:0.010000 episode_count: 22001. steps_count: 9733583.000000\n",
      "ep 3143: ep_len:500 episode reward: total was 7.910000. running mean: -7.221918\n",
      "ep 3143: ep_len:555 episode reward: total was -0.290000. running mean: -7.152599\n",
      "ep 3143: ep_len:610 episode reward: total was -7.030000. running mean: -7.151373\n",
      "ep 3143: ep_len:505 episode reward: total was 9.890000. running mean: -6.980959\n",
      "ep 3143: ep_len:3 episode reward: total was 0.000000. running mean: -6.911150\n",
      "ep 3143: ep_len:321 episode reward: total was 6.680000. running mean: -6.775238\n",
      "ep 3143: ep_len:525 episode reward: total was -22.850000. running mean: -6.935986\n",
      "epsilon:0.010000 episode_count: 22008. steps_count: 9736602.000000\n",
      "ep 3144: ep_len:680 episode reward: total was -11.630000. running mean: -6.982926\n",
      "ep 3144: ep_len:500 episode reward: total was 10.180000. running mean: -6.811297\n",
      "ep 3144: ep_len:550 episode reward: total was 11.970000. running mean: -6.623484\n",
      "ep 3144: ep_len:500 episode reward: total was -18.660000. running mean: -6.743849\n",
      "ep 3144: ep_len:127 episode reward: total was 5.560000. running mean: -6.620811\n",
      "ep 3144: ep_len:530 episode reward: total was -15.830000. running mean: -6.712902\n",
      "ep 3144: ep_len:525 episode reward: total was -5.530000. running mean: -6.701073\n",
      "epsilon:0.010000 episode_count: 22015. steps_count: 9740014.000000\n",
      "ep 3145: ep_len:500 episode reward: total was 5.400000. running mean: -6.580063\n",
      "ep 3145: ep_len:169 episode reward: total was 0.600000. running mean: -6.508262\n",
      "ep 3145: ep_len:500 episode reward: total was -20.480000. running mean: -6.647979\n",
      "ep 3145: ep_len:509 episode reward: total was 7.560000. running mean: -6.505900\n",
      "ep 3145: ep_len:3 episode reward: total was 0.000000. running mean: -6.440841\n",
      "ep 3145: ep_len:595 episode reward: total was 1.420000. running mean: -6.362232\n",
      "ep 3145: ep_len:590 episode reward: total was 0.220000. running mean: -6.296410\n",
      "epsilon:0.010000 episode_count: 22022. steps_count: 9742880.000000\n",
      "ep 3146: ep_len:500 episode reward: total was 18.820000. running mean: -6.045246\n",
      "ep 3146: ep_len:515 episode reward: total was -2.800000. running mean: -6.012793\n",
      "ep 3146: ep_len:640 episode reward: total was -14.310000. running mean: -6.095765\n",
      "ep 3146: ep_len:515 episode reward: total was -28.090000. running mean: -6.315708\n",
      "ep 3146: ep_len:89 episode reward: total was 5.050000. running mean: -6.202051\n",
      "ep 3146: ep_len:645 episode reward: total was -30.080000. running mean: -6.440830\n",
      "ep 3146: ep_len:585 episode reward: total was -28.160000. running mean: -6.658022\n",
      "epsilon:0.010000 episode_count: 22029. steps_count: 9746369.000000\n",
      "ep 3147: ep_len:590 episode reward: total was 7.000000. running mean: -6.521442\n",
      "ep 3147: ep_len:373 episode reward: total was -39.380000. running mean: -6.850027\n",
      "ep 3147: ep_len:500 episode reward: total was -6.510000. running mean: -6.846627\n",
      "ep 3147: ep_len:528 episode reward: total was 11.600000. running mean: -6.662161\n",
      "ep 3147: ep_len:3 episode reward: total was 0.000000. running mean: -6.595539\n",
      "ep 3147: ep_len:655 episode reward: total was -5.130000. running mean: -6.580884\n",
      "ep 3147: ep_len:322 episode reward: total was -47.840000. running mean: -6.993475\n",
      "epsilon:0.010000 episode_count: 22036. steps_count: 9749340.000000\n",
      "ep 3148: ep_len:103 episode reward: total was -0.950000. running mean: -6.933040\n",
      "ep 3148: ep_len:615 episode reward: total was -32.600000. running mean: -7.189710\n",
      "ep 3148: ep_len:464 episode reward: total was -1.770000. running mean: -7.135513\n",
      "ep 3148: ep_len:402 episode reward: total was 3.380000. running mean: -7.030358\n",
      "ep 3148: ep_len:3 episode reward: total was 0.000000. running mean: -6.960054\n",
      "ep 3148: ep_len:510 episode reward: total was 3.430000. running mean: -6.856153\n",
      "ep 3148: ep_len:600 episode reward: total was -5.380000. running mean: -6.841392\n",
      "epsilon:0.010000 episode_count: 22043. steps_count: 9752037.000000\n",
      "ep 3149: ep_len:670 episode reward: total was -13.150000. running mean: -6.904478\n",
      "ep 3149: ep_len:500 episode reward: total was 12.030000. running mean: -6.715133\n",
      "ep 3149: ep_len:500 episode reward: total was -32.130000. running mean: -6.969282\n",
      "ep 3149: ep_len:107 episode reward: total was 2.090000. running mean: -6.878689\n",
      "ep 3149: ep_len:3 episode reward: total was 0.000000. running mean: -6.809902\n",
      "ep 3149: ep_len:615 episode reward: total was 6.770000. running mean: -6.674103\n",
      "ep 3149: ep_len:505 episode reward: total was -30.090000. running mean: -6.908262\n",
      "epsilon:0.010000 episode_count: 22050. steps_count: 9754937.000000\n",
      "ep 3150: ep_len:500 episode reward: total was 11.900000. running mean: -6.720179\n",
      "ep 3150: ep_len:520 episode reward: total was 1.200000. running mean: -6.640978\n",
      "ep 3150: ep_len:535 episode reward: total was 0.280000. running mean: -6.571768\n",
      "ep 3150: ep_len:520 episode reward: total was -15.010000. running mean: -6.656150\n",
      "ep 3150: ep_len:3 episode reward: total was 0.000000. running mean: -6.589589\n",
      "ep 3150: ep_len:690 episode reward: total was 1.400000. running mean: -6.509693\n",
      "ep 3150: ep_len:500 episode reward: total was -12.340000. running mean: -6.567996\n",
      "epsilon:0.010000 episode_count: 22057. steps_count: 9758205.000000\n",
      "ep 3151: ep_len:520 episode reward: total was 0.090000. running mean: -6.501416\n",
      "ep 3151: ep_len:565 episode reward: total was 5.750000. running mean: -6.378902\n",
      "ep 3151: ep_len:665 episode reward: total was -7.190000. running mean: -6.387013\n",
      "ep 3151: ep_len:500 episode reward: total was -4.520000. running mean: -6.368343\n",
      "ep 3151: ep_len:45 episode reward: total was 3.000000. running mean: -6.274659\n",
      "ep 3151: ep_len:680 episode reward: total was -32.270000. running mean: -6.534613\n",
      "ep 3151: ep_len:520 episode reward: total was -7.860000. running mean: -6.547866\n",
      "epsilon:0.010000 episode_count: 22064. steps_count: 9761700.000000\n",
      "ep 3152: ep_len:585 episode reward: total was 1.450000. running mean: -6.467888\n",
      "ep 3152: ep_len:500 episode reward: total was 8.130000. running mean: -6.321909\n",
      "ep 3152: ep_len:585 episode reward: total was -22.650000. running mean: -6.485190\n",
      "ep 3152: ep_len:53 episode reward: total was 0.070000. running mean: -6.419638\n",
      "ep 3152: ep_len:83 episode reward: total was -9.480000. running mean: -6.450242\n",
      "ep 3152: ep_len:650 episode reward: total was -6.730000. running mean: -6.453039\n",
      "ep 3152: ep_len:295 episode reward: total was -3.800000. running mean: -6.426509\n",
      "epsilon:0.010000 episode_count: 22071. steps_count: 9764451.000000\n",
      "ep 3153: ep_len:630 episode reward: total was -26.710000. running mean: -6.629344\n",
      "ep 3153: ep_len:625 episode reward: total was 2.100000. running mean: -6.542050\n",
      "ep 3153: ep_len:560 episode reward: total was -9.780000. running mean: -6.574430\n",
      "ep 3153: ep_len:535 episode reward: total was -0.160000. running mean: -6.510285\n",
      "ep 3153: ep_len:92 episode reward: total was -11.940000. running mean: -6.564583\n",
      "ep 3153: ep_len:545 episode reward: total was 4.000000. running mean: -6.458937\n",
      "ep 3153: ep_len:545 episode reward: total was -5.490000. running mean: -6.449247\n",
      "epsilon:0.010000 episode_count: 22078. steps_count: 9767983.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3154: ep_len:500 episode reward: total was 13.740000. running mean: -6.247355\n",
      "ep 3154: ep_len:580 episode reward: total was 5.030000. running mean: -6.134581\n",
      "ep 3154: ep_len:500 episode reward: total was -7.890000. running mean: -6.152136\n",
      "ep 3154: ep_len:52 episode reward: total was -0.940000. running mean: -6.100014\n",
      "ep 3154: ep_len:78 episode reward: total was 6.530000. running mean: -5.973714\n",
      "ep 3154: ep_len:750 episode reward: total was -41.300000. running mean: -6.326977\n",
      "ep 3154: ep_len:510 episode reward: total was -3.780000. running mean: -6.301507\n",
      "epsilon:0.010000 episode_count: 22085. steps_count: 9770953.000000\n",
      "ep 3155: ep_len:500 episode reward: total was -8.860000. running mean: -6.327092\n",
      "ep 3155: ep_len:620 episode reward: total was -20.790000. running mean: -6.471721\n",
      "ep 3155: ep_len:575 episode reward: total was 5.910000. running mean: -6.347904\n",
      "ep 3155: ep_len:500 episode reward: total was 10.890000. running mean: -6.175525\n",
      "ep 3155: ep_len:3 episode reward: total was 0.000000. running mean: -6.113770\n",
      "ep 3155: ep_len:244 episode reward: total was 8.130000. running mean: -5.971332\n",
      "ep 3155: ep_len:500 episode reward: total was -0.210000. running mean: -5.913719\n",
      "epsilon:0.010000 episode_count: 22092. steps_count: 9773895.000000\n",
      "ep 3156: ep_len:590 episode reward: total was -9.420000. running mean: -5.948781\n",
      "ep 3156: ep_len:640 episode reward: total was 8.120000. running mean: -5.808094\n",
      "ep 3156: ep_len:680 episode reward: total was -7.580000. running mean: -5.825813\n",
      "ep 3156: ep_len:500 episode reward: total was 1.050000. running mean: -5.757055\n",
      "ep 3156: ep_len:3 episode reward: total was 0.000000. running mean: -5.699484\n",
      "ep 3156: ep_len:500 episode reward: total was -30.600000. running mean: -5.948489\n",
      "ep 3156: ep_len:319 episode reward: total was -11.330000. running mean: -6.002304\n",
      "epsilon:0.010000 episode_count: 22099. steps_count: 9777127.000000\n",
      "ep 3157: ep_len:500 episode reward: total was -21.890000. running mean: -6.161181\n",
      "ep 3157: ep_len:520 episode reward: total was -10.830000. running mean: -6.207869\n",
      "ep 3157: ep_len:444 episode reward: total was 6.270000. running mean: -6.083091\n",
      "ep 3157: ep_len:530 episode reward: total was -13.020000. running mean: -6.152460\n",
      "ep 3157: ep_len:3 episode reward: total was 0.000000. running mean: -6.090935\n",
      "ep 3157: ep_len:610 episode reward: total was 5.420000. running mean: -5.975826\n",
      "ep 3157: ep_len:275 episode reward: total was -6.840000. running mean: -5.984468\n",
      "epsilon:0.010000 episode_count: 22106. steps_count: 9780009.000000\n",
      "ep 3158: ep_len:580 episode reward: total was 6.110000. running mean: -5.863523\n",
      "ep 3158: ep_len:525 episode reward: total was -23.000000. running mean: -6.034888\n",
      "ep 3158: ep_len:595 episode reward: total was -6.320000. running mean: -6.037739\n",
      "ep 3158: ep_len:500 episode reward: total was -3.570000. running mean: -6.013061\n",
      "ep 3158: ep_len:81 episode reward: total was 3.510000. running mean: -5.917831\n",
      "ep 3158: ep_len:154 episode reward: total was 4.080000. running mean: -5.817853\n",
      "ep 3158: ep_len:640 episode reward: total was 2.740000. running mean: -5.732274\n",
      "epsilon:0.010000 episode_count: 22113. steps_count: 9783084.000000\n",
      "ep 3159: ep_len:500 episode reward: total was 6.900000. running mean: -5.605951\n",
      "ep 3159: ep_len:675 episode reward: total was -38.830000. running mean: -5.938192\n",
      "ep 3159: ep_len:369 episode reward: total was -27.880000. running mean: -6.157610\n",
      "ep 3159: ep_len:500 episode reward: total was -15.510000. running mean: -6.251134\n",
      "ep 3159: ep_len:3 episode reward: total was 0.000000. running mean: -6.188622\n",
      "ep 3159: ep_len:620 episode reward: total was -15.300000. running mean: -6.279736\n",
      "ep 3159: ep_len:500 episode reward: total was -1.020000. running mean: -6.227139\n",
      "epsilon:0.010000 episode_count: 22120. steps_count: 9786251.000000\n",
      "ep 3160: ep_len:500 episode reward: total was 11.860000. running mean: -6.046267\n",
      "ep 3160: ep_len:525 episode reward: total was 1.160000. running mean: -5.974205\n",
      "ep 3160: ep_len:635 episode reward: total was -1.190000. running mean: -5.926363\n",
      "ep 3160: ep_len:550 episode reward: total was 12.990000. running mean: -5.737199\n",
      "ep 3160: ep_len:100 episode reward: total was -7.940000. running mean: -5.759227\n",
      "ep 3160: ep_len:585 episode reward: total was -4.520000. running mean: -5.746835\n",
      "ep 3160: ep_len:620 episode reward: total was -8.070000. running mean: -5.770066\n",
      "epsilon:0.010000 episode_count: 22127. steps_count: 9789766.000000\n",
      "ep 3161: ep_len:510 episode reward: total was -27.460000. running mean: -5.986966\n",
      "ep 3161: ep_len:625 episode reward: total was -20.600000. running mean: -6.133096\n",
      "ep 3161: ep_len:640 episode reward: total was -1.380000. running mean: -6.085565\n",
      "ep 3161: ep_len:414 episode reward: total was 8.860000. running mean: -5.936110\n",
      "ep 3161: ep_len:3 episode reward: total was 0.000000. running mean: -5.876748\n",
      "ep 3161: ep_len:625 episode reward: total was 2.000000. running mean: -5.797981\n",
      "ep 3161: ep_len:328 episode reward: total was -16.290000. running mean: -5.902901\n",
      "epsilon:0.010000 episode_count: 22134. steps_count: 9792911.000000\n",
      "ep 3162: ep_len:215 episode reward: total was -0.400000. running mean: -5.847872\n",
      "ep 3162: ep_len:685 episode reward: total was 19.660000. running mean: -5.592793\n",
      "ep 3162: ep_len:605 episode reward: total was 1.630000. running mean: -5.520565\n",
      "ep 3162: ep_len:500 episode reward: total was -10.550000. running mean: -5.570860\n",
      "ep 3162: ep_len:3 episode reward: total was 0.000000. running mean: -5.515151\n",
      "ep 3162: ep_len:505 episode reward: total was -5.890000. running mean: -5.518900\n",
      "ep 3162: ep_len:640 episode reward: total was -8.310000. running mean: -5.546811\n",
      "epsilon:0.010000 episode_count: 22141. steps_count: 9796064.000000\n",
      "ep 3163: ep_len:535 episode reward: total was 2.130000. running mean: -5.470043\n",
      "ep 3163: ep_len:585 episode reward: total was -11.550000. running mean: -5.530842\n",
      "ep 3163: ep_len:620 episode reward: total was -3.520000. running mean: -5.510734\n",
      "ep 3163: ep_len:510 episode reward: total was -12.090000. running mean: -5.576526\n",
      "ep 3163: ep_len:3 episode reward: total was 0.000000. running mean: -5.520761\n",
      "ep 3163: ep_len:595 episode reward: total was -28.450000. running mean: -5.750054\n",
      "ep 3163: ep_len:565 episode reward: total was -9.350000. running mean: -5.786053\n",
      "epsilon:0.010000 episode_count: 22148. steps_count: 9799477.000000\n",
      "ep 3164: ep_len:223 episode reward: total was -14.890000. running mean: -5.877092\n",
      "ep 3164: ep_len:500 episode reward: total was -5.120000. running mean: -5.869522\n",
      "ep 3164: ep_len:79 episode reward: total was 1.050000. running mean: -5.800326\n",
      "ep 3164: ep_len:505 episode reward: total was 15.470000. running mean: -5.587623\n",
      "ep 3164: ep_len:67 episode reward: total was -0.990000. running mean: -5.541647\n",
      "ep 3164: ep_len:515 episode reward: total was 0.120000. running mean: -5.485030\n",
      "ep 3164: ep_len:306 episode reward: total was -2.830000. running mean: -5.458480\n",
      "epsilon:0.010000 episode_count: 22155. steps_count: 9801672.000000\n",
      "ep 3165: ep_len:500 episode reward: total was 0.910000. running mean: -5.394795\n",
      "ep 3165: ep_len:500 episode reward: total was 15.200000. running mean: -5.188847\n",
      "ep 3165: ep_len:500 episode reward: total was -6.480000. running mean: -5.201759\n",
      "ep 3165: ep_len:56 episode reward: total was 1.560000. running mean: -5.134141\n",
      "ep 3165: ep_len:114 episode reward: total was -6.440000. running mean: -5.147200\n",
      "ep 3165: ep_len:590 episode reward: total was -16.030000. running mean: -5.256028\n",
      "ep 3165: ep_len:500 episode reward: total was -6.800000. running mean: -5.271468\n",
      "epsilon:0.010000 episode_count: 22162. steps_count: 9804432.000000\n",
      "ep 3166: ep_len:920 episode reward: total was -99.230000. running mean: -6.211053\n",
      "ep 3166: ep_len:625 episode reward: total was -11.630000. running mean: -6.265242\n",
      "ep 3166: ep_len:625 episode reward: total was -6.920000. running mean: -6.271790\n",
      "ep 3166: ep_len:500 episode reward: total was 12.980000. running mean: -6.079272\n",
      "ep 3166: ep_len:3 episode reward: total was 0.000000. running mean: -6.018479\n",
      "ep 3166: ep_len:294 episode reward: total was 1.170000. running mean: -5.946595\n",
      "ep 3166: ep_len:590 episode reward: total was -26.070000. running mean: -6.147829\n",
      "epsilon:0.010000 episode_count: 22169. steps_count: 9807989.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3167: ep_len:500 episode reward: total was 2.310000. running mean: -6.063250\n",
      "ep 3167: ep_len:610 episode reward: total was -26.870000. running mean: -6.271318\n",
      "ep 3167: ep_len:540 episode reward: total was -5.590000. running mean: -6.264505\n",
      "ep 3167: ep_len:500 episode reward: total was -10.040000. running mean: -6.302260\n",
      "ep 3167: ep_len:3 episode reward: total was 0.000000. running mean: -6.239237\n",
      "ep 3167: ep_len:301 episode reward: total was 2.660000. running mean: -6.150245\n",
      "ep 3167: ep_len:515 episode reward: total was -15.090000. running mean: -6.239642\n",
      "epsilon:0.010000 episode_count: 22176. steps_count: 9810958.000000\n",
      "ep 3168: ep_len:605 episode reward: total was -26.240000. running mean: -6.439646\n",
      "ep 3168: ep_len:640 episode reward: total was 7.550000. running mean: -6.299749\n",
      "ep 3168: ep_len:500 episode reward: total was 0.500000. running mean: -6.231752\n",
      "ep 3168: ep_len:510 episode reward: total was -9.030000. running mean: -6.259734\n",
      "ep 3168: ep_len:3 episode reward: total was 0.000000. running mean: -6.197137\n",
      "ep 3168: ep_len:520 episode reward: total was -18.380000. running mean: -6.318966\n",
      "ep 3168: ep_len:580 episode reward: total was -14.010000. running mean: -6.395876\n",
      "epsilon:0.010000 episode_count: 22183. steps_count: 9814316.000000\n",
      "ep 3169: ep_len:655 episode reward: total was -16.210000. running mean: -6.494017\n",
      "ep 3169: ep_len:510 episode reward: total was 7.430000. running mean: -6.354777\n",
      "ep 3169: ep_len:670 episode reward: total was -24.290000. running mean: -6.534129\n",
      "ep 3169: ep_len:565 episode reward: total was -3.510000. running mean: -6.503888\n",
      "ep 3169: ep_len:3 episode reward: total was 0.000000. running mean: -6.438849\n",
      "ep 3169: ep_len:670 episode reward: total was -2.680000. running mean: -6.401261\n",
      "ep 3169: ep_len:605 episode reward: total was -21.020000. running mean: -6.547448\n",
      "epsilon:0.010000 episode_count: 22190. steps_count: 9817994.000000\n",
      "ep 3170: ep_len:575 episode reward: total was -15.710000. running mean: -6.639073\n",
      "ep 3170: ep_len:500 episode reward: total was -7.380000. running mean: -6.646483\n",
      "ep 3170: ep_len:540 episode reward: total was -7.600000. running mean: -6.656018\n",
      "ep 3170: ep_len:113 episode reward: total was 5.080000. running mean: -6.538658\n",
      "ep 3170: ep_len:3 episode reward: total was 0.000000. running mean: -6.473271\n",
      "ep 3170: ep_len:625 episode reward: total was -15.380000. running mean: -6.562338\n",
      "ep 3170: ep_len:510 episode reward: total was -8.830000. running mean: -6.585015\n",
      "epsilon:0.010000 episode_count: 22197. steps_count: 9820860.000000\n",
      "ep 3171: ep_len:500 episode reward: total was 5.230000. running mean: -6.466865\n",
      "ep 3171: ep_len:540 episode reward: total was -12.780000. running mean: -6.529996\n",
      "ep 3171: ep_len:590 episode reward: total was -7.400000. running mean: -6.538696\n",
      "ep 3171: ep_len:500 episode reward: total was 8.030000. running mean: -6.393009\n",
      "ep 3171: ep_len:3 episode reward: total was 0.000000. running mean: -6.329079\n",
      "ep 3171: ep_len:540 episode reward: total was 1.650000. running mean: -6.249288\n",
      "ep 3171: ep_len:600 episode reward: total was -4.990000. running mean: -6.236696\n",
      "epsilon:0.010000 episode_count: 22204. steps_count: 9824133.000000\n",
      "ep 3172: ep_len:920 episode reward: total was -125.770000. running mean: -7.432029\n",
      "ep 3172: ep_len:615 episode reward: total was -0.560000. running mean: -7.363308\n",
      "ep 3172: ep_len:530 episode reward: total was -8.730000. running mean: -7.376975\n",
      "ep 3172: ep_len:490 episode reward: total was -2.030000. running mean: -7.323505\n",
      "ep 3172: ep_len:95 episode reward: total was -8.440000. running mean: -7.334670\n",
      "ep 3172: ep_len:555 episode reward: total was -9.630000. running mean: -7.357624\n",
      "ep 3172: ep_len:575 episode reward: total was -43.040000. running mean: -7.714447\n",
      "epsilon:0.010000 episode_count: 22211. steps_count: 9827913.000000\n",
      "ep 3173: ep_len:595 episode reward: total was 0.990000. running mean: -7.627403\n",
      "ep 3173: ep_len:545 episode reward: total was -0.830000. running mean: -7.559429\n",
      "ep 3173: ep_len:535 episode reward: total was -0.090000. running mean: -7.484735\n",
      "ep 3173: ep_len:500 episode reward: total was 13.370000. running mean: -7.276187\n",
      "ep 3173: ep_len:3 episode reward: total was 0.000000. running mean: -7.203425\n",
      "ep 3173: ep_len:630 episode reward: total was 0.170000. running mean: -7.129691\n",
      "ep 3173: ep_len:545 episode reward: total was -2.300000. running mean: -7.081394\n",
      "epsilon:0.010000 episode_count: 22218. steps_count: 9831266.000000\n",
      "ep 3174: ep_len:615 episode reward: total was -25.560000. running mean: -7.266180\n",
      "ep 3174: ep_len:540 episode reward: total was 6.400000. running mean: -7.129519\n",
      "ep 3174: ep_len:550 episode reward: total was -5.200000. running mean: -7.110223\n",
      "ep 3174: ep_len:366 episode reward: total was 0.290000. running mean: -7.036221\n",
      "ep 3174: ep_len:3 episode reward: total was 0.000000. running mean: -6.965859\n",
      "ep 3174: ep_len:500 episode reward: total was -10.260000. running mean: -6.998800\n",
      "ep 3174: ep_len:525 episode reward: total was -18.890000. running mean: -7.117712\n",
      "epsilon:0.010000 episode_count: 22225. steps_count: 9834365.000000\n",
      "ep 3175: ep_len:241 episode reward: total was -16.310000. running mean: -7.209635\n",
      "ep 3175: ep_len:500 episode reward: total was -27.140000. running mean: -7.408939\n",
      "ep 3175: ep_len:500 episode reward: total was 5.060000. running mean: -7.284249\n",
      "ep 3175: ep_len:500 episode reward: total was 5.890000. running mean: -7.152507\n",
      "ep 3175: ep_len:3 episode reward: total was 0.000000. running mean: -7.080982\n",
      "ep 3175: ep_len:305 episode reward: total was 0.680000. running mean: -7.003372\n",
      "ep 3175: ep_len:565 episode reward: total was -11.940000. running mean: -7.052738\n",
      "epsilon:0.010000 episode_count: 22232. steps_count: 9836979.000000\n",
      "ep 3176: ep_len:500 episode reward: total was 18.300000. running mean: -6.799211\n",
      "ep 3176: ep_len:500 episode reward: total was -14.670000. running mean: -6.877919\n",
      "ep 3176: ep_len:670 episode reward: total was -22.410000. running mean: -7.033240\n",
      "ep 3176: ep_len:570 episode reward: total was -9.910000. running mean: -7.062007\n",
      "ep 3176: ep_len:89 episode reward: total was -8.940000. running mean: -7.080787\n",
      "ep 3176: ep_len:595 episode reward: total was -12.530000. running mean: -7.135279\n",
      "ep 3176: ep_len:530 episode reward: total was 1.220000. running mean: -7.051727\n",
      "epsilon:0.010000 episode_count: 22239. steps_count: 9840433.000000\n",
      "ep 3177: ep_len:500 episode reward: total was 16.310000. running mean: -6.818109\n",
      "ep 3177: ep_len:500 episode reward: total was 2.230000. running mean: -6.727628\n",
      "ep 3177: ep_len:555 episode reward: total was -3.190000. running mean: -6.692252\n",
      "ep 3177: ep_len:500 episode reward: total was -0.160000. running mean: -6.626929\n",
      "ep 3177: ep_len:3 episode reward: total was 0.000000. running mean: -6.560660\n",
      "ep 3177: ep_len:620 episode reward: total was -23.700000. running mean: -6.732053\n",
      "ep 3177: ep_len:565 episode reward: total was -9.520000. running mean: -6.759933\n",
      "epsilon:0.010000 episode_count: 22246. steps_count: 9843676.000000\n",
      "ep 3178: ep_len:221 episode reward: total was -7.920000. running mean: -6.771534\n",
      "ep 3178: ep_len:500 episode reward: total was 4.910000. running mean: -6.654718\n",
      "ep 3178: ep_len:630 episode reward: total was -11.930000. running mean: -6.707471\n",
      "ep 3178: ep_len:56 episode reward: total was 1.560000. running mean: -6.624796\n",
      "ep 3178: ep_len:97 episode reward: total was 7.060000. running mean: -6.487948\n",
      "ep 3178: ep_len:610 episode reward: total was 3.040000. running mean: -6.392669\n",
      "ep 3178: ep_len:545 episode reward: total was -2.920000. running mean: -6.357942\n",
      "epsilon:0.010000 episode_count: 22253. steps_count: 9846335.000000\n",
      "ep 3179: ep_len:505 episode reward: total was -0.410000. running mean: -6.298463\n",
      "ep 3179: ep_len:595 episode reward: total was -0.540000. running mean: -6.240878\n",
      "ep 3179: ep_len:660 episode reward: total was -8.380000. running mean: -6.262269\n",
      "ep 3179: ep_len:530 episode reward: total was -21.590000. running mean: -6.415547\n",
      "ep 3179: ep_len:91 episode reward: total was -11.450000. running mean: -6.465891\n",
      "ep 3179: ep_len:324 episode reward: total was 3.710000. running mean: -6.364132\n",
      "ep 3179: ep_len:630 episode reward: total was 0.820000. running mean: -6.292291\n",
      "epsilon:0.010000 episode_count: 22260. steps_count: 9849670.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3180: ep_len:510 episode reward: total was 9.490000. running mean: -6.134468\n",
      "ep 3180: ep_len:575 episode reward: total was -0.850000. running mean: -6.081623\n",
      "ep 3180: ep_len:72 episode reward: total was -0.460000. running mean: -6.025407\n",
      "ep 3180: ep_len:520 episode reward: total was -5.950000. running mean: -6.024653\n",
      "ep 3180: ep_len:124 episode reward: total was -7.440000. running mean: -6.038807\n",
      "ep 3180: ep_len:785 episode reward: total was -114.780000. running mean: -7.126219\n",
      "ep 3180: ep_len:635 episode reward: total was -54.940000. running mean: -7.604356\n",
      "epsilon:0.010000 episode_count: 22267. steps_count: 9852891.000000\n",
      "ep 3181: ep_len:555 episode reward: total was 10.450000. running mean: -7.423813\n",
      "ep 3181: ep_len:535 episode reward: total was -9.410000. running mean: -7.443675\n",
      "ep 3181: ep_len:655 episode reward: total was -7.150000. running mean: -7.440738\n",
      "ep 3181: ep_len:575 episode reward: total was 13.470000. running mean: -7.231631\n",
      "ep 3181: ep_len:3 episode reward: total was 0.000000. running mean: -7.159314\n",
      "ep 3181: ep_len:610 episode reward: total was -29.230000. running mean: -7.380021\n",
      "ep 3181: ep_len:500 episode reward: total was -7.780000. running mean: -7.384021\n",
      "epsilon:0.010000 episode_count: 22274. steps_count: 9856324.000000\n",
      "ep 3182: ep_len:215 episode reward: total was -2.390000. running mean: -7.334081\n",
      "ep 3182: ep_len:500 episode reward: total was 0.380000. running mean: -7.256940\n",
      "ep 3182: ep_len:565 episode reward: total was -1.160000. running mean: -7.195970\n",
      "ep 3182: ep_len:540 episode reward: total was 7.050000. running mean: -7.053511\n",
      "ep 3182: ep_len:92 episode reward: total was -6.950000. running mean: -7.052476\n",
      "ep 3182: ep_len:590 episode reward: total was -10.690000. running mean: -7.088851\n",
      "ep 3182: ep_len:545 episode reward: total was -20.390000. running mean: -7.221862\n",
      "epsilon:0.010000 episode_count: 22281. steps_count: 9859371.000000\n",
      "ep 3183: ep_len:695 episode reward: total was -19.190000. running mean: -7.341544\n",
      "ep 3183: ep_len:590 episode reward: total was 5.210000. running mean: -7.216028\n",
      "ep 3183: ep_len:580 episode reward: total was 7.480000. running mean: -7.069068\n",
      "ep 3183: ep_len:570 episode reward: total was 15.940000. running mean: -6.838977\n",
      "ep 3183: ep_len:28 episode reward: total was 0.000000. running mean: -6.770588\n",
      "ep 3183: ep_len:570 episode reward: total was -28.330000. running mean: -6.986182\n",
      "ep 3183: ep_len:545 episode reward: total was -5.700000. running mean: -6.973320\n",
      "epsilon:0.010000 episode_count: 22288. steps_count: 9862949.000000\n",
      "ep 3184: ep_len:500 episode reward: total was 6.820000. running mean: -6.835387\n",
      "ep 3184: ep_len:545 episode reward: total was -17.700000. running mean: -6.944033\n",
      "ep 3184: ep_len:625 episode reward: total was -2.340000. running mean: -6.897992\n",
      "ep 3184: ep_len:500 episode reward: total was 11.010000. running mean: -6.718913\n",
      "ep 3184: ep_len:3 episode reward: total was 0.000000. running mean: -6.651723\n",
      "ep 3184: ep_len:570 episode reward: total was -23.620000. running mean: -6.821406\n",
      "ep 3184: ep_len:580 episode reward: total was 4.570000. running mean: -6.707492\n",
      "epsilon:0.010000 episode_count: 22295. steps_count: 9866272.000000\n",
      "ep 3185: ep_len:860 episode reward: total was -45.120000. running mean: -7.091617\n",
      "ep 3185: ep_len:595 episode reward: total was -11.490000. running mean: -7.135601\n",
      "ep 3185: ep_len:79 episode reward: total was 1.050000. running mean: -7.053745\n",
      "ep 3185: ep_len:500 episode reward: total was 11.090000. running mean: -6.872308\n",
      "ep 3185: ep_len:3 episode reward: total was 0.000000. running mean: -6.803585\n",
      "ep 3185: ep_len:575 episode reward: total was -1.090000. running mean: -6.746449\n",
      "ep 3185: ep_len:550 episode reward: total was -1.250000. running mean: -6.691484\n",
      "epsilon:0.010000 episode_count: 22302. steps_count: 9869434.000000\n",
      "ep 3186: ep_len:565 episode reward: total was -15.210000. running mean: -6.776669\n",
      "ep 3186: ep_len:510 episode reward: total was 2.160000. running mean: -6.687303\n",
      "ep 3186: ep_len:79 episode reward: total was -0.970000. running mean: -6.630130\n",
      "ep 3186: ep_len:36 episode reward: total was 2.530000. running mean: -6.538528\n",
      "ep 3186: ep_len:3 episode reward: total was 0.000000. running mean: -6.473143\n",
      "ep 3186: ep_len:500 episode reward: total was 6.270000. running mean: -6.345712\n",
      "ep 3186: ep_len:595 episode reward: total was 0.500000. running mean: -6.277254\n",
      "epsilon:0.010000 episode_count: 22309. steps_count: 9871722.000000\n",
      "ep 3187: ep_len:505 episode reward: total was 11.350000. running mean: -6.100982\n",
      "ep 3187: ep_len:545 episode reward: total was -6.830000. running mean: -6.108272\n",
      "ep 3187: ep_len:715 episode reward: total was -70.340000. running mean: -6.750589\n",
      "ep 3187: ep_len:121 episode reward: total was 4.580000. running mean: -6.637284\n",
      "ep 3187: ep_len:116 episode reward: total was -0.920000. running mean: -6.580111\n",
      "ep 3187: ep_len:520 episode reward: total was 8.680000. running mean: -6.427510\n",
      "ep 3187: ep_len:600 episode reward: total was -2.000000. running mean: -6.383234\n",
      "epsilon:0.010000 episode_count: 22316. steps_count: 9874844.000000\n",
      "ep 3188: ep_len:259 episode reward: total was 3.630000. running mean: -6.283102\n",
      "ep 3188: ep_len:359 episode reward: total was -7.780000. running mean: -6.298071\n",
      "ep 3188: ep_len:550 episode reward: total was -6.290000. running mean: -6.297990\n",
      "ep 3188: ep_len:520 episode reward: total was 2.400000. running mean: -6.211010\n",
      "ep 3188: ep_len:3 episode reward: total was 0.000000. running mean: -6.148900\n",
      "ep 3188: ep_len:625 episode reward: total was 1.040000. running mean: -6.077011\n",
      "ep 3188: ep_len:600 episode reward: total was -3.770000. running mean: -6.053941\n",
      "epsilon:0.010000 episode_count: 22323. steps_count: 9877760.000000\n",
      "ep 3189: ep_len:505 episode reward: total was 6.970000. running mean: -5.923702\n",
      "ep 3189: ep_len:555 episode reward: total was -14.390000. running mean: -6.008365\n",
      "ep 3189: ep_len:560 episode reward: total was -16.770000. running mean: -6.115981\n",
      "ep 3189: ep_len:520 episode reward: total was 9.060000. running mean: -5.964221\n",
      "ep 3189: ep_len:53 episode reward: total was 5.000000. running mean: -5.854579\n",
      "ep 3189: ep_len:500 episode reward: total was -8.280000. running mean: -5.878833\n",
      "ep 3189: ep_len:525 episode reward: total was -2.040000. running mean: -5.840445\n",
      "epsilon:0.010000 episode_count: 22330. steps_count: 9880978.000000\n",
      "ep 3190: ep_len:125 episode reward: total was 2.590000. running mean: -5.756141\n",
      "ep 3190: ep_len:353 episode reward: total was -29.780000. running mean: -5.996379\n",
      "ep 3190: ep_len:500 episode reward: total was 3.410000. running mean: -5.902315\n",
      "ep 3190: ep_len:505 episode reward: total was -11.050000. running mean: -5.953792\n",
      "ep 3190: ep_len:91 episode reward: total was 4.050000. running mean: -5.853754\n",
      "ep 3190: ep_len:520 episode reward: total was -18.930000. running mean: -5.984517\n",
      "ep 3190: ep_len:505 episode reward: total was -5.570000. running mean: -5.980372\n",
      "epsilon:0.010000 episode_count: 22337. steps_count: 9883577.000000\n",
      "ep 3191: ep_len:500 episode reward: total was 1.320000. running mean: -5.907368\n",
      "ep 3191: ep_len:530 episode reward: total was 7.540000. running mean: -5.772894\n",
      "ep 3191: ep_len:439 episode reward: total was -18.770000. running mean: -5.902865\n",
      "ep 3191: ep_len:525 episode reward: total was 2.520000. running mean: -5.818637\n",
      "ep 3191: ep_len:43 episode reward: total was -3.500000. running mean: -5.795450\n",
      "ep 3191: ep_len:231 episode reward: total was 7.680000. running mean: -5.660696\n",
      "ep 3191: ep_len:525 episode reward: total was -3.960000. running mean: -5.643689\n",
      "epsilon:0.010000 episode_count: 22344. steps_count: 9886370.000000\n",
      "ep 3192: ep_len:570 episode reward: total was 17.980000. running mean: -5.407452\n",
      "ep 3192: ep_len:500 episode reward: total was -4.610000. running mean: -5.399477\n",
      "ep 3192: ep_len:550 episode reward: total was 1.970000. running mean: -5.325783\n",
      "ep 3192: ep_len:610 episode reward: total was 20.030000. running mean: -5.072225\n",
      "ep 3192: ep_len:3 episode reward: total was 0.000000. running mean: -5.021503\n",
      "ep 3192: ep_len:520 episode reward: total was -22.830000. running mean: -5.199588\n",
      "ep 3192: ep_len:515 episode reward: total was -8.300000. running mean: -5.230592\n",
      "epsilon:0.010000 episode_count: 22351. steps_count: 9889638.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3193: ep_len:520 episode reward: total was -1.360000. running mean: -5.191886\n",
      "ep 3193: ep_len:515 episode reward: total was -17.070000. running mean: -5.310667\n",
      "ep 3193: ep_len:640 episode reward: total was -0.890000. running mean: -5.266460\n",
      "ep 3193: ep_len:555 episode reward: total was 10.590000. running mean: -5.107896\n",
      "ep 3193: ep_len:3 episode reward: total was 0.000000. running mean: -5.056817\n",
      "ep 3193: ep_len:645 episode reward: total was -89.800000. running mean: -5.904248\n",
      "ep 3193: ep_len:535 episode reward: total was -21.880000. running mean: -6.064006\n",
      "epsilon:0.010000 episode_count: 22358. steps_count: 9893051.000000\n",
      "ep 3194: ep_len:104 episode reward: total was 5.600000. running mean: -5.947366\n",
      "ep 3194: ep_len:570 episode reward: total was -12.380000. running mean: -6.011692\n",
      "ep 3194: ep_len:645 episode reward: total was -2.180000. running mean: -5.973375\n",
      "ep 3194: ep_len:525 episode reward: total was -1.540000. running mean: -5.929042\n",
      "ep 3194: ep_len:3 episode reward: total was 0.000000. running mean: -5.869751\n",
      "ep 3194: ep_len:535 episode reward: total was 4.840000. running mean: -5.762654\n",
      "ep 3194: ep_len:570 episode reward: total was -24.350000. running mean: -5.948527\n",
      "epsilon:0.010000 episode_count: 22365. steps_count: 9896003.000000\n",
      "ep 3195: ep_len:500 episode reward: total was 7.240000. running mean: -5.816642\n",
      "ep 3195: ep_len:500 episode reward: total was 16.200000. running mean: -5.596475\n",
      "ep 3195: ep_len:640 episode reward: total was -20.920000. running mean: -5.749711\n",
      "ep 3195: ep_len:500 episode reward: total was 7.540000. running mean: -5.616814\n",
      "ep 3195: ep_len:82 episode reward: total was 5.500000. running mean: -5.505645\n",
      "ep 3195: ep_len:299 episode reward: total was -27.390000. running mean: -5.724489\n",
      "ep 3195: ep_len:580 episode reward: total was -18.070000. running mean: -5.847944\n",
      "epsilon:0.010000 episode_count: 22372. steps_count: 9899104.000000\n",
      "ep 3196: ep_len:570 episode reward: total was 4.100000. running mean: -5.748465\n",
      "ep 3196: ep_len:302 episode reward: total was -9.310000. running mean: -5.784080\n",
      "ep 3196: ep_len:439 episode reward: total was 4.300000. running mean: -5.683239\n",
      "ep 3196: ep_len:520 episode reward: total was 13.550000. running mean: -5.490907\n",
      "ep 3196: ep_len:3 episode reward: total was 0.000000. running mean: -5.435998\n",
      "ep 3196: ep_len:540 episode reward: total was -20.420000. running mean: -5.585838\n",
      "ep 3196: ep_len:585 episode reward: total was -1.470000. running mean: -5.544679\n",
      "epsilon:0.010000 episode_count: 22379. steps_count: 9902063.000000\n",
      "ep 3197: ep_len:595 episode reward: total was 11.160000. running mean: -5.377633\n",
      "ep 3197: ep_len:625 episode reward: total was -10.270000. running mean: -5.426556\n",
      "ep 3197: ep_len:630 episode reward: total was 0.720000. running mean: -5.365091\n",
      "ep 3197: ep_len:1075 episode reward: total was -108.350000. running mean: -6.394940\n",
      "ep 3197: ep_len:3 episode reward: total was 0.000000. running mean: -6.330990\n",
      "ep 3197: ep_len:585 episode reward: total was -2.620000. running mean: -6.293880\n",
      "ep 3197: ep_len:610 episode reward: total was -17.060000. running mean: -6.401542\n",
      "epsilon:0.010000 episode_count: 22386. steps_count: 9906186.000000\n",
      "ep 3198: ep_len:120 episode reward: total was 0.070000. running mean: -6.336826\n",
      "ep 3198: ep_len:545 episode reward: total was -52.100000. running mean: -6.794458\n",
      "ep 3198: ep_len:500 episode reward: total was 8.440000. running mean: -6.642113\n",
      "ep 3198: ep_len:500 episode reward: total was 7.080000. running mean: -6.504892\n",
      "ep 3198: ep_len:3 episode reward: total was 0.000000. running mean: -6.439843\n",
      "ep 3198: ep_len:500 episode reward: total was -0.750000. running mean: -6.382945\n",
      "ep 3198: ep_len:550 episode reward: total was -11.390000. running mean: -6.433015\n",
      "epsilon:0.010000 episode_count: 22393. steps_count: 9908904.000000\n",
      "ep 3199: ep_len:595 episode reward: total was 7.480000. running mean: -6.293885\n",
      "ep 3199: ep_len:625 episode reward: total was -48.500000. running mean: -6.715946\n",
      "ep 3199: ep_len:530 episode reward: total was -21.330000. running mean: -6.862087\n",
      "ep 3199: ep_len:520 episode reward: total was 2.110000. running mean: -6.772366\n",
      "ep 3199: ep_len:80 episode reward: total was 4.030000. running mean: -6.664342\n",
      "ep 3199: ep_len:336 episode reward: total was 5.210000. running mean: -6.545599\n",
      "ep 3199: ep_len:575 episode reward: total was 1.200000. running mean: -6.468143\n",
      "epsilon:0.010000 episode_count: 22400. steps_count: 9912165.000000\n",
      "ep 3200: ep_len:630 episode reward: total was -9.250000. running mean: -6.495962\n",
      "ep 3200: ep_len:545 episode reward: total was 16.450000. running mean: -6.266502\n",
      "ep 3200: ep_len:575 episode reward: total was 4.940000. running mean: -6.154437\n",
      "ep 3200: ep_len:525 episode reward: total was 9.590000. running mean: -5.996993\n",
      "ep 3200: ep_len:2 episode reward: total was 0.000000. running mean: -5.937023\n",
      "ep 3200: ep_len:313 episode reward: total was 1.150000. running mean: -5.866152\n",
      "ep 3200: ep_len:280 episode reward: total was -16.330000. running mean: -5.970791\n",
      "epsilon:0.010000 episode_count: 22407. steps_count: 9915035.000000\n",
      "ep 3201: ep_len:565 episode reward: total was 2.650000. running mean: -5.884583\n",
      "ep 3201: ep_len:515 episode reward: total was -2.300000. running mean: -5.848737\n",
      "ep 3201: ep_len:79 episode reward: total was 0.040000. running mean: -5.789850\n",
      "ep 3201: ep_len:640 episode reward: total was -74.680000. running mean: -6.478751\n",
      "ep 3201: ep_len:3 episode reward: total was 0.000000. running mean: -6.413964\n",
      "ep 3201: ep_len:505 episode reward: total was -4.480000. running mean: -6.394624\n",
      "ep 3201: ep_len:308 episode reward: total was -13.280000. running mean: -6.463478\n",
      "epsilon:0.010000 episode_count: 22414. steps_count: 9917650.000000\n",
      "ep 3202: ep_len:600 episode reward: total was -19.020000. running mean: -6.589043\n",
      "ep 3202: ep_len:500 episode reward: total was 16.210000. running mean: -6.361053\n",
      "ep 3202: ep_len:685 episode reward: total was -41.780000. running mean: -6.715242\n",
      "ep 3202: ep_len:510 episode reward: total was -15.520000. running mean: -6.803290\n",
      "ep 3202: ep_len:129 episode reward: total was 4.550000. running mean: -6.689757\n",
      "ep 3202: ep_len:830 episode reward: total was -59.840000. running mean: -7.221259\n",
      "ep 3202: ep_len:530 episode reward: total was -11.930000. running mean: -7.268347\n",
      "epsilon:0.010000 episode_count: 22421. steps_count: 9921434.000000\n",
      "ep 3203: ep_len:525 episode reward: total was -17.860000. running mean: -7.374263\n",
      "ep 3203: ep_len:565 episode reward: total was 4.570000. running mean: -7.254821\n",
      "ep 3203: ep_len:575 episode reward: total was -2.740000. running mean: -7.209672\n",
      "ep 3203: ep_len:565 episode reward: total was 5.460000. running mean: -7.082976\n",
      "ep 3203: ep_len:52 episode reward: total was 5.000000. running mean: -6.962146\n",
      "ep 3203: ep_len:595 episode reward: total was -0.150000. running mean: -6.894024\n",
      "ep 3203: ep_len:193 episode reward: total was -9.880000. running mean: -6.923884\n",
      "epsilon:0.010000 episode_count: 22428. steps_count: 9924504.000000\n",
      "ep 3204: ep_len:580 episode reward: total was -2.950000. running mean: -6.884145\n",
      "ep 3204: ep_len:525 episode reward: total was 6.080000. running mean: -6.754504\n",
      "ep 3204: ep_len:462 episode reward: total was -3.710000. running mean: -6.724059\n",
      "ep 3204: ep_len:487 episode reward: total was -20.010000. running mean: -6.856918\n",
      "ep 3204: ep_len:112 episode reward: total was 3.050000. running mean: -6.757849\n",
      "ep 3204: ep_len:645 episode reward: total was -20.760000. running mean: -6.897871\n",
      "ep 3204: ep_len:700 episode reward: total was -62.350000. running mean: -7.452392\n",
      "epsilon:0.010000 episode_count: 22435. steps_count: 9928015.000000\n",
      "ep 3205: ep_len:610 episode reward: total was 3.470000. running mean: -7.343168\n",
      "ep 3205: ep_len:500 episode reward: total was 17.240000. running mean: -7.097336\n",
      "ep 3205: ep_len:590 episode reward: total was -13.740000. running mean: -7.163763\n",
      "ep 3205: ep_len:640 episode reward: total was -64.120000. running mean: -7.733325\n",
      "ep 3205: ep_len:93 episode reward: total was 4.540000. running mean: -7.610592\n",
      "ep 3205: ep_len:670 episode reward: total was -1.640000. running mean: -7.550886\n",
      "ep 3205: ep_len:610 episode reward: total was -13.000000. running mean: -7.605377\n",
      "epsilon:0.010000 episode_count: 22442. steps_count: 9931728.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3206: ep_len:229 episode reward: total was 5.130000. running mean: -7.478024\n",
      "ep 3206: ep_len:280 episode reward: total was -8.860000. running mean: -7.491843\n",
      "ep 3206: ep_len:540 episode reward: total was -10.300000. running mean: -7.519925\n",
      "ep 3206: ep_len:47 episode reward: total was 1.070000. running mean: -7.434026\n",
      "ep 3206: ep_len:128 episode reward: total was 6.050000. running mean: -7.299185\n",
      "ep 3206: ep_len:645 episode reward: total was 9.040000. running mean: -7.135794\n",
      "ep 3206: ep_len:500 episode reward: total was -5.820000. running mean: -7.122636\n",
      "epsilon:0.010000 episode_count: 22449. steps_count: 9934097.000000\n",
      "ep 3207: ep_len:565 episode reward: total was -15.700000. running mean: -7.208409\n",
      "ep 3207: ep_len:500 episode reward: total was 6.860000. running mean: -7.067725\n",
      "ep 3207: ep_len:645 episode reward: total was 10.200000. running mean: -6.895048\n",
      "ep 3207: ep_len:585 episode reward: total was -12.470000. running mean: -6.950797\n",
      "ep 3207: ep_len:55 episode reward: total was 4.000000. running mean: -6.841289\n",
      "ep 3207: ep_len:241 episode reward: total was 8.620000. running mean: -6.686677\n",
      "ep 3207: ep_len:555 episode reward: total was 1.030000. running mean: -6.609510\n",
      "epsilon:0.010000 episode_count: 22456. steps_count: 9937243.000000\n",
      "ep 3208: ep_len:500 episode reward: total was 3.110000. running mean: -6.512315\n",
      "ep 3208: ep_len:590 episode reward: total was -8.550000. running mean: -6.532692\n",
      "ep 3208: ep_len:625 episode reward: total was 0.990000. running mean: -6.457465\n",
      "ep 3208: ep_len:500 episode reward: total was -29.140000. running mean: -6.684290\n",
      "ep 3208: ep_len:3 episode reward: total was 0.000000. running mean: -6.617447\n",
      "ep 3208: ep_len:530 episode reward: total was -4.950000. running mean: -6.600773\n",
      "ep 3208: ep_len:279 episode reward: total was -12.330000. running mean: -6.658065\n",
      "epsilon:0.010000 episode_count: 22463. steps_count: 9940270.000000\n",
      "ep 3209: ep_len:120 episode reward: total was 2.580000. running mean: -6.565684\n",
      "ep 3209: ep_len:530 episode reward: total was 21.200000. running mean: -6.288027\n",
      "ep 3209: ep_len:396 episode reward: total was 7.240000. running mean: -6.152747\n",
      "ep 3209: ep_len:545 episode reward: total was 4.930000. running mean: -6.041920\n",
      "ep 3209: ep_len:66 episode reward: total was 3.020000. running mean: -5.951300\n",
      "ep 3209: ep_len:530 episode reward: total was -20.350000. running mean: -6.095287\n",
      "ep 3209: ep_len:296 episode reward: total was -10.790000. running mean: -6.142235\n",
      "epsilon:0.010000 episode_count: 22470. steps_count: 9942753.000000\n",
      "ep 3210: ep_len:575 episode reward: total was -5.670000. running mean: -6.137512\n",
      "ep 3210: ep_len:645 episode reward: total was 21.980000. running mean: -5.856337\n",
      "ep 3210: ep_len:500 episode reward: total was -0.250000. running mean: -5.800274\n",
      "ep 3210: ep_len:550 episode reward: total was 3.410000. running mean: -5.708171\n",
      "ep 3210: ep_len:69 episode reward: total was -5.980000. running mean: -5.710889\n",
      "ep 3210: ep_len:500 episode reward: total was 0.120000. running mean: -5.652580\n",
      "ep 3210: ep_len:615 episode reward: total was -19.500000. running mean: -5.791055\n",
      "epsilon:0.010000 episode_count: 22477. steps_count: 9946207.000000\n",
      "ep 3211: ep_len:500 episode reward: total was 6.270000. running mean: -5.670444\n",
      "ep 3211: ep_len:565 episode reward: total was 16.890000. running mean: -5.444840\n",
      "ep 3211: ep_len:655 episode reward: total was -27.290000. running mean: -5.663291\n",
      "ep 3211: ep_len:110 episode reward: total was 2.600000. running mean: -5.580658\n",
      "ep 3211: ep_len:106 episode reward: total was 7.540000. running mean: -5.449452\n",
      "ep 3211: ep_len:555 episode reward: total was -12.690000. running mean: -5.521857\n",
      "ep 3211: ep_len:520 episode reward: total was -8.320000. running mean: -5.549839\n",
      "epsilon:0.010000 episode_count: 22484. steps_count: 9949218.000000\n",
      "ep 3212: ep_len:620 episode reward: total was -0.380000. running mean: -5.498140\n",
      "ep 3212: ep_len:570 episode reward: total was 5.170000. running mean: -5.391459\n",
      "ep 3212: ep_len:500 episode reward: total was 0.420000. running mean: -5.333344\n",
      "ep 3212: ep_len:510 episode reward: total was -20.080000. running mean: -5.480811\n",
      "ep 3212: ep_len:3 episode reward: total was 0.000000. running mean: -5.426003\n",
      "ep 3212: ep_len:540 episode reward: total was -2.020000. running mean: -5.391943\n",
      "ep 3212: ep_len:500 episode reward: total was -7.280000. running mean: -5.410823\n",
      "epsilon:0.010000 episode_count: 22491. steps_count: 9952461.000000\n",
      "ep 3213: ep_len:565 episode reward: total was -12.550000. running mean: -5.482215\n",
      "ep 3213: ep_len:505 episode reward: total was 9.700000. running mean: -5.330393\n",
      "ep 3213: ep_len:500 episode reward: total was 2.920000. running mean: -5.247889\n",
      "ep 3213: ep_len:565 episode reward: total was -36.620000. running mean: -5.561610\n",
      "ep 3213: ep_len:91 episode reward: total was -1.460000. running mean: -5.520594\n",
      "ep 3213: ep_len:510 episode reward: total was 6.950000. running mean: -5.395888\n",
      "ep 3213: ep_len:510 episode reward: total was -3.260000. running mean: -5.374529\n",
      "epsilon:0.010000 episode_count: 22498. steps_count: 9955707.000000\n",
      "ep 3214: ep_len:615 episode reward: total was 6.630000. running mean: -5.254484\n",
      "ep 3214: ep_len:505 episode reward: total was 0.710000. running mean: -5.194839\n",
      "ep 3214: ep_len:525 episode reward: total was 0.810000. running mean: -5.134791\n",
      "ep 3214: ep_len:395 episode reward: total was 7.380000. running mean: -5.009643\n",
      "ep 3214: ep_len:101 episode reward: total was 7.040000. running mean: -4.889146\n",
      "ep 3214: ep_len:500 episode reward: total was -28.960000. running mean: -5.129855\n",
      "ep 3214: ep_len:284 episode reward: total was -12.290000. running mean: -5.201456\n",
      "epsilon:0.010000 episode_count: 22505. steps_count: 9958632.000000\n",
      "ep 3215: ep_len:545 episode reward: total was 2.410000. running mean: -5.125342\n",
      "ep 3215: ep_len:1000 episode reward: total was -63.590000. running mean: -5.709988\n",
      "ep 3215: ep_len:590 episode reward: total was 6.150000. running mean: -5.591388\n",
      "ep 3215: ep_len:565 episode reward: total was -5.640000. running mean: -5.591874\n",
      "ep 3215: ep_len:93 episode reward: total was 4.020000. running mean: -5.495756\n",
      "ep 3215: ep_len:500 episode reward: total was -48.260000. running mean: -5.923398\n",
      "ep 3215: ep_len:535 episode reward: total was -5.910000. running mean: -5.923264\n",
      "epsilon:0.010000 episode_count: 22512. steps_count: 9962460.000000\n",
      "ep 3216: ep_len:640 episode reward: total was -15.720000. running mean: -6.021232\n",
      "ep 3216: ep_len:610 episode reward: total was -14.020000. running mean: -6.101219\n",
      "ep 3216: ep_len:520 episode reward: total was -6.080000. running mean: -6.101007\n",
      "ep 3216: ep_len:510 episode reward: total was 15.960000. running mean: -5.880397\n",
      "ep 3216: ep_len:88 episode reward: total was 4.040000. running mean: -5.781193\n",
      "ep 3216: ep_len:580 episode reward: total was -11.660000. running mean: -5.839981\n",
      "ep 3216: ep_len:675 episode reward: total was -33.190000. running mean: -6.113481\n",
      "epsilon:0.010000 episode_count: 22519. steps_count: 9966083.000000\n",
      "ep 3217: ep_len:580 episode reward: total was -20.200000. running mean: -6.254346\n",
      "ep 3217: ep_len:505 episode reward: total was 17.370000. running mean: -6.018103\n",
      "ep 3217: ep_len:670 episode reward: total was -29.020000. running mean: -6.248122\n",
      "ep 3217: ep_len:525 episode reward: total was 9.370000. running mean: -6.091941\n",
      "ep 3217: ep_len:3 episode reward: total was 0.000000. running mean: -6.031021\n",
      "ep 3217: ep_len:635 episode reward: total was -4.940000. running mean: -6.020111\n",
      "ep 3217: ep_len:550 episode reward: total was -27.150000. running mean: -6.231410\n",
      "epsilon:0.010000 episode_count: 22526. steps_count: 9969551.000000\n",
      "ep 3218: ep_len:505 episode reward: total was -21.430000. running mean: -6.383396\n",
      "ep 3218: ep_len:510 episode reward: total was -4.820000. running mean: -6.367762\n",
      "ep 3218: ep_len:410 episode reward: total was -0.790000. running mean: -6.311984\n",
      "ep 3218: ep_len:530 episode reward: total was 3.390000. running mean: -6.214964\n",
      "ep 3218: ep_len:3 episode reward: total was 0.000000. running mean: -6.152815\n",
      "ep 3218: ep_len:500 episode reward: total was 3.760000. running mean: -6.053687\n",
      "ep 3218: ep_len:515 episode reward: total was -15.130000. running mean: -6.144450\n",
      "epsilon:0.010000 episode_count: 22533. steps_count: 9972524.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3219: ep_len:229 episode reward: total was -3.380000. running mean: -6.116805\n",
      "ep 3219: ep_len:368 episode reward: total was -2.310000. running mean: -6.078737\n",
      "ep 3219: ep_len:615 episode reward: total was 3.970000. running mean: -5.978250\n",
      "ep 3219: ep_len:500 episode reward: total was -35.690000. running mean: -6.275367\n",
      "ep 3219: ep_len:3 episode reward: total was 0.000000. running mean: -6.212614\n",
      "ep 3219: ep_len:535 episode reward: total was 0.340000. running mean: -6.147088\n",
      "ep 3219: ep_len:535 episode reward: total was -18.070000. running mean: -6.266317\n",
      "epsilon:0.010000 episode_count: 22540. steps_count: 9975309.000000\n",
      "ep 3220: ep_len:500 episode reward: total was 3.900000. running mean: -6.164654\n",
      "ep 3220: ep_len:500 episode reward: total was 0.160000. running mean: -6.101407\n",
      "ep 3220: ep_len:565 episode reward: total was 3.630000. running mean: -6.004093\n",
      "ep 3220: ep_len:545 episode reward: total was -13.000000. running mean: -6.074052\n",
      "ep 3220: ep_len:3 episode reward: total was 0.000000. running mean: -6.013312\n",
      "ep 3220: ep_len:535 episode reward: total was 2.680000. running mean: -5.926378\n",
      "ep 3220: ep_len:550 episode reward: total was -18.090000. running mean: -6.048015\n",
      "epsilon:0.010000 episode_count: 22547. steps_count: 9978507.000000\n",
      "ep 3221: ep_len:130 episode reward: total was 3.610000. running mean: -5.951434\n",
      "ep 3221: ep_len:540 episode reward: total was 22.400000. running mean: -5.667920\n",
      "ep 3221: ep_len:540 episode reward: total was 3.410000. running mean: -5.577141\n",
      "ep 3221: ep_len:635 episode reward: total was -21.330000. running mean: -5.734670\n",
      "ep 3221: ep_len:3 episode reward: total was 0.000000. running mean: -5.677323\n",
      "ep 3221: ep_len:585 episode reward: total was -2.470000. running mean: -5.645250\n",
      "ep 3221: ep_len:625 episode reward: total was -14.550000. running mean: -5.734297\n",
      "epsilon:0.010000 episode_count: 22554. steps_count: 9981565.000000\n",
      "ep 3222: ep_len:645 episode reward: total was -11.670000. running mean: -5.793654\n",
      "ep 3222: ep_len:615 episode reward: total was -9.490000. running mean: -5.830618\n",
      "ep 3222: ep_len:845 episode reward: total was -28.090000. running mean: -6.053211\n",
      "ep 3222: ep_len:525 episode reward: total was -28.570000. running mean: -6.278379\n",
      "ep 3222: ep_len:53 episode reward: total was 5.000000. running mean: -6.165595\n",
      "ep 3222: ep_len:500 episode reward: total was -29.470000. running mean: -6.398640\n",
      "ep 3222: ep_len:610 episode reward: total was -9.350000. running mean: -6.428153\n",
      "epsilon:0.010000 episode_count: 22561. steps_count: 9985358.000000\n",
      "ep 3223: ep_len:210 episode reward: total was -0.370000. running mean: -6.367572\n",
      "ep 3223: ep_len:364 episode reward: total was -7.340000. running mean: -6.377296\n",
      "ep 3223: ep_len:635 episode reward: total was -9.060000. running mean: -6.404123\n",
      "ep 3223: ep_len:585 episode reward: total was -72.680000. running mean: -7.066882\n",
      "ep 3223: ep_len:83 episode reward: total was 6.020000. running mean: -6.936013\n",
      "ep 3223: ep_len:261 episode reward: total was 9.670000. running mean: -6.769953\n",
      "ep 3223: ep_len:168 episode reward: total was -13.880000. running mean: -6.841053\n",
      "epsilon:0.010000 episode_count: 22568. steps_count: 9987664.000000\n",
      "ep 3224: ep_len:540 episode reward: total was 8.430000. running mean: -6.688343\n",
      "ep 3224: ep_len:700 episode reward: total was -17.550000. running mean: -6.796959\n",
      "ep 3224: ep_len:650 episode reward: total was -25.080000. running mean: -6.979790\n",
      "ep 3224: ep_len:505 episode reward: total was -9.660000. running mean: -7.006592\n",
      "ep 3224: ep_len:3 episode reward: total was 0.000000. running mean: -6.936526\n",
      "ep 3224: ep_len:505 episode reward: total was -29.550000. running mean: -7.162661\n",
      "ep 3224: ep_len:580 episode reward: total was -22.890000. running mean: -7.319934\n",
      "epsilon:0.010000 episode_count: 22575. steps_count: 9991147.000000\n",
      "ep 3225: ep_len:88 episode reward: total was 1.540000. running mean: -7.231335\n",
      "ep 3225: ep_len:510 episode reward: total was 12.170000. running mean: -7.037321\n",
      "ep 3225: ep_len:500 episode reward: total was -13.460000. running mean: -7.101548\n",
      "ep 3225: ep_len:600 episode reward: total was 21.970000. running mean: -6.810833\n",
      "ep 3225: ep_len:54 episode reward: total was 5.000000. running mean: -6.692724\n",
      "ep 3225: ep_len:590 episode reward: total was -21.880000. running mean: -6.844597\n",
      "ep 3225: ep_len:510 episode reward: total was -8.590000. running mean: -6.862051\n",
      "epsilon:0.010000 episode_count: 22582. steps_count: 9993999.000000\n",
      "ep 3226: ep_len:565 episode reward: total was 9.430000. running mean: -6.699131\n",
      "ep 3226: ep_len:535 episode reward: total was 20.400000. running mean: -6.428139\n",
      "ep 3226: ep_len:560 episode reward: total was -37.450000. running mean: -6.738358\n",
      "ep 3226: ep_len:530 episode reward: total was -10.180000. running mean: -6.772774\n",
      "ep 3226: ep_len:3 episode reward: total was 0.000000. running mean: -6.705047\n",
      "ep 3226: ep_len:580 episode reward: total was -11.600000. running mean: -6.753996\n",
      "ep 3226: ep_len:565 episode reward: total was -25.090000. running mean: -6.937356\n",
      "epsilon:0.010000 episode_count: 22589. steps_count: 9997337.000000\n",
      "ep 3227: ep_len:610 episode reward: total was 3.430000. running mean: -6.833683\n",
      "ep 3227: ep_len:625 episode reward: total was 19.090000. running mean: -6.574446\n",
      "ep 3227: ep_len:570 episode reward: total was -29.400000. running mean: -6.802701\n",
      "ep 3227: ep_len:155 episode reward: total was 6.110000. running mean: -6.673574\n",
      "ep 3227: ep_len:3 episode reward: total was 0.000000. running mean: -6.606839\n",
      "ep 3227: ep_len:560 episode reward: total was 4.370000. running mean: -6.497070\n",
      "ep 3227: ep_len:565 episode reward: total was -24.190000. running mean: -6.673999\n",
      "epsilon:0.010000 episode_count: 22596. steps_count: 10000425.000000\n",
      "ep 3228: ep_len:590 episode reward: total was -1.540000. running mean: -6.622659\n",
      "ep 3228: ep_len:500 episode reward: total was -9.840000. running mean: -6.654833\n",
      "ep 3228: ep_len:560 episode reward: total was 1.310000. running mean: -6.575185\n",
      "ep 3228: ep_len:565 episode reward: total was 15.440000. running mean: -6.355033\n",
      "ep 3228: ep_len:3 episode reward: total was 0.000000. running mean: -6.291482\n",
      "ep 3228: ep_len:540 episode reward: total was -0.370000. running mean: -6.232268\n",
      "ep 3228: ep_len:184 episode reward: total was -8.920000. running mean: -6.259145\n",
      "epsilon:0.010000 episode_count: 22603. steps_count: 10003367.000000\n",
      "ep 3229: ep_len:585 episode reward: total was 1.550000. running mean: -6.181053\n",
      "ep 3229: ep_len:545 episode reward: total was -36.090000. running mean: -6.480143\n",
      "ep 3229: ep_len:585 episode reward: total was -2.760000. running mean: -6.442941\n",
      "ep 3229: ep_len:500 episode reward: total was -12.090000. running mean: -6.499412\n",
      "ep 3229: ep_len:85 episode reward: total was -10.940000. running mean: -6.543818\n",
      "ep 3229: ep_len:510 episode reward: total was 0.400000. running mean: -6.474380\n",
      "ep 3229: ep_len:515 episode reward: total was -11.820000. running mean: -6.527836\n",
      "epsilon:0.010000 episode_count: 22610. steps_count: 10006692.000000\n",
      "ep 3230: ep_len:500 episode reward: total was 2.890000. running mean: -6.433658\n",
      "ep 3230: ep_len:580 episode reward: total was 2.060000. running mean: -6.348721\n",
      "ep 3230: ep_len:580 episode reward: total was -33.330000. running mean: -6.618534\n",
      "ep 3230: ep_len:123 episode reward: total was 3.620000. running mean: -6.516148\n",
      "ep 3230: ep_len:110 episode reward: total was 4.520000. running mean: -6.405787\n",
      "ep 3230: ep_len:610 episode reward: total was -1.560000. running mean: -6.357329\n",
      "ep 3230: ep_len:550 episode reward: total was -4.060000. running mean: -6.334356\n",
      "epsilon:0.010000 episode_count: 22617. steps_count: 10009745.000000\n",
      "ep 3231: ep_len:590 episode reward: total was 11.450000. running mean: -6.156512\n",
      "ep 3231: ep_len:500 episode reward: total was 17.360000. running mean: -5.921347\n",
      "ep 3231: ep_len:500 episode reward: total was -4.810000. running mean: -5.910234\n",
      "ep 3231: ep_len:158 episode reward: total was 5.130000. running mean: -5.799831\n",
      "ep 3231: ep_len:3 episode reward: total was 0.000000. running mean: -5.741833\n",
      "ep 3231: ep_len:510 episode reward: total was 2.340000. running mean: -5.661015\n",
      "ep 3231: ep_len:600 episode reward: total was -10.780000. running mean: -5.712205\n",
      "epsilon:0.010000 episode_count: 22624. steps_count: 10012606.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3232: ep_len:530 episode reward: total was -11.500000. running mean: -5.770082\n",
      "ep 3232: ep_len:505 episode reward: total was -1.980000. running mean: -5.732182\n",
      "ep 3232: ep_len:530 episode reward: total was -10.230000. running mean: -5.777160\n",
      "ep 3232: ep_len:543 episode reward: total was -26.410000. running mean: -5.983488\n",
      "ep 3232: ep_len:3 episode reward: total was 0.000000. running mean: -5.923653\n",
      "ep 3232: ep_len:580 episode reward: total was 8.190000. running mean: -5.782517\n",
      "ep 3232: ep_len:500 episode reward: total was -14.960000. running mean: -5.874292\n",
      "epsilon:0.010000 episode_count: 22631. steps_count: 10015797.000000\n",
      "ep 3233: ep_len:590 episode reward: total was -0.390000. running mean: -5.819449\n",
      "ep 3233: ep_len:515 episode reward: total was 18.840000. running mean: -5.572854\n",
      "ep 3233: ep_len:575 episode reward: total was -6.270000. running mean: -5.579826\n",
      "ep 3233: ep_len:500 episode reward: total was -20.630000. running mean: -5.730327\n",
      "ep 3233: ep_len:52 episode reward: total was 5.000000. running mean: -5.623024\n",
      "ep 3233: ep_len:670 episode reward: total was -37.300000. running mean: -5.939794\n",
      "ep 3233: ep_len:500 episode reward: total was -22.800000. running mean: -6.108396\n",
      "epsilon:0.010000 episode_count: 22638. steps_count: 10019199.000000\n",
      "ep 3234: ep_len:565 episode reward: total was -18.240000. running mean: -6.229712\n",
      "ep 3234: ep_len:740 episode reward: total was -35.780000. running mean: -6.525215\n",
      "ep 3234: ep_len:645 episode reward: total was -19.720000. running mean: -6.657163\n",
      "ep 3234: ep_len:56 episode reward: total was -1.940000. running mean: -6.609991\n",
      "ep 3234: ep_len:3 episode reward: total was 0.000000. running mean: -6.543891\n",
      "ep 3234: ep_len:312 episode reward: total was -28.340000. running mean: -6.761852\n",
      "ep 3234: ep_len:500 episode reward: total was -1.540000. running mean: -6.709634\n",
      "epsilon:0.010000 episode_count: 22645. steps_count: 10022020.000000\n",
      "ep 3235: ep_len:520 episode reward: total was 6.770000. running mean: -6.574837\n",
      "ep 3235: ep_len:186 episode reward: total was -0.890000. running mean: -6.517989\n",
      "ep 3235: ep_len:500 episode reward: total was -2.500000. running mean: -6.477809\n",
      "ep 3235: ep_len:94 episode reward: total was 2.060000. running mean: -6.392431\n",
      "ep 3235: ep_len:84 episode reward: total was -4.950000. running mean: -6.378007\n",
      "ep 3235: ep_len:500 episode reward: total was 10.280000. running mean: -6.211427\n",
      "ep 3235: ep_len:505 episode reward: total was -9.130000. running mean: -6.240612\n",
      "epsilon:0.010000 episode_count: 22652. steps_count: 10024409.000000\n",
      "ep 3236: ep_len:500 episode reward: total was 3.760000. running mean: -6.140606\n",
      "ep 3236: ep_len:540 episode reward: total was -15.120000. running mean: -6.230400\n",
      "ep 3236: ep_len:555 episode reward: total was -4.350000. running mean: -6.211596\n",
      "ep 3236: ep_len:500 episode reward: total was -15.630000. running mean: -6.305780\n",
      "ep 3236: ep_len:3 episode reward: total was 0.000000. running mean: -6.242722\n",
      "ep 3236: ep_len:305 episode reward: total was -14.820000. running mean: -6.328495\n",
      "ep 3236: ep_len:515 episode reward: total was -4.540000. running mean: -6.310610\n",
      "epsilon:0.010000 episode_count: 22659. steps_count: 10027327.000000\n",
      "ep 3237: ep_len:116 episode reward: total was 1.560000. running mean: -6.231904\n",
      "ep 3237: ep_len:620 episode reward: total was -1.080000. running mean: -6.180385\n",
      "ep 3237: ep_len:550 episode reward: total was -3.750000. running mean: -6.156081\n",
      "ep 3237: ep_len:364 episode reward: total was -17.650000. running mean: -6.271021\n",
      "ep 3237: ep_len:3 episode reward: total was 0.000000. running mean: -6.208310\n",
      "ep 3237: ep_len:510 episode reward: total was -27.520000. running mean: -6.421427\n",
      "ep 3237: ep_len:530 episode reward: total was -3.280000. running mean: -6.390013\n",
      "epsilon:0.010000 episode_count: 22666. steps_count: 10030020.000000\n",
      "ep 3238: ep_len:510 episode reward: total was 1.600000. running mean: -6.310113\n",
      "ep 3238: ep_len:360 episode reward: total was -37.300000. running mean: -6.620012\n",
      "ep 3238: ep_len:590 episode reward: total was -1.380000. running mean: -6.567612\n",
      "ep 3238: ep_len:575 episode reward: total was 11.070000. running mean: -6.391235\n",
      "ep 3238: ep_len:50 episode reward: total was 4.510000. running mean: -6.282223\n",
      "ep 3238: ep_len:169 episode reward: total was 4.110000. running mean: -6.178301\n",
      "ep 3238: ep_len:228 episode reward: total was -7.340000. running mean: -6.189918\n",
      "epsilon:0.010000 episode_count: 22673. steps_count: 10032502.000000\n",
      "ep 3239: ep_len:500 episode reward: total was 5.400000. running mean: -6.074019\n",
      "ep 3239: ep_len:178 episode reward: total was 3.160000. running mean: -5.981678\n",
      "ep 3239: ep_len:575 episode reward: total was -7.250000. running mean: -5.994362\n",
      "ep 3239: ep_len:530 episode reward: total was 15.480000. running mean: -5.779618\n",
      "ep 3239: ep_len:84 episode reward: total was -3.940000. running mean: -5.761222\n",
      "ep 3239: ep_len:182 episode reward: total was 8.120000. running mean: -5.622410\n",
      "ep 3239: ep_len:620 episode reward: total was -19.890000. running mean: -5.765086\n",
      "epsilon:0.010000 episode_count: 22680. steps_count: 10035171.000000\n",
      "ep 3240: ep_len:535 episode reward: total was -32.930000. running mean: -6.036735\n",
      "ep 3240: ep_len:500 episode reward: total was -3.700000. running mean: -6.013367\n",
      "ep 3240: ep_len:500 episode reward: total was -2.580000. running mean: -5.979034\n",
      "ep 3240: ep_len:119 episode reward: total was 1.590000. running mean: -5.903343\n",
      "ep 3240: ep_len:3 episode reward: total was 0.000000. running mean: -5.844310\n",
      "ep 3240: ep_len:159 episode reward: total was 3.570000. running mean: -5.750167\n",
      "ep 3240: ep_len:580 episode reward: total was -0.510000. running mean: -5.697765\n",
      "epsilon:0.010000 episode_count: 22687. steps_count: 10037567.000000\n",
      "ep 3241: ep_len:122 episode reward: total was 3.070000. running mean: -5.610088\n",
      "ep 3241: ep_len:590 episode reward: total was 8.290000. running mean: -5.471087\n",
      "ep 3241: ep_len:615 episode reward: total was -1.510000. running mean: -5.431476\n",
      "ep 3241: ep_len:500 episode reward: total was -27.660000. running mean: -5.653761\n",
      "ep 3241: ep_len:3 episode reward: total was 0.000000. running mean: -5.597223\n",
      "ep 3241: ep_len:635 episode reward: total was 2.760000. running mean: -5.513651\n",
      "ep 3241: ep_len:540 episode reward: total was -7.950000. running mean: -5.538015\n",
      "epsilon:0.010000 episode_count: 22694. steps_count: 10040572.000000\n",
      "ep 3242: ep_len:500 episode reward: total was 16.310000. running mean: -5.319535\n",
      "ep 3242: ep_len:535 episode reward: total was -15.850000. running mean: -5.424839\n",
      "ep 3242: ep_len:580 episode reward: total was 1.960000. running mean: -5.350991\n",
      "ep 3242: ep_len:521 episode reward: total was 3.580000. running mean: -5.261681\n",
      "ep 3242: ep_len:3 episode reward: total was 0.000000. running mean: -5.209064\n",
      "ep 3242: ep_len:500 episode reward: total was 4.530000. running mean: -5.111673\n",
      "ep 3242: ep_len:560 episode reward: total was -0.030000. running mean: -5.060857\n",
      "epsilon:0.010000 episode_count: 22701. steps_count: 10043771.000000\n",
      "ep 3243: ep_len:575 episode reward: total was 11.050000. running mean: -4.899748\n",
      "ep 3243: ep_len:545 episode reward: total was 0.840000. running mean: -4.842351\n",
      "ep 3243: ep_len:500 episode reward: total was 0.510000. running mean: -4.788827\n",
      "ep 3243: ep_len:595 episode reward: total was 13.100000. running mean: -4.609939\n",
      "ep 3243: ep_len:41 episode reward: total was 2.500000. running mean: -4.538839\n",
      "ep 3243: ep_len:680 episode reward: total was 4.930000. running mean: -4.444151\n",
      "ep 3243: ep_len:800 episode reward: total was -120.860000. running mean: -5.608310\n",
      "epsilon:0.010000 episode_count: 22708. steps_count: 10047507.000000\n",
      "ep 3244: ep_len:560 episode reward: total was 5.640000. running mean: -5.495826\n",
      "ep 3244: ep_len:620 episode reward: total was 0.010000. running mean: -5.440768\n",
      "ep 3244: ep_len:605 episode reward: total was -4.220000. running mean: -5.428561\n",
      "ep 3244: ep_len:555 episode reward: total was -3.100000. running mean: -5.405275\n",
      "ep 3244: ep_len:52 episode reward: total was 5.000000. running mean: -5.301222\n",
      "ep 3244: ep_len:283 episode reward: total was 1.150000. running mean: -5.236710\n",
      "ep 3244: ep_len:500 episode reward: total was -4.320000. running mean: -5.227543\n",
      "epsilon:0.010000 episode_count: 22715. steps_count: 10050682.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3245: ep_len:520 episode reward: total was -16.310000. running mean: -5.338367\n",
      "ep 3245: ep_len:510 episode reward: total was -4.590000. running mean: -5.330884\n",
      "ep 3245: ep_len:64 episode reward: total was -2.960000. running mean: -5.307175\n",
      "ep 3245: ep_len:520 episode reward: total was 3.500000. running mean: -5.219103\n",
      "ep 3245: ep_len:91 episode reward: total was 8.030000. running mean: -5.086612\n",
      "ep 3245: ep_len:595 episode reward: total was -2.520000. running mean: -5.060946\n",
      "ep 3245: ep_len:555 episode reward: total was -38.270000. running mean: -5.393037\n",
      "epsilon:0.010000 episode_count: 22722. steps_count: 10053537.000000\n",
      "ep 3246: ep_len:500 episode reward: total was 6.270000. running mean: -5.276406\n",
      "ep 3246: ep_len:510 episode reward: total was 17.200000. running mean: -5.051642\n",
      "ep 3246: ep_len:625 episode reward: total was -35.110000. running mean: -5.352226\n",
      "ep 3246: ep_len:720 episode reward: total was -30.590000. running mean: -5.604603\n",
      "ep 3246: ep_len:3 episode reward: total was 0.000000. running mean: -5.548557\n",
      "ep 3246: ep_len:500 episode reward: total was 2.630000. running mean: -5.466772\n",
      "ep 3246: ep_len:287 episode reward: total was -3.830000. running mean: -5.450404\n",
      "epsilon:0.010000 episode_count: 22729. steps_count: 10056682.000000\n",
      "ep 3247: ep_len:206 episode reward: total was -1.860000. running mean: -5.414500\n",
      "ep 3247: ep_len:575 episode reward: total was -7.500000. running mean: -5.435355\n",
      "ep 3247: ep_len:416 episode reward: total was 5.770000. running mean: -5.323302\n",
      "ep 3247: ep_len:147 episode reward: total was 5.610000. running mean: -5.213969\n",
      "ep 3247: ep_len:3 episode reward: total was 0.000000. running mean: -5.161829\n",
      "ep 3247: ep_len:535 episode reward: total was -1.010000. running mean: -5.120311\n",
      "ep 3247: ep_len:615 episode reward: total was -1.470000. running mean: -5.083807\n",
      "epsilon:0.010000 episode_count: 22736. steps_count: 10059179.000000\n",
      "ep 3248: ep_len:505 episode reward: total was -3.410000. running mean: -5.067069\n",
      "ep 3248: ep_len:315 episode reward: total was -27.340000. running mean: -5.289799\n",
      "ep 3248: ep_len:625 episode reward: total was 7.270000. running mean: -5.164201\n",
      "ep 3248: ep_len:535 episode reward: total was -6.460000. running mean: -5.177159\n",
      "ep 3248: ep_len:93 episode reward: total was -10.440000. running mean: -5.229787\n",
      "ep 3248: ep_len:500 episode reward: total was 1.800000. running mean: -5.159489\n",
      "ep 3248: ep_len:540 episode reward: total was 1.100000. running mean: -5.096894\n",
      "epsilon:0.010000 episode_count: 22743. steps_count: 10062292.000000\n",
      "ep 3249: ep_len:570 episode reward: total was 6.450000. running mean: -4.981425\n",
      "ep 3249: ep_len:585 episode reward: total was -14.850000. running mean: -5.080111\n",
      "ep 3249: ep_len:505 episode reward: total was -3.440000. running mean: -5.063710\n",
      "ep 3249: ep_len:500 episode reward: total was 9.910000. running mean: -4.913973\n",
      "ep 3249: ep_len:101 episode reward: total was -3.930000. running mean: -4.904133\n",
      "ep 3249: ep_len:600 episode reward: total was -32.590000. running mean: -5.180992\n",
      "ep 3249: ep_len:525 episode reward: total was -6.230000. running mean: -5.191482\n",
      "epsilon:0.010000 episode_count: 22750. steps_count: 10065678.000000\n",
      "ep 3250: ep_len:595 episode reward: total was 5.940000. running mean: -5.080167\n",
      "ep 3250: ep_len:515 episode reward: total was -13.090000. running mean: -5.160265\n",
      "ep 3250: ep_len:565 episode reward: total was -7.250000. running mean: -5.181163\n",
      "ep 3250: ep_len:520 episode reward: total was -10.480000. running mean: -5.234151\n",
      "ep 3250: ep_len:3 episode reward: total was 0.000000. running mean: -5.181810\n",
      "ep 3250: ep_len:169 episode reward: total was 7.600000. running mean: -5.053992\n",
      "ep 3250: ep_len:550 episode reward: total was -17.770000. running mean: -5.181152\n",
      "epsilon:0.010000 episode_count: 22757. steps_count: 10068595.000000\n",
      "ep 3251: ep_len:257 episode reward: total was 8.160000. running mean: -5.047740\n",
      "ep 3251: ep_len:345 episode reward: total was -10.310000. running mean: -5.100363\n",
      "ep 3251: ep_len:560 episode reward: total was -6.100000. running mean: -5.110359\n",
      "ep 3251: ep_len:595 episode reward: total was 5.080000. running mean: -5.008455\n",
      "ep 3251: ep_len:102 episode reward: total was 7.040000. running mean: -4.887971\n",
      "ep 3251: ep_len:620 episode reward: total was 9.020000. running mean: -4.748891\n",
      "ep 3251: ep_len:184 episode reward: total was -4.910000. running mean: -4.750502\n",
      "epsilon:0.010000 episode_count: 22764. steps_count: 10071258.000000\n",
      "ep 3252: ep_len:134 episode reward: total was 4.590000. running mean: -4.657097\n",
      "ep 3252: ep_len:188 episode reward: total was -5.390000. running mean: -4.664426\n",
      "ep 3252: ep_len:535 episode reward: total was -9.840000. running mean: -4.716182\n",
      "ep 3252: ep_len:520 episode reward: total was 12.460000. running mean: -4.544420\n",
      "ep 3252: ep_len:3 episode reward: total was 0.000000. running mean: -4.498976\n",
      "ep 3252: ep_len:293 episode reward: total was -0.820000. running mean: -4.462186\n",
      "ep 3252: ep_len:600 episode reward: total was 1.770000. running mean: -4.399864\n",
      "epsilon:0.010000 episode_count: 22771. steps_count: 10073531.000000\n",
      "ep 3253: ep_len:625 episode reward: total was -1.800000. running mean: -4.373866\n",
      "ep 3253: ep_len:358 episode reward: total was -4.780000. running mean: -4.377927\n",
      "ep 3253: ep_len:500 episode reward: total was -5.040000. running mean: -4.384548\n",
      "ep 3253: ep_len:422 episode reward: total was 3.890000. running mean: -4.301802\n",
      "ep 3253: ep_len:3 episode reward: total was 0.000000. running mean: -4.258784\n",
      "ep 3253: ep_len:690 episode reward: total was 7.890000. running mean: -4.137296\n",
      "ep 3253: ep_len:540 episode reward: total was 3.230000. running mean: -4.063624\n",
      "epsilon:0.010000 episode_count: 22778. steps_count: 10076669.000000\n",
      "ep 3254: ep_len:101 episode reward: total was 0.070000. running mean: -4.022287\n",
      "ep 3254: ep_len:196 episode reward: total was 3.170000. running mean: -3.950364\n",
      "ep 3254: ep_len:79 episode reward: total was 1.050000. running mean: -3.900361\n",
      "ep 3254: ep_len:500 episode reward: total was 10.890000. running mean: -3.752457\n",
      "ep 3254: ep_len:48 episode reward: total was 4.500000. running mean: -3.669933\n",
      "ep 3254: ep_len:500 episode reward: total was -21.920000. running mean: -3.852433\n",
      "ep 3254: ep_len:335 episode reward: total was -7.270000. running mean: -3.886609\n",
      "epsilon:0.010000 episode_count: 22785. steps_count: 10078428.000000\n",
      "ep 3255: ep_len:117 episode reward: total was 2.080000. running mean: -3.826943\n",
      "ep 3255: ep_len:545 episode reward: total was -14.390000. running mean: -3.932573\n",
      "ep 3255: ep_len:500 episode reward: total was -5.880000. running mean: -3.952048\n",
      "ep 3255: ep_len:540 episode reward: total was 6.520000. running mean: -3.847327\n",
      "ep 3255: ep_len:77 episode reward: total was -8.450000. running mean: -3.893354\n",
      "ep 3255: ep_len:500 episode reward: total was -4.380000. running mean: -3.898220\n",
      "ep 3255: ep_len:309 episode reward: total was -7.300000. running mean: -3.932238\n",
      "epsilon:0.010000 episode_count: 22792. steps_count: 10081016.000000\n",
      "ep 3256: ep_len:585 episode reward: total was -3.040000. running mean: -3.923316\n",
      "ep 3256: ep_len:625 episode reward: total was 10.450000. running mean: -3.779583\n",
      "ep 3256: ep_len:520 episode reward: total was 2.950000. running mean: -3.712287\n",
      "ep 3256: ep_len:426 episode reward: total was 4.890000. running mean: -3.626264\n",
      "ep 3256: ep_len:3 episode reward: total was 0.000000. running mean: -3.590001\n",
      "ep 3256: ep_len:590 episode reward: total was 3.390000. running mean: -3.520201\n",
      "ep 3256: ep_len:605 episode reward: total was -0.850000. running mean: -3.493499\n",
      "epsilon:0.010000 episode_count: 22799. steps_count: 10084370.000000\n",
      "ep 3257: ep_len:204 episode reward: total was 5.600000. running mean: -3.402564\n",
      "ep 3257: ep_len:500 episode reward: total was -0.480000. running mean: -3.373339\n",
      "ep 3257: ep_len:500 episode reward: total was -9.050000. running mean: -3.430105\n",
      "ep 3257: ep_len:610 episode reward: total was 1.560000. running mean: -3.380204\n",
      "ep 3257: ep_len:53 episode reward: total was 0.500000. running mean: -3.341402\n",
      "ep 3257: ep_len:595 episode reward: total was 4.160000. running mean: -3.266388\n",
      "ep 3257: ep_len:201 episode reward: total was -4.350000. running mean: -3.277224\n",
      "epsilon:0.010000 episode_count: 22806. steps_count: 10087033.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3258: ep_len:610 episode reward: total was 12.640000. running mean: -3.118052\n",
      "ep 3258: ep_len:515 episode reward: total was -3.800000. running mean: -3.124872\n",
      "ep 3258: ep_len:500 episode reward: total was 1.430000. running mean: -3.079323\n",
      "ep 3258: ep_len:500 episode reward: total was -12.570000. running mean: -3.174230\n",
      "ep 3258: ep_len:3 episode reward: total was 0.000000. running mean: -3.142487\n",
      "ep 3258: ep_len:525 episode reward: total was -4.650000. running mean: -3.157562\n",
      "ep 3258: ep_len:186 episode reward: total was -8.390000. running mean: -3.209887\n",
      "epsilon:0.010000 episode_count: 22813. steps_count: 10089872.000000\n",
      "ep 3259: ep_len:510 episode reward: total was 14.480000. running mean: -3.032988\n",
      "ep 3259: ep_len:500 episode reward: total was 0.130000. running mean: -3.001358\n",
      "ep 3259: ep_len:382 episode reward: total was -11.830000. running mean: -3.089644\n",
      "ep 3259: ep_len:500 episode reward: total was 8.320000. running mean: -2.975548\n",
      "ep 3259: ep_len:3 episode reward: total was 0.000000. running mean: -2.945793\n",
      "ep 3259: ep_len:540 episode reward: total was -14.080000. running mean: -3.057135\n",
      "ep 3259: ep_len:209 episode reward: total was -6.880000. running mean: -3.095363\n",
      "epsilon:0.010000 episode_count: 22820. steps_count: 10092516.000000\n",
      "ep 3260: ep_len:580 episode reward: total was -25.420000. running mean: -3.318610\n",
      "ep 3260: ep_len:500 episode reward: total was -4.990000. running mean: -3.335324\n",
      "ep 3260: ep_len:630 episode reward: total was -3.680000. running mean: -3.338770\n",
      "ep 3260: ep_len:510 episode reward: total was -17.020000. running mean: -3.475583\n",
      "ep 3260: ep_len:113 episode reward: total was -11.460000. running mean: -3.555427\n",
      "ep 3260: ep_len:690 episode reward: total was -5.150000. running mean: -3.571372\n",
      "ep 3260: ep_len:580 episode reward: total was -13.880000. running mean: -3.674459\n",
      "epsilon:0.010000 episode_count: 22827. steps_count: 10096119.000000\n",
      "ep 3261: ep_len:675 episode reward: total was -20.210000. running mean: -3.839814\n",
      "ep 3261: ep_len:505 episode reward: total was 15.700000. running mean: -3.644416\n",
      "ep 3261: ep_len:590 episode reward: total was -19.050000. running mean: -3.798472\n",
      "ep 3261: ep_len:118 episode reward: total was 4.110000. running mean: -3.719387\n",
      "ep 3261: ep_len:3 episode reward: total was 0.000000. running mean: -3.682193\n",
      "ep 3261: ep_len:710 episode reward: total was -5.600000. running mean: -3.701371\n",
      "ep 3261: ep_len:590 episode reward: total was -20.020000. running mean: -3.864558\n",
      "epsilon:0.010000 episode_count: 22834. steps_count: 10099310.000000\n",
      "ep 3262: ep_len:500 episode reward: total was 0.730000. running mean: -3.818612\n",
      "ep 3262: ep_len:675 episode reward: total was -13.780000. running mean: -3.918226\n",
      "ep 3262: ep_len:79 episode reward: total was 1.050000. running mean: -3.868544\n",
      "ep 3262: ep_len:535 episode reward: total was -29.460000. running mean: -4.124458\n",
      "ep 3262: ep_len:85 episode reward: total was 4.040000. running mean: -4.042814\n",
      "ep 3262: ep_len:600 episode reward: total was 3.990000. running mean: -3.962486\n",
      "ep 3262: ep_len:515 episode reward: total was -7.570000. running mean: -3.998561\n",
      "epsilon:0.010000 episode_count: 22841. steps_count: 10102299.000000\n",
      "ep 3263: ep_len:560 episode reward: total was -13.720000. running mean: -4.095775\n",
      "ep 3263: ep_len:675 episode reward: total was -12.280000. running mean: -4.177617\n",
      "ep 3263: ep_len:540 episode reward: total was -3.290000. running mean: -4.168741\n",
      "ep 3263: ep_len:515 episode reward: total was -16.120000. running mean: -4.288254\n",
      "ep 3263: ep_len:3 episode reward: total was 0.000000. running mean: -4.245371\n",
      "ep 3263: ep_len:580 episode reward: total was -34.890000. running mean: -4.551817\n",
      "ep 3263: ep_len:303 episode reward: total was -2.270000. running mean: -4.528999\n",
      "epsilon:0.010000 episode_count: 22848. steps_count: 10105475.000000\n",
      "ep 3264: ep_len:500 episode reward: total was 13.860000. running mean: -4.345109\n",
      "ep 3264: ep_len:545 episode reward: total was -17.330000. running mean: -4.474958\n",
      "ep 3264: ep_len:570 episode reward: total was 9.460000. running mean: -4.335609\n",
      "ep 3264: ep_len:520 episode reward: total was 10.490000. running mean: -4.187353\n",
      "ep 3264: ep_len:92 episode reward: total was -11.940000. running mean: -4.264879\n",
      "ep 3264: ep_len:645 episode reward: total was -0.980000. running mean: -4.232030\n",
      "ep 3264: ep_len:620 episode reward: total was -44.470000. running mean: -4.634410\n",
      "epsilon:0.010000 episode_count: 22855. steps_count: 10108967.000000\n",
      "ep 3265: ep_len:218 episode reward: total was 5.630000. running mean: -4.531766\n",
      "ep 3265: ep_len:575 episode reward: total was 11.080000. running mean: -4.375648\n",
      "ep 3265: ep_len:645 episode reward: total was -0.650000. running mean: -4.338392\n",
      "ep 3265: ep_len:555 episode reward: total was 6.970000. running mean: -4.225308\n",
      "ep 3265: ep_len:3 episode reward: total was 0.000000. running mean: -4.183055\n",
      "ep 3265: ep_len:500 episode reward: total was -2.420000. running mean: -4.165424\n",
      "ep 3265: ep_len:540 episode reward: total was -11.920000. running mean: -4.242970\n",
      "epsilon:0.010000 episode_count: 22862. steps_count: 10112003.000000\n",
      "ep 3266: ep_len:520 episode reward: total was 6.360000. running mean: -4.136940\n",
      "ep 3266: ep_len:345 episode reward: total was -5.840000. running mean: -4.153971\n",
      "ep 3266: ep_len:585 episode reward: total was -1.380000. running mean: -4.126231\n",
      "ep 3266: ep_len:505 episode reward: total was 13.960000. running mean: -3.945369\n",
      "ep 3266: ep_len:90 episode reward: total was 2.030000. running mean: -3.885615\n",
      "ep 3266: ep_len:500 episode reward: total was 6.880000. running mean: -3.777959\n",
      "ep 3266: ep_len:585 episode reward: total was -12.880000. running mean: -3.868979\n",
      "epsilon:0.010000 episode_count: 22869. steps_count: 10115133.000000\n",
      "ep 3267: ep_len:236 episode reward: total was 5.120000. running mean: -3.779090\n",
      "ep 3267: ep_len:580 episode reward: total was -15.950000. running mean: -3.900799\n",
      "ep 3267: ep_len:444 episode reward: total was -12.280000. running mean: -3.984591\n",
      "ep 3267: ep_len:530 episode reward: total was 6.630000. running mean: -3.878445\n",
      "ep 3267: ep_len:3 episode reward: total was 0.000000. running mean: -3.839660\n",
      "ep 3267: ep_len:635 episode reward: total was 3.550000. running mean: -3.765764\n",
      "ep 3267: ep_len:565 episode reward: total was -13.110000. running mean: -3.859206\n",
      "epsilon:0.010000 episode_count: 22876. steps_count: 10118126.000000\n",
      "ep 3268: ep_len:580 episode reward: total was 15.030000. running mean: -3.670314\n",
      "ep 3268: ep_len:187 episode reward: total was 0.640000. running mean: -3.627211\n",
      "ep 3268: ep_len:500 episode reward: total was -1.640000. running mean: -3.607339\n",
      "ep 3268: ep_len:575 episode reward: total was 9.080000. running mean: -3.480465\n",
      "ep 3268: ep_len:3 episode reward: total was 0.000000. running mean: -3.445661\n",
      "ep 3268: ep_len:565 episode reward: total was -0.760000. running mean: -3.418804\n",
      "ep 3268: ep_len:510 episode reward: total was -1.490000. running mean: -3.399516\n",
      "epsilon:0.010000 episode_count: 22883. steps_count: 10121046.000000\n",
      "ep 3269: ep_len:555 episode reward: total was -17.770000. running mean: -3.543221\n",
      "ep 3269: ep_len:630 episode reward: total was 9.600000. running mean: -3.411789\n",
      "ep 3269: ep_len:550 episode reward: total was -11.340000. running mean: -3.491071\n",
      "ep 3269: ep_len:500 episode reward: total was -13.520000. running mean: -3.591360\n",
      "ep 3269: ep_len:102 episode reward: total was 4.530000. running mean: -3.510147\n",
      "ep 3269: ep_len:545 episode reward: total was 3.680000. running mean: -3.438245\n",
      "ep 3269: ep_len:340 episode reward: total was -1.230000. running mean: -3.416163\n",
      "epsilon:0.010000 episode_count: 22890. steps_count: 10124268.000000\n",
      "ep 3270: ep_len:560 episode reward: total was -0.910000. running mean: -3.391101\n",
      "ep 3270: ep_len:500 episode reward: total was 17.250000. running mean: -3.184690\n",
      "ep 3270: ep_len:515 episode reward: total was -4.080000. running mean: -3.193643\n",
      "ep 3270: ep_len:500 episode reward: total was 15.070000. running mean: -3.011007\n",
      "ep 3270: ep_len:55 episode reward: total was 2.500000. running mean: -2.955897\n",
      "ep 3270: ep_len:625 episode reward: total was 2.910000. running mean: -2.897238\n",
      "ep 3270: ep_len:530 episode reward: total was -5.330000. running mean: -2.921565\n",
      "epsilon:0.010000 episode_count: 22897. steps_count: 10127553.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3271: ep_len:194 episode reward: total was 3.620000. running mean: -2.856150\n",
      "ep 3271: ep_len:825 episode reward: total was -64.880000. running mean: -3.476388\n",
      "ep 3271: ep_len:580 episode reward: total was -3.180000. running mean: -3.473424\n",
      "ep 3271: ep_len:500 episode reward: total was 16.460000. running mean: -3.274090\n",
      "ep 3271: ep_len:3 episode reward: total was 0.000000. running mean: -3.241349\n",
      "ep 3271: ep_len:607 episode reward: total was -64.090000. running mean: -3.849836\n",
      "ep 3271: ep_len:590 episode reward: total was -1.980000. running mean: -3.831137\n",
      "epsilon:0.010000 episode_count: 22904. steps_count: 10130852.000000\n",
      "ep 3272: ep_len:500 episode reward: total was 7.390000. running mean: -3.718926\n",
      "ep 3272: ep_len:585 episode reward: total was -8.430000. running mean: -3.766037\n",
      "ep 3272: ep_len:595 episode reward: total was -30.530000. running mean: -4.033676\n",
      "ep 3272: ep_len:510 episode reward: total was 18.530000. running mean: -3.808039\n",
      "ep 3272: ep_len:90 episode reward: total was 3.040000. running mean: -3.739559\n",
      "ep 3272: ep_len:645 episode reward: total was -49.670000. running mean: -4.198863\n",
      "ep 3272: ep_len:595 episode reward: total was -4.970000. running mean: -4.206575\n",
      "epsilon:0.010000 episode_count: 22911. steps_count: 10134372.000000\n",
      "ep 3273: ep_len:515 episode reward: total was -16.880000. running mean: -4.333309\n",
      "ep 3273: ep_len:290 episode reward: total was -36.840000. running mean: -4.658376\n",
      "ep 3273: ep_len:685 episode reward: total was -18.260000. running mean: -4.794392\n",
      "ep 3273: ep_len:530 episode reward: total was -10.460000. running mean: -4.851048\n",
      "ep 3273: ep_len:73 episode reward: total was 5.540000. running mean: -4.747138\n",
      "ep 3273: ep_len:585 episode reward: total was -0.490000. running mean: -4.704566\n",
      "ep 3273: ep_len:500 episode reward: total was -3.390000. running mean: -4.691421\n",
      "epsilon:0.010000 episode_count: 22918. steps_count: 10137550.000000\n",
      "ep 3274: ep_len:217 episode reward: total was -5.390000. running mean: -4.698407\n",
      "ep 3274: ep_len:510 episode reward: total was -5.480000. running mean: -4.706223\n",
      "ep 3274: ep_len:620 episode reward: total was -4.500000. running mean: -4.704160\n",
      "ep 3274: ep_len:53 episode reward: total was -2.930000. running mean: -4.686419\n",
      "ep 3274: ep_len:56 episode reward: total was 0.000000. running mean: -4.639555\n",
      "ep 3274: ep_len:515 episode reward: total was -22.570000. running mean: -4.818859\n",
      "ep 3274: ep_len:500 episode reward: total was -47.790000. running mean: -5.248570\n",
      "epsilon:0.010000 episode_count: 22925. steps_count: 10140021.000000\n",
      "ep 3275: ep_len:500 episode reward: total was 4.600000. running mean: -5.150085\n",
      "ep 3275: ep_len:565 episode reward: total was 20.430000. running mean: -4.894284\n",
      "ep 3275: ep_len:500 episode reward: total was -11.000000. running mean: -4.955341\n",
      "ep 3275: ep_len:126 episode reward: total was 1.120000. running mean: -4.894588\n",
      "ep 3275: ep_len:132 episode reward: total was 7.560000. running mean: -4.770042\n",
      "ep 3275: ep_len:520 episode reward: total was -11.400000. running mean: -4.836341\n",
      "ep 3275: ep_len:550 episode reward: total was -18.460000. running mean: -4.972578\n",
      "epsilon:0.010000 episode_count: 22932. steps_count: 10142914.000000\n",
      "ep 3276: ep_len:590 episode reward: total was 2.980000. running mean: -4.893052\n",
      "ep 3276: ep_len:500 episode reward: total was 4.270000. running mean: -4.801422\n",
      "ep 3276: ep_len:61 episode reward: total was 2.550000. running mean: -4.727907\n",
      "ep 3276: ep_len:520 episode reward: total was -1.710000. running mean: -4.697728\n",
      "ep 3276: ep_len:92 episode reward: total was 4.540000. running mean: -4.605351\n",
      "ep 3276: ep_len:515 episode reward: total was -19.920000. running mean: -4.758497\n",
      "ep 3276: ep_len:565 episode reward: total was -20.520000. running mean: -4.916113\n",
      "epsilon:0.010000 episode_count: 22939. steps_count: 10145757.000000\n",
      "ep 3277: ep_len:595 episode reward: total was 0.550000. running mean: -4.861451\n",
      "ep 3277: ep_len:530 episode reward: total was -6.540000. running mean: -4.878237\n",
      "ep 3277: ep_len:500 episode reward: total was -21.920000. running mean: -5.048654\n",
      "ep 3277: ep_len:96 episode reward: total was -2.920000. running mean: -5.027368\n",
      "ep 3277: ep_len:132 episode reward: total was 7.560000. running mean: -4.901494\n",
      "ep 3277: ep_len:600 episode reward: total was 15.160000. running mean: -4.700879\n",
      "ep 3277: ep_len:510 episode reward: total was -23.980000. running mean: -4.893671\n",
      "epsilon:0.010000 episode_count: 22946. steps_count: 10148720.000000\n",
      "ep 3278: ep_len:208 episode reward: total was -3.910000. running mean: -4.883834\n",
      "ep 3278: ep_len:585 episode reward: total was 22.370000. running mean: -4.611295\n",
      "ep 3278: ep_len:595 episode reward: total was -26.030000. running mean: -4.825483\n",
      "ep 3278: ep_len:500 episode reward: total was 16.000000. running mean: -4.617228\n",
      "ep 3278: ep_len:99 episode reward: total was 5.040000. running mean: -4.520655\n",
      "ep 3278: ep_len:675 episode reward: total was 6.480000. running mean: -4.410649\n",
      "ep 3278: ep_len:505 episode reward: total was -13.130000. running mean: -4.497842\n",
      "epsilon:0.010000 episode_count: 22953. steps_count: 10151887.000000\n",
      "ep 3279: ep_len:565 episode reward: total was 8.680000. running mean: -4.366064\n",
      "ep 3279: ep_len:575 episode reward: total was -3.380000. running mean: -4.356203\n",
      "ep 3279: ep_len:580 episode reward: total was 5.880000. running mean: -4.253841\n",
      "ep 3279: ep_len:551 episode reward: total was -31.380000. running mean: -4.525103\n",
      "ep 3279: ep_len:3 episode reward: total was 0.000000. running mean: -4.479852\n",
      "ep 3279: ep_len:580 episode reward: total was -3.040000. running mean: -4.465453\n",
      "ep 3279: ep_len:530 episode reward: total was -4.540000. running mean: -4.466199\n",
      "epsilon:0.010000 episode_count: 22960. steps_count: 10155271.000000\n",
      "ep 3280: ep_len:620 episode reward: total was 13.180000. running mean: -4.289737\n",
      "ep 3280: ep_len:500 episode reward: total was -8.120000. running mean: -4.328039\n",
      "ep 3280: ep_len:595 episode reward: total was 6.900000. running mean: -4.215759\n",
      "ep 3280: ep_len:500 episode reward: total was 10.480000. running mean: -4.068801\n",
      "ep 3280: ep_len:91 episode reward: total was 6.040000. running mean: -3.967713\n",
      "ep 3280: ep_len:565 episode reward: total was -12.540000. running mean: -4.053436\n",
      "ep 3280: ep_len:167 episode reward: total was 1.120000. running mean: -4.001702\n",
      "epsilon:0.010000 episode_count: 22967. steps_count: 10158309.000000\n",
      "ep 3281: ep_len:555 episode reward: total was 9.500000. running mean: -3.866685\n",
      "ep 3281: ep_len:250 episode reward: total was -28.840000. running mean: -4.116418\n",
      "ep 3281: ep_len:640 episode reward: total was 11.200000. running mean: -3.963254\n",
      "ep 3281: ep_len:550 episode reward: total was 10.480000. running mean: -3.818821\n",
      "ep 3281: ep_len:116 episode reward: total was 4.560000. running mean: -3.735033\n",
      "ep 3281: ep_len:580 episode reward: total was -16.160000. running mean: -3.859283\n",
      "ep 3281: ep_len:505 episode reward: total was -16.430000. running mean: -3.984990\n",
      "epsilon:0.010000 episode_count: 22974. steps_count: 10161505.000000\n",
      "ep 3282: ep_len:640 episode reward: total was 8.260000. running mean: -3.862540\n",
      "ep 3282: ep_len:500 episode reward: total was -2.530000. running mean: -3.849215\n",
      "ep 3282: ep_len:630 episode reward: total was -21.980000. running mean: -4.030523\n",
      "ep 3282: ep_len:125 episode reward: total was 2.130000. running mean: -3.968917\n",
      "ep 3282: ep_len:3 episode reward: total was 0.000000. running mean: -3.929228\n",
      "ep 3282: ep_len:230 episode reward: total was 7.160000. running mean: -3.818336\n",
      "ep 3282: ep_len:605 episode reward: total was 5.760000. running mean: -3.722553\n",
      "epsilon:0.010000 episode_count: 22981. steps_count: 10164238.000000\n",
      "ep 3283: ep_len:500 episode reward: total was -17.430000. running mean: -3.859627\n",
      "ep 3283: ep_len:605 episode reward: total was 5.460000. running mean: -3.766431\n",
      "ep 3283: ep_len:431 episode reward: total was -19.740000. running mean: -3.926166\n",
      "ep 3283: ep_len:48 episode reward: total was -1.960000. running mean: -3.906505\n",
      "ep 3283: ep_len:3 episode reward: total was 0.000000. running mean: -3.867440\n",
      "ep 3283: ep_len:500 episode reward: total was -18.780000. running mean: -4.016565\n",
      "ep 3283: ep_len:530 episode reward: total was -5.580000. running mean: -4.032200\n",
      "epsilon:0.010000 episode_count: 22988. steps_count: 10166855.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3284: ep_len:630 episode reward: total was -14.730000. running mean: -4.139178\n",
      "ep 3284: ep_len:500 episode reward: total was 2.050000. running mean: -4.077286\n",
      "ep 3284: ep_len:570 episode reward: total was 2.030000. running mean: -4.016213\n",
      "ep 3284: ep_len:535 episode reward: total was 8.520000. running mean: -3.890851\n",
      "ep 3284: ep_len:91 episode reward: total was 5.550000. running mean: -3.796442\n",
      "ep 3284: ep_len:304 episode reward: total was 7.680000. running mean: -3.681678\n",
      "ep 3284: ep_len:185 episode reward: total was -4.350000. running mean: -3.688361\n",
      "epsilon:0.010000 episode_count: 22995. steps_count: 10169670.000000\n",
      "ep 3285: ep_len:685 episode reward: total was -13.730000. running mean: -3.788778\n",
      "ep 3285: ep_len:590 episode reward: total was -5.860000. running mean: -3.809490\n",
      "ep 3285: ep_len:590 episode reward: total was -9.220000. running mean: -3.863595\n",
      "ep 3285: ep_len:500 episode reward: total was -1.550000. running mean: -3.840459\n",
      "ep 3285: ep_len:89 episode reward: total was 6.030000. running mean: -3.741754\n",
      "ep 3285: ep_len:620 episode reward: total was 3.000000. running mean: -3.674337\n",
      "ep 3285: ep_len:206 episode reward: total was -4.860000. running mean: -3.686193\n",
      "epsilon:0.010000 episode_count: 23002. steps_count: 10172950.000000\n",
      "ep 3286: ep_len:570 episode reward: total was 3.610000. running mean: -3.613231\n",
      "ep 3286: ep_len:500 episode reward: total was -23.310000. running mean: -3.810199\n",
      "ep 3286: ep_len:670 episode reward: total was -1.120000. running mean: -3.783297\n",
      "ep 3286: ep_len:505 episode reward: total was 4.410000. running mean: -3.701364\n",
      "ep 3286: ep_len:3 episode reward: total was 0.000000. running mean: -3.664351\n",
      "ep 3286: ep_len:500 episode reward: total was 6.300000. running mean: -3.564707\n",
      "ep 3286: ep_len:500 episode reward: total was 0.450000. running mean: -3.524560\n",
      "epsilon:0.010000 episode_count: 23009. steps_count: 10176198.000000\n",
      "ep 3287: ep_len:600 episode reward: total was -11.240000. running mean: -3.601714\n",
      "ep 3287: ep_len:500 episode reward: total was 12.720000. running mean: -3.438497\n",
      "ep 3287: ep_len:670 episode reward: total was -25.240000. running mean: -3.656512\n",
      "ep 3287: ep_len:580 episode reward: total was -26.460000. running mean: -3.884547\n",
      "ep 3287: ep_len:3 episode reward: total was 0.000000. running mean: -3.845702\n",
      "ep 3287: ep_len:500 episode reward: total was -1.690000. running mean: -3.824145\n",
      "ep 3287: ep_len:625 episode reward: total was -26.100000. running mean: -4.046903\n",
      "epsilon:0.010000 episode_count: 23016. steps_count: 10179676.000000\n",
      "ep 3288: ep_len:585 episode reward: total was 9.490000. running mean: -3.911534\n",
      "ep 3288: ep_len:500 episode reward: total was 0.790000. running mean: -3.864519\n",
      "ep 3288: ep_len:500 episode reward: total was -5.850000. running mean: -3.884374\n",
      "ep 3288: ep_len:515 episode reward: total was 13.460000. running mean: -3.710930\n",
      "ep 3288: ep_len:122 episode reward: total was 3.560000. running mean: -3.638221\n",
      "ep 3288: ep_len:615 episode reward: total was 4.480000. running mean: -3.557038\n",
      "ep 3288: ep_len:342 episode reward: total was -7.260000. running mean: -3.594068\n",
      "epsilon:0.010000 episode_count: 23023. steps_count: 10182855.000000\n",
      "ep 3289: ep_len:219 episode reward: total was 2.140000. running mean: -3.536727\n",
      "ep 3289: ep_len:510 episode reward: total was -11.340000. running mean: -3.614760\n",
      "ep 3289: ep_len:595 episode reward: total was -42.620000. running mean: -4.004812\n",
      "ep 3289: ep_len:560 episode reward: total was 11.480000. running mean: -3.849964\n",
      "ep 3289: ep_len:3 episode reward: total was 0.000000. running mean: -3.811465\n",
      "ep 3289: ep_len:540 episode reward: total was -9.660000. running mean: -3.869950\n",
      "ep 3289: ep_len:585 episode reward: total was -49.200000. running mean: -4.323251\n",
      "epsilon:0.010000 episode_count: 23030. steps_count: 10185867.000000\n",
      "ep 3290: ep_len:500 episode reward: total was 11.290000. running mean: -4.167118\n",
      "ep 3290: ep_len:500 episode reward: total was 0.120000. running mean: -4.124247\n",
      "ep 3290: ep_len:457 episode reward: total was -2.720000. running mean: -4.110204\n",
      "ep 3290: ep_len:500 episode reward: total was 13.920000. running mean: -3.929902\n",
      "ep 3290: ep_len:3 episode reward: total was 0.000000. running mean: -3.890603\n",
      "ep 3290: ep_len:255 episode reward: total was 2.130000. running mean: -3.830397\n",
      "ep 3290: ep_len:525 episode reward: total was -2.310000. running mean: -3.815193\n",
      "epsilon:0.010000 episode_count: 23037. steps_count: 10188607.000000\n",
      "ep 3291: ep_len:625 episode reward: total was -15.230000. running mean: -3.929341\n",
      "ep 3291: ep_len:525 episode reward: total was -5.020000. running mean: -3.940248\n",
      "ep 3291: ep_len:570 episode reward: total was 1.910000. running mean: -3.881746\n",
      "ep 3291: ep_len:500 episode reward: total was -23.140000. running mean: -4.074328\n",
      "ep 3291: ep_len:3 episode reward: total was 0.000000. running mean: -4.033585\n",
      "ep 3291: ep_len:505 episode reward: total was 8.620000. running mean: -3.907049\n",
      "ep 3291: ep_len:615 episode reward: total was -21.890000. running mean: -4.086878\n",
      "epsilon:0.010000 episode_count: 23044. steps_count: 10191950.000000\n",
      "ep 3292: ep_len:253 episode reward: total was 4.630000. running mean: -3.999710\n",
      "ep 3292: ep_len:535 episode reward: total was 1.200000. running mean: -3.947713\n",
      "ep 3292: ep_len:590 episode reward: total was 2.970000. running mean: -3.878535\n",
      "ep 3292: ep_len:132 episode reward: total was 3.610000. running mean: -3.803650\n",
      "ep 3292: ep_len:104 episode reward: total was 4.530000. running mean: -3.720314\n",
      "ep 3292: ep_len:306 episode reward: total was 2.700000. running mean: -3.656110\n",
      "ep 3292: ep_len:500 episode reward: total was -9.310000. running mean: -3.712649\n",
      "epsilon:0.010000 episode_count: 23051. steps_count: 10194370.000000\n",
      "ep 3293: ep_len:575 episode reward: total was -1.890000. running mean: -3.694423\n",
      "ep 3293: ep_len:680 episode reward: total was -39.310000. running mean: -4.050579\n",
      "ep 3293: ep_len:555 episode reward: total was -8.250000. running mean: -4.092573\n",
      "ep 3293: ep_len:520 episode reward: total was 13.480000. running mean: -3.916847\n",
      "ep 3293: ep_len:3 episode reward: total was 0.000000. running mean: -3.877679\n",
      "ep 3293: ep_len:640 episode reward: total was -5.390000. running mean: -3.892802\n",
      "ep 3293: ep_len:315 episode reward: total was -5.790000. running mean: -3.911774\n",
      "epsilon:0.010000 episode_count: 23058. steps_count: 10197658.000000\n",
      "ep 3294: ep_len:635 episode reward: total was -10.650000. running mean: -3.979156\n",
      "ep 3294: ep_len:530 episode reward: total was -44.770000. running mean: -4.387065\n",
      "ep 3294: ep_len:660 episode reward: total was -5.900000. running mean: -4.402194\n",
      "ep 3294: ep_len:500 episode reward: total was -40.290000. running mean: -4.761072\n",
      "ep 3294: ep_len:55 episode reward: total was 4.000000. running mean: -4.673461\n",
      "ep 3294: ep_len:500 episode reward: total was -9.720000. running mean: -4.723927\n",
      "ep 3294: ep_len:500 episode reward: total was -15.600000. running mean: -4.832687\n",
      "epsilon:0.010000 episode_count: 23065. steps_count: 10201038.000000\n",
      "ep 3295: ep_len:595 episode reward: total was 16.990000. running mean: -4.614460\n",
      "ep 3295: ep_len:500 episode reward: total was 2.820000. running mean: -4.540116\n",
      "ep 3295: ep_len:585 episode reward: total was -3.090000. running mean: -4.525615\n",
      "ep 3295: ep_len:500 episode reward: total was -23.530000. running mean: -4.715659\n",
      "ep 3295: ep_len:55 episode reward: total was 4.000000. running mean: -4.628502\n",
      "ep 3295: ep_len:500 episode reward: total was -6.740000. running mean: -4.649617\n",
      "ep 3295: ep_len:565 episode reward: total was -39.120000. running mean: -4.994321\n",
      "epsilon:0.010000 episode_count: 23072. steps_count: 10204338.000000\n",
      "ep 3296: ep_len:500 episode reward: total was 7.770000. running mean: -4.866678\n",
      "ep 3296: ep_len:500 episode reward: total was 19.360000. running mean: -4.624411\n",
      "ep 3296: ep_len:500 episode reward: total was -9.450000. running mean: -4.672667\n",
      "ep 3296: ep_len:500 episode reward: total was 4.880000. running mean: -4.577140\n",
      "ep 3296: ep_len:94 episode reward: total was 5.520000. running mean: -4.476169\n",
      "ep 3296: ep_len:500 episode reward: total was -24.240000. running mean: -4.673807\n",
      "ep 3296: ep_len:211 episode reward: total was -5.860000. running mean: -4.685669\n",
      "epsilon:0.010000 episode_count: 23079. steps_count: 10207143.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3297: ep_len:660 episode reward: total was -6.620000. running mean: -4.705012\n",
      "ep 3297: ep_len:540 episode reward: total was 0.070000. running mean: -4.657262\n",
      "ep 3297: ep_len:620 episode reward: total was 5.510000. running mean: -4.555589\n",
      "ep 3297: ep_len:600 episode reward: total was 9.450000. running mean: -4.415534\n",
      "ep 3297: ep_len:45 episode reward: total was 3.000000. running mean: -4.341378\n",
      "ep 3297: ep_len:555 episode reward: total was 8.860000. running mean: -4.209364\n",
      "ep 3297: ep_len:620 episode reward: total was 1.230000. running mean: -4.154971\n",
      "epsilon:0.010000 episode_count: 23086. steps_count: 10210783.000000\n",
      "ep 3298: ep_len:545 episode reward: total was -4.280000. running mean: -4.156221\n",
      "ep 3298: ep_len:540 episode reward: total was 1.440000. running mean: -4.100259\n",
      "ep 3298: ep_len:580 episode reward: total was -6.750000. running mean: -4.126756\n",
      "ep 3298: ep_len:625 episode reward: total was 4.450000. running mean: -4.040989\n",
      "ep 3298: ep_len:40 episode reward: total was 2.500000. running mean: -3.975579\n",
      "ep 3298: ep_len:610 episode reward: total was 4.930000. running mean: -3.886523\n",
      "ep 3298: ep_len:565 episode reward: total was -13.470000. running mean: -3.982358\n",
      "epsilon:0.010000 episode_count: 23093. steps_count: 10214288.000000\n",
      "ep 3299: ep_len:565 episode reward: total was 10.010000. running mean: -3.842434\n",
      "ep 3299: ep_len:500 episode reward: total was 23.780000. running mean: -3.566210\n",
      "ep 3299: ep_len:595 episode reward: total was 5.000000. running mean: -3.480548\n",
      "ep 3299: ep_len:500 episode reward: total was 16.370000. running mean: -3.282042\n",
      "ep 3299: ep_len:98 episode reward: total was 6.540000. running mean: -3.183822\n",
      "ep 3299: ep_len:605 episode reward: total was -14.020000. running mean: -3.292184\n",
      "ep 3299: ep_len:211 episode reward: total was -1.330000. running mean: -3.272562\n",
      "epsilon:0.010000 episode_count: 23100. steps_count: 10217362.000000\n",
      "ep 3300: ep_len:585 episode reward: total was 8.490000. running mean: -3.154936\n",
      "ep 3300: ep_len:383 episode reward: total was -20.340000. running mean: -3.326787\n",
      "ep 3300: ep_len:570 episode reward: total was -9.400000. running mean: -3.387519\n",
      "ep 3300: ep_len:120 episode reward: total was 5.590000. running mean: -3.297744\n",
      "ep 3300: ep_len:99 episode reward: total was 5.540000. running mean: -3.209366\n",
      "ep 3300: ep_len:500 episode reward: total was 1.420000. running mean: -3.163073\n",
      "ep 3300: ep_len:500 episode reward: total was -6.450000. running mean: -3.195942\n",
      "epsilon:0.010000 episode_count: 23107. steps_count: 10220119.000000\n",
      "ep 3301: ep_len:545 episode reward: total was 9.680000. running mean: -3.067183\n",
      "ep 3301: ep_len:520 episode reward: total was 6.960000. running mean: -2.966911\n",
      "ep 3301: ep_len:580 episode reward: total was 3.830000. running mean: -2.898942\n",
      "ep 3301: ep_len:39 episode reward: total was -0.470000. running mean: -2.874652\n",
      "ep 3301: ep_len:105 episode reward: total was 3.530000. running mean: -2.810606\n",
      "ep 3301: ep_len:690 episode reward: total was 5.930000. running mean: -2.723200\n",
      "ep 3301: ep_len:500 episode reward: total was -2.300000. running mean: -2.718968\n",
      "epsilon:0.010000 episode_count: 23114. steps_count: 10223098.000000\n",
      "ep 3302: ep_len:620 episode reward: total was 5.520000. running mean: -2.636578\n",
      "ep 3302: ep_len:500 episode reward: total was -13.750000. running mean: -2.747712\n",
      "ep 3302: ep_len:369 episode reward: total was 6.170000. running mean: -2.658535\n",
      "ep 3302: ep_len:500 episode reward: total was 4.450000. running mean: -2.587450\n",
      "ep 3302: ep_len:112 episode reward: total was 5.040000. running mean: -2.511175\n",
      "ep 3302: ep_len:600 episode reward: total was 6.020000. running mean: -2.425863\n",
      "ep 3302: ep_len:520 episode reward: total was -13.430000. running mean: -2.535905\n",
      "epsilon:0.010000 episode_count: 23121. steps_count: 10226319.000000\n",
      "ep 3303: ep_len:640 episode reward: total was -7.670000. running mean: -2.587246\n",
      "ep 3303: ep_len:193 episode reward: total was -5.360000. running mean: -2.614973\n",
      "ep 3303: ep_len:590 episode reward: total was 9.950000. running mean: -2.489324\n",
      "ep 3303: ep_len:630 episode reward: total was 17.690000. running mean: -2.287530\n",
      "ep 3303: ep_len:3 episode reward: total was 0.000000. running mean: -2.264655\n",
      "ep 3303: ep_len:192 episode reward: total was 9.600000. running mean: -2.146008\n",
      "ep 3303: ep_len:540 episode reward: total was -15.980000. running mean: -2.284348\n",
      "epsilon:0.010000 episode_count: 23128. steps_count: 10229107.000000\n",
      "ep 3304: ep_len:222 episode reward: total was -2.930000. running mean: -2.290805\n",
      "ep 3304: ep_len:520 episode reward: total was -6.790000. running mean: -2.335797\n",
      "ep 3304: ep_len:685 episode reward: total was -1.120000. running mean: -2.323639\n",
      "ep 3304: ep_len:56 episode reward: total was 1.560000. running mean: -2.284803\n",
      "ep 3304: ep_len:3 episode reward: total was 0.000000. running mean: -2.261954\n",
      "ep 3304: ep_len:570 episode reward: total was 8.140000. running mean: -2.157935\n",
      "ep 3304: ep_len:595 episode reward: total was -8.590000. running mean: -2.222256\n",
      "epsilon:0.010000 episode_count: 23135. steps_count: 10231758.000000\n",
      "ep 3305: ep_len:620 episode reward: total was 16.500000. running mean: -2.035033\n",
      "ep 3305: ep_len:540 episode reward: total was 19.890000. running mean: -1.815783\n",
      "ep 3305: ep_len:530 episode reward: total was -2.070000. running mean: -1.818325\n",
      "ep 3305: ep_len:500 episode reward: total was 6.450000. running mean: -1.735642\n",
      "ep 3305: ep_len:3 episode reward: total was 0.000000. running mean: -1.718285\n",
      "ep 3305: ep_len:505 episode reward: total was 17.070000. running mean: -1.530402\n",
      "ep 3305: ep_len:317 episode reward: total was -0.300000. running mean: -1.518098\n",
      "epsilon:0.010000 episode_count: 23142. steps_count: 10234773.000000\n",
      "ep 3306: ep_len:550 episode reward: total was 3.570000. running mean: -1.467217\n",
      "ep 3306: ep_len:500 episode reward: total was -7.750000. running mean: -1.530045\n",
      "ep 3306: ep_len:500 episode reward: total was -11.850000. running mean: -1.633245\n",
      "ep 3306: ep_len:550 episode reward: total was 7.490000. running mean: -1.542012\n",
      "ep 3306: ep_len:104 episode reward: total was 6.550000. running mean: -1.461092\n",
      "ep 3306: ep_len:580 episode reward: total was -19.280000. running mean: -1.639281\n",
      "ep 3306: ep_len:341 episode reward: total was -5.240000. running mean: -1.675288\n",
      "epsilon:0.010000 episode_count: 23149. steps_count: 10237898.000000\n",
      "ep 3307: ep_len:510 episode reward: total was 3.580000. running mean: -1.622736\n",
      "ep 3307: ep_len:500 episode reward: total was 2.380000. running mean: -1.582708\n",
      "ep 3307: ep_len:555 episode reward: total was 8.490000. running mean: -1.481981\n",
      "ep 3307: ep_len:520 episode reward: total was -27.040000. running mean: -1.737561\n",
      "ep 3307: ep_len:3 episode reward: total was 0.000000. running mean: -1.720186\n",
      "ep 3307: ep_len:590 episode reward: total was -20.210000. running mean: -1.905084\n",
      "ep 3307: ep_len:570 episode reward: total was -11.090000. running mean: -1.996933\n",
      "epsilon:0.010000 episode_count: 23156. steps_count: 10241146.000000\n",
      "ep 3308: ep_len:720 episode reward: total was -19.220000. running mean: -2.169164\n",
      "ep 3308: ep_len:610 episode reward: total was -12.130000. running mean: -2.268772\n",
      "ep 3308: ep_len:500 episode reward: total was 10.420000. running mean: -2.141884\n",
      "ep 3308: ep_len:505 episode reward: total was 6.420000. running mean: -2.056265\n",
      "ep 3308: ep_len:93 episode reward: total was -11.940000. running mean: -2.155103\n",
      "ep 3308: ep_len:540 episode reward: total was 2.280000. running mean: -2.110752\n",
      "ep 3308: ep_len:163 episode reward: total was -8.920000. running mean: -2.178844\n",
      "epsilon:0.010000 episode_count: 23163. steps_count: 10244277.000000\n",
      "ep 3309: ep_len:645 episode reward: total was -9.740000. running mean: -2.254456\n",
      "ep 3309: ep_len:500 episode reward: total was -24.040000. running mean: -2.472311\n",
      "ep 3309: ep_len:545 episode reward: total was 2.970000. running mean: -2.417888\n",
      "ep 3309: ep_len:500 episode reward: total was -0.070000. running mean: -2.394409\n",
      "ep 3309: ep_len:3 episode reward: total was 0.000000. running mean: -2.370465\n",
      "ep 3309: ep_len:186 episode reward: total was 5.100000. running mean: -2.295761\n",
      "ep 3309: ep_len:545 episode reward: total was -3.450000. running mean: -2.307303\n",
      "epsilon:0.010000 episode_count: 23170. steps_count: 10247201.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3310: ep_len:595 episode reward: total was -12.750000. running mean: -2.411730\n",
      "ep 3310: ep_len:500 episode reward: total was -4.870000. running mean: -2.436313\n",
      "ep 3310: ep_len:615 episode reward: total was -44.590000. running mean: -2.857849\n",
      "ep 3310: ep_len:570 episode reward: total was -0.550000. running mean: -2.834771\n",
      "ep 3310: ep_len:3 episode reward: total was 0.000000. running mean: -2.806423\n",
      "ep 3310: ep_len:500 episode reward: total was -20.740000. running mean: -2.985759\n",
      "ep 3310: ep_len:515 episode reward: total was -15.970000. running mean: -3.115601\n",
      "epsilon:0.010000 episode_count: 23177. steps_count: 10250499.000000\n",
      "ep 3311: ep_len:500 episode reward: total was -13.910000. running mean: -3.223545\n",
      "ep 3311: ep_len:500 episode reward: total was -2.100000. running mean: -3.212310\n",
      "ep 3311: ep_len:360 episode reward: total was -12.370000. running mean: -3.303887\n",
      "ep 3311: ep_len:580 episode reward: total was -5.450000. running mean: -3.325348\n",
      "ep 3311: ep_len:3 episode reward: total was 0.000000. running mean: -3.292095\n",
      "ep 3311: ep_len:520 episode reward: total was -40.040000. running mean: -3.659574\n",
      "ep 3311: ep_len:350 episode reward: total was -5.770000. running mean: -3.680678\n",
      "epsilon:0.010000 episode_count: 23184. steps_count: 10253312.000000\n",
      "ep 3312: ep_len:120 episode reward: total was -3.450000. running mean: -3.678371\n",
      "ep 3312: ep_len:500 episode reward: total was 9.350000. running mean: -3.548087\n",
      "ep 3312: ep_len:525 episode reward: total was -7.150000. running mean: -3.584106\n",
      "ep 3312: ep_len:590 episode reward: total was -2.200000. running mean: -3.570265\n",
      "ep 3312: ep_len:3 episode reward: total was 0.000000. running mean: -3.534563\n",
      "ep 3312: ep_len:319 episode reward: total was -0.310000. running mean: -3.502317\n",
      "ep 3312: ep_len:585 episode reward: total was -32.650000. running mean: -3.793794\n",
      "epsilon:0.010000 episode_count: 23191. steps_count: 10255954.000000\n",
      "ep 3313: ep_len:500 episode reward: total was -2.160000. running mean: -3.777456\n",
      "ep 3313: ep_len:356 episode reward: total was -24.340000. running mean: -3.983081\n",
      "ep 3313: ep_len:500 episode reward: total was -13.570000. running mean: -4.078951\n",
      "ep 3313: ep_len:525 episode reward: total was 10.370000. running mean: -3.934461\n",
      "ep 3313: ep_len:98 episode reward: total was 6.050000. running mean: -3.834617\n",
      "ep 3313: ep_len:515 episode reward: total was -5.100000. running mean: -3.847270\n",
      "ep 3313: ep_len:590 episode reward: total was -7.390000. running mean: -3.882698\n",
      "epsilon:0.010000 episode_count: 23198. steps_count: 10259038.000000\n",
      "ep 3314: ep_len:242 episode reward: total was 0.610000. running mean: -3.837771\n",
      "ep 3314: ep_len:650 episode reward: total was 15.150000. running mean: -3.647893\n",
      "ep 3314: ep_len:625 episode reward: total was -4.410000. running mean: -3.655514\n",
      "ep 3314: ep_len:625 episode reward: total was -43.600000. running mean: -4.054959\n",
      "ep 3314: ep_len:94 episode reward: total was -10.440000. running mean: -4.118809\n",
      "ep 3314: ep_len:535 episode reward: total was -7.630000. running mean: -4.153921\n",
      "ep 3314: ep_len:605 episode reward: total was -17.530000. running mean: -4.287682\n",
      "epsilon:0.010000 episode_count: 23205. steps_count: 10262414.000000\n",
      "ep 3315: ep_len:500 episode reward: total was 10.800000. running mean: -4.136805\n",
      "ep 3315: ep_len:515 episode reward: total was 4.190000. running mean: -4.053537\n",
      "ep 3315: ep_len:354 episode reward: total was 6.230000. running mean: -3.950702\n",
      "ep 3315: ep_len:170 episode reward: total was 4.620000. running mean: -3.864995\n",
      "ep 3315: ep_len:3 episode reward: total was 0.000000. running mean: -3.826345\n",
      "ep 3315: ep_len:243 episode reward: total was 11.710000. running mean: -3.670981\n",
      "ep 3315: ep_len:500 episode reward: total was -9.760000. running mean: -3.731872\n",
      "epsilon:0.010000 episode_count: 23212. steps_count: 10264699.000000\n",
      "ep 3316: ep_len:195 episode reward: total was 2.100000. running mean: -3.673553\n",
      "ep 3316: ep_len:605 episode reward: total was -3.820000. running mean: -3.675017\n",
      "ep 3316: ep_len:720 episode reward: total was -35.070000. running mean: -3.988967\n",
      "ep 3316: ep_len:38 episode reward: total was 1.520000. running mean: -3.933877\n",
      "ep 3316: ep_len:3 episode reward: total was 0.000000. running mean: -3.894539\n",
      "ep 3316: ep_len:630 episode reward: total was -4.550000. running mean: -3.901093\n",
      "ep 3316: ep_len:595 episode reward: total was -19.580000. running mean: -4.057882\n",
      "epsilon:0.010000 episode_count: 23219. steps_count: 10267485.000000\n",
      "ep 3317: ep_len:645 episode reward: total was 12.190000. running mean: -3.895404\n",
      "ep 3317: ep_len:515 episode reward: total was -31.340000. running mean: -4.169849\n",
      "ep 3317: ep_len:411 episode reward: total was 6.720000. running mean: -4.060951\n",
      "ep 3317: ep_len:533 episode reward: total was -26.430000. running mean: -4.284641\n",
      "ep 3317: ep_len:3 episode reward: total was 0.000000. running mean: -4.241795\n",
      "ep 3317: ep_len:695 episode reward: total was 3.430000. running mean: -4.165077\n",
      "ep 3317: ep_len:525 episode reward: total was -12.840000. running mean: -4.251826\n",
      "epsilon:0.010000 episode_count: 23226. steps_count: 10270812.000000\n",
      "ep 3318: ep_len:535 episode reward: total was -15.270000. running mean: -4.362008\n",
      "ep 3318: ep_len:500 episode reward: total was 18.250000. running mean: -4.135888\n",
      "ep 3318: ep_len:550 episode reward: total was 4.930000. running mean: -4.045229\n",
      "ep 3318: ep_len:560 episode reward: total was -38.280000. running mean: -4.387577\n",
      "ep 3318: ep_len:36 episode reward: total was 1.000000. running mean: -4.333701\n",
      "ep 3318: ep_len:500 episode reward: total was 12.930000. running mean: -4.161064\n",
      "ep 3318: ep_len:600 episode reward: total was -4.910000. running mean: -4.168553\n",
      "epsilon:0.010000 episode_count: 23233. steps_count: 10274093.000000\n",
      "ep 3319: ep_len:630 episode reward: total was -2.120000. running mean: -4.148068\n",
      "ep 3319: ep_len:515 episode reward: total was -6.170000. running mean: -4.168287\n",
      "ep 3319: ep_len:630 episode reward: total was -0.870000. running mean: -4.135304\n",
      "ep 3319: ep_len:600 episode reward: total was -4.450000. running mean: -4.138451\n",
      "ep 3319: ep_len:3 episode reward: total was 0.000000. running mean: -4.097067\n",
      "ep 3319: ep_len:510 episode reward: total was 6.690000. running mean: -3.989196\n",
      "ep 3319: ep_len:540 episode reward: total was -3.200000. running mean: -3.981304\n",
      "epsilon:0.010000 episode_count: 23240. steps_count: 10277521.000000\n",
      "ep 3320: ep_len:500 episode reward: total was -0.660000. running mean: -3.948091\n",
      "ep 3320: ep_len:185 episode reward: total was -6.890000. running mean: -3.977510\n",
      "ep 3320: ep_len:610 episode reward: total was -1.580000. running mean: -3.953535\n",
      "ep 3320: ep_len:535 episode reward: total was -23.160000. running mean: -4.145600\n",
      "ep 3320: ep_len:106 episode reward: total was 8.550000. running mean: -4.018644\n",
      "ep 3320: ep_len:530 episode reward: total was -17.940000. running mean: -4.157857\n",
      "ep 3320: ep_len:540 episode reward: total was -14.340000. running mean: -4.259679\n",
      "epsilon:0.010000 episode_count: 23247. steps_count: 10280527.000000\n",
      "ep 3321: ep_len:110 episode reward: total was 1.580000. running mean: -4.201282\n",
      "ep 3321: ep_len:500 episode reward: total was 13.170000. running mean: -4.027569\n",
      "ep 3321: ep_len:580 episode reward: total was -13.740000. running mean: -4.124693\n",
      "ep 3321: ep_len:550 episode reward: total was 12.380000. running mean: -3.959646\n",
      "ep 3321: ep_len:3 episode reward: total was 0.000000. running mean: -3.920050\n",
      "ep 3321: ep_len:555 episode reward: total was -2.570000. running mean: -3.906550\n",
      "ep 3321: ep_len:560 episode reward: total was -3.220000. running mean: -3.899684\n",
      "epsilon:0.010000 episode_count: 23254. steps_count: 10283385.000000\n",
      "ep 3322: ep_len:610 episode reward: total was 1.040000. running mean: -3.850287\n",
      "ep 3322: ep_len:500 episode reward: total was 16.840000. running mean: -3.643384\n",
      "ep 3322: ep_len:575 episode reward: total was -1.460000. running mean: -3.621550\n",
      "ep 3322: ep_len:56 episode reward: total was 2.570000. running mean: -3.559635\n",
      "ep 3322: ep_len:104 episode reward: total was 6.550000. running mean: -3.458539\n",
      "ep 3322: ep_len:510 episode reward: total was -5.390000. running mean: -3.477853\n",
      "ep 3322: ep_len:585 episode reward: total was -8.030000. running mean: -3.523375\n",
      "epsilon:0.010000 episode_count: 23261. steps_count: 10286325.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3323: ep_len:500 episode reward: total was 8.260000. running mean: -3.405541\n",
      "ep 3323: ep_len:640 episode reward: total was -10.020000. running mean: -3.471686\n",
      "ep 3323: ep_len:500 episode reward: total was -4.400000. running mean: -3.480969\n",
      "ep 3323: ep_len:500 episode reward: total was -18.050000. running mean: -3.626659\n",
      "ep 3323: ep_len:3 episode reward: total was 0.000000. running mean: -3.590392\n",
      "ep 3323: ep_len:500 episode reward: total was 0.210000. running mean: -3.552388\n",
      "ep 3323: ep_len:560 episode reward: total was -4.930000. running mean: -3.566165\n",
      "epsilon:0.010000 episode_count: 23268. steps_count: 10289528.000000\n",
      "ep 3324: ep_len:500 episode reward: total was 4.850000. running mean: -3.482003\n",
      "ep 3324: ep_len:500 episode reward: total was -11.980000. running mean: -3.566983\n",
      "ep 3324: ep_len:625 episode reward: total was 2.080000. running mean: -3.510513\n",
      "ep 3324: ep_len:525 episode reward: total was -19.140000. running mean: -3.666808\n",
      "ep 3324: ep_len:3 episode reward: total was 0.000000. running mean: -3.630140\n",
      "ep 3324: ep_len:630 episode reward: total was -21.750000. running mean: -3.811338\n",
      "ep 3324: ep_len:525 episode reward: total was -6.810000. running mean: -3.841325\n",
      "epsilon:0.010000 episode_count: 23275. steps_count: 10292836.000000\n",
      "ep 3325: ep_len:500 episode reward: total was -1.570000. running mean: -3.818612\n",
      "ep 3325: ep_len:555 episode reward: total was -4.270000. running mean: -3.823126\n",
      "ep 3325: ep_len:550 episode reward: total was 0.830000. running mean: -3.776594\n",
      "ep 3325: ep_len:500 episode reward: total was -5.480000. running mean: -3.793629\n",
      "ep 3325: ep_len:95 episode reward: total was 4.060000. running mean: -3.715092\n",
      "ep 3325: ep_len:525 episode reward: total was -7.470000. running mean: -3.752641\n",
      "ep 3325: ep_len:500 episode reward: total was -15.450000. running mean: -3.869615\n",
      "epsilon:0.010000 episode_count: 23282. steps_count: 10296061.000000\n",
      "ep 3326: ep_len:194 episode reward: total was 5.580000. running mean: -3.775119\n",
      "ep 3326: ep_len:595 episode reward: total was 6.260000. running mean: -3.674768\n",
      "ep 3326: ep_len:605 episode reward: total was 4.590000. running mean: -3.592120\n",
      "ep 3326: ep_len:725 episode reward: total was -62.330000. running mean: -4.179499\n",
      "ep 3326: ep_len:3 episode reward: total was 0.000000. running mean: -4.137704\n",
      "ep 3326: ep_len:580 episode reward: total was -5.600000. running mean: -4.152327\n",
      "ep 3326: ep_len:342 episode reward: total was -4.290000. running mean: -4.153703\n",
      "epsilon:0.010000 episode_count: 23289. steps_count: 10299105.000000\n",
      "ep 3327: ep_len:615 episode reward: total was -14.300000. running mean: -4.255166\n",
      "ep 3327: ep_len:500 episode reward: total was 22.170000. running mean: -3.990915\n",
      "ep 3327: ep_len:610 episode reward: total was 5.910000. running mean: -3.891906\n",
      "ep 3327: ep_len:41 episode reward: total was 0.050000. running mean: -3.852487\n",
      "ep 3327: ep_len:3 episode reward: total was 0.000000. running mean: -3.813962\n",
      "ep 3327: ep_len:510 episode reward: total was 5.030000. running mean: -3.725522\n",
      "ep 3327: ep_len:500 episode reward: total was -3.420000. running mean: -3.722467\n",
      "epsilon:0.010000 episode_count: 23296. steps_count: 10301884.000000\n",
      "ep 3328: ep_len:670 episode reward: total was -2.620000. running mean: -3.711442\n",
      "ep 3328: ep_len:254 episode reward: total was -7.360000. running mean: -3.747928\n",
      "ep 3328: ep_len:765 episode reward: total was -60.250000. running mean: -4.312948\n",
      "ep 3328: ep_len:520 episode reward: total was 14.590000. running mean: -4.123919\n",
      "ep 3328: ep_len:3 episode reward: total was 0.000000. running mean: -4.082680\n",
      "ep 3328: ep_len:505 episode reward: total was 0.420000. running mean: -4.037653\n",
      "ep 3328: ep_len:580 episode reward: total was -5.390000. running mean: -4.051176\n",
      "epsilon:0.010000 episode_count: 23303. steps_count: 10305181.000000\n",
      "ep 3329: ep_len:242 episode reward: total was 5.130000. running mean: -3.959365\n",
      "ep 3329: ep_len:525 episode reward: total was -7.300000. running mean: -3.992771\n",
      "ep 3329: ep_len:550 episode reward: total was -10.800000. running mean: -4.060843\n",
      "ep 3329: ep_len:162 episode reward: total was 3.640000. running mean: -3.983835\n",
      "ep 3329: ep_len:53 episode reward: total was 5.000000. running mean: -3.893997\n",
      "ep 3329: ep_len:775 episode reward: total was -50.710000. running mean: -4.362157\n",
      "ep 3329: ep_len:520 episode reward: total was -2.720000. running mean: -4.345735\n",
      "epsilon:0.010000 episode_count: 23310. steps_count: 10308008.000000\n",
      "ep 3330: ep_len:500 episode reward: total was -1.180000. running mean: -4.314078\n",
      "ep 3330: ep_len:575 episode reward: total was 1.450000. running mean: -4.256437\n",
      "ep 3330: ep_len:590 episode reward: total was 0.900000. running mean: -4.204873\n",
      "ep 3330: ep_len:623 episode reward: total was -21.860000. running mean: -4.381424\n",
      "ep 3330: ep_len:51 episode reward: total was 5.000000. running mean: -4.287610\n",
      "ep 3330: ep_len:510 episode reward: total was 1.190000. running mean: -4.232833\n",
      "ep 3330: ep_len:500 episode reward: total was 1.890000. running mean: -4.171605\n",
      "epsilon:0.010000 episode_count: 23317. steps_count: 10311357.000000\n",
      "ep 3331: ep_len:630 episode reward: total was 2.110000. running mean: -4.108789\n",
      "ep 3331: ep_len:500 episode reward: total was -14.670000. running mean: -4.214401\n",
      "ep 3331: ep_len:500 episode reward: total was -4.290000. running mean: -4.215157\n",
      "ep 3331: ep_len:500 episode reward: total was -10.500000. running mean: -4.278006\n",
      "ep 3331: ep_len:109 episode reward: total was 6.040000. running mean: -4.174826\n",
      "ep 3331: ep_len:500 episode reward: total was -0.110000. running mean: -4.134177\n",
      "ep 3331: ep_len:820 episode reward: total was -38.220000. running mean: -4.475036\n",
      "epsilon:0.010000 episode_count: 23324. steps_count: 10314916.000000\n",
      "ep 3332: ep_len:239 episode reward: total was 2.640000. running mean: -4.403885\n",
      "ep 3332: ep_len:500 episode reward: total was -4.750000. running mean: -4.407346\n",
      "ep 3332: ep_len:580 episode reward: total was -18.780000. running mean: -4.551073\n",
      "ep 3332: ep_len:505 episode reward: total was 12.460000. running mean: -4.380962\n",
      "ep 3332: ep_len:3 episode reward: total was 0.000000. running mean: -4.337152\n",
      "ep 3332: ep_len:320 episode reward: total was 5.180000. running mean: -4.241981\n",
      "ep 3332: ep_len:640 episode reward: total was -19.760000. running mean: -4.397161\n",
      "epsilon:0.010000 episode_count: 23331. steps_count: 10317703.000000\n",
      "ep 3333: ep_len:650 episode reward: total was -49.330000. running mean: -4.846490\n",
      "ep 3333: ep_len:545 episode reward: total was 7.390000. running mean: -4.724125\n",
      "ep 3333: ep_len:590 episode reward: total was -29.620000. running mean: -4.973083\n",
      "ep 3333: ep_len:500 episode reward: total was -10.030000. running mean: -5.023653\n",
      "ep 3333: ep_len:3 episode reward: total was 0.000000. running mean: -4.973416\n",
      "ep 3333: ep_len:775 episode reward: total was -46.240000. running mean: -5.386082\n",
      "ep 3333: ep_len:605 episode reward: total was -9.080000. running mean: -5.423021\n",
      "epsilon:0.010000 episode_count: 23338. steps_count: 10321371.000000\n",
      "ep 3334: ep_len:765 episode reward: total was -30.750000. running mean: -5.676291\n",
      "ep 3334: ep_len:500 episode reward: total was -28.510000. running mean: -5.904628\n",
      "ep 3334: ep_len:590 episode reward: total was -4.100000. running mean: -5.886582\n",
      "ep 3334: ep_len:570 episode reward: total was 2.410000. running mean: -5.803616\n",
      "ep 3334: ep_len:3 episode reward: total was 0.000000. running mean: -5.745580\n",
      "ep 3334: ep_len:570 episode reward: total was -6.630000. running mean: -5.754424\n",
      "ep 3334: ep_len:341 episode reward: total was -5.850000. running mean: -5.755380\n",
      "epsilon:0.010000 episode_count: 23345. steps_count: 10324710.000000\n",
      "ep 3335: ep_len:575 episode reward: total was -22.230000. running mean: -5.920126\n",
      "ep 3335: ep_len:535 episode reward: total was -7.640000. running mean: -5.937325\n",
      "ep 3335: ep_len:500 episode reward: total was 2.370000. running mean: -5.854251\n",
      "ep 3335: ep_len:505 episode reward: total was -11.520000. running mean: -5.910909\n",
      "ep 3335: ep_len:47 episode reward: total was 4.500000. running mean: -5.806800\n",
      "ep 3335: ep_len:580 episode reward: total was -8.580000. running mean: -5.834532\n",
      "ep 3335: ep_len:510 episode reward: total was -7.090000. running mean: -5.847086\n",
      "epsilon:0.010000 episode_count: 23352. steps_count: 10327962.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3336: ep_len:117 episode reward: total was 5.570000. running mean: -5.732916\n",
      "ep 3336: ep_len:500 episode reward: total was 4.020000. running mean: -5.635386\n",
      "ep 3336: ep_len:525 episode reward: total was -9.830000. running mean: -5.677333\n",
      "ep 3336: ep_len:630 episode reward: total was 16.160000. running mean: -5.458959\n",
      "ep 3336: ep_len:3 episode reward: total was 0.000000. running mean: -5.404370\n",
      "ep 3336: ep_len:585 episode reward: total was 0.450000. running mean: -5.345826\n",
      "ep 3336: ep_len:555 episode reward: total was -34.700000. running mean: -5.639368\n",
      "epsilon:0.010000 episode_count: 23359. steps_count: 10330877.000000\n",
      "ep 3337: ep_len:615 episode reward: total was -34.030000. running mean: -5.923274\n",
      "ep 3337: ep_len:510 episode reward: total was 4.150000. running mean: -5.822541\n",
      "ep 3337: ep_len:500 episode reward: total was 0.810000. running mean: -5.756216\n",
      "ep 3337: ep_len:500 episode reward: total was -15.120000. running mean: -5.849854\n",
      "ep 3337: ep_len:3 episode reward: total was 0.000000. running mean: -5.791355\n",
      "ep 3337: ep_len:535 episode reward: total was 9.890000. running mean: -5.634542\n",
      "ep 3337: ep_len:565 episode reward: total was -2.560000. running mean: -5.603796\n",
      "epsilon:0.010000 episode_count: 23366. steps_count: 10334105.000000\n",
      "ep 3338: ep_len:500 episode reward: total was -21.860000. running mean: -5.766358\n",
      "ep 3338: ep_len:545 episode reward: total was 2.570000. running mean: -5.682995\n",
      "ep 3338: ep_len:620 episode reward: total was -21.050000. running mean: -5.836665\n",
      "ep 3338: ep_len:530 episode reward: total was 8.300000. running mean: -5.695298\n",
      "ep 3338: ep_len:95 episode reward: total was 5.530000. running mean: -5.583045\n",
      "ep 3338: ep_len:615 episode reward: total was 8.430000. running mean: -5.442915\n",
      "ep 3338: ep_len:172 episode reward: total was 0.170000. running mean: -5.386785\n",
      "epsilon:0.010000 episode_count: 23373. steps_count: 10337182.000000\n",
      "ep 3339: ep_len:565 episode reward: total was -9.180000. running mean: -5.424718\n",
      "ep 3339: ep_len:500 episode reward: total was -32.700000. running mean: -5.697470\n",
      "ep 3339: ep_len:590 episode reward: total was 0.980000. running mean: -5.630696\n",
      "ep 3339: ep_len:570 episode reward: total was 9.560000. running mean: -5.478789\n",
      "ep 3339: ep_len:83 episode reward: total was 3.020000. running mean: -5.393801\n",
      "ep 3339: ep_len:505 episode reward: total was 4.400000. running mean: -5.295863\n",
      "ep 3339: ep_len:187 episode reward: total was -6.400000. running mean: -5.306904\n",
      "epsilon:0.010000 episode_count: 23380. steps_count: 10340182.000000\n",
      "ep 3340: ep_len:205 episode reward: total was 3.590000. running mean: -5.217935\n",
      "ep 3340: ep_len:545 episode reward: total was 8.510000. running mean: -5.080656\n",
      "ep 3340: ep_len:575 episode reward: total was 0.430000. running mean: -5.025549\n",
      "ep 3340: ep_len:500 episode reward: total was -15.110000. running mean: -5.126394\n",
      "ep 3340: ep_len:3 episode reward: total was 0.000000. running mean: -5.075130\n",
      "ep 3340: ep_len:635 episode reward: total was -6.810000. running mean: -5.092479\n",
      "ep 3340: ep_len:320 episode reward: total was -8.340000. running mean: -5.124954\n",
      "epsilon:0.010000 episode_count: 23387. steps_count: 10342965.000000\n",
      "ep 3341: ep_len:205 episode reward: total was -19.350000. running mean: -5.267204\n",
      "ep 3341: ep_len:201 episode reward: total was -6.350000. running mean: -5.278032\n",
      "ep 3341: ep_len:66 episode reward: total was 0.020000. running mean: -5.225052\n",
      "ep 3341: ep_len:590 episode reward: total was 10.580000. running mean: -5.067001\n",
      "ep 3341: ep_len:95 episode reward: total was 3.540000. running mean: -4.980931\n",
      "ep 3341: ep_len:665 episode reward: total was 8.910000. running mean: -4.842022\n",
      "ep 3341: ep_len:505 episode reward: total was -0.240000. running mean: -4.796002\n",
      "epsilon:0.010000 episode_count: 23394. steps_count: 10345292.000000\n",
      "ep 3342: ep_len:565 episode reward: total was 12.120000. running mean: -4.626842\n",
      "ep 3342: ep_len:535 episode reward: total was 4.050000. running mean: -4.540073\n",
      "ep 3342: ep_len:500 episode reward: total was 0.590000. running mean: -4.488773\n",
      "ep 3342: ep_len:600 episode reward: total was 13.000000. running mean: -4.313885\n",
      "ep 3342: ep_len:3 episode reward: total was 0.000000. running mean: -4.270746\n",
      "ep 3342: ep_len:500 episode reward: total was 2.890000. running mean: -4.199139\n",
      "ep 3342: ep_len:520 episode reward: total was -19.420000. running mean: -4.351347\n",
      "epsilon:0.010000 episode_count: 23401. steps_count: 10348515.000000\n",
      "ep 3343: ep_len:585 episode reward: total was 9.970000. running mean: -4.208134\n",
      "ep 3343: ep_len:580 episode reward: total was 26.400000. running mean: -3.902052\n",
      "ep 3343: ep_len:615 episode reward: total was -8.420000. running mean: -3.947232\n",
      "ep 3343: ep_len:620 episode reward: total was -19.830000. running mean: -4.106060\n",
      "ep 3343: ep_len:3 episode reward: total was 0.000000. running mean: -4.064999\n",
      "ep 3343: ep_len:172 episode reward: total was 6.110000. running mean: -3.963249\n",
      "ep 3343: ep_len:575 episode reward: total was -2.540000. running mean: -3.949016\n",
      "epsilon:0.010000 episode_count: 23408. steps_count: 10351665.000000\n",
      "ep 3344: ep_len:500 episode reward: total was -13.930000. running mean: -4.048826\n",
      "ep 3344: ep_len:371 episode reward: total was -7.350000. running mean: -4.081838\n",
      "ep 3344: ep_len:505 episode reward: total was 1.440000. running mean: -4.026620\n",
      "ep 3344: ep_len:600 episode reward: total was 7.080000. running mean: -3.915553\n",
      "ep 3344: ep_len:3 episode reward: total was 0.000000. running mean: -3.876398\n",
      "ep 3344: ep_len:610 episode reward: total was -3.550000. running mean: -3.873134\n",
      "ep 3344: ep_len:211 episode reward: total was -6.380000. running mean: -3.898203\n",
      "epsilon:0.010000 episode_count: 23415. steps_count: 10354465.000000\n",
      "ep 3345: ep_len:590 episode reward: total was 10.470000. running mean: -3.754521\n",
      "ep 3345: ep_len:550 episode reward: total was 1.150000. running mean: -3.705475\n",
      "ep 3345: ep_len:500 episode reward: total was -2.640000. running mean: -3.694821\n",
      "ep 3345: ep_len:170 episode reward: total was 3.140000. running mean: -3.626472\n",
      "ep 3345: ep_len:3 episode reward: total was 0.000000. running mean: -3.590208\n",
      "ep 3345: ep_len:590 episode reward: total was -37.460000. running mean: -3.928906\n",
      "ep 3345: ep_len:500 episode reward: total was -27.520000. running mean: -4.164817\n",
      "epsilon:0.010000 episode_count: 23422. steps_count: 10357368.000000\n",
      "ep 3346: ep_len:595 episode reward: total was -15.670000. running mean: -4.279868\n",
      "ep 3346: ep_len:500 episode reward: total was 14.740000. running mean: -4.089670\n",
      "ep 3346: ep_len:585 episode reward: total was -16.970000. running mean: -4.218473\n",
      "ep 3346: ep_len:408 episode reward: total was 7.870000. running mean: -4.097588\n",
      "ep 3346: ep_len:3 episode reward: total was 0.000000. running mean: -4.056612\n",
      "ep 3346: ep_len:665 episode reward: total was 6.340000. running mean: -3.952646\n",
      "ep 3346: ep_len:283 episode reward: total was -3.350000. running mean: -3.946620\n",
      "epsilon:0.010000 episode_count: 23429. steps_count: 10360407.000000\n",
      "ep 3347: ep_len:585 episode reward: total was -20.190000. running mean: -4.109054\n",
      "ep 3347: ep_len:745 episode reward: total was -39.840000. running mean: -4.466363\n",
      "ep 3347: ep_len:605 episode reward: total was 0.250000. running mean: -4.419199\n",
      "ep 3347: ep_len:510 episode reward: total was -20.430000. running mean: -4.579307\n",
      "ep 3347: ep_len:2 episode reward: total was 0.000000. running mean: -4.533514\n",
      "ep 3347: ep_len:500 episode reward: total was -6.310000. running mean: -4.551279\n",
      "ep 3347: ep_len:530 episode reward: total was -1.360000. running mean: -4.519366\n",
      "epsilon:0.010000 episode_count: 23436. steps_count: 10363884.000000\n",
      "ep 3348: ep_len:225 episode reward: total was 2.620000. running mean: -4.447973\n",
      "ep 3348: ep_len:540 episode reward: total was 18.400000. running mean: -4.219493\n",
      "ep 3348: ep_len:525 episode reward: total was -3.310000. running mean: -4.210398\n",
      "ep 3348: ep_len:500 episode reward: total was 10.910000. running mean: -4.059194\n",
      "ep 3348: ep_len:3 episode reward: total was 0.000000. running mean: -4.018602\n",
      "ep 3348: ep_len:570 episode reward: total was -1.180000. running mean: -3.990216\n",
      "ep 3348: ep_len:525 episode reward: total was -3.400000. running mean: -3.984314\n",
      "epsilon:0.010000 episode_count: 23443. steps_count: 10366772.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3349: ep_len:695 episode reward: total was -8.660000. running mean: -4.031071\n",
      "ep 3349: ep_len:500 episode reward: total was 1.390000. running mean: -3.976860\n",
      "ep 3349: ep_len:384 episode reward: total was 3.200000. running mean: -3.905092\n",
      "ep 3349: ep_len:500 episode reward: total was 8.840000. running mean: -3.777641\n",
      "ep 3349: ep_len:3 episode reward: total was 0.000000. running mean: -3.739864\n",
      "ep 3349: ep_len:570 episode reward: total was 2.890000. running mean: -3.673566\n",
      "ep 3349: ep_len:338 episode reward: total was -7.850000. running mean: -3.715330\n",
      "epsilon:0.010000 episode_count: 23450. steps_count: 10369762.000000\n",
      "ep 3350: ep_len:500 episode reward: total was -14.480000. running mean: -3.822977\n",
      "ep 3350: ep_len:192 episode reward: total was -2.900000. running mean: -3.813747\n",
      "ep 3350: ep_len:555 episode reward: total was -6.740000. running mean: -3.843009\n",
      "ep 3350: ep_len:515 episode reward: total was 10.090000. running mean: -3.703679\n",
      "ep 3350: ep_len:50 episode reward: total was 3.500000. running mean: -3.631643\n",
      "ep 3350: ep_len:640 episode reward: total was 1.010000. running mean: -3.585226\n",
      "ep 3350: ep_len:520 episode reward: total was -3.060000. running mean: -3.579974\n",
      "epsilon:0.010000 episode_count: 23457. steps_count: 10372734.000000\n",
      "ep 3351: ep_len:585 episode reward: total was 11.030000. running mean: -3.433874\n",
      "ep 3351: ep_len:555 episode reward: total was -9.440000. running mean: -3.493935\n",
      "ep 3351: ep_len:620 episode reward: total was 0.110000. running mean: -3.457896\n",
      "ep 3351: ep_len:545 episode reward: total was 3.860000. running mean: -3.384717\n",
      "ep 3351: ep_len:3 episode reward: total was 0.000000. running mean: -3.350870\n",
      "ep 3351: ep_len:575 episode reward: total was 4.400000. running mean: -3.273361\n",
      "ep 3351: ep_len:535 episode reward: total was -8.570000. running mean: -3.326328\n",
      "epsilon:0.010000 episode_count: 23464. steps_count: 10376152.000000\n",
      "ep 3352: ep_len:228 episode reward: total was -0.900000. running mean: -3.302064\n",
      "ep 3352: ep_len:500 episode reward: total was 9.190000. running mean: -3.177144\n",
      "ep 3352: ep_len:545 episode reward: total was -24.890000. running mean: -3.394272\n",
      "ep 3352: ep_len:500 episode reward: total was 3.570000. running mean: -3.324630\n",
      "ep 3352: ep_len:49 episode reward: total was 4.500000. running mean: -3.246383\n",
      "ep 3352: ep_len:500 episode reward: total was 7.520000. running mean: -3.138719\n",
      "ep 3352: ep_len:580 episode reward: total was -41.250000. running mean: -3.519832\n",
      "epsilon:0.010000 episode_count: 23471. steps_count: 10379054.000000\n",
      "ep 3353: ep_len:222 episode reward: total was 2.120000. running mean: -3.463434\n",
      "ep 3353: ep_len:520 episode reward: total was 14.220000. running mean: -3.286600\n",
      "ep 3353: ep_len:630 episode reward: total was 1.340000. running mean: -3.240334\n",
      "ep 3353: ep_len:515 episode reward: total was -0.610000. running mean: -3.214030\n",
      "ep 3353: ep_len:89 episode reward: total was -5.950000. running mean: -3.241390\n",
      "ep 3353: ep_len:500 episode reward: total was 0.520000. running mean: -3.203776\n",
      "ep 3353: ep_len:585 episode reward: total was 0.510000. running mean: -3.166638\n",
      "epsilon:0.010000 episode_count: 23478. steps_count: 10382115.000000\n",
      "ep 3354: ep_len:530 episode reward: total was -8.240000. running mean: -3.217372\n",
      "ep 3354: ep_len:293 episode reward: total was -37.330000. running mean: -3.558498\n",
      "ep 3354: ep_len:655 episode reward: total was -20.250000. running mean: -3.725413\n",
      "ep 3354: ep_len:430 episode reward: total was 7.420000. running mean: -3.613959\n",
      "ep 3354: ep_len:131 episode reward: total was 6.560000. running mean: -3.512219\n",
      "ep 3354: ep_len:530 episode reward: total was -34.970000. running mean: -3.826797\n",
      "ep 3354: ep_len:500 episode reward: total was -22.990000. running mean: -4.018429\n",
      "epsilon:0.010000 episode_count: 23485. steps_count: 10385184.000000\n",
      "ep 3355: ep_len:500 episode reward: total was 2.820000. running mean: -3.950045\n",
      "ep 3355: ep_len:520 episode reward: total was -0.820000. running mean: -3.918745\n",
      "ep 3355: ep_len:560 episode reward: total was 1.370000. running mean: -3.865857\n",
      "ep 3355: ep_len:500 episode reward: total was -8.540000. running mean: -3.912598\n",
      "ep 3355: ep_len:3 episode reward: total was 0.000000. running mean: -3.873473\n",
      "ep 3355: ep_len:595 episode reward: total was 3.980000. running mean: -3.794938\n",
      "ep 3355: ep_len:296 episode reward: total was -3.320000. running mean: -3.790188\n",
      "epsilon:0.010000 episode_count: 23492. steps_count: 10388158.000000\n",
      "ep 3356: ep_len:635 episode reward: total was -3.640000. running mean: -3.788687\n",
      "ep 3356: ep_len:500 episode reward: total was 10.670000. running mean: -3.644100\n",
      "ep 3356: ep_len:690 episode reward: total was -3.680000. running mean: -3.644459\n",
      "ep 3356: ep_len:130 episode reward: total was 5.610000. running mean: -3.551914\n",
      "ep 3356: ep_len:3 episode reward: total was 0.000000. running mean: -3.516395\n",
      "ep 3356: ep_len:615 episode reward: total was -3.570000. running mean: -3.516931\n",
      "ep 3356: ep_len:620 episode reward: total was -9.750000. running mean: -3.579262\n",
      "epsilon:0.010000 episode_count: 23499. steps_count: 10391351.000000\n",
      "ep 3357: ep_len:500 episode reward: total was -11.010000. running mean: -3.653569\n",
      "ep 3357: ep_len:550 episode reward: total was 18.410000. running mean: -3.432933\n",
      "ep 3357: ep_len:655 episode reward: total was -3.630000. running mean: -3.434904\n",
      "ep 3357: ep_len:500 episode reward: total was -10.780000. running mean: -3.508355\n",
      "ep 3357: ep_len:90 episode reward: total was -3.470000. running mean: -3.507971\n",
      "ep 3357: ep_len:535 episode reward: total was -9.760000. running mean: -3.570492\n",
      "ep 3357: ep_len:326 episode reward: total was -9.830000. running mean: -3.633087\n",
      "epsilon:0.010000 episode_count: 23506. steps_count: 10394507.000000\n",
      "ep 3358: ep_len:202 episode reward: total was 3.610000. running mean: -3.560656\n",
      "ep 3358: ep_len:500 episode reward: total was 10.230000. running mean: -3.422749\n",
      "ep 3358: ep_len:625 episode reward: total was -0.050000. running mean: -3.389022\n",
      "ep 3358: ep_len:520 episode reward: total was -9.530000. running mean: -3.450432\n",
      "ep 3358: ep_len:3 episode reward: total was 0.000000. running mean: -3.415927\n",
      "ep 3358: ep_len:770 episode reward: total was -144.960000. running mean: -4.831368\n",
      "ep 3358: ep_len:595 episode reward: total was -4.490000. running mean: -4.827954\n",
      "epsilon:0.010000 episode_count: 23513. steps_count: 10397722.000000\n",
      "ep 3359: ep_len:215 episode reward: total was 0.090000. running mean: -4.778775\n",
      "ep 3359: ep_len:500 episode reward: total was -3.830000. running mean: -4.769287\n",
      "ep 3359: ep_len:725 episode reward: total was -27.590000. running mean: -4.997494\n",
      "ep 3359: ep_len:132 episode reward: total was 5.090000. running mean: -4.896619\n",
      "ep 3359: ep_len:3 episode reward: total was 0.000000. running mean: -4.847653\n",
      "ep 3359: ep_len:535 episode reward: total was -10.800000. running mean: -4.907177\n",
      "ep 3359: ep_len:525 episode reward: total was -17.860000. running mean: -5.036705\n",
      "epsilon:0.010000 episode_count: 23520. steps_count: 10400357.000000\n",
      "ep 3360: ep_len:585 episode reward: total was 11.470000. running mean: -4.871638\n",
      "ep 3360: ep_len:570 episode reward: total was -15.170000. running mean: -4.974621\n",
      "ep 3360: ep_len:540 episode reward: total was 1.630000. running mean: -4.908575\n",
      "ep 3360: ep_len:313 episode reward: total was 6.820000. running mean: -4.791289\n",
      "ep 3360: ep_len:108 episode reward: total was 6.040000. running mean: -4.682977\n",
      "ep 3360: ep_len:264 episode reward: total was 4.690000. running mean: -4.589247\n",
      "ep 3360: ep_len:560 episode reward: total was -39.650000. running mean: -4.939854\n",
      "epsilon:0.010000 episode_count: 23527. steps_count: 10403297.000000\n",
      "ep 3361: ep_len:605 episode reward: total was 8.990000. running mean: -4.800556\n",
      "ep 3361: ep_len:500 episode reward: total was 18.680000. running mean: -4.565750\n",
      "ep 3361: ep_len:565 episode reward: total was -35.390000. running mean: -4.873993\n",
      "ep 3361: ep_len:525 episode reward: total was -15.980000. running mean: -4.985053\n",
      "ep 3361: ep_len:45 episode reward: total was 3.000000. running mean: -4.905202\n",
      "ep 3361: ep_len:615 episode reward: total was 6.440000. running mean: -4.791750\n",
      "ep 3361: ep_len:615 episode reward: total was -17.360000. running mean: -4.917433\n",
      "epsilon:0.010000 episode_count: 23534. steps_count: 10406767.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3362: ep_len:640 episode reward: total was -16.700000. running mean: -5.035258\n",
      "ep 3362: ep_len:585 episode reward: total was -44.000000. running mean: -5.424906\n",
      "ep 3362: ep_len:535 episode reward: total was 6.960000. running mean: -5.301057\n",
      "ep 3362: ep_len:515 episode reward: total was 9.880000. running mean: -5.149246\n",
      "ep 3362: ep_len:3 episode reward: total was 0.000000. running mean: -5.097754\n",
      "ep 3362: ep_len:555 episode reward: total was -26.110000. running mean: -5.307876\n",
      "ep 3362: ep_len:500 episode reward: total was -14.040000. running mean: -5.395197\n",
      "epsilon:0.010000 episode_count: 23541. steps_count: 10410100.000000\n",
      "ep 3363: ep_len:206 episode reward: total was 3.620000. running mean: -5.305045\n",
      "ep 3363: ep_len:515 episode reward: total was 7.250000. running mean: -5.179495\n",
      "ep 3363: ep_len:545 episode reward: total was 2.440000. running mean: -5.103300\n",
      "ep 3363: ep_len:530 episode reward: total was 7.440000. running mean: -4.977867\n",
      "ep 3363: ep_len:96 episode reward: total was -3.960000. running mean: -4.967688\n",
      "ep 3363: ep_len:565 episode reward: total was -19.760000. running mean: -5.115611\n",
      "ep 3363: ep_len:500 episode reward: total was -38.790000. running mean: -5.452355\n",
      "epsilon:0.010000 episode_count: 23548. steps_count: 10413057.000000\n",
      "ep 3364: ep_len:585 episode reward: total was 12.460000. running mean: -5.273232\n",
      "ep 3364: ep_len:570 episode reward: total was 10.100000. running mean: -5.119499\n",
      "ep 3364: ep_len:73 episode reward: total was -0.460000. running mean: -5.072904\n",
      "ep 3364: ep_len:515 episode reward: total was -30.110000. running mean: -5.323275\n",
      "ep 3364: ep_len:3 episode reward: total was 0.000000. running mean: -5.270043\n",
      "ep 3364: ep_len:555 episode reward: total was -10.590000. running mean: -5.323242\n",
      "ep 3364: ep_len:500 episode reward: total was -1.290000. running mean: -5.282910\n",
      "epsilon:0.010000 episode_count: 23555. steps_count: 10415858.000000\n",
      "ep 3365: ep_len:650 episode reward: total was -24.240000. running mean: -5.472481\n",
      "ep 3365: ep_len:500 episode reward: total was 10.730000. running mean: -5.310456\n",
      "ep 3365: ep_len:505 episode reward: total was -0.590000. running mean: -5.263251\n",
      "ep 3365: ep_len:610 episode reward: total was 6.090000. running mean: -5.149719\n",
      "ep 3365: ep_len:110 episode reward: total was 2.050000. running mean: -5.077722\n",
      "ep 3365: ep_len:570 episode reward: total was -23.830000. running mean: -5.265244\n",
      "ep 3365: ep_len:575 episode reward: total was -19.390000. running mean: -5.406492\n",
      "epsilon:0.010000 episode_count: 23562. steps_count: 10419378.000000\n",
      "ep 3366: ep_len:121 episode reward: total was 2.070000. running mean: -5.331727\n",
      "ep 3366: ep_len:500 episode reward: total was 25.720000. running mean: -5.021210\n",
      "ep 3366: ep_len:394 episode reward: total was 7.720000. running mean: -4.893798\n",
      "ep 3366: ep_len:540 episode reward: total was -10.430000. running mean: -4.949160\n",
      "ep 3366: ep_len:3 episode reward: total was 0.000000. running mean: -4.899668\n",
      "ep 3366: ep_len:615 episode reward: total was -9.020000. running mean: -4.940871\n",
      "ep 3366: ep_len:298 episode reward: total was -6.290000. running mean: -4.954363\n",
      "epsilon:0.010000 episode_count: 23569. steps_count: 10421849.000000\n",
      "ep 3367: ep_len:228 episode reward: total was 4.610000. running mean: -4.858719\n",
      "ep 3367: ep_len:500 episode reward: total was 6.560000. running mean: -4.744532\n",
      "ep 3367: ep_len:525 episode reward: total was 0.090000. running mean: -4.696187\n",
      "ep 3367: ep_len:515 episode reward: total was 3.440000. running mean: -4.614825\n",
      "ep 3367: ep_len:82 episode reward: total was 3.540000. running mean: -4.533277\n",
      "ep 3367: ep_len:620 episode reward: total was -56.590000. running mean: -5.053844\n",
      "ep 3367: ep_len:575 episode reward: total was -8.540000. running mean: -5.088705\n",
      "epsilon:0.010000 episode_count: 23576. steps_count: 10424894.000000\n",
      "ep 3368: ep_len:580 episode reward: total was 6.540000. running mean: -4.972418\n",
      "ep 3368: ep_len:520 episode reward: total was 8.040000. running mean: -4.842294\n",
      "ep 3368: ep_len:615 episode reward: total was -24.710000. running mean: -5.040971\n",
      "ep 3368: ep_len:118 episode reward: total was 5.580000. running mean: -4.934761\n",
      "ep 3368: ep_len:3 episode reward: total was 0.000000. running mean: -4.885414\n",
      "ep 3368: ep_len:500 episode reward: total was -9.280000. running mean: -4.929360\n",
      "ep 3368: ep_len:315 episode reward: total was -0.740000. running mean: -4.887466\n",
      "epsilon:0.010000 episode_count: 23583. steps_count: 10427545.000000\n",
      "ep 3369: ep_len:575 episode reward: total was -17.730000. running mean: -5.015891\n",
      "ep 3369: ep_len:580 episode reward: total was 1.750000. running mean: -4.948232\n",
      "ep 3369: ep_len:590 episode reward: total was 4.530000. running mean: -4.853450\n",
      "ep 3369: ep_len:395 episode reward: total was 7.380000. running mean: -4.731116\n",
      "ep 3369: ep_len:94 episode reward: total was 3.530000. running mean: -4.648505\n",
      "ep 3369: ep_len:685 episode reward: total was 1.390000. running mean: -4.588119\n",
      "ep 3369: ep_len:209 episode reward: total was -7.830000. running mean: -4.620538\n",
      "epsilon:0.010000 episode_count: 23590. steps_count: 10430673.000000\n",
      "ep 3370: ep_len:505 episode reward: total was 0.660000. running mean: -4.567733\n",
      "ep 3370: ep_len:665 episode reward: total was 5.090000. running mean: -4.471156\n",
      "ep 3370: ep_len:500 episode reward: total was 3.380000. running mean: -4.392644\n",
      "ep 3370: ep_len:500 episode reward: total was -30.550000. running mean: -4.654218\n",
      "ep 3370: ep_len:3 episode reward: total was 0.000000. running mean: -4.607675\n",
      "ep 3370: ep_len:645 episode reward: total was -8.700000. running mean: -4.648599\n",
      "ep 3370: ep_len:540 episode reward: total was -10.360000. running mean: -4.705713\n",
      "epsilon:0.010000 episode_count: 23597. steps_count: 10434031.000000\n",
      "ep 3371: ep_len:500 episode reward: total was 7.420000. running mean: -4.584456\n",
      "ep 3371: ep_len:500 episode reward: total was 5.370000. running mean: -4.484911\n",
      "ep 3371: ep_len:530 episode reward: total was 6.970000. running mean: -4.370362\n",
      "ep 3371: ep_len:520 episode reward: total was -9.000000. running mean: -4.416658\n",
      "ep 3371: ep_len:3 episode reward: total was 0.000000. running mean: -4.372492\n",
      "ep 3371: ep_len:620 episode reward: total was -37.000000. running mean: -4.698767\n",
      "ep 3371: ep_len:510 episode reward: total was -4.930000. running mean: -4.701079\n",
      "epsilon:0.010000 episode_count: 23604. steps_count: 10437214.000000\n",
      "ep 3372: ep_len:630 episode reward: total was -27.750000. running mean: -4.931568\n",
      "ep 3372: ep_len:183 episode reward: total was -1.390000. running mean: -4.896153\n",
      "ep 3372: ep_len:580 episode reward: total was 5.160000. running mean: -4.795591\n",
      "ep 3372: ep_len:510 episode reward: total was -32.200000. running mean: -5.069635\n",
      "ep 3372: ep_len:3 episode reward: total was 0.000000. running mean: -5.018939\n",
      "ep 3372: ep_len:580 episode reward: total was 2.940000. running mean: -4.939349\n",
      "ep 3372: ep_len:595 episode reward: total was 0.230000. running mean: -4.887656\n",
      "epsilon:0.010000 episode_count: 23611. steps_count: 10440295.000000\n",
      "ep 3373: ep_len:194 episode reward: total was -10.370000. running mean: -4.942479\n",
      "ep 3373: ep_len:690 episode reward: total was -28.270000. running mean: -5.175755\n",
      "ep 3373: ep_len:575 episode reward: total was -8.210000. running mean: -5.206097\n",
      "ep 3373: ep_len:580 episode reward: total was -11.470000. running mean: -5.268736\n",
      "ep 3373: ep_len:48 episode reward: total was 4.500000. running mean: -5.171049\n",
      "ep 3373: ep_len:322 episode reward: total was -20.320000. running mean: -5.322538\n",
      "ep 3373: ep_len:535 episode reward: total was -13.310000. running mean: -5.402413\n",
      "epsilon:0.010000 episode_count: 23618. steps_count: 10443239.000000\n",
      "ep 3374: ep_len:590 episode reward: total was -9.300000. running mean: -5.441389\n",
      "ep 3374: ep_len:605 episode reward: total was 8.540000. running mean: -5.301575\n",
      "ep 3374: ep_len:625 episode reward: total was 8.000000. running mean: -5.168559\n",
      "ep 3374: ep_len:615 episode reward: total was 11.170000. running mean: -5.005173\n",
      "ep 3374: ep_len:90 episode reward: total was -3.450000. running mean: -4.989622\n",
      "ep 3374: ep_len:625 episode reward: total was -7.860000. running mean: -5.018326\n",
      "ep 3374: ep_len:550 episode reward: total was -20.340000. running mean: -5.171542\n",
      "epsilon:0.010000 episode_count: 23625. steps_count: 10446939.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3375: ep_len:525 episode reward: total was -15.030000. running mean: -5.270127\n",
      "ep 3375: ep_len:550 episode reward: total was -12.270000. running mean: -5.340126\n",
      "ep 3375: ep_len:500 episode reward: total was -29.610000. running mean: -5.582824\n",
      "ep 3375: ep_len:525 episode reward: total was -14.530000. running mean: -5.672296\n",
      "ep 3375: ep_len:129 episode reward: total was 3.540000. running mean: -5.580173\n",
      "ep 3375: ep_len:580 episode reward: total was -3.180000. running mean: -5.556171\n",
      "ep 3375: ep_len:327 episode reward: total was -1.290000. running mean: -5.513510\n",
      "epsilon:0.010000 episode_count: 23632. steps_count: 10450075.000000\n",
      "ep 3376: ep_len:635 episode reward: total was -5.660000. running mean: -5.514975\n",
      "ep 3376: ep_len:500 episode reward: total was 8.230000. running mean: -5.377525\n",
      "ep 3376: ep_len:830 episode reward: total was -35.560000. running mean: -5.679350\n",
      "ep 3376: ep_len:510 episode reward: total was 13.480000. running mean: -5.487756\n",
      "ep 3376: ep_len:3 episode reward: total was 0.000000. running mean: -5.432879\n",
      "ep 3376: ep_len:302 episode reward: total was 3.700000. running mean: -5.341550\n",
      "ep 3376: ep_len:615 episode reward: total was -4.010000. running mean: -5.328234\n",
      "epsilon:0.010000 episode_count: 23639. steps_count: 10453470.000000\n",
      "ep 3377: ep_len:650 episode reward: total was -22.770000. running mean: -5.502652\n",
      "ep 3377: ep_len:273 episode reward: total was -7.900000. running mean: -5.526625\n",
      "ep 3377: ep_len:570 episode reward: total was -1.700000. running mean: -5.488359\n",
      "ep 3377: ep_len:505 episode reward: total was -7.520000. running mean: -5.508676\n",
      "ep 3377: ep_len:96 episode reward: total was 6.540000. running mean: -5.388189\n",
      "ep 3377: ep_len:620 episode reward: total was -13.160000. running mean: -5.465907\n",
      "ep 3377: ep_len:565 episode reward: total was -4.580000. running mean: -5.457048\n",
      "epsilon:0.010000 episode_count: 23646. steps_count: 10456749.000000\n",
      "ep 3378: ep_len:500 episode reward: total was 0.680000. running mean: -5.395677\n",
      "ep 3378: ep_len:570 episode reward: total was 16.890000. running mean: -5.172821\n",
      "ep 3378: ep_len:381 episode reward: total was -17.340000. running mean: -5.294492\n",
      "ep 3378: ep_len:509 episode reward: total was 11.080000. running mean: -5.130747\n",
      "ep 3378: ep_len:48 episode reward: total was -6.000000. running mean: -5.139440\n",
      "ep 3378: ep_len:505 episode reward: total was -14.520000. running mean: -5.233246\n",
      "ep 3378: ep_len:520 episode reward: total was -17.380000. running mean: -5.354713\n",
      "epsilon:0.010000 episode_count: 23653. steps_count: 10459782.000000\n",
      "ep 3379: ep_len:665 episode reward: total was -12.120000. running mean: -5.422366\n",
      "ep 3379: ep_len:515 episode reward: total was -9.940000. running mean: -5.467542\n",
      "ep 3379: ep_len:825 episode reward: total was -54.220000. running mean: -5.955067\n",
      "ep 3379: ep_len:530 episode reward: total was 11.910000. running mean: -5.776416\n",
      "ep 3379: ep_len:3 episode reward: total was 0.000000. running mean: -5.718652\n",
      "ep 3379: ep_len:186 episode reward: total was 8.130000. running mean: -5.580166\n",
      "ep 3379: ep_len:275 episode reward: total was -3.790000. running mean: -5.562264\n",
      "epsilon:0.010000 episode_count: 23660. steps_count: 10462781.000000\n",
      "ep 3380: ep_len:194 episode reward: total was -6.390000. running mean: -5.570541\n",
      "ep 3380: ep_len:622 episode reward: total was -52.930000. running mean: -6.044136\n",
      "ep 3380: ep_len:580 episode reward: total was -24.860000. running mean: -6.232294\n",
      "ep 3380: ep_len:500 episode reward: total was 11.410000. running mean: -6.055872\n",
      "ep 3380: ep_len:3 episode reward: total was 0.000000. running mean: -5.995313\n",
      "ep 3380: ep_len:655 episode reward: total was -1.570000. running mean: -5.951060\n",
      "ep 3380: ep_len:720 episode reward: total was -44.790000. running mean: -6.339449\n",
      "epsilon:0.010000 episode_count: 23667. steps_count: 10466055.000000\n",
      "ep 3381: ep_len:134 episode reward: total was 2.570000. running mean: -6.250355\n",
      "ep 3381: ep_len:298 episode reward: total was -4.330000. running mean: -6.231151\n",
      "ep 3381: ep_len:685 episode reward: total was -1.550000. running mean: -6.184340\n",
      "ep 3381: ep_len:391 episode reward: total was 1.380000. running mean: -6.108696\n",
      "ep 3381: ep_len:49 episode reward: total was 4.500000. running mean: -6.002609\n",
      "ep 3381: ep_len:615 episode reward: total was -22.290000. running mean: -6.165483\n",
      "ep 3381: ep_len:500 episode reward: total was -12.240000. running mean: -6.226228\n",
      "epsilon:0.010000 episode_count: 23674. steps_count: 10468727.000000\n",
      "ep 3382: ep_len:565 episode reward: total was -17.660000. running mean: -6.340566\n",
      "ep 3382: ep_len:725 episode reward: total was -36.880000. running mean: -6.645960\n",
      "ep 3382: ep_len:374 episode reward: total was 6.240000. running mean: -6.517101\n",
      "ep 3382: ep_len:54 episode reward: total was 2.070000. running mean: -6.431230\n",
      "ep 3382: ep_len:3 episode reward: total was 0.000000. running mean: -6.366917\n",
      "ep 3382: ep_len:500 episode reward: total was -6.340000. running mean: -6.366648\n",
      "ep 3382: ep_len:540 episode reward: total was -13.120000. running mean: -6.434182\n",
      "epsilon:0.010000 episode_count: 23681. steps_count: 10471488.000000\n",
      "ep 3383: ep_len:500 episode reward: total was -0.140000. running mean: -6.371240\n",
      "ep 3383: ep_len:276 episode reward: total was -3.850000. running mean: -6.346028\n",
      "ep 3383: ep_len:635 episode reward: total was 1.870000. running mean: -6.263867\n",
      "ep 3383: ep_len:590 episode reward: total was -34.090000. running mean: -6.542129\n",
      "ep 3383: ep_len:80 episode reward: total was -7.430000. running mean: -6.551007\n",
      "ep 3383: ep_len:515 episode reward: total was -13.230000. running mean: -6.617797\n",
      "ep 3383: ep_len:550 episode reward: total was -8.600000. running mean: -6.637619\n",
      "epsilon:0.010000 episode_count: 23688. steps_count: 10474634.000000\n",
      "ep 3384: ep_len:615 episode reward: total was -15.570000. running mean: -6.726943\n",
      "ep 3384: ep_len:585 episode reward: total was -12.570000. running mean: -6.785374\n",
      "ep 3384: ep_len:525 episode reward: total was -12.290000. running mean: -6.840420\n",
      "ep 3384: ep_len:510 episode reward: total was 5.580000. running mean: -6.716216\n",
      "ep 3384: ep_len:49 episode reward: total was 4.500000. running mean: -6.604054\n",
      "ep 3384: ep_len:570 episode reward: total was 3.350000. running mean: -6.504513\n",
      "ep 3384: ep_len:319 episode reward: total was -3.790000. running mean: -6.477368\n",
      "epsilon:0.010000 episode_count: 23695. steps_count: 10477807.000000\n",
      "ep 3385: ep_len:640 episode reward: total was -2.420000. running mean: -6.436794\n",
      "ep 3385: ep_len:615 episode reward: total was 24.940000. running mean: -6.123026\n",
      "ep 3385: ep_len:620 episode reward: total was 0.460000. running mean: -6.057196\n",
      "ep 3385: ep_len:505 episode reward: total was 6.580000. running mean: -5.930824\n",
      "ep 3385: ep_len:94 episode reward: total was -8.940000. running mean: -5.960916\n",
      "ep 3385: ep_len:500 episode reward: total was 2.440000. running mean: -5.876907\n",
      "ep 3385: ep_len:500 episode reward: total was -1.430000. running mean: -5.832438\n",
      "epsilon:0.010000 episode_count: 23702. steps_count: 10481281.000000\n",
      "ep 3386: ep_len:260 episode reward: total was 4.130000. running mean: -5.732813\n",
      "ep 3386: ep_len:500 episode reward: total was 8.630000. running mean: -5.589185\n",
      "ep 3386: ep_len:369 episode reward: total was -1.820000. running mean: -5.551493\n",
      "ep 3386: ep_len:148 episode reward: total was 4.100000. running mean: -5.454978\n",
      "ep 3386: ep_len:93 episode reward: total was -12.950000. running mean: -5.529929\n",
      "ep 3386: ep_len:575 episode reward: total was 5.970000. running mean: -5.414929\n",
      "ep 3386: ep_len:500 episode reward: total was -15.480000. running mean: -5.515580\n",
      "epsilon:0.010000 episode_count: 23709. steps_count: 10483726.000000\n",
      "ep 3387: ep_len:570 episode reward: total was 0.090000. running mean: -5.459524\n",
      "ep 3387: ep_len:168 episode reward: total was -6.410000. running mean: -5.469029\n",
      "ep 3387: ep_len:575 episode reward: total was -8.090000. running mean: -5.495239\n",
      "ep 3387: ep_len:114 episode reward: total was -0.930000. running mean: -5.449586\n",
      "ep 3387: ep_len:3 episode reward: total was 0.000000. running mean: -5.395090\n",
      "ep 3387: ep_len:540 episode reward: total was -35.570000. running mean: -5.696839\n",
      "ep 3387: ep_len:500 episode reward: total was -24.110000. running mean: -5.880971\n",
      "epsilon:0.010000 episode_count: 23716. steps_count: 10486196.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3388: ep_len:535 episode reward: total was 2.860000. running mean: -5.793561\n",
      "ep 3388: ep_len:515 episode reward: total was -2.630000. running mean: -5.761926\n",
      "ep 3388: ep_len:625 episode reward: total was -11.250000. running mean: -5.816806\n",
      "ep 3388: ep_len:530 episode reward: total was 14.470000. running mean: -5.613938\n",
      "ep 3388: ep_len:3 episode reward: total was 0.000000. running mean: -5.557799\n",
      "ep 3388: ep_len:520 episode reward: total was 0.280000. running mean: -5.499421\n",
      "ep 3388: ep_len:188 episode reward: total was -25.960000. running mean: -5.704027\n",
      "epsilon:0.010000 episode_count: 23723. steps_count: 10489112.000000\n",
      "ep 3389: ep_len:580 episode reward: total was 3.320000. running mean: -5.613787\n",
      "ep 3389: ep_len:585 episode reward: total was 19.580000. running mean: -5.361849\n",
      "ep 3389: ep_len:525 episode reward: total was -0.230000. running mean: -5.310530\n",
      "ep 3389: ep_len:500 episode reward: total was -11.540000. running mean: -5.372825\n",
      "ep 3389: ep_len:3 episode reward: total was 0.000000. running mean: -5.319097\n",
      "ep 3389: ep_len:625 episode reward: total was -2.020000. running mean: -5.286106\n",
      "ep 3389: ep_len:344 episode reward: total was -12.830000. running mean: -5.361545\n",
      "epsilon:0.010000 episode_count: 23730. steps_count: 10492274.000000\n",
      "ep 3390: ep_len:520 episode reward: total was -13.340000. running mean: -5.441329\n",
      "ep 3390: ep_len:545 episode reward: total was 17.880000. running mean: -5.208116\n",
      "ep 3390: ep_len:685 episode reward: total was -2.590000. running mean: -5.181935\n",
      "ep 3390: ep_len:520 episode reward: total was 8.450000. running mean: -5.045615\n",
      "ep 3390: ep_len:3 episode reward: total was 0.000000. running mean: -4.995159\n",
      "ep 3390: ep_len:685 episode reward: total was 3.410000. running mean: -4.911108\n",
      "ep 3390: ep_len:600 episode reward: total was -9.530000. running mean: -4.957297\n",
      "epsilon:0.010000 episode_count: 23737. steps_count: 10495832.000000\n",
      "ep 3391: ep_len:198 episode reward: total was 3.600000. running mean: -4.871724\n",
      "ep 3391: ep_len:625 episode reward: total was -30.850000. running mean: -5.131506\n",
      "ep 3391: ep_len:600 episode reward: total was 4.470000. running mean: -5.035491\n",
      "ep 3391: ep_len:500 episode reward: total was 9.940000. running mean: -4.885736\n",
      "ep 3391: ep_len:3 episode reward: total was 0.000000. running mean: -4.836879\n",
      "ep 3391: ep_len:505 episode reward: total was -22.480000. running mean: -5.013310\n",
      "ep 3391: ep_len:345 episode reward: total was -9.330000. running mean: -5.056477\n",
      "epsilon:0.010000 episode_count: 23744. steps_count: 10498608.000000\n",
      "ep 3392: ep_len:575 episode reward: total was 4.920000. running mean: -4.956712\n",
      "ep 3392: ep_len:625 episode reward: total was 22.580000. running mean: -4.681345\n",
      "ep 3392: ep_len:500 episode reward: total was 9.870000. running mean: -4.535832\n",
      "ep 3392: ep_len:500 episode reward: total was -5.470000. running mean: -4.545173\n",
      "ep 3392: ep_len:3 episode reward: total was 0.000000. running mean: -4.499722\n",
      "ep 3392: ep_len:645 episode reward: total was -2.700000. running mean: -4.481725\n",
      "ep 3392: ep_len:194 episode reward: total was -10.400000. running mean: -4.540907\n",
      "epsilon:0.010000 episode_count: 23751. steps_count: 10501650.000000\n",
      "ep 3393: ep_len:205 episode reward: total was -15.850000. running mean: -4.653998\n",
      "ep 3393: ep_len:595 episode reward: total was -45.450000. running mean: -5.061958\n",
      "ep 3393: ep_len:560 episode reward: total was 2.810000. running mean: -4.983239\n",
      "ep 3393: ep_len:120 episode reward: total was 5.610000. running mean: -4.877306\n",
      "ep 3393: ep_len:106 episode reward: total was 7.540000. running mean: -4.753133\n",
      "ep 3393: ep_len:250 episode reward: total was 6.680000. running mean: -4.638802\n",
      "ep 3393: ep_len:195 episode reward: total was -10.420000. running mean: -4.696614\n",
      "epsilon:0.010000 episode_count: 23758. steps_count: 10503681.000000\n",
      "ep 3394: ep_len:500 episode reward: total was 8.780000. running mean: -4.561848\n",
      "ep 3394: ep_len:500 episode reward: total was -22.850000. running mean: -4.744729\n",
      "ep 3394: ep_len:645 episode reward: total was -13.200000. running mean: -4.829282\n",
      "ep 3394: ep_len:525 episode reward: total was -10.030000. running mean: -4.881289\n",
      "ep 3394: ep_len:3 episode reward: total was 0.000000. running mean: -4.832476\n",
      "ep 3394: ep_len:500 episode reward: total was -16.670000. running mean: -4.950851\n",
      "ep 3394: ep_len:555 episode reward: total was -5.060000. running mean: -4.951943\n",
      "epsilon:0.010000 episode_count: 23765. steps_count: 10506909.000000\n",
      "ep 3395: ep_len:134 episode reward: total was -10.430000. running mean: -5.006724\n",
      "ep 3395: ep_len:500 episode reward: total was 5.890000. running mean: -4.897756\n",
      "ep 3395: ep_len:525 episode reward: total was -6.690000. running mean: -4.915679\n",
      "ep 3395: ep_len:500 episode reward: total was 14.950000. running mean: -4.717022\n",
      "ep 3395: ep_len:3 episode reward: total was 0.000000. running mean: -4.669852\n",
      "ep 3395: ep_len:540 episode reward: total was 0.460000. running mean: -4.618553\n",
      "ep 3395: ep_len:510 episode reward: total was -8.130000. running mean: -4.653668\n",
      "epsilon:0.010000 episode_count: 23772. steps_count: 10509621.000000\n",
      "ep 3396: ep_len:610 episode reward: total was 8.000000. running mean: -4.527131\n",
      "ep 3396: ep_len:580 episode reward: total was 7.850000. running mean: -4.403360\n",
      "ep 3396: ep_len:545 episode reward: total was -6.220000. running mean: -4.421526\n",
      "ep 3396: ep_len:565 episode reward: total was 17.000000. running mean: -4.207311\n",
      "ep 3396: ep_len:3 episode reward: total was 0.000000. running mean: -4.165238\n",
      "ep 3396: ep_len:510 episode reward: total was 7.900000. running mean: -4.044585\n",
      "ep 3396: ep_len:590 episode reward: total was -20.790000. running mean: -4.212039\n",
      "epsilon:0.010000 episode_count: 23779. steps_count: 10513024.000000\n",
      "ep 3397: ep_len:500 episode reward: total was -16.390000. running mean: -4.333819\n",
      "ep 3397: ep_len:505 episode reward: total was -5.840000. running mean: -4.348881\n",
      "ep 3397: ep_len:334 episode reward: total was -8.360000. running mean: -4.388992\n",
      "ep 3397: ep_len:605 episode reward: total was 7.120000. running mean: -4.273902\n",
      "ep 3397: ep_len:3 episode reward: total was 0.000000. running mean: -4.231163\n",
      "ep 3397: ep_len:255 episode reward: total was 3.120000. running mean: -4.157652\n",
      "ep 3397: ep_len:585 episode reward: total was -8.440000. running mean: -4.200475\n",
      "epsilon:0.010000 episode_count: 23786. steps_count: 10515811.000000\n",
      "ep 3398: ep_len:515 episode reward: total was 5.460000. running mean: -4.103870\n",
      "ep 3398: ep_len:600 episode reward: total was -7.570000. running mean: -4.138532\n",
      "ep 3398: ep_len:70 episode reward: total was -2.970000. running mean: -4.126846\n",
      "ep 3398: ep_len:627 episode reward: total was -28.360000. running mean: -4.369178\n",
      "ep 3398: ep_len:3 episode reward: total was 0.000000. running mean: -4.325486\n",
      "ep 3398: ep_len:505 episode reward: total was 3.570000. running mean: -4.246531\n",
      "ep 3398: ep_len:555 episode reward: total was -11.340000. running mean: -4.317466\n",
      "epsilon:0.010000 episode_count: 23793. steps_count: 10518686.000000\n",
      "ep 3399: ep_len:590 episode reward: total was -20.180000. running mean: -4.476091\n",
      "ep 3399: ep_len:305 episode reward: total was -36.340000. running mean: -4.794730\n",
      "ep 3399: ep_len:500 episode reward: total was 4.540000. running mean: -4.701383\n",
      "ep 3399: ep_len:500 episode reward: total was 8.980000. running mean: -4.564569\n",
      "ep 3399: ep_len:3 episode reward: total was 0.000000. running mean: -4.518923\n",
      "ep 3399: ep_len:640 episode reward: total was -42.150000. running mean: -4.895234\n",
      "ep 3399: ep_len:540 episode reward: total was -14.500000. running mean: -4.991282\n",
      "epsilon:0.010000 episode_count: 23800. steps_count: 10521764.000000\n",
      "ep 3400: ep_len:595 episode reward: total was 1.590000. running mean: -4.925469\n",
      "ep 3400: ep_len:500 episode reward: total was -18.600000. running mean: -5.062214\n",
      "ep 3400: ep_len:630 episode reward: total was -0.880000. running mean: -5.020392\n",
      "ep 3400: ep_len:595 episode reward: total was 14.490000. running mean: -4.825288\n",
      "ep 3400: ep_len:3 episode reward: total was 0.000000. running mean: -4.777035\n",
      "ep 3400: ep_len:510 episode reward: total was 5.390000. running mean: -4.675365\n",
      "ep 3400: ep_len:510 episode reward: total was -24.600000. running mean: -4.874611\n",
      "epsilon:0.010000 episode_count: 23807. steps_count: 10525107.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3401: ep_len:620 episode reward: total was 0.610000. running mean: -4.819765\n",
      "ep 3401: ep_len:500 episode reward: total was 18.680000. running mean: -4.584768\n",
      "ep 3401: ep_len:570 episode reward: total was -24.340000. running mean: -4.782320\n",
      "ep 3401: ep_len:500 episode reward: total was -35.620000. running mean: -5.090697\n",
      "ep 3401: ep_len:107 episode reward: total was -7.930000. running mean: -5.119090\n",
      "ep 3401: ep_len:575 episode reward: total was 2.380000. running mean: -5.044099\n",
      "ep 3401: ep_len:510 episode reward: total was -7.730000. running mean: -5.070958\n",
      "epsilon:0.010000 episode_count: 23814. steps_count: 10528489.000000\n",
      "ep 3402: ep_len:500 episode reward: total was 8.430000. running mean: -4.935948\n",
      "ep 3402: ep_len:635 episode reward: total was -19.260000. running mean: -5.079189\n",
      "ep 3402: ep_len:615 episode reward: total was -16.490000. running mean: -5.193297\n",
      "ep 3402: ep_len:500 episode reward: total was -6.510000. running mean: -5.206464\n",
      "ep 3402: ep_len:3 episode reward: total was 0.000000. running mean: -5.154399\n",
      "ep 3402: ep_len:500 episode reward: total was 2.290000. running mean: -5.079955\n",
      "ep 3402: ep_len:625 episode reward: total was -14.800000. running mean: -5.177156\n",
      "epsilon:0.010000 episode_count: 23821. steps_count: 10531867.000000\n",
      "ep 3403: ep_len:210 episode reward: total was -11.830000. running mean: -5.243684\n",
      "ep 3403: ep_len:500 episode reward: total was -17.850000. running mean: -5.369747\n",
      "ep 3403: ep_len:459 episode reward: total was -18.310000. running mean: -5.499150\n",
      "ep 3403: ep_len:520 episode reward: total was -4.450000. running mean: -5.488658\n",
      "ep 3403: ep_len:3 episode reward: total was 0.000000. running mean: -5.433772\n",
      "ep 3403: ep_len:600 episode reward: total was 5.460000. running mean: -5.324834\n",
      "ep 3403: ep_len:600 episode reward: total was 5.320000. running mean: -5.218386\n",
      "epsilon:0.010000 episode_count: 23828. steps_count: 10534759.000000\n",
      "ep 3404: ep_len:229 episode reward: total was 3.630000. running mean: -5.129902\n",
      "ep 3404: ep_len:560 episode reward: total was -35.950000. running mean: -5.438103\n",
      "ep 3404: ep_len:610 episode reward: total was -9.170000. running mean: -5.475422\n",
      "ep 3404: ep_len:510 episode reward: total was -8.850000. running mean: -5.509168\n",
      "ep 3404: ep_len:3 episode reward: total was 0.000000. running mean: -5.454076\n",
      "ep 3404: ep_len:755 episode reward: total was -34.740000. running mean: -5.746935\n",
      "ep 3404: ep_len:530 episode reward: total was -17.980000. running mean: -5.869266\n",
      "epsilon:0.010000 episode_count: 23835. steps_count: 10537956.000000\n",
      "ep 3405: ep_len:515 episode reward: total was -0.480000. running mean: -5.815373\n",
      "ep 3405: ep_len:500 episode reward: total was -37.200000. running mean: -6.129219\n",
      "ep 3405: ep_len:555 episode reward: total was -21.840000. running mean: -6.286327\n",
      "ep 3405: ep_len:550 episode reward: total was -20.020000. running mean: -6.423664\n",
      "ep 3405: ep_len:3 episode reward: total was 0.000000. running mean: -6.359427\n",
      "ep 3405: ep_len:505 episode reward: total was -2.980000. running mean: -6.325633\n",
      "ep 3405: ep_len:500 episode reward: total was -9.610000. running mean: -6.358477\n",
      "epsilon:0.010000 episode_count: 23842. steps_count: 10541084.000000\n",
      "ep 3406: ep_len:535 episode reward: total was -19.770000. running mean: -6.492592\n",
      "ep 3406: ep_len:770 episode reward: total was -29.120000. running mean: -6.718866\n",
      "ep 3406: ep_len:515 episode reward: total was -1.970000. running mean: -6.671377\n",
      "ep 3406: ep_len:500 episode reward: total was -6.970000. running mean: -6.674364\n",
      "ep 3406: ep_len:97 episode reward: total was 1.060000. running mean: -6.597020\n",
      "ep 3406: ep_len:525 episode reward: total was -9.610000. running mean: -6.627150\n",
      "ep 3406: ep_len:320 episode reward: total was -47.430000. running mean: -7.035178\n",
      "epsilon:0.010000 episode_count: 23849. steps_count: 10544346.000000\n",
      "ep 3407: ep_len:565 episode reward: total was -6.410000. running mean: -7.028927\n",
      "ep 3407: ep_len:590 episode reward: total was 2.090000. running mean: -6.937737\n",
      "ep 3407: ep_len:570 episode reward: total was 8.490000. running mean: -6.783460\n",
      "ep 3407: ep_len:520 episode reward: total was -8.540000. running mean: -6.801025\n",
      "ep 3407: ep_len:3 episode reward: total was 0.000000. running mean: -6.733015\n",
      "ep 3407: ep_len:500 episode reward: total was -32.740000. running mean: -6.993085\n",
      "ep 3407: ep_len:595 episode reward: total was -6.010000. running mean: -6.983254\n",
      "epsilon:0.010000 episode_count: 23856. steps_count: 10547689.000000\n",
      "ep 3408: ep_len:116 episode reward: total was -6.430000. running mean: -6.977721\n",
      "ep 3408: ep_len:520 episode reward: total was -7.030000. running mean: -6.978244\n",
      "ep 3408: ep_len:500 episode reward: total was 7.100000. running mean: -6.837462\n",
      "ep 3408: ep_len:500 episode reward: total was -15.520000. running mean: -6.924287\n",
      "ep 3408: ep_len:102 episode reward: total was 5.540000. running mean: -6.799644\n",
      "ep 3408: ep_len:525 episode reward: total was -36.300000. running mean: -7.094648\n",
      "ep 3408: ep_len:307 episode reward: total was -47.400000. running mean: -7.497701\n",
      "epsilon:0.010000 episode_count: 23863. steps_count: 10550259.000000\n",
      "ep 3409: ep_len:206 episode reward: total was 2.100000. running mean: -7.401724\n",
      "ep 3409: ep_len:505 episode reward: total was 0.600000. running mean: -7.321707\n",
      "ep 3409: ep_len:500 episode reward: total was -4.870000. running mean: -7.297190\n",
      "ep 3409: ep_len:391 episode reward: total was -28.130000. running mean: -7.505518\n",
      "ep 3409: ep_len:112 episode reward: total was -1.940000. running mean: -7.449863\n",
      "ep 3409: ep_len:510 episode reward: total was -22.990000. running mean: -7.605264\n",
      "ep 3409: ep_len:505 episode reward: total was -16.540000. running mean: -7.694612\n",
      "epsilon:0.010000 episode_count: 23870. steps_count: 10552988.000000\n",
      "ep 3410: ep_len:500 episode reward: total was 19.340000. running mean: -7.424266\n",
      "ep 3410: ep_len:500 episode reward: total was 8.250000. running mean: -7.267523\n",
      "ep 3410: ep_len:590 episode reward: total was -6.210000. running mean: -7.256948\n",
      "ep 3410: ep_len:505 episode reward: total was 13.120000. running mean: -7.053178\n",
      "ep 3410: ep_len:3 episode reward: total was 0.000000. running mean: -6.982646\n",
      "ep 3410: ep_len:500 episode reward: total was 0.930000. running mean: -6.903520\n",
      "ep 3410: ep_len:346 episode reward: total was -10.770000. running mean: -6.942185\n",
      "epsilon:0.010000 episode_count: 23877. steps_count: 10555932.000000\n",
      "ep 3411: ep_len:605 episode reward: total was -15.160000. running mean: -7.024363\n",
      "ep 3411: ep_len:500 episode reward: total was -30.040000. running mean: -7.254519\n",
      "ep 3411: ep_len:575 episode reward: total was -6.910000. running mean: -7.251074\n",
      "ep 3411: ep_len:500 episode reward: total was -16.100000. running mean: -7.339563\n",
      "ep 3411: ep_len:82 episode reward: total was -10.950000. running mean: -7.375668\n",
      "ep 3411: ep_len:580 episode reward: total was -46.990000. running mean: -7.771811\n",
      "ep 3411: ep_len:211 episode reward: total was -6.380000. running mean: -7.757893\n",
      "epsilon:0.010000 episode_count: 23884. steps_count: 10558985.000000\n",
      "ep 3412: ep_len:575 episode reward: total was 1.690000. running mean: -7.663414\n",
      "ep 3412: ep_len:520 episode reward: total was 13.890000. running mean: -7.447880\n",
      "ep 3412: ep_len:575 episode reward: total was 0.330000. running mean: -7.370101\n",
      "ep 3412: ep_len:129 episode reward: total was 5.110000. running mean: -7.245300\n",
      "ep 3412: ep_len:40 episode reward: total was 2.500000. running mean: -7.147847\n",
      "ep 3412: ep_len:640 episode reward: total was -28.040000. running mean: -7.356769\n",
      "ep 3412: ep_len:500 episode reward: total was -30.430000. running mean: -7.587501\n",
      "epsilon:0.010000 episode_count: 23891. steps_count: 10561964.000000\n",
      "ep 3413: ep_len:525 episode reward: total was -23.860000. running mean: -7.750226\n",
      "ep 3413: ep_len:344 episode reward: total was -41.320000. running mean: -8.085924\n",
      "ep 3413: ep_len:500 episode reward: total was -7.090000. running mean: -8.075964\n",
      "ep 3413: ep_len:515 episode reward: total was -10.000000. running mean: -8.095205\n",
      "ep 3413: ep_len:132 episode reward: total was 6.060000. running mean: -7.953653\n",
      "ep 3413: ep_len:540 episode reward: total was -22.900000. running mean: -8.103116\n",
      "ep 3413: ep_len:600 episode reward: total was -10.290000. running mean: -8.124985\n",
      "epsilon:0.010000 episode_count: 23898. steps_count: 10565120.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3414: ep_len:590 episode reward: total was 0.900000. running mean: -8.034735\n",
      "ep 3414: ep_len:361 episode reward: total was -37.350000. running mean: -8.327888\n",
      "ep 3414: ep_len:70 episode reward: total was -2.450000. running mean: -8.269109\n",
      "ep 3414: ep_len:505 episode reward: total was -6.990000. running mean: -8.256318\n",
      "ep 3414: ep_len:3 episode reward: total was 0.000000. running mean: -8.173755\n",
      "ep 3414: ep_len:296 episode reward: total was -13.840000. running mean: -8.230417\n",
      "ep 3414: ep_len:500 episode reward: total was -3.700000. running mean: -8.185113\n",
      "epsilon:0.010000 episode_count: 23905. steps_count: 10567445.000000\n",
      "ep 3415: ep_len:232 episode reward: total was 4.600000. running mean: -8.057262\n",
      "ep 3415: ep_len:560 episode reward: total was 0.420000. running mean: -7.972489\n",
      "ep 3415: ep_len:575 episode reward: total was 2.500000. running mean: -7.867764\n",
      "ep 3415: ep_len:132 episode reward: total was 6.100000. running mean: -7.728087\n",
      "ep 3415: ep_len:106 episode reward: total was 3.530000. running mean: -7.615506\n",
      "ep 3415: ep_len:590 episode reward: total was -6.500000. running mean: -7.604351\n",
      "ep 3415: ep_len:525 episode reward: total was -6.350000. running mean: -7.591807\n",
      "epsilon:0.010000 episode_count: 23912. steps_count: 10570165.000000\n",
      "ep 3416: ep_len:116 episode reward: total was 2.570000. running mean: -7.490189\n",
      "ep 3416: ep_len:540 episode reward: total was -25.970000. running mean: -7.674987\n",
      "ep 3416: ep_len:540 episode reward: total was -21.840000. running mean: -7.816637\n",
      "ep 3416: ep_len:610 episode reward: total was 0.030000. running mean: -7.738171\n",
      "ep 3416: ep_len:3 episode reward: total was 0.000000. running mean: -7.660789\n",
      "ep 3416: ep_len:500 episode reward: total was 2.630000. running mean: -7.557881\n",
      "ep 3416: ep_len:500 episode reward: total was -10.980000. running mean: -7.592103\n",
      "epsilon:0.010000 episode_count: 23919. steps_count: 10572974.000000\n",
      "ep 3417: ep_len:735 episode reward: total was -62.830000. running mean: -8.144482\n",
      "ep 3417: ep_len:500 episode reward: total was -2.610000. running mean: -8.089137\n",
      "ep 3417: ep_len:655 episode reward: total was -23.770000. running mean: -8.245945\n",
      "ep 3417: ep_len:500 episode reward: total was 7.550000. running mean: -8.087986\n",
      "ep 3417: ep_len:3 episode reward: total was 0.000000. running mean: -8.007106\n",
      "ep 3417: ep_len:515 episode reward: total was -36.480000. running mean: -8.291835\n",
      "ep 3417: ep_len:540 episode reward: total was -1.820000. running mean: -8.227117\n",
      "epsilon:0.010000 episode_count: 23926. steps_count: 10576422.000000\n",
      "ep 3418: ep_len:214 episode reward: total was 7.150000. running mean: -8.073346\n",
      "ep 3418: ep_len:515 episode reward: total was -5.600000. running mean: -8.048612\n",
      "ep 3418: ep_len:530 episode reward: total was -5.930000. running mean: -8.027426\n",
      "ep 3418: ep_len:500 episode reward: total was -30.260000. running mean: -8.249752\n",
      "ep 3418: ep_len:3 episode reward: total was 0.000000. running mean: -8.167254\n",
      "ep 3418: ep_len:500 episode reward: total was 3.470000. running mean: -8.050882\n",
      "ep 3418: ep_len:595 episode reward: total was 2.000000. running mean: -7.950373\n",
      "epsilon:0.010000 episode_count: 23933. steps_count: 10579279.000000\n",
      "ep 3419: ep_len:220 episode reward: total was 4.660000. running mean: -7.824269\n",
      "ep 3419: ep_len:610 episode reward: total was -9.130000. running mean: -7.837326\n",
      "ep 3419: ep_len:500 episode reward: total was 1.080000. running mean: -7.748153\n",
      "ep 3419: ep_len:505 episode reward: total was 2.400000. running mean: -7.646672\n",
      "ep 3419: ep_len:3 episode reward: total was 0.000000. running mean: -7.570205\n",
      "ep 3419: ep_len:625 episode reward: total was 5.340000. running mean: -7.441103\n",
      "ep 3419: ep_len:195 episode reward: total was -8.340000. running mean: -7.450092\n",
      "epsilon:0.010000 episode_count: 23940. steps_count: 10581937.000000\n",
      "ep 3420: ep_len:655 episode reward: total was -35.830000. running mean: -7.733891\n",
      "ep 3420: ep_len:279 episode reward: total was -29.850000. running mean: -7.955052\n",
      "ep 3420: ep_len:570 episode reward: total was -7.030000. running mean: -7.945801\n",
      "ep 3420: ep_len:500 episode reward: total was -13.580000. running mean: -8.002143\n",
      "ep 3420: ep_len:3 episode reward: total was 0.000000. running mean: -7.922122\n",
      "ep 3420: ep_len:500 episode reward: total was -21.750000. running mean: -8.060401\n",
      "ep 3420: ep_len:580 episode reward: total was -21.960000. running mean: -8.199397\n",
      "epsilon:0.010000 episode_count: 23947. steps_count: 10585024.000000\n",
      "ep 3421: ep_len:595 episode reward: total was -2.500000. running mean: -8.142403\n",
      "ep 3421: ep_len:500 episode reward: total was 3.320000. running mean: -8.027779\n",
      "ep 3421: ep_len:640 episode reward: total was -2.190000. running mean: -7.969401\n",
      "ep 3421: ep_len:170 episode reward: total was 3.660000. running mean: -7.853107\n",
      "ep 3421: ep_len:80 episode reward: total was 3.540000. running mean: -7.739176\n",
      "ep 3421: ep_len:505 episode reward: total was 0.910000. running mean: -7.652684\n",
      "ep 3421: ep_len:555 episode reward: total was -5.020000. running mean: -7.626357\n",
      "epsilon:0.010000 episode_count: 23954. steps_count: 10588069.000000\n",
      "ep 3422: ep_len:610 episode reward: total was -20.280000. running mean: -7.752894\n",
      "ep 3422: ep_len:352 episode reward: total was -36.330000. running mean: -8.038665\n",
      "ep 3422: ep_len:520 episode reward: total was -9.350000. running mean: -8.051778\n",
      "ep 3422: ep_len:708 episode reward: total was -15.370000. running mean: -8.124960\n",
      "ep 3422: ep_len:3 episode reward: total was 0.000000. running mean: -8.043711\n",
      "ep 3422: ep_len:500 episode reward: total was -25.760000. running mean: -8.220874\n",
      "ep 3422: ep_len:525 episode reward: total was -9.610000. running mean: -8.234765\n",
      "epsilon:0.010000 episode_count: 23961. steps_count: 10591287.000000\n",
      "ep 3423: ep_len:725 episode reward: total was -25.740000. running mean: -8.409817\n",
      "ep 3423: ep_len:500 episode reward: total was 1.390000. running mean: -8.311819\n",
      "ep 3423: ep_len:555 episode reward: total was -7.810000. running mean: -8.306801\n",
      "ep 3423: ep_len:379 episode reward: total was 1.350000. running mean: -8.210233\n",
      "ep 3423: ep_len:3 episode reward: total was 0.000000. running mean: -8.128131\n",
      "ep 3423: ep_len:252 episode reward: total was 1.150000. running mean: -8.035349\n",
      "ep 3423: ep_len:211 episode reward: total was -2.340000. running mean: -7.978396\n",
      "epsilon:0.010000 episode_count: 23968. steps_count: 10593912.000000\n",
      "ep 3424: ep_len:500 episode reward: total was -15.870000. running mean: -8.057312\n",
      "ep 3424: ep_len:500 episode reward: total was 5.570000. running mean: -7.921039\n",
      "ep 3424: ep_len:500 episode reward: total was -5.330000. running mean: -7.895128\n",
      "ep 3424: ep_len:500 episode reward: total was -26.650000. running mean: -8.082677\n",
      "ep 3424: ep_len:3 episode reward: total was 0.000000. running mean: -8.001850\n",
      "ep 3424: ep_len:550 episode reward: total was -37.540000. running mean: -8.297232\n",
      "ep 3424: ep_len:510 episode reward: total was -9.450000. running mean: -8.308759\n",
      "epsilon:0.010000 episode_count: 23975. steps_count: 10596975.000000\n",
      "ep 3425: ep_len:545 episode reward: total was 1.140000. running mean: -8.214272\n",
      "ep 3425: ep_len:570 episode reward: total was 22.440000. running mean: -7.907729\n",
      "ep 3425: ep_len:452 episode reward: total was 11.280000. running mean: -7.715852\n",
      "ep 3425: ep_len:535 episode reward: total was -19.050000. running mean: -7.829193\n",
      "ep 3425: ep_len:3 episode reward: total was 0.000000. running mean: -7.750901\n",
      "ep 3425: ep_len:308 episode reward: total was 4.170000. running mean: -7.631692\n",
      "ep 3425: ep_len:500 episode reward: total was -32.680000. running mean: -7.882175\n",
      "epsilon:0.010000 episode_count: 23982. steps_count: 10599888.000000\n",
      "ep 3426: ep_len:515 episode reward: total was 0.620000. running mean: -7.797154\n",
      "ep 3426: ep_len:630 episode reward: total was 0.440000. running mean: -7.714782\n",
      "ep 3426: ep_len:500 episode reward: total was -5.510000. running mean: -7.692734\n",
      "ep 3426: ep_len:535 episode reward: total was 11.940000. running mean: -7.496407\n",
      "ep 3426: ep_len:3 episode reward: total was 0.000000. running mean: -7.421443\n",
      "ep 3426: ep_len:500 episode reward: total was -43.020000. running mean: -7.777428\n",
      "ep 3426: ep_len:600 episode reward: total was -22.110000. running mean: -7.920754\n",
      "epsilon:0.010000 episode_count: 23989. steps_count: 10603171.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3427: ep_len:515 episode reward: total was -3.140000. running mean: -7.872947\n",
      "ep 3427: ep_len:510 episode reward: total was -40.090000. running mean: -8.195117\n",
      "ep 3427: ep_len:500 episode reward: total was -11.560000. running mean: -8.228766\n",
      "ep 3427: ep_len:500 episode reward: total was -22.640000. running mean: -8.372878\n",
      "ep 3427: ep_len:3 episode reward: total was 0.000000. running mean: -8.289150\n",
      "ep 3427: ep_len:600 episode reward: total was 4.460000. running mean: -8.161658\n",
      "ep 3427: ep_len:295 episode reward: total was -26.890000. running mean: -8.348942\n",
      "epsilon:0.010000 episode_count: 23996. steps_count: 10606094.000000\n",
      "ep 3428: ep_len:217 episode reward: total was 0.130000. running mean: -8.264152\n",
      "ep 3428: ep_len:550 episode reward: total was 1.000000. running mean: -8.171511\n",
      "ep 3428: ep_len:500 episode reward: total was 1.770000. running mean: -8.072095\n",
      "ep 3428: ep_len:530 episode reward: total was 7.430000. running mean: -7.917075\n",
      "ep 3428: ep_len:3 episode reward: total was 0.000000. running mean: -7.837904\n",
      "ep 3428: ep_len:530 episode reward: total was -15.080000. running mean: -7.910325\n",
      "ep 3428: ep_len:575 episode reward: total was -21.010000. running mean: -8.041321\n",
      "epsilon:0.010000 episode_count: 24003. steps_count: 10608999.000000\n",
      "ep 3429: ep_len:520 episode reward: total was -12.200000. running mean: -8.082908\n",
      "ep 3429: ep_len:520 episode reward: total was -8.460000. running mean: -8.086679\n",
      "ep 3429: ep_len:500 episode reward: total was 0.410000. running mean: -8.001712\n",
      "ep 3429: ep_len:505 episode reward: total was -18.090000. running mean: -8.102595\n",
      "ep 3429: ep_len:3 episode reward: total was 0.000000. running mean: -8.021569\n",
      "ep 3429: ep_len:630 episode reward: total was 2.530000. running mean: -7.916054\n",
      "ep 3429: ep_len:540 episode reward: total was -20.460000. running mean: -8.041493\n",
      "epsilon:0.010000 episode_count: 24010. steps_count: 10612217.000000\n",
      "ep 3430: ep_len:585 episode reward: total was -9.460000. running mean: -8.055678\n",
      "ep 3430: ep_len:545 episode reward: total was -14.530000. running mean: -8.120421\n",
      "ep 3430: ep_len:635 episode reward: total was -23.060000. running mean: -8.269817\n",
      "ep 3430: ep_len:500 episode reward: total was -25.670000. running mean: -8.443819\n",
      "ep 3430: ep_len:3 episode reward: total was 0.000000. running mean: -8.359381\n",
      "ep 3430: ep_len:615 episode reward: total was -30.030000. running mean: -8.576087\n",
      "ep 3430: ep_len:595 episode reward: total was -15.000000. running mean: -8.640326\n",
      "epsilon:0.010000 episode_count: 24017. steps_count: 10615695.000000\n",
      "ep 3431: ep_len:520 episode reward: total was 1.090000. running mean: -8.543023\n",
      "ep 3431: ep_len:500 episode reward: total was -37.600000. running mean: -8.833593\n",
      "ep 3431: ep_len:580 episode reward: total was -21.240000. running mean: -8.957657\n",
      "ep 3431: ep_len:55 episode reward: total was 0.060000. running mean: -8.867480\n",
      "ep 3431: ep_len:85 episode reward: total was 3.030000. running mean: -8.748505\n",
      "ep 3431: ep_len:500 episode reward: total was -28.100000. running mean: -8.942020\n",
      "ep 3431: ep_len:305 episode reward: total was -11.810000. running mean: -8.970700\n",
      "epsilon:0.010000 episode_count: 24024. steps_count: 10618240.000000\n",
      "ep 3432: ep_len:645 episode reward: total was -7.230000. running mean: -8.953293\n",
      "ep 3432: ep_len:565 episode reward: total was -7.470000. running mean: -8.938460\n",
      "ep 3432: ep_len:400 episode reward: total was 4.740000. running mean: -8.801676\n",
      "ep 3432: ep_len:500 episode reward: total was -9.630000. running mean: -8.809959\n",
      "ep 3432: ep_len:95 episode reward: total was 3.540000. running mean: -8.686459\n",
      "ep 3432: ep_len:500 episode reward: total was -8.280000. running mean: -8.682395\n",
      "ep 3432: ep_len:301 episode reward: total was -9.830000. running mean: -8.693871\n",
      "epsilon:0.010000 episode_count: 24031. steps_count: 10621246.000000\n",
      "ep 3433: ep_len:570 episode reward: total was 10.100000. running mean: -8.505932\n",
      "ep 3433: ep_len:500 episode reward: total was 13.200000. running mean: -8.288873\n",
      "ep 3433: ep_len:525 episode reward: total was -10.320000. running mean: -8.309184\n",
      "ep 3433: ep_len:585 episode reward: total was 9.070000. running mean: -8.135392\n",
      "ep 3433: ep_len:93 episode reward: total was 3.530000. running mean: -8.018738\n",
      "ep 3433: ep_len:515 episode reward: total was 1.650000. running mean: -7.922051\n",
      "ep 3433: ep_len:184 episode reward: total was -1.360000. running mean: -7.856430\n",
      "epsilon:0.010000 episode_count: 24038. steps_count: 10624218.000000\n",
      "ep 3434: ep_len:535 episode reward: total was -11.260000. running mean: -7.890466\n",
      "ep 3434: ep_len:510 episode reward: total was -29.030000. running mean: -8.101861\n",
      "ep 3434: ep_len:450 episode reward: total was -16.220000. running mean: -8.183043\n",
      "ep 3434: ep_len:515 episode reward: total was -26.620000. running mean: -8.367412\n",
      "ep 3434: ep_len:126 episode reward: total was 6.050000. running mean: -8.223238\n",
      "ep 3434: ep_len:500 episode reward: total was 2.750000. running mean: -8.113506\n",
      "ep 3434: ep_len:540 episode reward: total was -46.160000. running mean: -8.493971\n",
      "epsilon:0.010000 episode_count: 24045. steps_count: 10627394.000000\n",
      "ep 3435: ep_len:505 episode reward: total was -1.630000. running mean: -8.425331\n",
      "ep 3435: ep_len:555 episode reward: total was 16.370000. running mean: -8.177378\n",
      "ep 3435: ep_len:595 episode reward: total was 1.470000. running mean: -8.080904\n",
      "ep 3435: ep_len:500 episode reward: total was -35.200000. running mean: -8.352095\n",
      "ep 3435: ep_len:3 episode reward: total was 0.000000. running mean: -8.268574\n",
      "ep 3435: ep_len:515 episode reward: total was -35.160000. running mean: -8.537488\n",
      "ep 3435: ep_len:555 episode reward: total was -3.040000. running mean: -8.482513\n",
      "epsilon:0.010000 episode_count: 24052. steps_count: 10630622.000000\n",
      "ep 3436: ep_len:229 episode reward: total was 2.130000. running mean: -8.376388\n",
      "ep 3436: ep_len:510 episode reward: total was 1.750000. running mean: -8.275124\n",
      "ep 3436: ep_len:630 episode reward: total was -15.250000. running mean: -8.344873\n",
      "ep 3436: ep_len:550 episode reward: total was 10.490000. running mean: -8.156524\n",
      "ep 3436: ep_len:3 episode reward: total was 0.000000. running mean: -8.074959\n",
      "ep 3436: ep_len:565 episode reward: total was -5.690000. running mean: -8.051109\n",
      "ep 3436: ep_len:745 episode reward: total was -53.440000. running mean: -8.504998\n",
      "epsilon:0.010000 episode_count: 24059. steps_count: 10633854.000000\n",
      "ep 3437: ep_len:570 episode reward: total was -41.200000. running mean: -8.831948\n",
      "ep 3437: ep_len:500 episode reward: total was -24.760000. running mean: -8.991229\n",
      "ep 3437: ep_len:500 episode reward: total was -5.270000. running mean: -8.954017\n",
      "ep 3437: ep_len:500 episode reward: total was -2.180000. running mean: -8.886276\n",
      "ep 3437: ep_len:3 episode reward: total was 0.000000. running mean: -8.797414\n",
      "ep 3437: ep_len:500 episode reward: total was -3.270000. running mean: -8.742140\n",
      "ep 3437: ep_len:321 episode reward: total was -4.760000. running mean: -8.702318\n",
      "epsilon:0.010000 episode_count: 24066. steps_count: 10636748.000000\n",
      "ep 3438: ep_len:215 episode reward: total was -0.430000. running mean: -8.619595\n",
      "ep 3438: ep_len:500 episode reward: total was -11.340000. running mean: -8.646799\n",
      "ep 3438: ep_len:423 episode reward: total was -9.750000. running mean: -8.657831\n",
      "ep 3438: ep_len:121 episode reward: total was 4.580000. running mean: -8.525453\n",
      "ep 3438: ep_len:51 episode reward: total was 5.000000. running mean: -8.390198\n",
      "ep 3438: ep_len:505 episode reward: total was -7.480000. running mean: -8.381096\n",
      "ep 3438: ep_len:500 episode reward: total was -18.170000. running mean: -8.478985\n",
      "epsilon:0.010000 episode_count: 24073. steps_count: 10639063.000000\n",
      "ep 3439: ep_len:525 episode reward: total was 1.630000. running mean: -8.377895\n",
      "ep 3439: ep_len:565 episode reward: total was 26.430000. running mean: -8.029816\n",
      "ep 3439: ep_len:585 episode reward: total was -1.920000. running mean: -7.968718\n",
      "ep 3439: ep_len:369 episode reward: total was -7.710000. running mean: -7.966131\n",
      "ep 3439: ep_len:88 episode reward: total was 5.540000. running mean: -7.831070\n",
      "ep 3439: ep_len:505 episode reward: total was -12.780000. running mean: -7.880559\n",
      "ep 3439: ep_len:211 episode reward: total was -1.330000. running mean: -7.815054\n",
      "epsilon:0.010000 episode_count: 24080. steps_count: 10641911.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3440: ep_len:224 episode reward: total was 2.580000. running mean: -7.711103\n",
      "ep 3440: ep_len:500 episode reward: total was -7.900000. running mean: -7.712992\n",
      "ep 3440: ep_len:575 episode reward: total was -29.330000. running mean: -7.929162\n",
      "ep 3440: ep_len:545 episode reward: total was -38.020000. running mean: -8.230070\n",
      "ep 3440: ep_len:2 episode reward: total was 0.000000. running mean: -8.147770\n",
      "ep 3440: ep_len:245 episode reward: total was 6.640000. running mean: -7.999892\n",
      "ep 3440: ep_len:615 episode reward: total was -7.560000. running mean: -7.995493\n",
      "epsilon:0.010000 episode_count: 24087. steps_count: 10644617.000000\n",
      "ep 3441: ep_len:645 episode reward: total was -10.140000. running mean: -8.016938\n",
      "ep 3441: ep_len:353 episode reward: total was -36.360000. running mean: -8.300369\n",
      "ep 3441: ep_len:575 episode reward: total was -0.390000. running mean: -8.221265\n",
      "ep 3441: ep_len:500 episode reward: total was -0.080000. running mean: -8.139852\n",
      "ep 3441: ep_len:106 episode reward: total was 6.040000. running mean: -7.998054\n",
      "ep 3441: ep_len:510 episode reward: total was -31.000000. running mean: -8.228073\n",
      "ep 3441: ep_len:525 episode reward: total was -18.660000. running mean: -8.332393\n",
      "epsilon:0.010000 episode_count: 24094. steps_count: 10647831.000000\n",
      "ep 3442: ep_len:249 episode reward: total was 6.610000. running mean: -8.182969\n",
      "ep 3442: ep_len:570 episode reward: total was 8.600000. running mean: -8.015139\n",
      "ep 3442: ep_len:565 episode reward: total was -2.710000. running mean: -7.962088\n",
      "ep 3442: ep_len:510 episode reward: total was 0.840000. running mean: -7.874067\n",
      "ep 3442: ep_len:3 episode reward: total was 0.000000. running mean: -7.795326\n",
      "ep 3442: ep_len:147 episode reward: total was 6.610000. running mean: -7.651273\n",
      "ep 3442: ep_len:525 episode reward: total was -20.460000. running mean: -7.779360\n",
      "epsilon:0.010000 episode_count: 24101. steps_count: 10650400.000000\n",
      "ep 3443: ep_len:500 episode reward: total was 9.810000. running mean: -7.603466\n",
      "ep 3443: ep_len:274 episode reward: total was -32.350000. running mean: -7.850932\n",
      "ep 3443: ep_len:680 episode reward: total was -1.590000. running mean: -7.788323\n",
      "ep 3443: ep_len:510 episode reward: total was -10.010000. running mean: -7.810539\n",
      "ep 3443: ep_len:3 episode reward: total was 0.000000. running mean: -7.732434\n",
      "ep 3443: ep_len:500 episode reward: total was 1.390000. running mean: -7.641210\n",
      "ep 3443: ep_len:540 episode reward: total was -45.190000. running mean: -8.016697\n",
      "epsilon:0.010000 episode_count: 24108. steps_count: 10653407.000000\n",
      "ep 3444: ep_len:500 episode reward: total was 6.760000. running mean: -7.868930\n",
      "ep 3444: ep_len:595 episode reward: total was 1.760000. running mean: -7.772641\n",
      "ep 3444: ep_len:500 episode reward: total was 0.100000. running mean: -7.693915\n",
      "ep 3444: ep_len:149 episode reward: total was 5.090000. running mean: -7.566076\n",
      "ep 3444: ep_len:3 episode reward: total was 0.000000. running mean: -7.490415\n",
      "ep 3444: ep_len:640 episode reward: total was 8.890000. running mean: -7.326611\n",
      "ep 3444: ep_len:575 episode reward: total was -18.890000. running mean: -7.442245\n",
      "epsilon:0.010000 episode_count: 24115. steps_count: 10656369.000000\n",
      "ep 3445: ep_len:240 episode reward: total was -19.340000. running mean: -7.561222\n",
      "ep 3445: ep_len:267 episode reward: total was -31.840000. running mean: -7.804010\n",
      "ep 3445: ep_len:565 episode reward: total was -35.690000. running mean: -8.082870\n",
      "ep 3445: ep_len:550 episode reward: total was 9.410000. running mean: -7.907941\n",
      "ep 3445: ep_len:3 episode reward: total was 0.000000. running mean: -7.828862\n",
      "ep 3445: ep_len:500 episode reward: total was -15.970000. running mean: -7.910273\n",
      "ep 3445: ep_len:630 episode reward: total was -3.250000. running mean: -7.863670\n",
      "epsilon:0.010000 episode_count: 24122. steps_count: 10659124.000000\n",
      "ep 3446: ep_len:530 episode reward: total was 5.380000. running mean: -7.731234\n",
      "ep 3446: ep_len:500 episode reward: total was 1.220000. running mean: -7.641721\n",
      "ep 3446: ep_len:550 episode reward: total was -7.630000. running mean: -7.641604\n",
      "ep 3446: ep_len:520 episode reward: total was -10.610000. running mean: -7.671288\n",
      "ep 3446: ep_len:91 episode reward: total was -1.460000. running mean: -7.609175\n",
      "ep 3446: ep_len:605 episode reward: total was 2.960000. running mean: -7.503483\n",
      "ep 3446: ep_len:545 episode reward: total was -13.380000. running mean: -7.562249\n",
      "epsilon:0.010000 episode_count: 24129. steps_count: 10662465.000000\n",
      "ep 3447: ep_len:545 episode reward: total was -11.700000. running mean: -7.603626\n",
      "ep 3447: ep_len:525 episode reward: total was 8.100000. running mean: -7.446590\n",
      "ep 3447: ep_len:78 episode reward: total was 0.040000. running mean: -7.371724\n",
      "ep 3447: ep_len:500 episode reward: total was 8.410000. running mean: -7.213907\n",
      "ep 3447: ep_len:3 episode reward: total was 0.000000. running mean: -7.141768\n",
      "ep 3447: ep_len:525 episode reward: total was -11.240000. running mean: -7.182750\n",
      "ep 3447: ep_len:500 episode reward: total was -8.750000. running mean: -7.198422\n",
      "epsilon:0.010000 episode_count: 24136. steps_count: 10665141.000000\n",
      "ep 3448: ep_len:505 episode reward: total was -3.790000. running mean: -7.164338\n",
      "ep 3448: ep_len:500 episode reward: total was -4.840000. running mean: -7.141095\n",
      "ep 3448: ep_len:610 episode reward: total was -0.370000. running mean: -7.073384\n",
      "ep 3448: ep_len:625 episode reward: total was -12.970000. running mean: -7.132350\n",
      "ep 3448: ep_len:90 episode reward: total was 4.540000. running mean: -7.015627\n",
      "ep 3448: ep_len:605 episode reward: total was 2.990000. running mean: -6.915570\n",
      "ep 3448: ep_len:575 episode reward: total was -19.400000. running mean: -7.040415\n",
      "epsilon:0.010000 episode_count: 24143. steps_count: 10668651.000000\n",
      "ep 3449: ep_len:620 episode reward: total was 3.640000. running mean: -6.933610\n",
      "ep 3449: ep_len:181 episode reward: total was -2.430000. running mean: -6.888574\n",
      "ep 3449: ep_len:515 episode reward: total was -0.930000. running mean: -6.828989\n",
      "ep 3449: ep_len:500 episode reward: total was -10.580000. running mean: -6.866499\n",
      "ep 3449: ep_len:3 episode reward: total was 0.000000. running mean: -6.797834\n",
      "ep 3449: ep_len:178 episode reward: total was 6.120000. running mean: -6.668655\n",
      "ep 3449: ep_len:510 episode reward: total was -22.940000. running mean: -6.831369\n",
      "epsilon:0.010000 episode_count: 24150. steps_count: 10671158.000000\n",
      "ep 3450: ep_len:535 episode reward: total was 5.590000. running mean: -6.707155\n",
      "ep 3450: ep_len:500 episode reward: total was 16.720000. running mean: -6.472884\n",
      "ep 3450: ep_len:403 episode reward: total was 4.220000. running mean: -6.365955\n",
      "ep 3450: ep_len:408 episode reward: total was 4.860000. running mean: -6.253695\n",
      "ep 3450: ep_len:98 episode reward: total was -9.450000. running mean: -6.285658\n",
      "ep 3450: ep_len:500 episode reward: total was -23.780000. running mean: -6.460602\n",
      "ep 3450: ep_len:500 episode reward: total was -11.570000. running mean: -6.511696\n",
      "epsilon:0.010000 episode_count: 24157. steps_count: 10674102.000000\n",
      "ep 3451: ep_len:510 episode reward: total was -10.830000. running mean: -6.554879\n",
      "ep 3451: ep_len:590 episode reward: total was 15.370000. running mean: -6.335630\n",
      "ep 3451: ep_len:550 episode reward: total was 5.450000. running mean: -6.217774\n",
      "ep 3451: ep_len:505 episode reward: total was -30.130000. running mean: -6.456896\n",
      "ep 3451: ep_len:3 episode reward: total was 0.000000. running mean: -6.392327\n",
      "ep 3451: ep_len:590 episode reward: total was -7.050000. running mean: -6.398904\n",
      "ep 3451: ep_len:510 episode reward: total was -7.580000. running mean: -6.410715\n",
      "epsilon:0.010000 episode_count: 24164. steps_count: 10677360.000000\n",
      "ep 3452: ep_len:125 episode reward: total was 6.110000. running mean: -6.285507\n",
      "ep 3452: ep_len:520 episode reward: total was -2.820000. running mean: -6.250852\n",
      "ep 3452: ep_len:429 episode reward: total was -11.790000. running mean: -6.306244\n",
      "ep 3452: ep_len:510 episode reward: total was 8.580000. running mean: -6.157381\n",
      "ep 3452: ep_len:34 episode reward: total was 2.000000. running mean: -6.075808\n",
      "ep 3452: ep_len:565 episode reward: total was 9.140000. running mean: -5.923650\n",
      "ep 3452: ep_len:535 episode reward: total was -10.490000. running mean: -5.969313\n",
      "epsilon:0.010000 episode_count: 24171. steps_count: 10680078.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3453: ep_len:530 episode reward: total was -20.940000. running mean: -6.119020\n",
      "ep 3453: ep_len:500 episode reward: total was 2.870000. running mean: -6.029130\n",
      "ep 3453: ep_len:590 episode reward: total was 7.960000. running mean: -5.889238\n",
      "ep 3453: ep_len:530 episode reward: total was 7.950000. running mean: -5.750846\n",
      "ep 3453: ep_len:3 episode reward: total was 0.000000. running mean: -5.693338\n",
      "ep 3453: ep_len:327 episode reward: total was -4.810000. running mean: -5.684504\n",
      "ep 3453: ep_len:290 episode reward: total was -4.320000. running mean: -5.670859\n",
      "epsilon:0.010000 episode_count: 24178. steps_count: 10682848.000000\n",
      "ep 3454: ep_len:134 episode reward: total was 3.580000. running mean: -5.578351\n",
      "ep 3454: ep_len:505 episode reward: total was -36.060000. running mean: -5.883167\n",
      "ep 3454: ep_len:575 episode reward: total was 3.450000. running mean: -5.789835\n",
      "ep 3454: ep_len:600 episode reward: total was 3.070000. running mean: -5.701237\n",
      "ep 3454: ep_len:3 episode reward: total was 0.000000. running mean: -5.644225\n",
      "ep 3454: ep_len:640 episode reward: total was 4.130000. running mean: -5.546482\n",
      "ep 3454: ep_len:565 episode reward: total was -32.140000. running mean: -5.812418\n",
      "epsilon:0.010000 episode_count: 24185. steps_count: 10685870.000000\n",
      "ep 3455: ep_len:555 episode reward: total was 7.070000. running mean: -5.683593\n",
      "ep 3455: ep_len:535 episode reward: total was 12.880000. running mean: -5.497958\n",
      "ep 3455: ep_len:74 episode reward: total was 0.550000. running mean: -5.437478\n",
      "ep 3455: ep_len:132 episode reward: total was 2.600000. running mean: -5.357103\n",
      "ep 3455: ep_len:3 episode reward: total was 0.000000. running mean: -5.303532\n",
      "ep 3455: ep_len:590 episode reward: total was -26.090000. running mean: -5.511397\n",
      "ep 3455: ep_len:500 episode reward: total was -4.220000. running mean: -5.498483\n",
      "epsilon:0.010000 episode_count: 24192. steps_count: 10688259.000000\n",
      "ep 3456: ep_len:550 episode reward: total was 0.140000. running mean: -5.442098\n",
      "ep 3456: ep_len:500 episode reward: total was -49.740000. running mean: -5.885077\n",
      "ep 3456: ep_len:371 episode reward: total was -23.380000. running mean: -6.060026\n",
      "ep 3456: ep_len:570 episode reward: total was 9.910000. running mean: -5.900326\n",
      "ep 3456: ep_len:3 episode reward: total was 0.000000. running mean: -5.841323\n",
      "ep 3456: ep_len:500 episode reward: total was -25.240000. running mean: -6.035310\n",
      "ep 3456: ep_len:505 episode reward: total was -2.810000. running mean: -6.003056\n",
      "epsilon:0.010000 episode_count: 24199. steps_count: 10691258.000000\n",
      "ep 3457: ep_len:655 episode reward: total was 8.200000. running mean: -5.861026\n",
      "ep 3457: ep_len:585 episode reward: total was -20.610000. running mean: -6.008516\n",
      "ep 3457: ep_len:620 episode reward: total was 3.950000. running mean: -5.908930\n",
      "ep 3457: ep_len:510 episode reward: total was -15.580000. running mean: -6.005641\n",
      "ep 3457: ep_len:87 episode reward: total was 5.540000. running mean: -5.890185\n",
      "ep 3457: ep_len:700 episode reward: total was 1.420000. running mean: -5.817083\n",
      "ep 3457: ep_len:600 episode reward: total was -1.530000. running mean: -5.774212\n",
      "epsilon:0.010000 episode_count: 24206. steps_count: 10695015.000000\n",
      "ep 3458: ep_len:565 episode reward: total was -2.080000. running mean: -5.737270\n",
      "ep 3458: ep_len:530 episode reward: total was 15.410000. running mean: -5.525797\n",
      "ep 3458: ep_len:610 episode reward: total was -7.560000. running mean: -5.546139\n",
      "ep 3458: ep_len:56 episode reward: total was 2.570000. running mean: -5.464978\n",
      "ep 3458: ep_len:3 episode reward: total was 0.000000. running mean: -5.410328\n",
      "ep 3458: ep_len:505 episode reward: total was 4.490000. running mean: -5.311325\n",
      "ep 3458: ep_len:505 episode reward: total was -5.050000. running mean: -5.308712\n",
      "epsilon:0.010000 episode_count: 24213. steps_count: 10697789.000000\n",
      "ep 3459: ep_len:505 episode reward: total was -26.360000. running mean: -5.519224\n",
      "ep 3459: ep_len:680 episode reward: total was -31.840000. running mean: -5.782432\n",
      "ep 3459: ep_len:555 episode reward: total was -2.130000. running mean: -5.745908\n",
      "ep 3459: ep_len:500 episode reward: total was -20.100000. running mean: -5.889449\n",
      "ep 3459: ep_len:3 episode reward: total was 0.000000. running mean: -5.830554\n",
      "ep 3459: ep_len:625 episode reward: total was 0.010000. running mean: -5.772149\n",
      "ep 3459: ep_len:520 episode reward: total was -4.000000. running mean: -5.754427\n",
      "epsilon:0.010000 episode_count: 24220. steps_count: 10701177.000000\n",
      "ep 3460: ep_len:224 episode reward: total was 6.130000. running mean: -5.635583\n",
      "ep 3460: ep_len:540 episode reward: total was -18.530000. running mean: -5.764527\n",
      "ep 3460: ep_len:79 episode reward: total was 0.040000. running mean: -5.706482\n",
      "ep 3460: ep_len:56 episode reward: total was 2.570000. running mean: -5.623717\n",
      "ep 3460: ep_len:3 episode reward: total was 0.000000. running mean: -5.567480\n",
      "ep 3460: ep_len:500 episode reward: total was 1.650000. running mean: -5.495305\n",
      "ep 3460: ep_len:500 episode reward: total was -4.190000. running mean: -5.482252\n",
      "epsilon:0.010000 episode_count: 24227. steps_count: 10703079.000000\n",
      "ep 3461: ep_len:500 episode reward: total was 10.940000. running mean: -5.318030\n",
      "ep 3461: ep_len:605 episode reward: total was 12.560000. running mean: -5.139249\n",
      "ep 3461: ep_len:630 episode reward: total was -11.010000. running mean: -5.197957\n",
      "ep 3461: ep_len:565 episode reward: total was 4.390000. running mean: -5.102077\n",
      "ep 3461: ep_len:3 episode reward: total was 0.000000. running mean: -5.051056\n",
      "ep 3461: ep_len:595 episode reward: total was 10.180000. running mean: -4.898746\n",
      "ep 3461: ep_len:180 episode reward: total was -9.410000. running mean: -4.943858\n",
      "epsilon:0.010000 episode_count: 24234. steps_count: 10706157.000000\n",
      "ep 3462: ep_len:640 episode reward: total was -8.650000. running mean: -4.980920\n",
      "ep 3462: ep_len:500 episode reward: total was -18.950000. running mean: -5.120611\n",
      "ep 3462: ep_len:515 episode reward: total was -1.750000. running mean: -5.086904\n",
      "ep 3462: ep_len:520 episode reward: total was -30.190000. running mean: -5.337935\n",
      "ep 3462: ep_len:3 episode reward: total was 0.000000. running mean: -5.284556\n",
      "ep 3462: ep_len:555 episode reward: total was -0.200000. running mean: -5.233711\n",
      "ep 3462: ep_len:500 episode reward: total was -8.390000. running mean: -5.265273\n",
      "epsilon:0.010000 episode_count: 24241. steps_count: 10709390.000000\n",
      "ep 3463: ep_len:560 episode reward: total was 11.970000. running mean: -5.092921\n",
      "ep 3463: ep_len:500 episode reward: total was -18.950000. running mean: -5.231491\n",
      "ep 3463: ep_len:580 episode reward: total was 0.340000. running mean: -5.175777\n",
      "ep 3463: ep_len:639 episode reward: total was -26.330000. running mean: -5.387319\n",
      "ep 3463: ep_len:3 episode reward: total was 0.000000. running mean: -5.333446\n",
      "ep 3463: ep_len:530 episode reward: total was 10.600000. running mean: -5.174111\n",
      "ep 3463: ep_len:198 episode reward: total was -2.340000. running mean: -5.145770\n",
      "epsilon:0.010000 episode_count: 24248. steps_count: 10712400.000000\n",
      "ep 3464: ep_len:590 episode reward: total was -6.940000. running mean: -5.163712\n",
      "ep 3464: ep_len:500 episode reward: total was -5.680000. running mean: -5.168875\n",
      "ep 3464: ep_len:620 episode reward: total was -0.410000. running mean: -5.121286\n",
      "ep 3464: ep_len:630 episode reward: total was 10.090000. running mean: -4.969174\n",
      "ep 3464: ep_len:3 episode reward: total was 0.000000. running mean: -4.919482\n",
      "ep 3464: ep_len:530 episode reward: total was 1.830000. running mean: -4.851987\n",
      "ep 3464: ep_len:211 episode reward: total was 2.220000. running mean: -4.781267\n",
      "epsilon:0.010000 episode_count: 24255. steps_count: 10715484.000000\n",
      "ep 3465: ep_len:500 episode reward: total was 11.460000. running mean: -4.618855\n",
      "ep 3465: ep_len:510 episode reward: total was 3.140000. running mean: -4.541266\n",
      "ep 3465: ep_len:535 episode reward: total was -0.240000. running mean: -4.498253\n",
      "ep 3465: ep_len:520 episode reward: total was -12.040000. running mean: -4.573671\n",
      "ep 3465: ep_len:3 episode reward: total was 0.000000. running mean: -4.527934\n",
      "ep 3465: ep_len:805 episode reward: total was -41.130000. running mean: -4.893955\n",
      "ep 3465: ep_len:282 episode reward: total was -0.230000. running mean: -4.847315\n",
      "epsilon:0.010000 episode_count: 24262. steps_count: 10718639.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3466: ep_len:615 episode reward: total was 3.620000. running mean: -4.762642\n",
      "ep 3466: ep_len:745 episode reward: total was -42.380000. running mean: -5.138816\n",
      "ep 3466: ep_len:515 episode reward: total was -8.760000. running mean: -5.175027\n",
      "ep 3466: ep_len:529 episode reward: total was -42.020000. running mean: -5.543477\n",
      "ep 3466: ep_len:3 episode reward: total was 0.000000. running mean: -5.488042\n",
      "ep 3466: ep_len:314 episode reward: total was 4.150000. running mean: -5.391662\n",
      "ep 3466: ep_len:291 episode reward: total was -2.320000. running mean: -5.360945\n",
      "epsilon:0.010000 episode_count: 24269. steps_count: 10721651.000000\n",
      "ep 3467: ep_len:500 episode reward: total was -10.390000. running mean: -5.411236\n",
      "ep 3467: ep_len:500 episode reward: total was 26.180000. running mean: -5.095324\n",
      "ep 3467: ep_len:650 episode reward: total was 2.390000. running mean: -5.020470\n",
      "ep 3467: ep_len:630 episode reward: total was -36.930000. running mean: -5.339566\n",
      "ep 3467: ep_len:69 episode reward: total was -13.000000. running mean: -5.416170\n",
      "ep 3467: ep_len:500 episode reward: total was -22.460000. running mean: -5.586608\n",
      "ep 3467: ep_len:605 episode reward: total was -17.950000. running mean: -5.710242\n",
      "epsilon:0.010000 episode_count: 24276. steps_count: 10725105.000000\n",
      "ep 3468: ep_len:212 episode reward: total was 3.600000. running mean: -5.617140\n",
      "ep 3468: ep_len:500 episode reward: total was -16.530000. running mean: -5.726268\n",
      "ep 3468: ep_len:377 episode reward: total was 1.200000. running mean: -5.657006\n",
      "ep 3468: ep_len:500 episode reward: total was 13.570000. running mean: -5.464736\n",
      "ep 3468: ep_len:3 episode reward: total was 0.000000. running mean: -5.410088\n",
      "ep 3468: ep_len:321 episode reward: total was 3.680000. running mean: -5.319187\n",
      "ep 3468: ep_len:540 episode reward: total was -5.580000. running mean: -5.321795\n",
      "epsilon:0.010000 episode_count: 24283. steps_count: 10727558.000000\n",
      "ep 3469: ep_len:570 episode reward: total was -14.220000. running mean: -5.410778\n",
      "ep 3469: ep_len:182 episode reward: total was -10.880000. running mean: -5.465470\n",
      "ep 3469: ep_len:660 episode reward: total was -16.140000. running mean: -5.572215\n",
      "ep 3469: ep_len:505 episode reward: total was 10.550000. running mean: -5.410993\n",
      "ep 3469: ep_len:3 episode reward: total was 0.000000. running mean: -5.356883\n",
      "ep 3469: ep_len:500 episode reward: total was -2.740000. running mean: -5.330714\n",
      "ep 3469: ep_len:500 episode reward: total was -18.690000. running mean: -5.464307\n",
      "epsilon:0.010000 episode_count: 24290. steps_count: 10730478.000000\n",
      "ep 3470: ep_len:510 episode reward: total was -9.870000. running mean: -5.508364\n",
      "ep 3470: ep_len:632 episode reward: total was -61.020000. running mean: -6.063480\n",
      "ep 3470: ep_len:575 episode reward: total was -7.660000. running mean: -6.079446\n",
      "ep 3470: ep_len:530 episode reward: total was -7.230000. running mean: -6.090951\n",
      "ep 3470: ep_len:3 episode reward: total was 0.000000. running mean: -6.030042\n",
      "ep 3470: ep_len:520 episode reward: total was -5.050000. running mean: -6.020241\n",
      "ep 3470: ep_len:332 episode reward: total was -0.270000. running mean: -5.962739\n",
      "epsilon:0.010000 episode_count: 24297. steps_count: 10733580.000000\n",
      "ep 3471: ep_len:510 episode reward: total was 14.940000. running mean: -5.753711\n",
      "ep 3471: ep_len:550 episode reward: total was 21.910000. running mean: -5.477074\n",
      "ep 3471: ep_len:605 episode reward: total was -13.510000. running mean: -5.557403\n",
      "ep 3471: ep_len:740 episode reward: total was -71.960000. running mean: -6.221429\n",
      "ep 3471: ep_len:86 episode reward: total was 4.530000. running mean: -6.113915\n",
      "ep 3471: ep_len:500 episode reward: total was -9.430000. running mean: -6.147076\n",
      "ep 3471: ep_len:565 episode reward: total was -11.080000. running mean: -6.196405\n",
      "epsilon:0.010000 episode_count: 24304. steps_count: 10737136.000000\n",
      "ep 3472: ep_len:500 episode reward: total was 10.880000. running mean: -6.025641\n",
      "ep 3472: ep_len:535 episode reward: total was -1.860000. running mean: -5.983985\n",
      "ep 3472: ep_len:585 episode reward: total was -5.150000. running mean: -5.975645\n",
      "ep 3472: ep_len:825 episode reward: total was -55.920000. running mean: -6.475088\n",
      "ep 3472: ep_len:3 episode reward: total was 0.000000. running mean: -6.410338\n",
      "ep 3472: ep_len:615 episode reward: total was 5.490000. running mean: -6.291334\n",
      "ep 3472: ep_len:550 episode reward: total was -36.790000. running mean: -6.596321\n",
      "epsilon:0.010000 episode_count: 24311. steps_count: 10740749.000000\n",
      "ep 3473: ep_len:675 episode reward: total was -21.160000. running mean: -6.741958\n",
      "ep 3473: ep_len:540 episode reward: total was -43.230000. running mean: -7.106838\n",
      "ep 3473: ep_len:670 episode reward: total was -1.580000. running mean: -7.051570\n",
      "ep 3473: ep_len:385 episode reward: total was 5.850000. running mean: -6.922554\n",
      "ep 3473: ep_len:93 episode reward: total was -10.470000. running mean: -6.958028\n",
      "ep 3473: ep_len:545 episode reward: total was -10.430000. running mean: -6.992748\n",
      "ep 3473: ep_len:500 episode reward: total was -21.920000. running mean: -7.142021\n",
      "epsilon:0.010000 episode_count: 24318. steps_count: 10744157.000000\n",
      "ep 3474: ep_len:630 episode reward: total was -8.640000. running mean: -7.157000\n",
      "ep 3474: ep_len:201 episode reward: total was -4.330000. running mean: -7.128730\n",
      "ep 3474: ep_len:535 episode reward: total was -33.430000. running mean: -7.391743\n",
      "ep 3474: ep_len:570 episode reward: total was 15.450000. running mean: -7.163326\n",
      "ep 3474: ep_len:3 episode reward: total was 0.000000. running mean: -7.091692\n",
      "ep 3474: ep_len:580 episode reward: total was 3.920000. running mean: -6.981576\n",
      "ep 3474: ep_len:500 episode reward: total was -9.880000. running mean: -7.010560\n",
      "epsilon:0.010000 episode_count: 24325. steps_count: 10747176.000000\n",
      "ep 3475: ep_len:570 episode reward: total was -13.670000. running mean: -7.077154\n",
      "ep 3475: ep_len:500 episode reward: total was -24.010000. running mean: -7.246483\n",
      "ep 3475: ep_len:575 episode reward: total was -12.790000. running mean: -7.301918\n",
      "ep 3475: ep_len:500 episode reward: total was -37.070000. running mean: -7.599599\n",
      "ep 3475: ep_len:96 episode reward: total was 7.030000. running mean: -7.453303\n",
      "ep 3475: ep_len:510 episode reward: total was 6.890000. running mean: -7.309870\n",
      "ep 3475: ep_len:349 episode reward: total was -1.250000. running mean: -7.249271\n",
      "epsilon:0.010000 episode_count: 24332. steps_count: 10750276.000000\n",
      "ep 3476: ep_len:500 episode reward: total was -13.900000. running mean: -7.315778\n",
      "ep 3476: ep_len:565 episode reward: total was -9.990000. running mean: -7.342520\n",
      "ep 3476: ep_len:550 episode reward: total was -3.670000. running mean: -7.305795\n",
      "ep 3476: ep_len:530 episode reward: total was 11.090000. running mean: -7.121837\n",
      "ep 3476: ep_len:3 episode reward: total was 0.000000. running mean: -7.050619\n",
      "ep 3476: ep_len:500 episode reward: total was -45.690000. running mean: -7.437013\n",
      "ep 3476: ep_len:530 episode reward: total was -32.160000. running mean: -7.684243\n",
      "epsilon:0.010000 episode_count: 24339. steps_count: 10753454.000000\n",
      "ep 3477: ep_len:780 episode reward: total was -60.200000. running mean: -8.209400\n",
      "ep 3477: ep_len:170 episode reward: total was -6.890000. running mean: -8.196206\n",
      "ep 3477: ep_len:645 episode reward: total was -3.120000. running mean: -8.145444\n",
      "ep 3477: ep_len:515 episode reward: total was -26.100000. running mean: -8.324990\n",
      "ep 3477: ep_len:118 episode reward: total was 6.060000. running mean: -8.181140\n",
      "ep 3477: ep_len:500 episode reward: total was -52.060000. running mean: -8.619928\n",
      "ep 3477: ep_len:590 episode reward: total was -7.980000. running mean: -8.613529\n",
      "epsilon:0.010000 episode_count: 24346. steps_count: 10756772.000000\n",
      "ep 3478: ep_len:173 episode reward: total was 0.580000. running mean: -8.521594\n",
      "ep 3478: ep_len:590 episode reward: total was 10.590000. running mean: -8.330478\n",
      "ep 3478: ep_len:59 episode reward: total was 0.550000. running mean: -8.241673\n",
      "ep 3478: ep_len:565 episode reward: total was 8.950000. running mean: -8.069756\n",
      "ep 3478: ep_len:3 episode reward: total was 0.000000. running mean: -7.989059\n",
      "ep 3478: ep_len:505 episode reward: total was -19.860000. running mean: -8.107768\n",
      "ep 3478: ep_len:825 episode reward: total was -44.180000. running mean: -8.468491\n",
      "epsilon:0.010000 episode_count: 24353. steps_count: 10759492.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3479: ep_len:263 episode reward: total was -3.890000. running mean: -8.422706\n",
      "ep 3479: ep_len:560 episode reward: total was 19.970000. running mean: -8.138779\n",
      "ep 3479: ep_len:595 episode reward: total was -2.510000. running mean: -8.082491\n",
      "ep 3479: ep_len:540 episode reward: total was -47.840000. running mean: -8.480066\n",
      "ep 3479: ep_len:3 episode reward: total was 0.000000. running mean: -8.395265\n",
      "ep 3479: ep_len:186 episode reward: total was 8.130000. running mean: -8.230013\n",
      "ep 3479: ep_len:550 episode reward: total was -2.320000. running mean: -8.170912\n",
      "epsilon:0.010000 episode_count: 24360. steps_count: 10762189.000000\n",
      "ep 3480: ep_len:530 episode reward: total was 7.700000. running mean: -8.012203\n",
      "ep 3480: ep_len:500 episode reward: total was -21.710000. running mean: -8.149181\n",
      "ep 3480: ep_len:500 episode reward: total was 2.780000. running mean: -8.039889\n",
      "ep 3480: ep_len:505 episode reward: total was 1.070000. running mean: -7.948791\n",
      "ep 3480: ep_len:3 episode reward: total was 0.000000. running mean: -7.869303\n",
      "ep 3480: ep_len:765 episode reward: total was -25.200000. running mean: -8.042610\n",
      "ep 3480: ep_len:307 episode reward: total was 0.220000. running mean: -7.959984\n",
      "epsilon:0.010000 episode_count: 24367. steps_count: 10765299.000000\n",
      "ep 3481: ep_len:590 episode reward: total was -22.200000. running mean: -8.102384\n",
      "ep 3481: ep_len:500 episode reward: total was -22.970000. running mean: -8.251060\n",
      "ep 3481: ep_len:640 episode reward: total was -28.790000. running mean: -8.456449\n",
      "ep 3481: ep_len:420 episode reward: total was 2.370000. running mean: -8.348185\n",
      "ep 3481: ep_len:3 episode reward: total was 0.000000. running mean: -8.264703\n",
      "ep 3481: ep_len:655 episode reward: total was -34.310000. running mean: -8.525156\n",
      "ep 3481: ep_len:580 episode reward: total was -11.570000. running mean: -8.555604\n",
      "epsilon:0.010000 episode_count: 24374. steps_count: 10768687.000000\n",
      "ep 3482: ep_len:585 episode reward: total was -39.610000. running mean: -8.866148\n",
      "ep 3482: ep_len:560 episode reward: total was -2.470000. running mean: -8.802187\n",
      "ep 3482: ep_len:565 episode reward: total was -7.190000. running mean: -8.786065\n",
      "ep 3482: ep_len:625 episode reward: total was 5.580000. running mean: -8.642404\n",
      "ep 3482: ep_len:3 episode reward: total was 0.000000. running mean: -8.555980\n",
      "ep 3482: ep_len:540 episode reward: total was -2.820000. running mean: -8.498620\n",
      "ep 3482: ep_len:198 episode reward: total was -4.330000. running mean: -8.456934\n",
      "epsilon:0.010000 episode_count: 24381. steps_count: 10771763.000000\n",
      "ep 3483: ep_len:600 episode reward: total was 4.130000. running mean: -8.331065\n",
      "ep 3483: ep_len:570 episode reward: total was 4.730000. running mean: -8.200454\n",
      "ep 3483: ep_len:640 episode reward: total was -11.680000. running mean: -8.235250\n",
      "ep 3483: ep_len:515 episode reward: total was 2.060000. running mean: -8.132297\n",
      "ep 3483: ep_len:3 episode reward: total was 0.000000. running mean: -8.050974\n",
      "ep 3483: ep_len:720 episode reward: total was 0.880000. running mean: -7.961665\n",
      "ep 3483: ep_len:580 episode reward: total was 2.020000. running mean: -7.861848\n",
      "epsilon:0.010000 episode_count: 24388. steps_count: 10775391.000000\n",
      "ep 3484: ep_len:620 episode reward: total was -7.680000. running mean: -7.860029\n",
      "ep 3484: ep_len:1110 episode reward: total was -154.610000. running mean: -9.327529\n",
      "ep 3484: ep_len:645 episode reward: total was 5.410000. running mean: -9.180154\n",
      "ep 3484: ep_len:525 episode reward: total was -11.040000. running mean: -9.198752\n",
      "ep 3484: ep_len:85 episode reward: total was -10.940000. running mean: -9.216165\n",
      "ep 3484: ep_len:505 episode reward: total was 7.610000. running mean: -9.047903\n",
      "ep 3484: ep_len:500 episode reward: total was 4.380000. running mean: -8.913624\n",
      "epsilon:0.010000 episode_count: 24395. steps_count: 10779381.000000\n",
      "ep 3485: ep_len:134 episode reward: total was 2.570000. running mean: -8.798788\n",
      "ep 3485: ep_len:775 episode reward: total was -34.270000. running mean: -9.053500\n",
      "ep 3485: ep_len:610 episode reward: total was 0.990000. running mean: -8.953065\n",
      "ep 3485: ep_len:500 episode reward: total was -7.000000. running mean: -8.933534\n",
      "ep 3485: ep_len:3 episode reward: total was 0.000000. running mean: -8.844199\n",
      "ep 3485: ep_len:500 episode reward: total was 2.280000. running mean: -8.732957\n",
      "ep 3485: ep_len:610 episode reward: total was -12.900000. running mean: -8.774627\n",
      "epsilon:0.010000 episode_count: 24402. steps_count: 10782513.000000\n",
      "ep 3486: ep_len:545 episode reward: total was -22.320000. running mean: -8.910081\n",
      "ep 3486: ep_len:635 episode reward: total was -9.120000. running mean: -8.912180\n",
      "ep 3486: ep_len:540 episode reward: total was 1.820000. running mean: -8.804859\n",
      "ep 3486: ep_len:145 episode reward: total was -0.410000. running mean: -8.720910\n",
      "ep 3486: ep_len:112 episode reward: total was 7.060000. running mean: -8.563101\n",
      "ep 3486: ep_len:600 episode reward: total was 4.030000. running mean: -8.437170\n",
      "ep 3486: ep_len:630 episode reward: total was -1.880000. running mean: -8.371598\n",
      "epsilon:0.010000 episode_count: 24409. steps_count: 10785720.000000\n",
      "ep 3487: ep_len:262 episode reward: total was 4.130000. running mean: -8.246582\n",
      "ep 3487: ep_len:585 episode reward: total was -4.270000. running mean: -8.206816\n",
      "ep 3487: ep_len:690 episode reward: total was -45.290000. running mean: -8.577648\n",
      "ep 3487: ep_len:515 episode reward: total was 10.400000. running mean: -8.387872\n",
      "ep 3487: ep_len:89 episode reward: total was -11.950000. running mean: -8.423493\n",
      "ep 3487: ep_len:530 episode reward: total was -23.850000. running mean: -8.577758\n",
      "ep 3487: ep_len:795 episode reward: total was -41.240000. running mean: -8.904380\n",
      "epsilon:0.010000 episode_count: 24416. steps_count: 10789186.000000\n",
      "ep 3488: ep_len:575 episode reward: total was 6.000000. running mean: -8.755337\n",
      "ep 3488: ep_len:345 episode reward: total was -8.840000. running mean: -8.756183\n",
      "ep 3488: ep_len:650 episode reward: total was -0.670000. running mean: -8.675321\n",
      "ep 3488: ep_len:170 episode reward: total was 5.160000. running mean: -8.536968\n",
      "ep 3488: ep_len:3 episode reward: total was 0.000000. running mean: -8.451599\n",
      "ep 3488: ep_len:530 episode reward: total was 1.420000. running mean: -8.352883\n",
      "ep 3488: ep_len:825 episode reward: total was -45.220000. running mean: -8.721554\n",
      "epsilon:0.010000 episode_count: 24423. steps_count: 10792284.000000\n",
      "ep 3489: ep_len:625 episode reward: total was -24.810000. running mean: -8.882438\n",
      "ep 3489: ep_len:575 episode reward: total was 18.930000. running mean: -8.604314\n",
      "ep 3489: ep_len:585 episode reward: total was -2.520000. running mean: -8.543471\n",
      "ep 3489: ep_len:56 episode reward: total was 2.570000. running mean: -8.432336\n",
      "ep 3489: ep_len:89 episode reward: total was -10.940000. running mean: -8.457413\n",
      "ep 3489: ep_len:555 episode reward: total was -30.940000. running mean: -8.682238\n",
      "ep 3489: ep_len:339 episode reward: total was -2.250000. running mean: -8.617916\n",
      "epsilon:0.010000 episode_count: 24430. steps_count: 10795108.000000\n",
      "ep 3490: ep_len:555 episode reward: total was 4.130000. running mean: -8.490437\n",
      "ep 3490: ep_len:500 episode reward: total was 3.040000. running mean: -8.375133\n",
      "ep 3490: ep_len:570 episode reward: total was 1.600000. running mean: -8.275381\n",
      "ep 3490: ep_len:575 episode reward: total was -49.760000. running mean: -8.690227\n",
      "ep 3490: ep_len:91 episode reward: total was -4.970000. running mean: -8.653025\n",
      "ep 3490: ep_len:640 episode reward: total was 0.840000. running mean: -8.558095\n",
      "ep 3490: ep_len:500 episode reward: total was -9.480000. running mean: -8.567314\n",
      "epsilon:0.010000 episode_count: 24437. steps_count: 10798539.000000\n",
      "ep 3491: ep_len:650 episode reward: total was -14.690000. running mean: -8.628541\n",
      "ep 3491: ep_len:500 episode reward: total was 1.650000. running mean: -8.525755\n",
      "ep 3491: ep_len:61 episode reward: total was 4.540000. running mean: -8.395098\n",
      "ep 3491: ep_len:374 episode reward: total was 6.360000. running mean: -8.247547\n",
      "ep 3491: ep_len:3 episode reward: total was 0.000000. running mean: -8.165071\n",
      "ep 3491: ep_len:500 episode reward: total was 2.250000. running mean: -8.060921\n",
      "ep 3491: ep_len:520 episode reward: total was -7.800000. running mean: -8.058311\n",
      "epsilon:0.010000 episode_count: 24444. steps_count: 10801147.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3492: ep_len:580 episode reward: total was 2.160000. running mean: -7.956128\n",
      "ep 3492: ep_len:575 episode reward: total was 8.730000. running mean: -7.789267\n",
      "ep 3492: ep_len:555 episode reward: total was 1.160000. running mean: -7.699774\n",
      "ep 3492: ep_len:500 episode reward: total was -4.000000. running mean: -7.662777\n",
      "ep 3492: ep_len:3 episode reward: total was 0.000000. running mean: -7.586149\n",
      "ep 3492: ep_len:545 episode reward: total was -6.450000. running mean: -7.574787\n",
      "ep 3492: ep_len:580 episode reward: total was -2.890000. running mean: -7.527940\n",
      "epsilon:0.010000 episode_count: 24451. steps_count: 10804485.000000\n",
      "ep 3493: ep_len:550 episode reward: total was -23.520000. running mean: -7.687860\n",
      "ep 3493: ep_len:500 episode reward: total was -28.400000. running mean: -7.894982\n",
      "ep 3493: ep_len:580 episode reward: total was -1.000000. running mean: -7.826032\n",
      "ep 3493: ep_len:750 episode reward: total was -46.690000. running mean: -8.214671\n",
      "ep 3493: ep_len:100 episode reward: total was 5.540000. running mean: -8.077125\n",
      "ep 3493: ep_len:230 episode reward: total was 3.150000. running mean: -7.964853\n",
      "ep 3493: ep_len:200 episode reward: total was -0.830000. running mean: -7.893505\n",
      "epsilon:0.010000 episode_count: 24458. steps_count: 10807395.000000\n",
      "ep 3494: ep_len:206 episode reward: total was 2.610000. running mean: -7.788470\n",
      "ep 3494: ep_len:535 episode reward: total was -34.830000. running mean: -8.058885\n",
      "ep 3494: ep_len:431 episode reward: total was -4.290000. running mean: -8.021196\n",
      "ep 3494: ep_len:515 episode reward: total was -7.030000. running mean: -8.011284\n",
      "ep 3494: ep_len:3 episode reward: total was 0.000000. running mean: -7.931172\n",
      "ep 3494: ep_len:685 episode reward: total was -8.590000. running mean: -7.937760\n",
      "ep 3494: ep_len:307 episode reward: total was -2.260000. running mean: -7.880982\n",
      "epsilon:0.010000 episode_count: 24465. steps_count: 10810077.000000\n",
      "ep 3495: ep_len:500 episode reward: total was -6.700000. running mean: -7.869172\n",
      "ep 3495: ep_len:660 episode reward: total was -28.820000. running mean: -8.078681\n",
      "ep 3495: ep_len:500 episode reward: total was -0.970000. running mean: -8.007594\n",
      "ep 3495: ep_len:111 episode reward: total was 3.080000. running mean: -7.896718\n",
      "ep 3495: ep_len:3 episode reward: total was 0.000000. running mean: -7.817751\n",
      "ep 3495: ep_len:630 episode reward: total was -13.290000. running mean: -7.872473\n",
      "ep 3495: ep_len:295 episode reward: total was -7.300000. running mean: -7.866748\n",
      "epsilon:0.010000 episode_count: 24472. steps_count: 10812776.000000\n",
      "ep 3496: ep_len:249 episode reward: total was 8.170000. running mean: -7.706381\n",
      "ep 3496: ep_len:179 episode reward: total was -2.900000. running mean: -7.658317\n",
      "ep 3496: ep_len:575 episode reward: total was 1.890000. running mean: -7.562834\n",
      "ep 3496: ep_len:500 episode reward: total was 7.920000. running mean: -7.408006\n",
      "ep 3496: ep_len:38 episode reward: total was -2.500000. running mean: -7.358926\n",
      "ep 3496: ep_len:500 episode reward: total was 5.280000. running mean: -7.232536\n",
      "ep 3496: ep_len:595 episode reward: total was 2.270000. running mean: -7.137511\n",
      "epsilon:0.010000 episode_count: 24479. steps_count: 10815412.000000\n",
      "ep 3497: ep_len:505 episode reward: total was 2.100000. running mean: -7.045136\n",
      "ep 3497: ep_len:580 episode reward: total was -1.380000. running mean: -6.988485\n",
      "ep 3497: ep_len:570 episode reward: total was -7.750000. running mean: -6.996100\n",
      "ep 3497: ep_len:500 episode reward: total was 11.490000. running mean: -6.811239\n",
      "ep 3497: ep_len:96 episode reward: total was 5.040000. running mean: -6.692726\n",
      "ep 3497: ep_len:620 episode reward: total was -29.020000. running mean: -6.915999\n",
      "ep 3497: ep_len:660 episode reward: total was -11.750000. running mean: -6.964339\n",
      "epsilon:0.010000 episode_count: 24486. steps_count: 10818943.000000\n",
      "ep 3498: ep_len:123 episode reward: total was 3.590000. running mean: -6.858796\n",
      "ep 3498: ep_len:500 episode reward: total was -8.200000. running mean: -6.872208\n",
      "ep 3498: ep_len:535 episode reward: total was -30.450000. running mean: -7.107986\n",
      "ep 3498: ep_len:565 episode reward: total was -5.930000. running mean: -7.096206\n",
      "ep 3498: ep_len:3 episode reward: total was 0.000000. running mean: -7.025244\n",
      "ep 3498: ep_len:510 episode reward: total was -5.420000. running mean: -7.009191\n",
      "ep 3498: ep_len:211 episode reward: total was 3.720000. running mean: -6.901899\n",
      "epsilon:0.010000 episode_count: 24493. steps_count: 10821390.000000\n",
      "ep 3499: ep_len:550 episode reward: total was 7.190000. running mean: -6.760980\n",
      "ep 3499: ep_len:500 episode reward: total was 14.890000. running mean: -6.544471\n",
      "ep 3499: ep_len:545 episode reward: total was -4.750000. running mean: -6.526526\n",
      "ep 3499: ep_len:56 episode reward: total was 2.570000. running mean: -6.435561\n",
      "ep 3499: ep_len:3 episode reward: total was 0.000000. running mean: -6.371205\n",
      "ep 3499: ep_len:585 episode reward: total was -5.030000. running mean: -6.357793\n",
      "ep 3499: ep_len:500 episode reward: total was -2.660000. running mean: -6.320815\n",
      "epsilon:0.010000 episode_count: 24500. steps_count: 10824129.000000\n",
      "ep 3500: ep_len:665 episode reward: total was -8.630000. running mean: -6.343907\n",
      "ep 3500: ep_len:525 episode reward: total was 2.080000. running mean: -6.259668\n",
      "ep 3500: ep_len:500 episode reward: total was 8.920000. running mean: -6.107871\n",
      "ep 3500: ep_len:121 episode reward: total was 3.100000. running mean: -6.015792\n",
      "ep 3500: ep_len:3 episode reward: total was 0.000000. running mean: -5.955634\n",
      "ep 3500: ep_len:234 episode reward: total was 7.650000. running mean: -5.819578\n",
      "ep 3500: ep_len:555 episode reward: total was -2.060000. running mean: -5.781982\n",
      "epsilon:0.010000 episode_count: 24507. steps_count: 10826732.000000\n",
      "ep 3501: ep_len:510 episode reward: total was 7.470000. running mean: -5.649463\n",
      "ep 3501: ep_len:500 episode reward: total was 19.720000. running mean: -5.395768\n",
      "ep 3501: ep_len:500 episode reward: total was -21.610000. running mean: -5.557910\n",
      "ep 3501: ep_len:167 episode reward: total was 3.130000. running mean: -5.471031\n",
      "ep 3501: ep_len:99 episode reward: total was 8.040000. running mean: -5.335921\n",
      "ep 3501: ep_len:680 episode reward: total was 5.420000. running mean: -5.228362\n",
      "ep 3501: ep_len:555 episode reward: total was -12.380000. running mean: -5.299878\n",
      "epsilon:0.010000 episode_count: 24514. steps_count: 10829743.000000\n",
      "ep 3502: ep_len:575 episode reward: total was 2.910000. running mean: -5.217779\n",
      "ep 3502: ep_len:630 episode reward: total was -30.840000. running mean: -5.474001\n",
      "ep 3502: ep_len:423 episode reward: total was 10.760000. running mean: -5.311661\n",
      "ep 3502: ep_len:510 episode reward: total was -2.010000. running mean: -5.278645\n",
      "ep 3502: ep_len:2 episode reward: total was 0.000000. running mean: -5.225858\n",
      "ep 3502: ep_len:625 episode reward: total was -57.240000. running mean: -5.746000\n",
      "ep 3502: ep_len:274 episode reward: total was -9.860000. running mean: -5.787140\n",
      "epsilon:0.010000 episode_count: 24521. steps_count: 10832782.000000\n",
      "ep 3503: ep_len:610 episode reward: total was 6.110000. running mean: -5.668168\n",
      "ep 3503: ep_len:580 episode reward: total was 10.490000. running mean: -5.506587\n",
      "ep 3503: ep_len:640 episode reward: total was -11.540000. running mean: -5.566921\n",
      "ep 3503: ep_len:55 episode reward: total was 1.070000. running mean: -5.500552\n",
      "ep 3503: ep_len:78 episode reward: total was 0.040000. running mean: -5.445146\n",
      "ep 3503: ep_len:565 episode reward: total was 8.110000. running mean: -5.309595\n",
      "ep 3503: ep_len:500 episode reward: total was -7.710000. running mean: -5.333599\n",
      "epsilon:0.010000 episode_count: 24528. steps_count: 10835810.000000\n",
      "ep 3504: ep_len:620 episode reward: total was -1.680000. running mean: -5.297063\n",
      "ep 3504: ep_len:359 episode reward: total was -8.360000. running mean: -5.327692\n",
      "ep 3504: ep_len:560 episode reward: total was -13.980000. running mean: -5.414215\n",
      "ep 3504: ep_len:389 episode reward: total was 2.850000. running mean: -5.331573\n",
      "ep 3504: ep_len:90 episode reward: total was 3.040000. running mean: -5.247857\n",
      "ep 3504: ep_len:510 episode reward: total was -3.260000. running mean: -5.227979\n",
      "ep 3504: ep_len:306 episode reward: total was 0.220000. running mean: -5.173499\n",
      "epsilon:0.010000 episode_count: 24535. steps_count: 10838644.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3505: ep_len:585 episode reward: total was 6.970000. running mean: -5.052064\n",
      "ep 3505: ep_len:685 episode reward: total was -22.330000. running mean: -5.224843\n",
      "ep 3505: ep_len:615 episode reward: total was 1.980000. running mean: -5.152795\n",
      "ep 3505: ep_len:500 episode reward: total was 6.020000. running mean: -5.041067\n",
      "ep 3505: ep_len:40 episode reward: total was -3.000000. running mean: -5.020656\n",
      "ep 3505: ep_len:660 episode reward: total was 2.840000. running mean: -4.942050\n",
      "ep 3505: ep_len:540 episode reward: total was -50.770000. running mean: -5.400329\n",
      "epsilon:0.010000 episode_count: 24542. steps_count: 10842269.000000\n",
      "ep 3506: ep_len:620 episode reward: total was -10.130000. running mean: -5.447626\n",
      "ep 3506: ep_len:505 episode reward: total was 13.340000. running mean: -5.259750\n",
      "ep 3506: ep_len:565 episode reward: total was -15.240000. running mean: -5.359552\n",
      "ep 3506: ep_len:500 episode reward: total was -14.670000. running mean: -5.452657\n",
      "ep 3506: ep_len:3 episode reward: total was 0.000000. running mean: -5.398130\n",
      "ep 3506: ep_len:585 episode reward: total was 2.410000. running mean: -5.320049\n",
      "ep 3506: ep_len:520 episode reward: total was -13.340000. running mean: -5.400248\n",
      "epsilon:0.010000 episode_count: 24549. steps_count: 10845567.000000\n",
      "ep 3507: ep_len:565 episode reward: total was -2.980000. running mean: -5.376046\n",
      "ep 3507: ep_len:500 episode reward: total was 1.040000. running mean: -5.311885\n",
      "ep 3507: ep_len:500 episode reward: total was -0.450000. running mean: -5.263266\n",
      "ep 3507: ep_len:515 episode reward: total was 3.360000. running mean: -5.177034\n",
      "ep 3507: ep_len:89 episode reward: total was 2.020000. running mean: -5.105063\n",
      "ep 3507: ep_len:580 episode reward: total was 3.460000. running mean: -5.019413\n",
      "ep 3507: ep_len:211 episode reward: total was -1.330000. running mean: -4.982519\n",
      "epsilon:0.010000 episode_count: 24556. steps_count: 10848527.000000\n",
      "ep 3508: ep_len:535 episode reward: total was -14.310000. running mean: -5.075794\n",
      "ep 3508: ep_len:510 episode reward: total was 9.750000. running mean: -4.927536\n",
      "ep 3508: ep_len:500 episode reward: total was 10.770000. running mean: -4.770560\n",
      "ep 3508: ep_len:500 episode reward: total was -3.540000. running mean: -4.758255\n",
      "ep 3508: ep_len:3 episode reward: total was 0.000000. running mean: -4.710672\n",
      "ep 3508: ep_len:625 episode reward: total was -4.160000. running mean: -4.705165\n",
      "ep 3508: ep_len:281 episode reward: total was 4.760000. running mean: -4.610514\n",
      "epsilon:0.010000 episode_count: 24563. steps_count: 10851481.000000\n",
      "ep 3509: ep_len:500 episode reward: total was 7.450000. running mean: -4.489909\n",
      "ep 3509: ep_len:500 episode reward: total was 2.660000. running mean: -4.418409\n",
      "ep 3509: ep_len:585 episode reward: total was -22.750000. running mean: -4.601725\n",
      "ep 3509: ep_len:500 episode reward: total was 0.030000. running mean: -4.555408\n",
      "ep 3509: ep_len:3 episode reward: total was 0.000000. running mean: -4.509854\n",
      "ep 3509: ep_len:500 episode reward: total was 1.220000. running mean: -4.452555\n",
      "ep 3509: ep_len:500 episode reward: total was -10.490000. running mean: -4.512930\n",
      "epsilon:0.010000 episode_count: 24570. steps_count: 10854569.000000\n",
      "ep 3510: ep_len:118 episode reward: total was 1.070000. running mean: -4.457101\n",
      "ep 3510: ep_len:580 episode reward: total was -10.470000. running mean: -4.517230\n",
      "ep 3510: ep_len:710 episode reward: total was 7.930000. running mean: -4.392757\n",
      "ep 3510: ep_len:530 episode reward: total was 9.450000. running mean: -4.254330\n",
      "ep 3510: ep_len:3 episode reward: total was 0.000000. running mean: -4.211786\n",
      "ep 3510: ep_len:580 episode reward: total was 0.340000. running mean: -4.166269\n",
      "ep 3510: ep_len:335 episode reward: total was -5.800000. running mean: -4.182606\n",
      "epsilon:0.010000 episode_count: 24577. steps_count: 10857425.000000\n",
      "ep 3511: ep_len:530 episode reward: total was 3.880000. running mean: -4.101980\n",
      "ep 3511: ep_len:605 episode reward: total was -0.040000. running mean: -4.061360\n",
      "ep 3511: ep_len:615 episode reward: total was 8.010000. running mean: -3.940646\n",
      "ep 3511: ep_len:56 episode reward: total was 1.560000. running mean: -3.885640\n",
      "ep 3511: ep_len:3 episode reward: total was 0.000000. running mean: -3.846784\n",
      "ep 3511: ep_len:585 episode reward: total was -1.640000. running mean: -3.824716\n",
      "ep 3511: ep_len:715 episode reward: total was -40.550000. running mean: -4.191969\n",
      "epsilon:0.010000 episode_count: 24584. steps_count: 10860534.000000\n",
      "ep 3512: ep_len:505 episode reward: total was -17.440000. running mean: -4.324449\n",
      "ep 3512: ep_len:585 episode reward: total was 9.110000. running mean: -4.190104\n",
      "ep 3512: ep_len:570 episode reward: total was -4.720000. running mean: -4.195403\n",
      "ep 3512: ep_len:520 episode reward: total was -30.070000. running mean: -4.454149\n",
      "ep 3512: ep_len:53 episode reward: total was 5.000000. running mean: -4.359608\n",
      "ep 3512: ep_len:625 episode reward: total was -34.420000. running mean: -4.660212\n",
      "ep 3512: ep_len:200 episode reward: total was -0.830000. running mean: -4.621910\n",
      "epsilon:0.010000 episode_count: 24591. steps_count: 10863592.000000\n",
      "ep 3513: ep_len:530 episode reward: total was -2.590000. running mean: -4.601591\n",
      "ep 3513: ep_len:575 episode reward: total was -36.070000. running mean: -4.916275\n",
      "ep 3513: ep_len:530 episode reward: total was -2.810000. running mean: -4.895212\n",
      "ep 3513: ep_len:510 episode reward: total was 0.970000. running mean: -4.836560\n",
      "ep 3513: ep_len:3 episode reward: total was 0.000000. running mean: -4.788194\n",
      "ep 3513: ep_len:590 episode reward: total was -28.920000. running mean: -5.029512\n",
      "ep 3513: ep_len:204 episode reward: total was -2.850000. running mean: -5.007717\n",
      "epsilon:0.010000 episode_count: 24598. steps_count: 10866534.000000\n",
      "ep 3514: ep_len:500 episode reward: total was -16.880000. running mean: -5.126440\n",
      "ep 3514: ep_len:565 episode reward: total was -5.910000. running mean: -5.134276\n",
      "ep 3514: ep_len:535 episode reward: total was -4.100000. running mean: -5.123933\n",
      "ep 3514: ep_len:500 episode reward: total was -16.670000. running mean: -5.239393\n",
      "ep 3514: ep_len:37 episode reward: total was -2.500000. running mean: -5.212000\n",
      "ep 3514: ep_len:500 episode reward: total was -2.910000. running mean: -5.188980\n",
      "ep 3514: ep_len:540 episode reward: total was -4.980000. running mean: -5.186890\n",
      "epsilon:0.010000 episode_count: 24605. steps_count: 10869711.000000\n",
      "ep 3515: ep_len:690 episode reward: total was -4.110000. running mean: -5.176121\n",
      "ep 3515: ep_len:500 episode reward: total was -2.360000. running mean: -5.147960\n",
      "ep 3515: ep_len:500 episode reward: total was -19.000000. running mean: -5.286480\n",
      "ep 3515: ep_len:595 episode reward: total was 9.410000. running mean: -5.139515\n",
      "ep 3515: ep_len:3 episode reward: total was 0.000000. running mean: -5.088120\n",
      "ep 3515: ep_len:545 episode reward: total was -12.040000. running mean: -5.157639\n",
      "ep 3515: ep_len:640 episode reward: total was -11.400000. running mean: -5.220063\n",
      "epsilon:0.010000 episode_count: 24612. steps_count: 10873184.000000\n",
      "ep 3516: ep_len:239 episode reward: total was -10.860000. running mean: -5.276462\n",
      "ep 3516: ep_len:650 episode reward: total was -30.340000. running mean: -5.527097\n",
      "ep 3516: ep_len:500 episode reward: total was 6.050000. running mean: -5.411326\n",
      "ep 3516: ep_len:145 episode reward: total was 3.610000. running mean: -5.321113\n",
      "ep 3516: ep_len:3 episode reward: total was 0.000000. running mean: -5.267902\n",
      "ep 3516: ep_len:575 episode reward: total was -5.040000. running mean: -5.265623\n",
      "ep 3516: ep_len:565 episode reward: total was -16.530000. running mean: -5.378267\n",
      "epsilon:0.010000 episode_count: 24619. steps_count: 10875861.000000\n",
      "ep 3517: ep_len:655 episode reward: total was -3.170000. running mean: -5.356184\n",
      "ep 3517: ep_len:500 episode reward: total was -1.600000. running mean: -5.318622\n",
      "ep 3517: ep_len:625 episode reward: total was -12.430000. running mean: -5.389736\n",
      "ep 3517: ep_len:500 episode reward: total was 13.890000. running mean: -5.196939\n",
      "ep 3517: ep_len:3 episode reward: total was 0.000000. running mean: -5.144969\n",
      "ep 3517: ep_len:690 episode reward: total was 3.420000. running mean: -5.059319\n",
      "ep 3517: ep_len:525 episode reward: total was -16.530000. running mean: -5.174026\n",
      "epsilon:0.010000 episode_count: 24626. steps_count: 10879359.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3518: ep_len:660 episode reward: total was -4.690000. running mean: -5.169186\n",
      "ep 3518: ep_len:500 episode reward: total was 6.090000. running mean: -5.056594\n",
      "ep 3518: ep_len:565 episode reward: total was -2.690000. running mean: -5.032928\n",
      "ep 3518: ep_len:510 episode reward: total was -13.040000. running mean: -5.112999\n",
      "ep 3518: ep_len:3 episode reward: total was 0.000000. running mean: -5.061869\n",
      "ep 3518: ep_len:530 episode reward: total was -5.100000. running mean: -5.062250\n",
      "ep 3518: ep_len:595 episode reward: total was -16.970000. running mean: -5.181328\n",
      "epsilon:0.010000 episode_count: 24633. steps_count: 10882722.000000\n",
      "ep 3519: ep_len:540 episode reward: total was -5.190000. running mean: -5.181414\n",
      "ep 3519: ep_len:505 episode reward: total was -18.150000. running mean: -5.311100\n",
      "ep 3519: ep_len:555 episode reward: total was 0.820000. running mean: -5.249789\n",
      "ep 3519: ep_len:505 episode reward: total was -7.550000. running mean: -5.272791\n",
      "ep 3519: ep_len:3 episode reward: total was 0.000000. running mean: -5.220064\n",
      "ep 3519: ep_len:590 episode reward: total was -16.110000. running mean: -5.328963\n",
      "ep 3519: ep_len:600 episode reward: total was -1.030000. running mean: -5.285973\n",
      "epsilon:0.010000 episode_count: 24640. steps_count: 10886020.000000\n",
      "ep 3520: ep_len:500 episode reward: total was 9.390000. running mean: -5.139214\n",
      "ep 3520: ep_len:585 episode reward: total was 17.160000. running mean: -4.916221\n",
      "ep 3520: ep_len:660 episode reward: total was 6.230000. running mean: -4.804759\n",
      "ep 3520: ep_len:535 episode reward: total was -5.090000. running mean: -4.807612\n",
      "ep 3520: ep_len:3 episode reward: total was 0.000000. running mean: -4.759535\n",
      "ep 3520: ep_len:520 episode reward: total was -7.710000. running mean: -4.789040\n",
      "ep 3520: ep_len:500 episode reward: total was -15.240000. running mean: -4.893550\n",
      "epsilon:0.010000 episode_count: 24647. steps_count: 10889323.000000\n",
      "ep 3521: ep_len:535 episode reward: total was 11.960000. running mean: -4.725014\n",
      "ep 3521: ep_len:625 episode reward: total was 2.470000. running mean: -4.653064\n",
      "ep 3521: ep_len:650 episode reward: total was 0.140000. running mean: -4.605133\n",
      "ep 3521: ep_len:393 episode reward: total was 4.860000. running mean: -4.510482\n",
      "ep 3521: ep_len:106 episode reward: total was 7.540000. running mean: -4.389977\n",
      "ep 3521: ep_len:580 episode reward: total was -13.250000. running mean: -4.478578\n",
      "ep 3521: ep_len:525 episode reward: total was -22.390000. running mean: -4.657692\n",
      "epsilon:0.010000 episode_count: 24654. steps_count: 10892737.000000\n",
      "ep 3522: ep_len:213 episode reward: total was -2.390000. running mean: -4.635015\n",
      "ep 3522: ep_len:570 episode reward: total was -9.010000. running mean: -4.678765\n",
      "ep 3522: ep_len:540 episode reward: total was -0.220000. running mean: -4.634177\n",
      "ep 3522: ep_len:580 episode reward: total was 14.080000. running mean: -4.447035\n",
      "ep 3522: ep_len:3 episode reward: total was 0.000000. running mean: -4.402565\n",
      "ep 3522: ep_len:560 episode reward: total was -1.660000. running mean: -4.375139\n",
      "ep 3522: ep_len:580 episode reward: total was -6.050000. running mean: -4.391888\n",
      "epsilon:0.010000 episode_count: 24661. steps_count: 10895783.000000\n",
      "ep 3523: ep_len:585 episode reward: total was -0.560000. running mean: -4.353569\n",
      "ep 3523: ep_len:630 episode reward: total was 14.660000. running mean: -4.163433\n",
      "ep 3523: ep_len:610 episode reward: total was -28.730000. running mean: -4.409099\n",
      "ep 3523: ep_len:500 episode reward: total was 6.360000. running mean: -4.301408\n",
      "ep 3523: ep_len:1 episode reward: total was 0.000000. running mean: -4.258394\n",
      "ep 3523: ep_len:545 episode reward: total was -1.230000. running mean: -4.228110\n",
      "ep 3523: ep_len:530 episode reward: total was -19.930000. running mean: -4.385129\n",
      "epsilon:0.010000 episode_count: 24668. steps_count: 10899184.000000\n",
      "ep 3524: ep_len:615 episode reward: total was -14.990000. running mean: -4.491178\n",
      "ep 3524: ep_len:520 episode reward: total was -4.640000. running mean: -4.492666\n",
      "ep 3524: ep_len:373 episode reward: total was -8.780000. running mean: -4.535539\n",
      "ep 3524: ep_len:625 episode reward: total was 8.700000. running mean: -4.403184\n",
      "ep 3524: ep_len:3 episode reward: total was 0.000000. running mean: -4.359152\n",
      "ep 3524: ep_len:535 episode reward: total was -2.170000. running mean: -4.337260\n",
      "ep 3524: ep_len:620 episode reward: total was -12.040000. running mean: -4.414288\n",
      "epsilon:0.010000 episode_count: 24675. steps_count: 10902475.000000\n",
      "ep 3525: ep_len:500 episode reward: total was -10.320000. running mean: -4.473345\n",
      "ep 3525: ep_len:500 episode reward: total was 21.270000. running mean: -4.215911\n",
      "ep 3525: ep_len:645 episode reward: total was -4.860000. running mean: -4.222352\n",
      "ep 3525: ep_len:570 episode reward: total was -5.500000. running mean: -4.235129\n",
      "ep 3525: ep_len:3 episode reward: total was 0.000000. running mean: -4.192778\n",
      "ep 3525: ep_len:530 episode reward: total was -29.990000. running mean: -4.450750\n",
      "ep 3525: ep_len:590 episode reward: total was -15.940000. running mean: -4.565642\n",
      "epsilon:0.010000 episode_count: 24682. steps_count: 10905813.000000\n",
      "ep 3526: ep_len:650 episode reward: total was -37.810000. running mean: -4.898086\n",
      "ep 3526: ep_len:500 episode reward: total was 3.870000. running mean: -4.810405\n",
      "ep 3526: ep_len:525 episode reward: total was -11.240000. running mean: -4.874701\n",
      "ep 3526: ep_len:520 episode reward: total was -10.050000. running mean: -4.926454\n",
      "ep 3526: ep_len:3 episode reward: total was 0.000000. running mean: -4.877189\n",
      "ep 3526: ep_len:515 episode reward: total was 0.590000. running mean: -4.822517\n",
      "ep 3526: ep_len:540 episode reward: total was -32.160000. running mean: -5.095892\n",
      "epsilon:0.010000 episode_count: 24689. steps_count: 10909066.000000\n",
      "ep 3527: ep_len:585 episode reward: total was -8.180000. running mean: -5.126733\n",
      "ep 3527: ep_len:525 episode reward: total was 25.350000. running mean: -4.821966\n",
      "ep 3527: ep_len:710 episode reward: total was -47.200000. running mean: -5.245746\n",
      "ep 3527: ep_len:545 episode reward: total was -6.470000. running mean: -5.257989\n",
      "ep 3527: ep_len:94 episode reward: total was 4.540000. running mean: -5.160009\n",
      "ep 3527: ep_len:585 episode reward: total was 5.460000. running mean: -5.053809\n",
      "ep 3527: ep_len:540 episode reward: total was -4.570000. running mean: -5.048971\n",
      "epsilon:0.010000 episode_count: 24696. steps_count: 10912650.000000\n",
      "ep 3528: ep_len:565 episode reward: total was -3.510000. running mean: -5.033581\n",
      "ep 3528: ep_len:550 episode reward: total was -6.500000. running mean: -5.048245\n",
      "ep 3528: ep_len:525 episode reward: total was -4.270000. running mean: -5.040463\n",
      "ep 3528: ep_len:500 episode reward: total was -6.020000. running mean: -5.050258\n",
      "ep 3528: ep_len:113 episode reward: total was 7.550000. running mean: -4.924256\n",
      "ep 3528: ep_len:500 episode reward: total was -8.780000. running mean: -4.962813\n",
      "ep 3528: ep_len:545 episode reward: total was -39.030000. running mean: -5.303485\n",
      "epsilon:0.010000 episode_count: 24703. steps_count: 10915948.000000\n",
      "ep 3529: ep_len:560 episode reward: total was -9.670000. running mean: -5.347150\n",
      "ep 3529: ep_len:500 episode reward: total was 1.540000. running mean: -5.278279\n",
      "ep 3529: ep_len:505 episode reward: total was -36.430000. running mean: -5.589796\n",
      "ep 3529: ep_len:500 episode reward: total was 4.920000. running mean: -5.484698\n",
      "ep 3529: ep_len:111 episode reward: total was -13.950000. running mean: -5.569351\n",
      "ep 3529: ep_len:575 episode reward: total was -2.140000. running mean: -5.535057\n",
      "ep 3529: ep_len:510 episode reward: total was -2.220000. running mean: -5.501907\n",
      "epsilon:0.010000 episode_count: 24710. steps_count: 10919209.000000\n",
      "ep 3530: ep_len:123 episode reward: total was 0.070000. running mean: -5.446188\n",
      "ep 3530: ep_len:354 episode reward: total was -40.840000. running mean: -5.800126\n",
      "ep 3530: ep_len:500 episode reward: total was -0.250000. running mean: -5.744625\n",
      "ep 3530: ep_len:118 episode reward: total was 7.080000. running mean: -5.616378\n",
      "ep 3530: ep_len:120 episode reward: total was 6.560000. running mean: -5.494615\n",
      "ep 3530: ep_len:695 episode reward: total was 11.480000. running mean: -5.324868\n",
      "ep 3530: ep_len:655 episode reward: total was -2.680000. running mean: -5.298420\n",
      "epsilon:0.010000 episode_count: 24717. steps_count: 10921774.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3531: ep_len:600 episode reward: total was 7.640000. running mean: -5.169036\n",
      "ep 3531: ep_len:530 episode reward: total was 11.220000. running mean: -5.005145\n",
      "ep 3531: ep_len:545 episode reward: total was -3.100000. running mean: -4.986094\n",
      "ep 3531: ep_len:510 episode reward: total was 6.410000. running mean: -4.872133\n",
      "ep 3531: ep_len:3 episode reward: total was 0.000000. running mean: -4.823411\n",
      "ep 3531: ep_len:535 episode reward: total was -6.240000. running mean: -4.837577\n",
      "ep 3531: ep_len:685 episode reward: total was -8.680000. running mean: -4.876002\n",
      "epsilon:0.010000 episode_count: 24724. steps_count: 10925182.000000\n",
      "ep 3532: ep_len:540 episode reward: total was -25.660000. running mean: -5.083842\n",
      "ep 3532: ep_len:595 episode reward: total was -7.390000. running mean: -5.106903\n",
      "ep 3532: ep_len:590 episode reward: total was -18.790000. running mean: -5.243734\n",
      "ep 3532: ep_len:510 episode reward: total was -6.460000. running mean: -5.255897\n",
      "ep 3532: ep_len:3 episode reward: total was 0.000000. running mean: -5.203338\n",
      "ep 3532: ep_len:535 episode reward: total was -1.820000. running mean: -5.169504\n",
      "ep 3532: ep_len:595 episode reward: total was -3.500000. running mean: -5.152809\n",
      "epsilon:0.010000 episode_count: 24731. steps_count: 10928550.000000\n",
      "ep 3533: ep_len:685 episode reward: total was -3.600000. running mean: -5.137281\n",
      "ep 3533: ep_len:500 episode reward: total was 19.230000. running mean: -4.893608\n",
      "ep 3533: ep_len:389 episode reward: total was -2.310000. running mean: -4.867772\n",
      "ep 3533: ep_len:610 episode reward: total was -43.810000. running mean: -5.257195\n",
      "ep 3533: ep_len:52 episode reward: total was 3.500000. running mean: -5.169623\n",
      "ep 3533: ep_len:680 episode reward: total was -32.140000. running mean: -5.439327\n",
      "ep 3533: ep_len:192 episode reward: total was -0.360000. running mean: -5.388533\n",
      "epsilon:0.010000 episode_count: 24738. steps_count: 10931658.000000\n",
      "ep 3534: ep_len:580 episode reward: total was 7.470000. running mean: -5.259948\n",
      "ep 3534: ep_len:292 episode reward: total was -37.360000. running mean: -5.580948\n",
      "ep 3534: ep_len:449 episode reward: total was 2.730000. running mean: -5.497839\n",
      "ep 3534: ep_len:56 episode reward: total was 1.560000. running mean: -5.427261\n",
      "ep 3534: ep_len:3 episode reward: total was 0.000000. running mean: -5.372988\n",
      "ep 3534: ep_len:625 episode reward: total was 7.380000. running mean: -5.245458\n",
      "ep 3534: ep_len:535 episode reward: total was -5.260000. running mean: -5.245603\n",
      "epsilon:0.010000 episode_count: 24745. steps_count: 10934198.000000\n",
      "ep 3535: ep_len:560 episode reward: total was 11.530000. running mean: -5.077847\n",
      "ep 3535: ep_len:500 episode reward: total was -16.380000. running mean: -5.190869\n",
      "ep 3535: ep_len:570 episode reward: total was -5.210000. running mean: -5.191060\n",
      "ep 3535: ep_len:500 episode reward: total was 12.070000. running mean: -5.018450\n",
      "ep 3535: ep_len:3 episode reward: total was 0.000000. running mean: -4.968265\n",
      "ep 3535: ep_len:775 episode reward: total was -39.230000. running mean: -5.310883\n",
      "ep 3535: ep_len:660 episode reward: total was -33.800000. running mean: -5.595774\n",
      "epsilon:0.010000 episode_count: 24752. steps_count: 10937766.000000\n",
      "ep 3536: ep_len:620 episode reward: total was 0.720000. running mean: -5.532616\n",
      "ep 3536: ep_len:575 episode reward: total was -9.380000. running mean: -5.571090\n",
      "ep 3536: ep_len:600 episode reward: total was -2.980000. running mean: -5.545179\n",
      "ep 3536: ep_len:510 episode reward: total was 4.880000. running mean: -5.440927\n",
      "ep 3536: ep_len:89 episode reward: total was 4.040000. running mean: -5.346118\n",
      "ep 3536: ep_len:650 episode reward: total was -84.300000. running mean: -6.135657\n",
      "ep 3536: ep_len:600 episode reward: total was -1.500000. running mean: -6.089300\n",
      "epsilon:0.010000 episode_count: 24759. steps_count: 10941410.000000\n",
      "ep 3537: ep_len:540 episode reward: total was -7.790000. running mean: -6.106307\n",
      "ep 3537: ep_len:500 episode reward: total was 6.550000. running mean: -5.979744\n",
      "ep 3537: ep_len:505 episode reward: total was -1.050000. running mean: -5.930447\n",
      "ep 3537: ep_len:580 episode reward: total was 7.480000. running mean: -5.796342\n",
      "ep 3537: ep_len:3 episode reward: total was 0.000000. running mean: -5.738379\n",
      "ep 3537: ep_len:500 episode reward: total was 2.140000. running mean: -5.659595\n",
      "ep 3537: ep_len:535 episode reward: total was -2.200000. running mean: -5.624999\n",
      "epsilon:0.010000 episode_count: 24766. steps_count: 10944573.000000\n",
      "ep 3538: ep_len:610 episode reward: total was -14.510000. running mean: -5.713849\n",
      "ep 3538: ep_len:625 episode reward: total was -29.430000. running mean: -5.951011\n",
      "ep 3538: ep_len:500 episode reward: total was 8.580000. running mean: -5.805700\n",
      "ep 3538: ep_len:510 episode reward: total was 14.050000. running mean: -5.607143\n",
      "ep 3538: ep_len:50 episode reward: total was 2.000000. running mean: -5.531072\n",
      "ep 3538: ep_len:525 episode reward: total was 7.130000. running mean: -5.404461\n",
      "ep 3538: ep_len:650 episode reward: total was -5.750000. running mean: -5.407917\n",
      "epsilon:0.010000 episode_count: 24773. steps_count: 10948043.000000\n",
      "ep 3539: ep_len:535 episode reward: total was 11.390000. running mean: -5.239937\n",
      "ep 3539: ep_len:500 episode reward: total was -12.490000. running mean: -5.312438\n",
      "ep 3539: ep_len:398 episode reward: total was 8.220000. running mean: -5.177114\n",
      "ep 3539: ep_len:535 episode reward: total was -11.490000. running mean: -5.240243\n",
      "ep 3539: ep_len:103 episode reward: total was 6.550000. running mean: -5.122340\n",
      "ep 3539: ep_len:500 episode reward: total was 6.240000. running mean: -5.008717\n",
      "ep 3539: ep_len:505 episode reward: total was -0.580000. running mean: -4.964430\n",
      "epsilon:0.010000 episode_count: 24780. steps_count: 10951119.000000\n",
      "ep 3540: ep_len:550 episode reward: total was 12.490000. running mean: -4.789885\n",
      "ep 3540: ep_len:505 episode reward: total was 8.690000. running mean: -4.655086\n",
      "ep 3540: ep_len:645 episode reward: total was 1.140000. running mean: -4.597136\n",
      "ep 3540: ep_len:425 episode reward: total was -33.630000. running mean: -4.887464\n",
      "ep 3540: ep_len:3 episode reward: total was 0.000000. running mean: -4.838590\n",
      "ep 3540: ep_len:164 episode reward: total was 3.120000. running mean: -4.759004\n",
      "ep 3540: ep_len:500 episode reward: total was -1.050000. running mean: -4.721914\n",
      "epsilon:0.010000 episode_count: 24787. steps_count: 10953911.000000\n",
      "ep 3541: ep_len:249 episode reward: total was 8.140000. running mean: -4.593294\n",
      "ep 3541: ep_len:292 episode reward: total was -29.850000. running mean: -4.845862\n",
      "ep 3541: ep_len:383 episode reward: total was 7.250000. running mean: -4.724903\n",
      "ep 3541: ep_len:500 episode reward: total was -6.480000. running mean: -4.742454\n",
      "ep 3541: ep_len:3 episode reward: total was 0.000000. running mean: -4.695029\n",
      "ep 3541: ep_len:670 episode reward: total was 11.950000. running mean: -4.528579\n",
      "ep 3541: ep_len:565 episode reward: total was -9.210000. running mean: -4.575393\n",
      "epsilon:0.010000 episode_count: 24794. steps_count: 10956573.000000\n",
      "ep 3542: ep_len:265 episode reward: total was 7.170000. running mean: -4.457939\n",
      "ep 3542: ep_len:505 episode reward: total was 16.370000. running mean: -4.249660\n",
      "ep 3542: ep_len:79 episode reward: total was 0.040000. running mean: -4.206763\n",
      "ep 3542: ep_len:126 episode reward: total was -1.890000. running mean: -4.183596\n",
      "ep 3542: ep_len:3 episode reward: total was 0.000000. running mean: -4.141760\n",
      "ep 3542: ep_len:500 episode reward: total was -2.960000. running mean: -4.129942\n",
      "ep 3542: ep_len:342 episode reward: total was -22.270000. running mean: -4.311343\n",
      "epsilon:0.010000 episode_count: 24801. steps_count: 10958393.000000\n",
      "ep 3543: ep_len:134 episode reward: total was 6.610000. running mean: -4.202129\n",
      "ep 3543: ep_len:500 episode reward: total was -6.430000. running mean: -4.224408\n",
      "ep 3543: ep_len:645 episode reward: total was -5.380000. running mean: -4.235964\n",
      "ep 3543: ep_len:595 episode reward: total was -2.900000. running mean: -4.222604\n",
      "ep 3543: ep_len:3 episode reward: total was 0.000000. running mean: -4.180378\n",
      "ep 3543: ep_len:555 episode reward: total was 5.400000. running mean: -4.084574\n",
      "ep 3543: ep_len:565 episode reward: total was -4.520000. running mean: -4.088929\n",
      "epsilon:0.010000 episode_count: 24808. steps_count: 10961390.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3544: ep_len:610 episode reward: total was -3.870000. running mean: -4.086739\n",
      "ep 3544: ep_len:287 episode reward: total was -36.850000. running mean: -4.414372\n",
      "ep 3544: ep_len:640 episode reward: total was -29.340000. running mean: -4.663628\n",
      "ep 3544: ep_len:155 episode reward: total was 5.130000. running mean: -4.565692\n",
      "ep 3544: ep_len:64 episode reward: total was -8.970000. running mean: -4.609735\n",
      "ep 3544: ep_len:500 episode reward: total was -0.230000. running mean: -4.565938\n",
      "ep 3544: ep_len:560 episode reward: total was -30.880000. running mean: -4.829078\n",
      "epsilon:0.010000 episode_count: 24815. steps_count: 10964206.000000\n",
      "ep 3545: ep_len:600 episode reward: total was -20.330000. running mean: -4.984088\n",
      "ep 3545: ep_len:500 episode reward: total was -10.830000. running mean: -5.042547\n",
      "ep 3545: ep_len:560 episode reward: total was -11.080000. running mean: -5.102921\n",
      "ep 3545: ep_len:50 episode reward: total was 0.070000. running mean: -5.051192\n",
      "ep 3545: ep_len:51 episode reward: total was -10.000000. running mean: -5.100680\n",
      "ep 3545: ep_len:505 episode reward: total was -0.900000. running mean: -5.058673\n",
      "ep 3545: ep_len:610 episode reward: total was -6.530000. running mean: -5.073387\n",
      "epsilon:0.010000 episode_count: 24822. steps_count: 10967082.000000\n",
      "ep 3546: ep_len:655 episode reward: total was -19.180000. running mean: -5.214453\n",
      "ep 3546: ep_len:172 episode reward: total was -3.860000. running mean: -5.200908\n",
      "ep 3546: ep_len:585 episode reward: total was -17.070000. running mean: -5.319599\n",
      "ep 3546: ep_len:540 episode reward: total was 16.460000. running mean: -5.101803\n",
      "ep 3546: ep_len:3 episode reward: total was 0.000000. running mean: -5.050785\n",
      "ep 3546: ep_len:545 episode reward: total was -1.720000. running mean: -5.017477\n",
      "ep 3546: ep_len:595 episode reward: total was -26.070000. running mean: -5.228002\n",
      "epsilon:0.010000 episode_count: 24829. steps_count: 10970177.000000\n",
      "ep 3547: ep_len:110 episode reward: total was 0.570000. running mean: -5.170022\n",
      "ep 3547: ep_len:500 episode reward: total was -3.520000. running mean: -5.153522\n",
      "ep 3547: ep_len:452 episode reward: total was 6.750000. running mean: -5.034487\n",
      "ep 3547: ep_len:535 episode reward: total was -14.430000. running mean: -5.128442\n",
      "ep 3547: ep_len:98 episode reward: total was 6.050000. running mean: -5.016658\n",
      "ep 3547: ep_len:500 episode reward: total was -15.740000. running mean: -5.123891\n",
      "ep 3547: ep_len:206 episode reward: total was -1.830000. running mean: -5.090952\n",
      "epsilon:0.010000 episode_count: 24836. steps_count: 10972578.000000\n",
      "ep 3548: ep_len:525 episode reward: total was 12.950000. running mean: -4.910543\n",
      "ep 3548: ep_len:540 episode reward: total was 22.870000. running mean: -4.632737\n",
      "ep 3548: ep_len:615 episode reward: total was 2.700000. running mean: -4.559410\n",
      "ep 3548: ep_len:505 episode reward: total was -9.530000. running mean: -4.609116\n",
      "ep 3548: ep_len:3 episode reward: total was 0.000000. running mean: -4.563025\n",
      "ep 3548: ep_len:500 episode reward: total was 11.090000. running mean: -4.406494\n",
      "ep 3548: ep_len:530 episode reward: total was -23.060000. running mean: -4.593029\n",
      "epsilon:0.010000 episode_count: 24843. steps_count: 10975796.000000\n",
      "ep 3549: ep_len:560 episode reward: total was -7.170000. running mean: -4.618799\n",
      "ep 3549: ep_len:540 episode reward: total was -19.010000. running mean: -4.762711\n",
      "ep 3549: ep_len:550 episode reward: total was 4.350000. running mean: -4.671584\n",
      "ep 3549: ep_len:494 episode reward: total was -21.950000. running mean: -4.844368\n",
      "ep 3549: ep_len:3 episode reward: total was 0.000000. running mean: -4.795925\n",
      "ep 3549: ep_len:244 episode reward: total was 3.630000. running mean: -4.711665\n",
      "ep 3549: ep_len:545 episode reward: total was -17.030000. running mean: -4.834849\n",
      "epsilon:0.010000 episode_count: 24850. steps_count: 10978732.000000\n",
      "ep 3550: ep_len:500 episode reward: total was 7.280000. running mean: -4.713700\n",
      "ep 3550: ep_len:538 episode reward: total was -39.980000. running mean: -5.066363\n",
      "ep 3550: ep_len:570 episode reward: total was -6.250000. running mean: -5.078200\n",
      "ep 3550: ep_len:580 episode reward: total was 10.510000. running mean: -4.922318\n",
      "ep 3550: ep_len:3 episode reward: total was 0.000000. running mean: -4.873094\n",
      "ep 3550: ep_len:500 episode reward: total was 6.260000. running mean: -4.761763\n",
      "ep 3550: ep_len:505 episode reward: total was -27.680000. running mean: -4.990946\n",
      "epsilon:0.010000 episode_count: 24857. steps_count: 10981928.000000\n",
      "ep 3551: ep_len:510 episode reward: total was -11.070000. running mean: -5.051736\n",
      "ep 3551: ep_len:500 episode reward: total was -9.390000. running mean: -5.095119\n",
      "ep 3551: ep_len:62 episode reward: total was 0.530000. running mean: -5.038868\n",
      "ep 3551: ep_len:426 episode reward: total was -25.600000. running mean: -5.244479\n",
      "ep 3551: ep_len:102 episode reward: total was 5.540000. running mean: -5.136634\n",
      "ep 3551: ep_len:650 episode reward: total was 4.210000. running mean: -5.043168\n",
      "ep 3551: ep_len:302 episode reward: total was -10.290000. running mean: -5.095636\n",
      "epsilon:0.010000 episode_count: 24864. steps_count: 10984480.000000\n",
      "ep 3552: ep_len:600 episode reward: total was 8.980000. running mean: -4.954880\n",
      "ep 3552: ep_len:193 episode reward: total was -6.390000. running mean: -4.969231\n",
      "ep 3552: ep_len:565 episode reward: total was 0.860000. running mean: -4.910939\n",
      "ep 3552: ep_len:545 episode reward: total was 9.480000. running mean: -4.767029\n",
      "ep 3552: ep_len:72 episode reward: total was 2.020000. running mean: -4.699159\n",
      "ep 3552: ep_len:540 episode reward: total was -9.880000. running mean: -4.750968\n",
      "ep 3552: ep_len:500 episode reward: total was -4.380000. running mean: -4.747258\n",
      "epsilon:0.010000 episode_count: 24871. steps_count: 10987495.000000\n",
      "ep 3553: ep_len:565 episode reward: total was 9.980000. running mean: -4.599985\n",
      "ep 3553: ep_len:184 episode reward: total was -2.370000. running mean: -4.577685\n",
      "ep 3553: ep_len:500 episode reward: total was -8.020000. running mean: -4.612109\n",
      "ep 3553: ep_len:570 episode reward: total was -2.480000. running mean: -4.590787\n",
      "ep 3553: ep_len:76 episode reward: total was 4.020000. running mean: -4.504680\n",
      "ep 3553: ep_len:695 episode reward: total was 1.470000. running mean: -4.444933\n",
      "ep 3553: ep_len:555 episode reward: total was -39.090000. running mean: -4.791383\n",
      "epsilon:0.010000 episode_count: 24878. steps_count: 10990640.000000\n",
      "ep 3554: ep_len:600 episode reward: total was 8.630000. running mean: -4.657170\n",
      "ep 3554: ep_len:510 episode reward: total was -14.400000. running mean: -4.754598\n",
      "ep 3554: ep_len:540 episode reward: total was 1.420000. running mean: -4.692852\n",
      "ep 3554: ep_len:500 episode reward: total was -8.040000. running mean: -4.726323\n",
      "ep 3554: ep_len:3 episode reward: total was 0.000000. running mean: -4.679060\n",
      "ep 3554: ep_len:500 episode reward: total was -32.280000. running mean: -4.955070\n",
      "ep 3554: ep_len:500 episode reward: total was 1.190000. running mean: -4.893619\n",
      "epsilon:0.010000 episode_count: 24885. steps_count: 10993793.000000\n",
      "ep 3555: ep_len:200 episode reward: total was -1.900000. running mean: -4.863683\n",
      "ep 3555: ep_len:510 episode reward: total was -12.430000. running mean: -4.939346\n",
      "ep 3555: ep_len:610 episode reward: total was -5.400000. running mean: -4.943952\n",
      "ep 3555: ep_len:500 episode reward: total was -24.690000. running mean: -5.141413\n",
      "ep 3555: ep_len:3 episode reward: total was 0.000000. running mean: -5.089999\n",
      "ep 3555: ep_len:500 episode reward: total was -14.280000. running mean: -5.181899\n",
      "ep 3555: ep_len:535 episode reward: total was -9.460000. running mean: -5.224680\n",
      "epsilon:0.010000 episode_count: 24892. steps_count: 10996651.000000\n",
      "ep 3556: ep_len:655 episode reward: total was -4.930000. running mean: -5.221733\n",
      "ep 3556: ep_len:585 episode reward: total was -19.640000. running mean: -5.365916\n",
      "ep 3556: ep_len:500 episode reward: total was -34.110000. running mean: -5.653357\n",
      "ep 3556: ep_len:700 episode reward: total was -29.460000. running mean: -5.891423\n",
      "ep 3556: ep_len:3 episode reward: total was 0.000000. running mean: -5.832509\n",
      "ep 3556: ep_len:520 episode reward: total was -34.920000. running mean: -6.123384\n",
      "ep 3556: ep_len:550 episode reward: total was -9.550000. running mean: -6.157650\n",
      "epsilon:0.010000 episode_count: 24899. steps_count: 11000164.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3557: ep_len:585 episode reward: total was 8.580000. running mean: -6.010273\n",
      "ep 3557: ep_len:500 episode reward: total was 19.210000. running mean: -5.758071\n",
      "ep 3557: ep_len:530 episode reward: total was -3.740000. running mean: -5.737890\n",
      "ep 3557: ep_len:530 episode reward: total was 12.920000. running mean: -5.551311\n",
      "ep 3557: ep_len:3 episode reward: total was 0.000000. running mean: -5.495798\n",
      "ep 3557: ep_len:530 episode reward: total was -25.460000. running mean: -5.695440\n",
      "ep 3557: ep_len:510 episode reward: total was -10.990000. running mean: -5.748385\n",
      "epsilon:0.010000 episode_count: 24906. steps_count: 11003352.000000\n",
      "ep 3558: ep_len:202 episode reward: total was 2.600000. running mean: -5.664902\n",
      "ep 3558: ep_len:630 episode reward: total was -27.810000. running mean: -5.886353\n",
      "ep 3558: ep_len:580 episode reward: total was -18.080000. running mean: -6.008289\n",
      "ep 3558: ep_len:555 episode reward: total was 6.950000. running mean: -5.878706\n",
      "ep 3558: ep_len:3 episode reward: total was 0.000000. running mean: -5.819919\n",
      "ep 3558: ep_len:500 episode reward: total was 2.260000. running mean: -5.739120\n",
      "ep 3558: ep_len:530 episode reward: total was -17.870000. running mean: -5.860429\n",
      "epsilon:0.010000 episode_count: 24913. steps_count: 11006352.000000\n",
      "ep 3559: ep_len:605 episode reward: total was 1.380000. running mean: -5.788024\n",
      "ep 3559: ep_len:615 episode reward: total was -24.140000. running mean: -5.971544\n",
      "ep 3559: ep_len:645 episode reward: total was -28.350000. running mean: -6.195329\n",
      "ep 3559: ep_len:500 episode reward: total was -2.000000. running mean: -6.153375\n",
      "ep 3559: ep_len:106 episode reward: total was 6.040000. running mean: -6.031442\n",
      "ep 3559: ep_len:500 episode reward: total was -13.000000. running mean: -6.101127\n",
      "ep 3559: ep_len:211 episode reward: total was -0.320000. running mean: -6.043316\n",
      "epsilon:0.010000 episode_count: 24920. steps_count: 11009534.000000\n",
      "ep 3560: ep_len:850 episode reward: total was -48.140000. running mean: -6.464283\n",
      "ep 3560: ep_len:505 episode reward: total was 3.880000. running mean: -6.360840\n",
      "ep 3560: ep_len:600 episode reward: total was -8.410000. running mean: -6.381332\n",
      "ep 3560: ep_len:720 episode reward: total was -123.960000. running mean: -7.557118\n",
      "ep 3560: ep_len:3 episode reward: total was 0.000000. running mean: -7.481547\n",
      "ep 3560: ep_len:630 episode reward: total was -29.850000. running mean: -7.705232\n",
      "ep 3560: ep_len:211 episode reward: total was -1.330000. running mean: -7.641479\n",
      "epsilon:0.010000 episode_count: 24927. steps_count: 11013053.000000\n",
      "ep 3561: ep_len:505 episode reward: total was 6.420000. running mean: -7.500865\n",
      "ep 3561: ep_len:595 episode reward: total was -3.010000. running mean: -7.455956\n",
      "ep 3561: ep_len:700 episode reward: total was -5.160000. running mean: -7.432996\n",
      "ep 3561: ep_len:160 episode reward: total was 3.150000. running mean: -7.327166\n",
      "ep 3561: ep_len:104 episode reward: total was 4.050000. running mean: -7.213395\n",
      "ep 3561: ep_len:261 episode reward: total was 8.660000. running mean: -7.054661\n",
      "ep 3561: ep_len:515 episode reward: total was -38.030000. running mean: -7.364414\n",
      "epsilon:0.010000 episode_count: 24934. steps_count: 11015893.000000\n",
      "ep 3562: ep_len:560 episode reward: total was -20.560000. running mean: -7.496370\n",
      "ep 3562: ep_len:670 episode reward: total was -105.420000. running mean: -8.475606\n",
      "ep 3562: ep_len:396 episode reward: total was -0.770000. running mean: -8.398550\n",
      "ep 3562: ep_len:500 episode reward: total was -20.560000. running mean: -8.520165\n",
      "ep 3562: ep_len:113 episode reward: total was 7.550000. running mean: -8.359463\n",
      "ep 3562: ep_len:510 episode reward: total was 3.460000. running mean: -8.241268\n",
      "ep 3562: ep_len:575 episode reward: total was -17.930000. running mean: -8.338156\n",
      "epsilon:0.010000 episode_count: 24941. steps_count: 11019217.000000\n",
      "ep 3563: ep_len:640 episode reward: total was -22.730000. running mean: -8.482074\n",
      "ep 3563: ep_len:630 episode reward: total was -4.570000. running mean: -8.442954\n",
      "ep 3563: ep_len:392 episode reward: total was 5.210000. running mean: -8.306424\n",
      "ep 3563: ep_len:515 episode reward: total was 6.450000. running mean: -8.158860\n",
      "ep 3563: ep_len:3 episode reward: total was 0.000000. running mean: -8.077271\n",
      "ep 3563: ep_len:595 episode reward: total was -17.480000. running mean: -8.171298\n",
      "ep 3563: ep_len:187 episode reward: total was 2.690000. running mean: -8.062685\n",
      "epsilon:0.010000 episode_count: 24948. steps_count: 11022179.000000\n",
      "ep 3564: ep_len:187 episode reward: total was 4.600000. running mean: -7.936059\n",
      "ep 3564: ep_len:500 episode reward: total was -16.730000. running mean: -8.023998\n",
      "ep 3564: ep_len:500 episode reward: total was -7.060000. running mean: -8.014358\n",
      "ep 3564: ep_len:510 episode reward: total was -9.920000. running mean: -8.033414\n",
      "ep 3564: ep_len:88 episode reward: total was 5.540000. running mean: -7.897680\n",
      "ep 3564: ep_len:500 episode reward: total was 11.540000. running mean: -7.703303\n",
      "ep 3564: ep_len:590 episode reward: total was -7.060000. running mean: -7.696870\n",
      "epsilon:0.010000 episode_count: 24955. steps_count: 11025054.000000\n",
      "ep 3565: ep_len:520 episode reward: total was -18.360000. running mean: -7.803502\n",
      "ep 3565: ep_len:500 episode reward: total was -9.510000. running mean: -7.820567\n",
      "ep 3565: ep_len:79 episode reward: total was 0.040000. running mean: -7.741961\n",
      "ep 3565: ep_len:500 episode reward: total was 8.870000. running mean: -7.575841\n",
      "ep 3565: ep_len:3 episode reward: total was 0.000000. running mean: -7.500083\n",
      "ep 3565: ep_len:500 episode reward: total was -38.620000. running mean: -7.811282\n",
      "ep 3565: ep_len:510 episode reward: total was -14.620000. running mean: -7.879369\n",
      "epsilon:0.010000 episode_count: 24962. steps_count: 11027666.000000\n",
      "ep 3566: ep_len:515 episode reward: total was -31.500000. running mean: -8.115576\n",
      "ep 3566: ep_len:500 episode reward: total was -13.040000. running mean: -8.164820\n",
      "ep 3566: ep_len:392 episode reward: total was 11.260000. running mean: -7.970572\n",
      "ep 3566: ep_len:510 episode reward: total was -1.150000. running mean: -7.902366\n",
      "ep 3566: ep_len:3 episode reward: total was 0.000000. running mean: -7.823342\n",
      "ep 3566: ep_len:615 episode reward: total was 1.480000. running mean: -7.730309\n",
      "ep 3566: ep_len:550 episode reward: total was -33.580000. running mean: -7.988806\n",
      "epsilon:0.010000 episode_count: 24969. steps_count: 11030751.000000\n",
      "ep 3567: ep_len:700 episode reward: total was -28.270000. running mean: -8.191618\n",
      "ep 3567: ep_len:590 episode reward: total was -13.920000. running mean: -8.248902\n",
      "ep 3567: ep_len:500 episode reward: total was -0.970000. running mean: -8.176113\n",
      "ep 3567: ep_len:595 episode reward: total was -45.160000. running mean: -8.545951\n",
      "ep 3567: ep_len:48 episode reward: total was 4.500000. running mean: -8.415492\n",
      "ep 3567: ep_len:515 episode reward: total was 0.410000. running mean: -8.327237\n",
      "ep 3567: ep_len:500 episode reward: total was -31.560000. running mean: -8.559565\n",
      "epsilon:0.010000 episode_count: 24976. steps_count: 11034199.000000\n",
      "ep 3568: ep_len:530 episode reward: total was -36.000000. running mean: -8.833969\n",
      "ep 3568: ep_len:500 episode reward: total was -20.510000. running mean: -8.950729\n",
      "ep 3568: ep_len:79 episode reward: total was -2.960000. running mean: -8.890822\n",
      "ep 3568: ep_len:585 episode reward: total was 8.520000. running mean: -8.716714\n",
      "ep 3568: ep_len:111 episode reward: total was 6.540000. running mean: -8.564147\n",
      "ep 3568: ep_len:595 episode reward: total was 4.090000. running mean: -8.437605\n",
      "ep 3568: ep_len:500 episode reward: total was -31.670000. running mean: -8.669929\n",
      "epsilon:0.010000 episode_count: 24983. steps_count: 11037099.000000\n",
      "ep 3569: ep_len:610 episode reward: total was 7.970000. running mean: -8.503530\n",
      "ep 3569: ep_len:500 episode reward: total was -7.140000. running mean: -8.489895\n",
      "ep 3569: ep_len:650 episode reward: total was -20.290000. running mean: -8.607896\n",
      "ep 3569: ep_len:525 episode reward: total was 7.970000. running mean: -8.442117\n",
      "ep 3569: ep_len:3 episode reward: total was 0.000000. running mean: -8.357695\n",
      "ep 3569: ep_len:570 episode reward: total was 9.930000. running mean: -8.174819\n",
      "ep 3569: ep_len:500 episode reward: total was -12.650000. running mean: -8.219570\n",
      "epsilon:0.010000 episode_count: 24990. steps_count: 11040457.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3570: ep_len:132 episode reward: total was -11.440000. running mean: -8.251775\n",
      "ep 3570: ep_len:165 episode reward: total was -4.940000. running mean: -8.218657\n",
      "ep 3570: ep_len:515 episode reward: total was -18.300000. running mean: -8.319470\n",
      "ep 3570: ep_len:515 episode reward: total was 1.860000. running mean: -8.217676\n",
      "ep 3570: ep_len:3 episode reward: total was 0.000000. running mean: -8.135499\n",
      "ep 3570: ep_len:500 episode reward: total was -61.720000. running mean: -8.671344\n",
      "ep 3570: ep_len:505 episode reward: total was -14.620000. running mean: -8.730830\n",
      "epsilon:0.010000 episode_count: 24997. steps_count: 11042792.000000\n",
      "ep 3571: ep_len:580 episode reward: total was 13.390000. running mean: -8.509622\n",
      "ep 3571: ep_len:625 episode reward: total was -7.580000. running mean: -8.500326\n",
      "ep 3571: ep_len:660 episode reward: total was -0.610000. running mean: -8.421423\n",
      "ep 3571: ep_len:505 episode reward: total was 8.940000. running mean: -8.247808\n",
      "ep 3571: ep_len:3 episode reward: total was 0.000000. running mean: -8.165330\n",
      "ep 3571: ep_len:590 episode reward: total was 3.450000. running mean: -8.049177\n",
      "ep 3571: ep_len:560 episode reward: total was -48.580000. running mean: -8.454485\n",
      "epsilon:0.010000 episode_count: 25004. steps_count: 11046315.000000\n",
      "ep 3572: ep_len:500 episode reward: total was 7.890000. running mean: -8.291040\n",
      "ep 3572: ep_len:595 episode reward: total was -18.300000. running mean: -8.391130\n",
      "ep 3572: ep_len:590 episode reward: total was -1.690000. running mean: -8.324119\n",
      "ep 3572: ep_len:162 episode reward: total was 4.630000. running mean: -8.194578\n",
      "ep 3572: ep_len:79 episode reward: total was 6.010000. running mean: -8.052532\n",
      "ep 3572: ep_len:545 episode reward: total was 3.590000. running mean: -7.936106\n",
      "ep 3572: ep_len:201 episode reward: total was -9.430000. running mean: -7.951045\n",
      "epsilon:0.010000 episode_count: 25011. steps_count: 11048987.000000\n",
      "ep 3573: ep_len:255 episode reward: total was 1.610000. running mean: -7.855435\n",
      "ep 3573: ep_len:500 episode reward: total was 18.720000. running mean: -7.589681\n",
      "ep 3573: ep_len:500 episode reward: total was -3.020000. running mean: -7.543984\n",
      "ep 3573: ep_len:500 episode reward: total was -21.140000. running mean: -7.679944\n",
      "ep 3573: ep_len:1 episode reward: total was 0.000000. running mean: -7.603144\n",
      "ep 3573: ep_len:665 episode reward: total was -15.720000. running mean: -7.684313\n",
      "ep 3573: ep_len:175 episode reward: total was -8.440000. running mean: -7.691870\n",
      "epsilon:0.010000 episode_count: 25018. steps_count: 11051583.000000\n",
      "ep 3574: ep_len:510 episode reward: total was -7.580000. running mean: -7.690751\n",
      "ep 3574: ep_len:635 episode reward: total was -21.880000. running mean: -7.832644\n",
      "ep 3574: ep_len:565 episode reward: total was 6.470000. running mean: -7.689617\n",
      "ep 3574: ep_len:163 episode reward: total was 7.630000. running mean: -7.536421\n",
      "ep 3574: ep_len:3 episode reward: total was 0.000000. running mean: -7.461057\n",
      "ep 3574: ep_len:515 episode reward: total was -3.680000. running mean: -7.423246\n",
      "ep 3574: ep_len:610 episode reward: total was -14.860000. running mean: -7.497614\n",
      "epsilon:0.010000 episode_count: 25025. steps_count: 11054584.000000\n",
      "ep 3575: ep_len:510 episode reward: total was 3.860000. running mean: -7.384038\n",
      "ep 3575: ep_len:285 episode reward: total was -4.840000. running mean: -7.358597\n",
      "ep 3575: ep_len:520 episode reward: total was -20.550000. running mean: -7.490511\n",
      "ep 3575: ep_len:510 episode reward: total was 6.570000. running mean: -7.349906\n",
      "ep 3575: ep_len:52 episode reward: total was 2.000000. running mean: -7.256407\n",
      "ep 3575: ep_len:565 episode reward: total was -2.570000. running mean: -7.209543\n",
      "ep 3575: ep_len:550 episode reward: total was -25.180000. running mean: -7.389248\n",
      "epsilon:0.010000 episode_count: 25032. steps_count: 11057576.000000\n",
      "ep 3576: ep_len:540 episode reward: total was -2.050000. running mean: -7.335855\n",
      "ep 3576: ep_len:625 episode reward: total was -43.890000. running mean: -7.701397\n",
      "ep 3576: ep_len:366 episode reward: total was 5.160000. running mean: -7.572783\n",
      "ep 3576: ep_len:560 episode reward: total was 9.430000. running mean: -7.402755\n",
      "ep 3576: ep_len:3 episode reward: total was 0.000000. running mean: -7.328727\n",
      "ep 3576: ep_len:595 episode reward: total was 0.920000. running mean: -7.246240\n",
      "ep 3576: ep_len:342 episode reward: total was -4.320000. running mean: -7.216978\n",
      "epsilon:0.010000 episode_count: 25039. steps_count: 11060607.000000\n",
      "ep 3577: ep_len:570 episode reward: total was -15.550000. running mean: -7.300308\n",
      "ep 3577: ep_len:500 episode reward: total was -20.840000. running mean: -7.435705\n",
      "ep 3577: ep_len:645 episode reward: total was -5.700000. running mean: -7.418348\n",
      "ep 3577: ep_len:585 episode reward: total was 19.980000. running mean: -7.144364\n",
      "ep 3577: ep_len:99 episode reward: total was 5.040000. running mean: -7.022521\n",
      "ep 3577: ep_len:731 episode reward: total was -46.800000. running mean: -7.420295\n",
      "ep 3577: ep_len:500 episode reward: total was -26.150000. running mean: -7.607592\n",
      "epsilon:0.010000 episode_count: 25046. steps_count: 11064237.000000\n",
      "ep 3578: ep_len:570 episode reward: total was -16.240000. running mean: -7.693917\n",
      "ep 3578: ep_len:510 episode reward: total was 6.310000. running mean: -7.553877\n",
      "ep 3578: ep_len:500 episode reward: total was -9.480000. running mean: -7.573139\n",
      "ep 3578: ep_len:500 episode reward: total was -3.120000. running mean: -7.528607\n",
      "ep 3578: ep_len:56 episode reward: total was 5.500000. running mean: -7.398321\n",
      "ep 3578: ep_len:635 episode reward: total was -5.190000. running mean: -7.376238\n",
      "ep 3578: ep_len:595 episode reward: total was -12.530000. running mean: -7.427776\n",
      "epsilon:0.010000 episode_count: 25053. steps_count: 11067603.000000\n",
      "ep 3579: ep_len:505 episode reward: total was -22.390000. running mean: -7.577398\n",
      "ep 3579: ep_len:500 episode reward: total was 16.180000. running mean: -7.339824\n",
      "ep 3579: ep_len:570 episode reward: total was -0.080000. running mean: -7.267226\n",
      "ep 3579: ep_len:535 episode reward: total was 12.510000. running mean: -7.069453\n",
      "ep 3579: ep_len:45 episode reward: total was 3.000000. running mean: -6.968759\n",
      "ep 3579: ep_len:615 episode reward: total was -2.350000. running mean: -6.922571\n",
      "ep 3579: ep_len:550 episode reward: total was -11.460000. running mean: -6.967945\n",
      "epsilon:0.010000 episode_count: 25060. steps_count: 11070923.000000\n",
      "ep 3580: ep_len:600 episode reward: total was 6.150000. running mean: -6.836766\n",
      "ep 3580: ep_len:500 episode reward: total was -6.460000. running mean: -6.832998\n",
      "ep 3580: ep_len:680 episode reward: total was 3.950000. running mean: -6.725168\n",
      "ep 3580: ep_len:500 episode reward: total was -20.620000. running mean: -6.864117\n",
      "ep 3580: ep_len:88 episode reward: total was 4.040000. running mean: -6.755076\n",
      "ep 3580: ep_len:243 episode reward: total was 7.180000. running mean: -6.615725\n",
      "ep 3580: ep_len:560 episode reward: total was -7.530000. running mean: -6.624868\n",
      "epsilon:0.010000 episode_count: 25067. steps_count: 11074094.000000\n",
      "ep 3581: ep_len:610 episode reward: total was 8.140000. running mean: -6.477219\n",
      "ep 3581: ep_len:655 episode reward: total was 9.620000. running mean: -6.316247\n",
      "ep 3581: ep_len:65 episode reward: total was -2.460000. running mean: -6.277684\n",
      "ep 3581: ep_len:116 episode reward: total was 1.570000. running mean: -6.199207\n",
      "ep 3581: ep_len:3 episode reward: total was 0.000000. running mean: -6.137215\n",
      "ep 3581: ep_len:560 episode reward: total was -17.180000. running mean: -6.247643\n",
      "ep 3581: ep_len:560 episode reward: total was -6.050000. running mean: -6.245667\n",
      "epsilon:0.010000 episode_count: 25074. steps_count: 11076663.000000\n",
      "ep 3582: ep_len:505 episode reward: total was -3.410000. running mean: -6.217310\n",
      "ep 3582: ep_len:610 episode reward: total was -10.090000. running mean: -6.256037\n",
      "ep 3582: ep_len:555 episode reward: total was -6.530000. running mean: -6.258777\n",
      "ep 3582: ep_len:151 episode reward: total was 6.110000. running mean: -6.135089\n",
      "ep 3582: ep_len:80 episode reward: total was -5.980000. running mean: -6.133538\n",
      "ep 3582: ep_len:500 episode reward: total was -0.740000. running mean: -6.079603\n",
      "ep 3582: ep_len:180 episode reward: total was -4.910000. running mean: -6.067907\n",
      "epsilon:0.010000 episode_count: 25081. steps_count: 11079244.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3583: ep_len:530 episode reward: total was 7.020000. running mean: -5.937027\n",
      "ep 3583: ep_len:339 episode reward: total was -15.930000. running mean: -6.036957\n",
      "ep 3583: ep_len:376 episode reward: total was -4.840000. running mean: -6.024988\n",
      "ep 3583: ep_len:580 episode reward: total was 8.620000. running mean: -5.878538\n",
      "ep 3583: ep_len:3 episode reward: total was 0.000000. running mean: -5.819752\n",
      "ep 3583: ep_len:575 episode reward: total was -16.740000. running mean: -5.928955\n",
      "ep 3583: ep_len:610 episode reward: total was -3.750000. running mean: -5.907165\n",
      "epsilon:0.010000 episode_count: 25088. steps_count: 11082257.000000\n",
      "ep 3584: ep_len:595 episode reward: total was -29.260000. running mean: -6.140694\n",
      "ep 3584: ep_len:555 episode reward: total was 6.570000. running mean: -6.013587\n",
      "ep 3584: ep_len:540 episode reward: total was -28.040000. running mean: -6.233851\n",
      "ep 3584: ep_len:500 episode reward: total was -39.190000. running mean: -6.563412\n",
      "ep 3584: ep_len:3 episode reward: total was 0.000000. running mean: -6.497778\n",
      "ep 3584: ep_len:665 episode reward: total was 1.840000. running mean: -6.414400\n",
      "ep 3584: ep_len:530 episode reward: total was -5.390000. running mean: -6.404156\n",
      "epsilon:0.010000 episode_count: 25095. steps_count: 11085645.000000\n",
      "ep 3585: ep_len:630 episode reward: total was -2.410000. running mean: -6.364215\n",
      "ep 3585: ep_len:690 episode reward: total was -6.710000. running mean: -6.367673\n",
      "ep 3585: ep_len:625 episode reward: total was 1.480000. running mean: -6.289196\n",
      "ep 3585: ep_len:560 episode reward: total was 8.910000. running mean: -6.137204\n",
      "ep 3585: ep_len:96 episode reward: total was 5.530000. running mean: -6.020532\n",
      "ep 3585: ep_len:615 episode reward: total was -22.230000. running mean: -6.182627\n",
      "ep 3585: ep_len:510 episode reward: total was -14.890000. running mean: -6.269700\n",
      "epsilon:0.010000 episode_count: 25102. steps_count: 11089371.000000\n",
      "ep 3586: ep_len:540 episode reward: total was -1.480000. running mean: -6.221803\n",
      "ep 3586: ep_len:277 episode reward: total was -1.370000. running mean: -6.173285\n",
      "ep 3586: ep_len:520 episode reward: total was -2.600000. running mean: -6.137552\n",
      "ep 3586: ep_len:535 episode reward: total was -3.490000. running mean: -6.111077\n",
      "ep 3586: ep_len:3 episode reward: total was 0.000000. running mean: -6.049966\n",
      "ep 3586: ep_len:500 episode reward: total was -18.910000. running mean: -6.178567\n",
      "ep 3586: ep_len:309 episode reward: total was -2.780000. running mean: -6.144581\n",
      "epsilon:0.010000 episode_count: 25109. steps_count: 11092055.000000\n",
      "ep 3587: ep_len:555 episode reward: total was -13.730000. running mean: -6.220435\n",
      "ep 3587: ep_len:500 episode reward: total was -3.020000. running mean: -6.188431\n",
      "ep 3587: ep_len:449 episode reward: total was -4.260000. running mean: -6.169146\n",
      "ep 3587: ep_len:510 episode reward: total was -25.100000. running mean: -6.358455\n",
      "ep 3587: ep_len:3 episode reward: total was 0.000000. running mean: -6.294870\n",
      "ep 3587: ep_len:625 episode reward: total was 6.180000. running mean: -6.170122\n",
      "ep 3587: ep_len:265 episode reward: total was -4.330000. running mean: -6.151720\n",
      "epsilon:0.010000 episode_count: 25116. steps_count: 11094962.000000\n",
      "ep 3588: ep_len:545 episode reward: total was -22.060000. running mean: -6.310803\n",
      "ep 3588: ep_len:515 episode reward: total was 4.850000. running mean: -6.199195\n",
      "ep 3588: ep_len:500 episode reward: total was -26.920000. running mean: -6.406403\n",
      "ep 3588: ep_len:500 episode reward: total was -9.980000. running mean: -6.442139\n",
      "ep 3588: ep_len:100 episode reward: total was 2.570000. running mean: -6.352018\n",
      "ep 3588: ep_len:535 episode reward: total was -1.350000. running mean: -6.301998\n",
      "ep 3588: ep_len:530 episode reward: total was -14.820000. running mean: -6.387178\n",
      "epsilon:0.010000 episode_count: 25123. steps_count: 11098187.000000\n",
      "ep 3589: ep_len:259 episode reward: total was 5.130000. running mean: -6.272006\n",
      "ep 3589: ep_len:595 episode reward: total was 15.650000. running mean: -6.052786\n",
      "ep 3589: ep_len:550 episode reward: total was -5.530000. running mean: -6.047558\n",
      "ep 3589: ep_len:510 episode reward: total was -20.340000. running mean: -6.190482\n",
      "ep 3589: ep_len:96 episode reward: total was 2.530000. running mean: -6.103278\n",
      "ep 3589: ep_len:560 episode reward: total was -34.430000. running mean: -6.386545\n",
      "ep 3589: ep_len:610 episode reward: total was -8.560000. running mean: -6.408279\n",
      "epsilon:0.010000 episode_count: 25130. steps_count: 11101367.000000\n",
      "ep 3590: ep_len:535 episode reward: total was 0.630000. running mean: -6.337897\n",
      "ep 3590: ep_len:535 episode reward: total was -19.630000. running mean: -6.470818\n",
      "ep 3590: ep_len:675 episode reward: total was -5.610000. running mean: -6.462209\n",
      "ep 3590: ep_len:44 episode reward: total was -3.470000. running mean: -6.432287\n",
      "ep 3590: ep_len:3 episode reward: total was 0.000000. running mean: -6.367964\n",
      "ep 3590: ep_len:620 episode reward: total was -6.040000. running mean: -6.364685\n",
      "ep 3590: ep_len:565 episode reward: total was -21.970000. running mean: -6.520738\n",
      "epsilon:0.010000 episode_count: 25137. steps_count: 11104344.000000\n",
      "ep 3591: ep_len:178 episode reward: total was 5.060000. running mean: -6.404931\n",
      "ep 3591: ep_len:500 episode reward: total was -24.320000. running mean: -6.584081\n",
      "ep 3591: ep_len:520 episode reward: total was -27.410000. running mean: -6.792340\n",
      "ep 3591: ep_len:500 episode reward: total was 10.630000. running mean: -6.618117\n",
      "ep 3591: ep_len:3 episode reward: total was 0.000000. running mean: -6.551936\n",
      "ep 3591: ep_len:500 episode reward: total was -10.010000. running mean: -6.586517\n",
      "ep 3591: ep_len:525 episode reward: total was -8.940000. running mean: -6.610051\n",
      "epsilon:0.010000 episode_count: 25144. steps_count: 11107070.000000\n",
      "ep 3592: ep_len:505 episode reward: total was 0.950000. running mean: -6.534451\n",
      "ep 3592: ep_len:500 episode reward: total was 11.190000. running mean: -6.357206\n",
      "ep 3592: ep_len:78 episode reward: total was 1.050000. running mean: -6.283134\n",
      "ep 3592: ep_len:402 episode reward: total was 5.390000. running mean: -6.166403\n",
      "ep 3592: ep_len:3 episode reward: total was 0.000000. running mean: -6.104739\n",
      "ep 3592: ep_len:605 episode reward: total was -91.360000. running mean: -6.957292\n",
      "ep 3592: ep_len:560 episode reward: total was -8.460000. running mean: -6.972319\n",
      "epsilon:0.010000 episode_count: 25151. steps_count: 11109723.000000\n",
      "ep 3593: ep_len:229 episode reward: total was -0.410000. running mean: -6.906695\n",
      "ep 3593: ep_len:287 episode reward: total was -36.330000. running mean: -7.200928\n",
      "ep 3593: ep_len:595 episode reward: total was -10.540000. running mean: -7.234319\n",
      "ep 3593: ep_len:132 episode reward: total was 1.590000. running mean: -7.146076\n",
      "ep 3593: ep_len:3 episode reward: total was 0.000000. running mean: -7.074615\n",
      "ep 3593: ep_len:645 episode reward: total was -19.380000. running mean: -7.197669\n",
      "ep 3593: ep_len:500 episode reward: total was -26.590000. running mean: -7.391592\n",
      "epsilon:0.010000 episode_count: 25158. steps_count: 11112114.000000\n",
      "ep 3594: ep_len:500 episode reward: total was -5.170000. running mean: -7.369376\n",
      "ep 3594: ep_len:670 episode reward: total was -21.320000. running mean: -7.508883\n",
      "ep 3594: ep_len:469 episode reward: total was 2.740000. running mean: -7.406394\n",
      "ep 3594: ep_len:112 episode reward: total was 2.570000. running mean: -7.306630\n",
      "ep 3594: ep_len:47 episode reward: total was 4.500000. running mean: -7.188564\n",
      "ep 3594: ep_len:500 episode reward: total was -0.250000. running mean: -7.119178\n",
      "ep 3594: ep_len:500 episode reward: total was -30.090000. running mean: -7.348886\n",
      "epsilon:0.010000 episode_count: 25165. steps_count: 11114912.000000\n",
      "ep 3595: ep_len:211 episode reward: total was 1.090000. running mean: -7.264497\n",
      "ep 3595: ep_len:595 episode reward: total was -54.640000. running mean: -7.738252\n",
      "ep 3595: ep_len:585 episode reward: total was -17.850000. running mean: -7.839370\n",
      "ep 3595: ep_len:132 episode reward: total was 7.110000. running mean: -7.689876\n",
      "ep 3595: ep_len:3 episode reward: total was 0.000000. running mean: -7.612977\n",
      "ep 3595: ep_len:168 episode reward: total was 6.630000. running mean: -7.470548\n",
      "ep 3595: ep_len:560 episode reward: total was -34.620000. running mean: -7.742042\n",
      "epsilon:0.010000 episode_count: 25172. steps_count: 11117166.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3596: ep_len:615 episode reward: total was 2.530000. running mean: -7.639322\n",
      "ep 3596: ep_len:171 episode reward: total was -2.910000. running mean: -7.592029\n",
      "ep 3596: ep_len:705 episode reward: total was -16.660000. running mean: -7.682708\n",
      "ep 3596: ep_len:170 episode reward: total was 4.150000. running mean: -7.564381\n",
      "ep 3596: ep_len:3 episode reward: total was 0.000000. running mean: -7.488737\n",
      "ep 3596: ep_len:1060 episode reward: total was -84.490000. running mean: -8.258750\n",
      "ep 3596: ep_len:535 episode reward: total was -1.610000. running mean: -8.192262\n",
      "epsilon:0.010000 episode_count: 25179. steps_count: 11120425.000000\n",
      "ep 3597: ep_len:500 episode reward: total was 1.940000. running mean: -8.090940\n",
      "ep 3597: ep_len:500 episode reward: total was -2.680000. running mean: -8.036830\n",
      "ep 3597: ep_len:555 episode reward: total was -3.370000. running mean: -7.990162\n",
      "ep 3597: ep_len:56 episode reward: total was 2.570000. running mean: -7.884561\n",
      "ep 3597: ep_len:3 episode reward: total was 0.000000. running mean: -7.805715\n",
      "ep 3597: ep_len:505 episode reward: total was -4.880000. running mean: -7.776458\n",
      "ep 3597: ep_len:267 episode reward: total was -1.330000. running mean: -7.711993\n",
      "epsilon:0.010000 episode_count: 25186. steps_count: 11122811.000000\n",
      "ep 3598: ep_len:620 episode reward: total was 5.650000. running mean: -7.578373\n",
      "ep 3598: ep_len:500 episode reward: total was 6.630000. running mean: -7.436290\n",
      "ep 3598: ep_len:540 episode reward: total was -9.720000. running mean: -7.459127\n",
      "ep 3598: ep_len:500 episode reward: total was 6.850000. running mean: -7.316035\n",
      "ep 3598: ep_len:3 episode reward: total was 0.000000. running mean: -7.242875\n",
      "ep 3598: ep_len:550 episode reward: total was 7.940000. running mean: -7.091046\n",
      "ep 3598: ep_len:580 episode reward: total was -20.950000. running mean: -7.229636\n",
      "epsilon:0.010000 episode_count: 25193. steps_count: 11126104.000000\n",
      "ep 3599: ep_len:236 episode reward: total was 4.630000. running mean: -7.111039\n",
      "ep 3599: ep_len:500 episode reward: total was 16.690000. running mean: -6.873029\n",
      "ep 3599: ep_len:500 episode reward: total was -7.460000. running mean: -6.878899\n",
      "ep 3599: ep_len:530 episode reward: total was 9.590000. running mean: -6.714210\n",
      "ep 3599: ep_len:81 episode reward: total was 4.000000. running mean: -6.607068\n",
      "ep 3599: ep_len:600 episode reward: total was -19.300000. running mean: -6.733997\n",
      "ep 3599: ep_len:560 episode reward: total was -16.140000. running mean: -6.828057\n",
      "epsilon:0.010000 episode_count: 25200. steps_count: 11129111.000000\n",
      "ep 3600: ep_len:660 episode reward: total was -14.150000. running mean: -6.901276\n",
      "ep 3600: ep_len:585 episode reward: total was 16.400000. running mean: -6.668264\n",
      "ep 3600: ep_len:550 episode reward: total was 4.900000. running mean: -6.552581\n",
      "ep 3600: ep_len:500 episode reward: total was -5.270000. running mean: -6.539755\n",
      "ep 3600: ep_len:3 episode reward: total was 0.000000. running mean: -6.474358\n",
      "ep 3600: ep_len:555 episode reward: total was 1.820000. running mean: -6.391414\n",
      "ep 3600: ep_len:510 episode reward: total was -7.660000. running mean: -6.404100\n",
      "epsilon:0.010000 episode_count: 25207. steps_count: 11132474.000000\n",
      "ep 3601: ep_len:505 episode reward: total was -22.440000. running mean: -6.564459\n",
      "ep 3601: ep_len:615 episode reward: total was -9.140000. running mean: -6.590214\n",
      "ep 3601: ep_len:620 episode reward: total was -30.800000. running mean: -6.832312\n",
      "ep 3601: ep_len:500 episode reward: total was 7.900000. running mean: -6.684989\n",
      "ep 3601: ep_len:90 episode reward: total was 5.060000. running mean: -6.567539\n",
      "ep 3601: ep_len:585 episode reward: total was 4.390000. running mean: -6.457964\n",
      "ep 3601: ep_len:500 episode reward: total was -27.690000. running mean: -6.670284\n",
      "epsilon:0.010000 episode_count: 25214. steps_count: 11135889.000000\n",
      "ep 3602: ep_len:220 episode reward: total was 1.110000. running mean: -6.592481\n",
      "ep 3602: ep_len:500 episode reward: total was 2.030000. running mean: -6.506257\n",
      "ep 3602: ep_len:675 episode reward: total was 4.830000. running mean: -6.392894\n",
      "ep 3602: ep_len:530 episode reward: total was -0.600000. running mean: -6.334965\n",
      "ep 3602: ep_len:3 episode reward: total was 0.000000. running mean: -6.271615\n",
      "ep 3602: ep_len:510 episode reward: total was -4.860000. running mean: -6.257499\n",
      "ep 3602: ep_len:600 episode reward: total was -15.370000. running mean: -6.348624\n",
      "epsilon:0.010000 episode_count: 25221. steps_count: 11138927.000000\n",
      "ep 3603: ep_len:530 episode reward: total was -26.390000. running mean: -6.549038\n",
      "ep 3603: ep_len:595 episode reward: total was -4.630000. running mean: -6.529848\n",
      "ep 3603: ep_len:76 episode reward: total was 0.040000. running mean: -6.464149\n",
      "ep 3603: ep_len:530 episode reward: total was 11.930000. running mean: -6.280208\n",
      "ep 3603: ep_len:125 episode reward: total was 2.040000. running mean: -6.197006\n",
      "ep 3603: ep_len:645 episode reward: total was -11.330000. running mean: -6.248336\n",
      "ep 3603: ep_len:520 episode reward: total was -22.460000. running mean: -6.410452\n",
      "epsilon:0.010000 episode_count: 25228. steps_count: 11141948.000000\n",
      "ep 3604: ep_len:500 episode reward: total was -40.830000. running mean: -6.754648\n",
      "ep 3604: ep_len:555 episode reward: total was 7.380000. running mean: -6.613301\n",
      "ep 3604: ep_len:580 episode reward: total was -37.830000. running mean: -6.925468\n",
      "ep 3604: ep_len:500 episode reward: total was -12.540000. running mean: -6.981613\n",
      "ep 3604: ep_len:92 episode reward: total was 6.560000. running mean: -6.846197\n",
      "ep 3604: ep_len:505 episode reward: total was 2.560000. running mean: -6.752135\n",
      "ep 3604: ep_len:159 episode reward: total was -5.390000. running mean: -6.738514\n",
      "epsilon:0.010000 episode_count: 25235. steps_count: 11144839.000000\n",
      "ep 3605: ep_len:590 episode reward: total was 2.420000. running mean: -6.646929\n",
      "ep 3605: ep_len:620 episode reward: total was 4.980000. running mean: -6.530660\n",
      "ep 3605: ep_len:570 episode reward: total was -21.850000. running mean: -6.683853\n",
      "ep 3605: ep_len:125 episode reward: total was -0.900000. running mean: -6.626014\n",
      "ep 3605: ep_len:3 episode reward: total was 0.000000. running mean: -6.559754\n",
      "ep 3605: ep_len:575 episode reward: total was 2.350000. running mean: -6.470657\n",
      "ep 3605: ep_len:500 episode reward: total was -25.120000. running mean: -6.657150\n",
      "epsilon:0.010000 episode_count: 25242. steps_count: 11147822.000000\n",
      "ep 3606: ep_len:585 episode reward: total was 9.490000. running mean: -6.495679\n",
      "ep 3606: ep_len:590 episode reward: total was -22.570000. running mean: -6.656422\n",
      "ep 3606: ep_len:540 episode reward: total was -5.740000. running mean: -6.647258\n",
      "ep 3606: ep_len:545 episode reward: total was 7.900000. running mean: -6.501785\n",
      "ep 3606: ep_len:50 episode reward: total was 3.500000. running mean: -6.401767\n",
      "ep 3606: ep_len:695 episode reward: total was 6.920000. running mean: -6.268550\n",
      "ep 3606: ep_len:500 episode reward: total was -1.190000. running mean: -6.217764\n",
      "epsilon:0.010000 episode_count: 25249. steps_count: 11151327.000000\n",
      "ep 3607: ep_len:500 episode reward: total was -12.100000. running mean: -6.276586\n",
      "ep 3607: ep_len:525 episode reward: total was -18.180000. running mean: -6.395621\n",
      "ep 3607: ep_len:640 episode reward: total was -2.220000. running mean: -6.353864\n",
      "ep 3607: ep_len:500 episode reward: total was 7.140000. running mean: -6.218926\n",
      "ep 3607: ep_len:3 episode reward: total was 0.000000. running mean: -6.156736\n",
      "ep 3607: ep_len:515 episode reward: total was -11.450000. running mean: -6.209669\n",
      "ep 3607: ep_len:500 episode reward: total was 0.940000. running mean: -6.138172\n",
      "epsilon:0.010000 episode_count: 25256. steps_count: 11154510.000000\n",
      "ep 3608: ep_len:580 episode reward: total was 6.110000. running mean: -6.015691\n",
      "ep 3608: ep_len:510 episode reward: total was 8.680000. running mean: -5.868734\n",
      "ep 3608: ep_len:465 episode reward: total was 2.770000. running mean: -5.782346\n",
      "ep 3608: ep_len:500 episode reward: total was 10.120000. running mean: -5.623323\n",
      "ep 3608: ep_len:104 episode reward: total was 5.540000. running mean: -5.511690\n",
      "ep 3608: ep_len:500 episode reward: total was -29.560000. running mean: -5.752173\n",
      "ep 3608: ep_len:204 episode reward: total was -0.830000. running mean: -5.702951\n",
      "epsilon:0.010000 episode_count: 25263. steps_count: 11157373.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3609: ep_len:575 episode reward: total was -10.980000. running mean: -5.755722\n",
      "ep 3609: ep_len:605 episode reward: total was 2.240000. running mean: -5.675764\n",
      "ep 3609: ep_len:500 episode reward: total was 9.820000. running mean: -5.520807\n",
      "ep 3609: ep_len:149 episode reward: total was 8.140000. running mean: -5.384199\n",
      "ep 3609: ep_len:98 episode reward: total was 7.530000. running mean: -5.255057\n",
      "ep 3609: ep_len:269 episode reward: total was -1.870000. running mean: -5.221206\n",
      "ep 3609: ep_len:329 episode reward: total was -2.760000. running mean: -5.196594\n",
      "epsilon:0.010000 episode_count: 25270. steps_count: 11159898.000000\n",
      "ep 3610: ep_len:500 episode reward: total was 14.320000. running mean: -5.001428\n",
      "ep 3610: ep_len:500 episode reward: total was -11.790000. running mean: -5.069314\n",
      "ep 3610: ep_len:870 episode reward: total was -101.090000. running mean: -6.029521\n",
      "ep 3610: ep_len:515 episode reward: total was -5.490000. running mean: -6.024126\n",
      "ep 3610: ep_len:3 episode reward: total was 0.000000. running mean: -5.963884\n",
      "ep 3610: ep_len:540 episode reward: total was -4.990000. running mean: -5.954145\n",
      "ep 3610: ep_len:500 episode reward: total was -3.310000. running mean: -5.927704\n",
      "epsilon:0.010000 episode_count: 25277. steps_count: 11163326.000000\n",
      "ep 3611: ep_len:540 episode reward: total was 5.200000. running mean: -5.816427\n",
      "ep 3611: ep_len:595 episode reward: total was -6.560000. running mean: -5.823863\n",
      "ep 3611: ep_len:580 episode reward: total was -2.860000. running mean: -5.794224\n",
      "ep 3611: ep_len:510 episode reward: total was -23.140000. running mean: -5.967682\n",
      "ep 3611: ep_len:85 episode reward: total was -11.950000. running mean: -6.027505\n",
      "ep 3611: ep_len:500 episode reward: total was -64.390000. running mean: -6.611130\n",
      "ep 3611: ep_len:600 episode reward: total was -53.640000. running mean: -7.081419\n",
      "epsilon:0.010000 episode_count: 25284. steps_count: 11166736.000000\n",
      "ep 3612: ep_len:199 episode reward: total was 3.080000. running mean: -6.979804\n",
      "ep 3612: ep_len:565 episode reward: total was -0.590000. running mean: -6.915906\n",
      "ep 3612: ep_len:550 episode reward: total was -10.620000. running mean: -6.952947\n",
      "ep 3612: ep_len:500 episode reward: total was -18.140000. running mean: -7.064818\n",
      "ep 3612: ep_len:55 episode reward: total was 4.000000. running mean: -6.954170\n",
      "ep 3612: ep_len:167 episode reward: total was 6.620000. running mean: -6.818428\n",
      "ep 3612: ep_len:500 episode reward: total was 4.000000. running mean: -6.710244\n",
      "epsilon:0.010000 episode_count: 25291. steps_count: 11169272.000000\n",
      "ep 3613: ep_len:650 episode reward: total was 5.160000. running mean: -6.591541\n",
      "ep 3613: ep_len:515 episode reward: total was -3.280000. running mean: -6.558426\n",
      "ep 3613: ep_len:640 episode reward: total was 3.180000. running mean: -6.461042\n",
      "ep 3613: ep_len:500 episode reward: total was 1.510000. running mean: -6.381331\n",
      "ep 3613: ep_len:3 episode reward: total was 0.000000. running mean: -6.317518\n",
      "ep 3613: ep_len:239 episode reward: total was 7.670000. running mean: -6.177643\n",
      "ep 3613: ep_len:321 episode reward: total was 1.210000. running mean: -6.103766\n",
      "epsilon:0.010000 episode_count: 25298. steps_count: 11172140.000000\n",
      "ep 3614: ep_len:500 episode reward: total was 5.400000. running mean: -5.988729\n",
      "ep 3614: ep_len:535 episode reward: total was -1.770000. running mean: -5.946541\n",
      "ep 3614: ep_len:530 episode reward: total was -18.860000. running mean: -6.075676\n",
      "ep 3614: ep_len:570 episode reward: total was 13.460000. running mean: -5.880319\n",
      "ep 3614: ep_len:3 episode reward: total was 0.000000. running mean: -5.821516\n",
      "ep 3614: ep_len:590 episode reward: total was -50.610000. running mean: -6.269401\n",
      "ep 3614: ep_len:520 episode reward: total was 0.980000. running mean: -6.196907\n",
      "epsilon:0.010000 episode_count: 25305. steps_count: 11175388.000000\n",
      "ep 3615: ep_len:575 episode reward: total was -12.670000. running mean: -6.261638\n",
      "ep 3615: ep_len:575 episode reward: total was 2.890000. running mean: -6.170121\n",
      "ep 3615: ep_len:635 episode reward: total was 8.020000. running mean: -6.028220\n",
      "ep 3615: ep_len:500 episode reward: total was -8.560000. running mean: -6.053538\n",
      "ep 3615: ep_len:3 episode reward: total was 0.000000. running mean: -5.993003\n",
      "ep 3615: ep_len:595 episode reward: total was -0.030000. running mean: -5.933373\n",
      "ep 3615: ep_len:510 episode reward: total was 3.990000. running mean: -5.834139\n",
      "epsilon:0.010000 episode_count: 25312. steps_count: 11178781.000000\n",
      "ep 3616: ep_len:500 episode reward: total was -14.940000. running mean: -5.925197\n",
      "ep 3616: ep_len:500 episode reward: total was -0.210000. running mean: -5.868045\n",
      "ep 3616: ep_len:690 episode reward: total was -24.290000. running mean: -6.052265\n",
      "ep 3616: ep_len:575 episode reward: total was 11.480000. running mean: -5.876942\n",
      "ep 3616: ep_len:94 episode reward: total was 6.040000. running mean: -5.757773\n",
      "ep 3616: ep_len:505 episode reward: total was -1.480000. running mean: -5.714995\n",
      "ep 3616: ep_len:345 episode reward: total was -12.760000. running mean: -5.785445\n",
      "epsilon:0.010000 episode_count: 25319. steps_count: 11181990.000000\n",
      "ep 3617: ep_len:650 episode reward: total was -15.180000. running mean: -5.879391\n",
      "ep 3617: ep_len:670 episode reward: total was -11.740000. running mean: -5.937997\n",
      "ep 3617: ep_len:451 episode reward: total was 7.240000. running mean: -5.806217\n",
      "ep 3617: ep_len:531 episode reward: total was 8.610000. running mean: -5.662055\n",
      "ep 3617: ep_len:48 episode reward: total was 4.500000. running mean: -5.560434\n",
      "ep 3617: ep_len:700 episode reward: total was -12.260000. running mean: -5.627430\n",
      "ep 3617: ep_len:595 episode reward: total was -58.700000. running mean: -6.158156\n",
      "epsilon:0.010000 episode_count: 25326. steps_count: 11185635.000000\n",
      "ep 3618: ep_len:515 episode reward: total was -11.360000. running mean: -6.210174\n",
      "ep 3618: ep_len:610 episode reward: total was -3.830000. running mean: -6.186372\n",
      "ep 3618: ep_len:400 episode reward: total was -16.230000. running mean: -6.286809\n",
      "ep 3618: ep_len:510 episode reward: total was 1.090000. running mean: -6.213040\n",
      "ep 3618: ep_len:91 episode reward: total was 6.040000. running mean: -6.090510\n",
      "ep 3618: ep_len:540 episode reward: total was -2.930000. running mean: -6.058905\n",
      "ep 3618: ep_len:620 episode reward: total was 7.080000. running mean: -5.927516\n",
      "epsilon:0.010000 episode_count: 25333. steps_count: 11188921.000000\n",
      "ep 3619: ep_len:620 episode reward: total was 4.610000. running mean: -5.822141\n",
      "ep 3619: ep_len:500 episode reward: total was 11.900000. running mean: -5.644919\n",
      "ep 3619: ep_len:550 episode reward: total was -0.580000. running mean: -5.594270\n",
      "ep 3619: ep_len:500 episode reward: total was -5.500000. running mean: -5.593327\n",
      "ep 3619: ep_len:3 episode reward: total was 0.000000. running mean: -5.537394\n",
      "ep 3619: ep_len:585 episode reward: total was -14.710000. running mean: -5.629120\n",
      "ep 3619: ep_len:620 episode reward: total was -7.870000. running mean: -5.651529\n",
      "epsilon:0.010000 episode_count: 25340. steps_count: 11192299.000000\n",
      "ep 3620: ep_len:690 episode reward: total was -9.160000. running mean: -5.686614\n",
      "ep 3620: ep_len:505 episode reward: total was 1.550000. running mean: -5.614248\n",
      "ep 3620: ep_len:555 episode reward: total was -18.510000. running mean: -5.743205\n",
      "ep 3620: ep_len:610 episode reward: total was 14.520000. running mean: -5.540573\n",
      "ep 3620: ep_len:3 episode reward: total was 0.000000. running mean: -5.485167\n",
      "ep 3620: ep_len:700 episode reward: total was -26.770000. running mean: -5.698016\n",
      "ep 3620: ep_len:179 episode reward: total was 3.160000. running mean: -5.609435\n",
      "epsilon:0.010000 episode_count: 25347. steps_count: 11195541.000000\n",
      "ep 3621: ep_len:550 episode reward: total was -12.710000. running mean: -5.680441\n",
      "ep 3621: ep_len:565 episode reward: total was 1.420000. running mean: -5.609437\n",
      "ep 3621: ep_len:500 episode reward: total was -6.340000. running mean: -5.616742\n",
      "ep 3621: ep_len:565 episode reward: total was 12.500000. running mean: -5.435575\n",
      "ep 3621: ep_len:109 episode reward: total was 0.560000. running mean: -5.375619\n",
      "ep 3621: ep_len:525 episode reward: total was -23.570000. running mean: -5.557563\n",
      "ep 3621: ep_len:201 episode reward: total was -1.320000. running mean: -5.515187\n",
      "epsilon:0.010000 episode_count: 25354. steps_count: 11198556.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3622: ep_len:610 episode reward: total was -49.620000. running mean: -5.956235\n",
      "ep 3622: ep_len:615 episode reward: total was -0.430000. running mean: -5.900973\n",
      "ep 3622: ep_len:525 episode reward: total was 4.950000. running mean: -5.792463\n",
      "ep 3622: ep_len:615 episode reward: total was -12.920000. running mean: -5.863739\n",
      "ep 3622: ep_len:2 episode reward: total was 0.000000. running mean: -5.805101\n",
      "ep 3622: ep_len:500 episode reward: total was -5.280000. running mean: -5.799850\n",
      "ep 3622: ep_len:550 episode reward: total was 5.550000. running mean: -5.686352\n",
      "epsilon:0.010000 episode_count: 25361. steps_count: 11201973.000000\n",
      "ep 3623: ep_len:545 episode reward: total was -7.170000. running mean: -5.701188\n",
      "ep 3623: ep_len:500 episode reward: total was -3.890000. running mean: -5.683076\n",
      "ep 3623: ep_len:605 episode reward: total was 6.520000. running mean: -5.561046\n",
      "ep 3623: ep_len:590 episode reward: total was 6.030000. running mean: -5.445135\n",
      "ep 3623: ep_len:51 episode reward: total was 5.000000. running mean: -5.340684\n",
      "ep 3623: ep_len:560 episode reward: total was -9.770000. running mean: -5.384977\n",
      "ep 3623: ep_len:600 episode reward: total was -38.140000. running mean: -5.712527\n",
      "epsilon:0.010000 episode_count: 25368. steps_count: 11205424.000000\n",
      "ep 3624: ep_len:585 episode reward: total was 15.530000. running mean: -5.500102\n",
      "ep 3624: ep_len:610 episode reward: total was 0.570000. running mean: -5.439401\n",
      "ep 3624: ep_len:368 episode reward: total was -10.330000. running mean: -5.488307\n",
      "ep 3624: ep_len:510 episode reward: total was 1.960000. running mean: -5.413824\n",
      "ep 3624: ep_len:3 episode reward: total was 0.000000. running mean: -5.359686\n",
      "ep 3624: ep_len:585 episode reward: total was -7.090000. running mean: -5.376989\n",
      "ep 3624: ep_len:515 episode reward: total was -17.910000. running mean: -5.502319\n",
      "epsilon:0.010000 episode_count: 25375. steps_count: 11208600.000000\n",
      "ep 3625: ep_len:235 episode reward: total was 10.170000. running mean: -5.345596\n",
      "ep 3625: ep_len:500 episode reward: total was -39.600000. running mean: -5.688140\n",
      "ep 3625: ep_len:535 episode reward: total was 0.250000. running mean: -5.628758\n",
      "ep 3625: ep_len:535 episode reward: total was 8.460000. running mean: -5.487871\n",
      "ep 3625: ep_len:87 episode reward: total was 4.040000. running mean: -5.392592\n",
      "ep 3625: ep_len:861 episode reward: total was -48.650000. running mean: -5.825166\n",
      "ep 3625: ep_len:303 episode reward: total was -2.760000. running mean: -5.794514\n",
      "epsilon:0.010000 episode_count: 25382. steps_count: 11211656.000000\n",
      "ep 3626: ep_len:595 episode reward: total was -1.990000. running mean: -5.756469\n",
      "ep 3626: ep_len:500 episode reward: total was -26.540000. running mean: -5.964305\n",
      "ep 3626: ep_len:406 episode reward: total was 4.700000. running mean: -5.857662\n",
      "ep 3626: ep_len:510 episode reward: total was 12.520000. running mean: -5.673885\n",
      "ep 3626: ep_len:3 episode reward: total was 0.000000. running mean: -5.617146\n",
      "ep 3626: ep_len:575 episode reward: total was -7.660000. running mean: -5.637575\n",
      "ep 3626: ep_len:575 episode reward: total was -16.510000. running mean: -5.746299\n",
      "epsilon:0.010000 episode_count: 25389. steps_count: 11214820.000000\n",
      "ep 3627: ep_len:670 episode reward: total was -15.660000. running mean: -5.845436\n",
      "ep 3627: ep_len:560 episode reward: total was 13.330000. running mean: -5.653682\n",
      "ep 3627: ep_len:575 episode reward: total was 6.360000. running mean: -5.533545\n",
      "ep 3627: ep_len:500 episode reward: total was 7.860000. running mean: -5.399609\n",
      "ep 3627: ep_len:55 episode reward: total was 4.000000. running mean: -5.305613\n",
      "ep 3627: ep_len:660 episode reward: total was -12.120000. running mean: -5.373757\n",
      "ep 3627: ep_len:545 episode reward: total was -9.580000. running mean: -5.415820\n",
      "epsilon:0.010000 episode_count: 25396. steps_count: 11218385.000000\n",
      "ep 3628: ep_len:690 episode reward: total was -3.530000. running mean: -5.396961\n",
      "ep 3628: ep_len:505 episode reward: total was 6.820000. running mean: -5.274792\n",
      "ep 3628: ep_len:610 episode reward: total was -6.860000. running mean: -5.290644\n",
      "ep 3628: ep_len:590 episode reward: total was 19.650000. running mean: -5.041237\n",
      "ep 3628: ep_len:3 episode reward: total was 0.000000. running mean: -4.990825\n",
      "ep 3628: ep_len:575 episode reward: total was -14.660000. running mean: -5.087517\n",
      "ep 3628: ep_len:291 episode reward: total was -3.330000. running mean: -5.069942\n",
      "epsilon:0.010000 episode_count: 25403. steps_count: 11221649.000000\n",
      "ep 3629: ep_len:253 episode reward: total was 4.140000. running mean: -4.977842\n",
      "ep 3629: ep_len:580 episode reward: total was 19.860000. running mean: -4.729464\n",
      "ep 3629: ep_len:680 episode reward: total was -3.150000. running mean: -4.713669\n",
      "ep 3629: ep_len:620 episode reward: total was 3.540000. running mean: -4.631132\n",
      "ep 3629: ep_len:3 episode reward: total was 0.000000. running mean: -4.584821\n",
      "ep 3629: ep_len:585 episode reward: total was 11.110000. running mean: -4.427873\n",
      "ep 3629: ep_len:520 episode reward: total was 1.500000. running mean: -4.368594\n",
      "epsilon:0.010000 episode_count: 25410. steps_count: 11224890.000000\n",
      "ep 3630: ep_len:580 episode reward: total was -7.210000. running mean: -4.397008\n",
      "ep 3630: ep_len:620 episode reward: total was 1.930000. running mean: -4.333738\n",
      "ep 3630: ep_len:565 episode reward: total was -7.710000. running mean: -4.367501\n",
      "ep 3630: ep_len:117 episode reward: total was 5.090000. running mean: -4.272926\n",
      "ep 3630: ep_len:3 episode reward: total was 0.000000. running mean: -4.230196\n",
      "ep 3630: ep_len:500 episode reward: total was -18.710000. running mean: -4.374995\n",
      "ep 3630: ep_len:595 episode reward: total was -4.490000. running mean: -4.376145\n",
      "epsilon:0.010000 episode_count: 25417. steps_count: 11227870.000000\n",
      "ep 3631: ep_len:500 episode reward: total was 3.620000. running mean: -4.296183\n",
      "ep 3631: ep_len:515 episode reward: total was -19.990000. running mean: -4.453121\n",
      "ep 3631: ep_len:540 episode reward: total was -17.210000. running mean: -4.580690\n",
      "ep 3631: ep_len:132 episode reward: total was 4.600000. running mean: -4.488883\n",
      "ep 3631: ep_len:2 episode reward: total was 0.000000. running mean: -4.443994\n",
      "ep 3631: ep_len:500 episode reward: total was -2.840000. running mean: -4.427954\n",
      "ep 3631: ep_len:600 episode reward: total was -5.970000. running mean: -4.443375\n",
      "epsilon:0.010000 episode_count: 25424. steps_count: 11230659.000000\n",
      "ep 3632: ep_len:540 episode reward: total was -1.140000. running mean: -4.410341\n",
      "ep 3632: ep_len:540 episode reward: total was -17.600000. running mean: -4.542238\n",
      "ep 3632: ep_len:655 episode reward: total was 1.130000. running mean: -4.485515\n",
      "ep 3632: ep_len:505 episode reward: total was 11.050000. running mean: -4.330160\n",
      "ep 3632: ep_len:3 episode reward: total was 0.000000. running mean: -4.286859\n",
      "ep 3632: ep_len:600 episode reward: total was -22.050000. running mean: -4.464490\n",
      "ep 3632: ep_len:304 episode reward: total was -2.790000. running mean: -4.447745\n",
      "epsilon:0.010000 episode_count: 25431. steps_count: 11233806.000000\n",
      "ep 3633: ep_len:500 episode reward: total was -5.800000. running mean: -4.461268\n",
      "ep 3633: ep_len:540 episode reward: total was -22.760000. running mean: -4.644255\n",
      "ep 3633: ep_len:79 episode reward: total was -3.450000. running mean: -4.632312\n",
      "ep 3633: ep_len:580 episode reward: total was 6.510000. running mean: -4.520889\n",
      "ep 3633: ep_len:87 episode reward: total was 4.530000. running mean: -4.430380\n",
      "ep 3633: ep_len:555 episode reward: total was 6.640000. running mean: -4.319677\n",
      "ep 3633: ep_len:595 episode reward: total was 3.100000. running mean: -4.245480\n",
      "epsilon:0.010000 episode_count: 25438. steps_count: 11236742.000000\n",
      "ep 3634: ep_len:227 episode reward: total was 7.150000. running mean: -4.131525\n",
      "ep 3634: ep_len:545 episode reward: total was -26.400000. running mean: -4.354210\n",
      "ep 3634: ep_len:625 episode reward: total was -8.130000. running mean: -4.391968\n",
      "ep 3634: ep_len:520 episode reward: total was 8.040000. running mean: -4.267648\n",
      "ep 3634: ep_len:98 episode reward: total was -9.940000. running mean: -4.324372\n",
      "ep 3634: ep_len:500 episode reward: total was 5.370000. running mean: -4.227428\n",
      "ep 3634: ep_len:315 episode reward: total was -5.790000. running mean: -4.243054\n",
      "epsilon:0.010000 episode_count: 25445. steps_count: 11239572.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3635: ep_len:600 episode reward: total was 3.440000. running mean: -4.166223\n",
      "ep 3635: ep_len:500 episode reward: total was 13.850000. running mean: -3.986061\n",
      "ep 3635: ep_len:635 episode reward: total was 1.090000. running mean: -3.935300\n",
      "ep 3635: ep_len:525 episode reward: total was -15.110000. running mean: -4.047047\n",
      "ep 3635: ep_len:3 episode reward: total was 0.000000. running mean: -4.006577\n",
      "ep 3635: ep_len:660 episode reward: total was 11.900000. running mean: -3.847511\n",
      "ep 3635: ep_len:595 episode reward: total was -14.780000. running mean: -3.956836\n",
      "epsilon:0.010000 episode_count: 25452. steps_count: 11243090.000000\n",
      "ep 3636: ep_len:550 episode reward: total was -14.230000. running mean: -4.059567\n",
      "ep 3636: ep_len:525 episode reward: total was -23.950000. running mean: -4.258472\n",
      "ep 3636: ep_len:377 episode reward: total was 6.680000. running mean: -4.149087\n",
      "ep 3636: ep_len:505 episode reward: total was 5.560000. running mean: -4.051996\n",
      "ep 3636: ep_len:83 episode reward: total was 1.000000. running mean: -4.001476\n",
      "ep 3636: ep_len:635 episode reward: total was 2.330000. running mean: -3.938161\n",
      "ep 3636: ep_len:272 episode reward: total was -4.300000. running mean: -3.941780\n",
      "epsilon:0.010000 episode_count: 25459. steps_count: 11246037.000000\n",
      "ep 3637: ep_len:650 episode reward: total was -0.060000. running mean: -3.902962\n",
      "ep 3637: ep_len:550 episode reward: total was 14.890000. running mean: -3.715032\n",
      "ep 3637: ep_len:545 episode reward: total was 5.390000. running mean: -3.623982\n",
      "ep 3637: ep_len:500 episode reward: total was -32.620000. running mean: -3.913942\n",
      "ep 3637: ep_len:3 episode reward: total was 0.000000. running mean: -3.874803\n",
      "ep 3637: ep_len:545 episode reward: total was -12.250000. running mean: -3.958555\n",
      "ep 3637: ep_len:510 episode reward: total was -14.400000. running mean: -4.062969\n",
      "epsilon:0.010000 episode_count: 25466. steps_count: 11249340.000000\n",
      "ep 3638: ep_len:580 episode reward: total was 4.640000. running mean: -3.975940\n",
      "ep 3638: ep_len:565 episode reward: total was 8.120000. running mean: -3.854980\n",
      "ep 3638: ep_len:535 episode reward: total was 4.450000. running mean: -3.771930\n",
      "ep 3638: ep_len:51 episode reward: total was -2.460000. running mean: -3.758811\n",
      "ep 3638: ep_len:3 episode reward: total was 0.000000. running mean: -3.721223\n",
      "ep 3638: ep_len:630 episode reward: total was -26.090000. running mean: -3.944911\n",
      "ep 3638: ep_len:560 episode reward: total was -21.050000. running mean: -4.115962\n",
      "epsilon:0.010000 episode_count: 25473. steps_count: 11252264.000000\n",
      "ep 3639: ep_len:630 episode reward: total was -13.690000. running mean: -4.211702\n",
      "ep 3639: ep_len:620 episode reward: total was -58.050000. running mean: -4.750085\n",
      "ep 3639: ep_len:500 episode reward: total was 3.520000. running mean: -4.667384\n",
      "ep 3639: ep_len:505 episode reward: total was 9.890000. running mean: -4.521810\n",
      "ep 3639: ep_len:3 episode reward: total was 0.000000. running mean: -4.476592\n",
      "ep 3639: ep_len:747 episode reward: total was -68.910000. running mean: -5.120926\n",
      "ep 3639: ep_len:211 episode reward: total was -5.370000. running mean: -5.123417\n",
      "epsilon:0.010000 episode_count: 25480. steps_count: 11255480.000000\n",
      "ep 3640: ep_len:550 episode reward: total was -14.750000. running mean: -5.219683\n",
      "ep 3640: ep_len:500 episode reward: total was -0.820000. running mean: -5.175686\n",
      "ep 3640: ep_len:422 episode reward: total was -27.360000. running mean: -5.397529\n",
      "ep 3640: ep_len:409 episode reward: total was 5.860000. running mean: -5.284954\n",
      "ep 3640: ep_len:3 episode reward: total was 0.000000. running mean: -5.232104\n",
      "ep 3640: ep_len:515 episode reward: total was -2.360000. running mean: -5.203383\n",
      "ep 3640: ep_len:711 episode reward: total was -68.020000. running mean: -5.831549\n",
      "epsilon:0.010000 episode_count: 25487. steps_count: 11258590.000000\n",
      "ep 3641: ep_len:585 episode reward: total was -1.440000. running mean: -5.787634\n",
      "ep 3641: ep_len:620 episode reward: total was -4.570000. running mean: -5.775458\n",
      "ep 3641: ep_len:570 episode reward: total was -1.110000. running mean: -5.728803\n",
      "ep 3641: ep_len:500 episode reward: total was 15.100000. running mean: -5.520515\n",
      "ep 3641: ep_len:3 episode reward: total was 0.000000. running mean: -5.465310\n",
      "ep 3641: ep_len:665 episode reward: total was 9.370000. running mean: -5.316957\n",
      "ep 3641: ep_len:525 episode reward: total was -9.050000. running mean: -5.354287\n",
      "epsilon:0.010000 episode_count: 25494. steps_count: 11262058.000000\n",
      "ep 3642: ep_len:655 episode reward: total was -0.110000. running mean: -5.301844\n",
      "ep 3642: ep_len:500 episode reward: total was 4.680000. running mean: -5.202026\n",
      "ep 3642: ep_len:660 episode reward: total was -32.270000. running mean: -5.472706\n",
      "ep 3642: ep_len:500 episode reward: total was -47.630000. running mean: -5.894279\n",
      "ep 3642: ep_len:3 episode reward: total was 0.000000. running mean: -5.835336\n",
      "ep 3642: ep_len:500 episode reward: total was -4.600000. running mean: -5.822982\n",
      "ep 3642: ep_len:585 episode reward: total was -13.570000. running mean: -5.900453\n",
      "epsilon:0.010000 episode_count: 25501. steps_count: 11265461.000000\n",
      "ep 3643: ep_len:211 episode reward: total was 2.590000. running mean: -5.815548\n",
      "ep 3643: ep_len:630 episode reward: total was -7.110000. running mean: -5.828493\n",
      "ep 3643: ep_len:550 episode reward: total was -2.250000. running mean: -5.792708\n",
      "ep 3643: ep_len:53 episode reward: total was -4.950000. running mean: -5.784281\n",
      "ep 3643: ep_len:3 episode reward: total was 0.000000. running mean: -5.726438\n",
      "ep 3643: ep_len:335 episode reward: total was -1.340000. running mean: -5.682573\n",
      "ep 3643: ep_len:605 episode reward: total was -7.060000. running mean: -5.696348\n",
      "epsilon:0.010000 episode_count: 25508. steps_count: 11267848.000000\n",
      "ep 3644: ep_len:555 episode reward: total was -6.660000. running mean: -5.705984\n",
      "ep 3644: ep_len:515 episode reward: total was -17.880000. running mean: -5.827724\n",
      "ep 3644: ep_len:66 episode reward: total was 1.030000. running mean: -5.759147\n",
      "ep 3644: ep_len:510 episode reward: total was -18.580000. running mean: -5.887356\n",
      "ep 3644: ep_len:3 episode reward: total was 0.000000. running mean: -5.828482\n",
      "ep 3644: ep_len:500 episode reward: total was -5.760000. running mean: -5.827797\n",
      "ep 3644: ep_len:630 episode reward: total was -10.070000. running mean: -5.870219\n",
      "epsilon:0.010000 episode_count: 25515. steps_count: 11270627.000000\n",
      "ep 3645: ep_len:565 episode reward: total was 14.450000. running mean: -5.667017\n",
      "ep 3645: ep_len:570 episode reward: total was -20.570000. running mean: -5.816047\n",
      "ep 3645: ep_len:580 episode reward: total was -3.200000. running mean: -5.789886\n",
      "ep 3645: ep_len:157 episode reward: total was 3.100000. running mean: -5.700988\n",
      "ep 3645: ep_len:97 episode reward: total was 5.040000. running mean: -5.593578\n",
      "ep 3645: ep_len:580 episode reward: total was -5.010000. running mean: -5.587742\n",
      "ep 3645: ep_len:650 episode reward: total was -23.340000. running mean: -5.765265\n",
      "epsilon:0.010000 episode_count: 25522. steps_count: 11273826.000000\n",
      "ep 3646: ep_len:510 episode reward: total was -9.810000. running mean: -5.805712\n",
      "ep 3646: ep_len:535 episode reward: total was 23.930000. running mean: -5.508355\n",
      "ep 3646: ep_len:610 episode reward: total was 4.380000. running mean: -5.409471\n",
      "ep 3646: ep_len:407 episode reward: total was 1.870000. running mean: -5.336676\n",
      "ep 3646: ep_len:126 episode reward: total was 7.060000. running mean: -5.212710\n",
      "ep 3646: ep_len:500 episode reward: total was -9.310000. running mean: -5.253683\n",
      "ep 3646: ep_len:555 episode reward: total was -10.900000. running mean: -5.310146\n",
      "epsilon:0.010000 episode_count: 25529. steps_count: 11277069.000000\n",
      "ep 3647: ep_len:520 episode reward: total was -5.160000. running mean: -5.308644\n",
      "ep 3647: ep_len:625 episode reward: total was -5.960000. running mean: -5.315158\n",
      "ep 3647: ep_len:540 episode reward: total was 3.390000. running mean: -5.228106\n",
      "ep 3647: ep_len:600 episode reward: total was 10.050000. running mean: -5.075325\n",
      "ep 3647: ep_len:3 episode reward: total was 0.000000. running mean: -5.024572\n",
      "ep 3647: ep_len:500 episode reward: total was 1.420000. running mean: -4.960126\n",
      "ep 3647: ep_len:580 episode reward: total was -14.020000. running mean: -5.050725\n",
      "epsilon:0.010000 episode_count: 25536. steps_count: 11280437.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3648: ep_len:232 episode reward: total was 2.640000. running mean: -4.973818\n",
      "ep 3648: ep_len:510 episode reward: total was 0.200000. running mean: -4.922080\n",
      "ep 3648: ep_len:500 episode reward: total was 6.870000. running mean: -4.804159\n",
      "ep 3648: ep_len:520 episode reward: total was -21.060000. running mean: -4.966717\n",
      "ep 3648: ep_len:122 episode reward: total was 6.560000. running mean: -4.851450\n",
      "ep 3648: ep_len:500 episode reward: total was -4.410000. running mean: -4.847036\n",
      "ep 3648: ep_len:500 episode reward: total was -46.190000. running mean: -5.260465\n",
      "epsilon:0.010000 episode_count: 25543. steps_count: 11283321.000000\n",
      "ep 3649: ep_len:128 episode reward: total was -4.940000. running mean: -5.257261\n",
      "ep 3649: ep_len:500 episode reward: total was 20.240000. running mean: -5.002288\n",
      "ep 3649: ep_len:71 episode reward: total was -1.990000. running mean: -4.972165\n",
      "ep 3649: ep_len:170 episode reward: total was 5.140000. running mean: -4.871043\n",
      "ep 3649: ep_len:3 episode reward: total was 0.000000. running mean: -4.822333\n",
      "ep 3649: ep_len:318 episode reward: total was 1.160000. running mean: -4.762510\n",
      "ep 3649: ep_len:500 episode reward: total was -8.260000. running mean: -4.797485\n",
      "epsilon:0.010000 episode_count: 25550. steps_count: 11285011.000000\n",
      "ep 3650: ep_len:525 episode reward: total was -1.270000. running mean: -4.762210\n",
      "ep 3650: ep_len:500 episode reward: total was -6.480000. running mean: -4.779388\n",
      "ep 3650: ep_len:545 episode reward: total was 1.140000. running mean: -4.720194\n",
      "ep 3650: ep_len:545 episode reward: total was 7.520000. running mean: -4.597792\n",
      "ep 3650: ep_len:3 episode reward: total was 0.000000. running mean: -4.551814\n",
      "ep 3650: ep_len:273 episode reward: total was 0.180000. running mean: -4.504496\n",
      "ep 3650: ep_len:580 episode reward: total was -10.820000. running mean: -4.567651\n",
      "epsilon:0.010000 episode_count: 25557. steps_count: 11287982.000000\n",
      "ep 3651: ep_len:540 episode reward: total was -0.400000. running mean: -4.525974\n",
      "ep 3651: ep_len:500 episode reward: total was 18.320000. running mean: -4.297515\n",
      "ep 3651: ep_len:560 episode reward: total was 0.360000. running mean: -4.250939\n",
      "ep 3651: ep_len:56 episode reward: total was 0.060000. running mean: -4.207830\n",
      "ep 3651: ep_len:83 episode reward: total was 4.520000. running mean: -4.120552\n",
      "ep 3651: ep_len:500 episode reward: total was -25.270000. running mean: -4.332046\n",
      "ep 3651: ep_len:191 episode reward: total was -6.390000. running mean: -4.352626\n",
      "epsilon:0.010000 episode_count: 25564. steps_count: 11290412.000000\n",
      "ep 3652: ep_len:117 episode reward: total was -8.970000. running mean: -4.398799\n",
      "ep 3652: ep_len:555 episode reward: total was -8.800000. running mean: -4.442811\n",
      "ep 3652: ep_len:560 episode reward: total was -9.740000. running mean: -4.495783\n",
      "ep 3652: ep_len:520 episode reward: total was -6.990000. running mean: -4.520726\n",
      "ep 3652: ep_len:120 episode reward: total was 7.540000. running mean: -4.400118\n",
      "ep 3652: ep_len:500 episode reward: total was -31.790000. running mean: -4.674017\n",
      "ep 3652: ep_len:550 episode reward: total was -10.090000. running mean: -4.728177\n",
      "epsilon:0.010000 episode_count: 25571. steps_count: 11293334.000000\n",
      "ep 3653: ep_len:615 episode reward: total was 5.990000. running mean: -4.620995\n",
      "ep 3653: ep_len:560 episode reward: total was -19.870000. running mean: -4.773485\n",
      "ep 3653: ep_len:1010 episode reward: total was -73.190000. running mean: -5.457650\n",
      "ep 3653: ep_len:56 episode reward: total was 0.550000. running mean: -5.397574\n",
      "ep 3653: ep_len:3 episode reward: total was 0.000000. running mean: -5.343598\n",
      "ep 3653: ep_len:510 episode reward: total was 9.460000. running mean: -5.195562\n",
      "ep 3653: ep_len:650 episode reward: total was -34.060000. running mean: -5.484206\n",
      "epsilon:0.010000 episode_count: 25578. steps_count: 11296738.000000\n",
      "ep 3654: ep_len:520 episode reward: total was -5.120000. running mean: -5.480564\n",
      "ep 3654: ep_len:500 episode reward: total was 27.230000. running mean: -5.153459\n",
      "ep 3654: ep_len:500 episode reward: total was -11.340000. running mean: -5.215324\n",
      "ep 3654: ep_len:500 episode reward: total was -17.090000. running mean: -5.334071\n",
      "ep 3654: ep_len:3 episode reward: total was 0.000000. running mean: -5.280730\n",
      "ep 3654: ep_len:650 episode reward: total was -2.690000. running mean: -5.254823\n",
      "ep 3654: ep_len:685 episode reward: total was -35.890000. running mean: -5.561175\n",
      "epsilon:0.010000 episode_count: 25585. steps_count: 11300096.000000\n",
      "ep 3655: ep_len:580 episode reward: total was 3.120000. running mean: -5.474363\n",
      "ep 3655: ep_len:565 episode reward: total was -1.350000. running mean: -5.433119\n",
      "ep 3655: ep_len:565 episode reward: total was 1.470000. running mean: -5.364088\n",
      "ep 3655: ep_len:595 episode reward: total was 1.370000. running mean: -5.296747\n",
      "ep 3655: ep_len:103 episode reward: total was 4.530000. running mean: -5.198480\n",
      "ep 3655: ep_len:590 episode reward: total was -2.640000. running mean: -5.172895\n",
      "ep 3655: ep_len:520 episode reward: total was -13.590000. running mean: -5.257066\n",
      "epsilon:0.010000 episode_count: 25592. steps_count: 11303614.000000\n",
      "ep 3656: ep_len:259 episode reward: total was -11.890000. running mean: -5.323395\n",
      "ep 3656: ep_len:605 episode reward: total was -50.520000. running mean: -5.775361\n",
      "ep 3656: ep_len:575 episode reward: total was -12.220000. running mean: -5.839808\n",
      "ep 3656: ep_len:49 episode reward: total was 1.540000. running mean: -5.766010\n",
      "ep 3656: ep_len:3 episode reward: total was 0.000000. running mean: -5.708350\n",
      "ep 3656: ep_len:500 episode reward: total was 0.800000. running mean: -5.643266\n",
      "ep 3656: ep_len:540 episode reward: total was -1.570000. running mean: -5.602533\n",
      "epsilon:0.010000 episode_count: 25599. steps_count: 11306145.000000\n",
      "ep 3657: ep_len:500 episode reward: total was 5.430000. running mean: -5.492208\n",
      "ep 3657: ep_len:500 episode reward: total was 14.700000. running mean: -5.290286\n",
      "ep 3657: ep_len:605 episode reward: total was -1.630000. running mean: -5.253683\n",
      "ep 3657: ep_len:540 episode reward: total was 10.480000. running mean: -5.096346\n",
      "ep 3657: ep_len:90 episode reward: total was 3.040000. running mean: -5.014983\n",
      "ep 3657: ep_len:505 episode reward: total was -10.700000. running mean: -5.071833\n",
      "ep 3657: ep_len:550 episode reward: total was -8.560000. running mean: -5.106715\n",
      "epsilon:0.010000 episode_count: 25606. steps_count: 11309435.000000\n",
      "ep 3658: ep_len:500 episode reward: total was 15.330000. running mean: -4.902348\n",
      "ep 3658: ep_len:535 episode reward: total was 23.920000. running mean: -4.614124\n",
      "ep 3658: ep_len:555 episode reward: total was -18.810000. running mean: -4.756083\n",
      "ep 3658: ep_len:545 episode reward: total was 10.990000. running mean: -4.598622\n",
      "ep 3658: ep_len:3 episode reward: total was 0.000000. running mean: -4.552636\n",
      "ep 3658: ep_len:550 episode reward: total was 8.680000. running mean: -4.420309\n",
      "ep 3658: ep_len:337 episode reward: total was -7.760000. running mean: -4.453706\n",
      "epsilon:0.010000 episode_count: 25613. steps_count: 11312460.000000\n",
      "ep 3659: ep_len:133 episode reward: total was 1.070000. running mean: -4.398469\n",
      "ep 3659: ep_len:530 episode reward: total was 22.940000. running mean: -4.125085\n",
      "ep 3659: ep_len:595 episode reward: total was 11.150000. running mean: -3.972334\n",
      "ep 3659: ep_len:625 episode reward: total was -1.470000. running mean: -3.947310\n",
      "ep 3659: ep_len:3 episode reward: total was 0.000000. running mean: -3.907837\n",
      "ep 3659: ep_len:635 episode reward: total was 11.540000. running mean: -3.753359\n",
      "ep 3659: ep_len:615 episode reward: total was -36.060000. running mean: -4.076425\n",
      "epsilon:0.010000 episode_count: 25620. steps_count: 11315596.000000\n",
      "ep 3660: ep_len:555 episode reward: total was -30.210000. running mean: -4.337761\n",
      "ep 3660: ep_len:540 episode reward: total was -1.530000. running mean: -4.309683\n",
      "ep 3660: ep_len:429 episode reward: total was -0.250000. running mean: -4.269087\n",
      "ep 3660: ep_len:575 episode reward: total was -2.520000. running mean: -4.251596\n",
      "ep 3660: ep_len:3 episode reward: total was 0.000000. running mean: -4.209080\n",
      "ep 3660: ep_len:500 episode reward: total was 12.060000. running mean: -4.046389\n",
      "ep 3660: ep_len:211 episode reward: total was 0.690000. running mean: -3.999025\n",
      "epsilon:0.010000 episode_count: 25627. steps_count: 11318409.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3661: ep_len:500 episode reward: total was 7.310000. running mean: -3.885935\n",
      "ep 3661: ep_len:500 episode reward: total was 15.710000. running mean: -3.689976\n",
      "ep 3661: ep_len:650 episode reward: total was -16.740000. running mean: -3.820476\n",
      "ep 3661: ep_len:535 episode reward: total was 3.520000. running mean: -3.747071\n",
      "ep 3661: ep_len:3 episode reward: total was 0.000000. running mean: -3.709600\n",
      "ep 3661: ep_len:580 episode reward: total was -0.040000. running mean: -3.672904\n",
      "ep 3661: ep_len:520 episode reward: total was -10.390000. running mean: -3.740075\n",
      "epsilon:0.010000 episode_count: 25634. steps_count: 11321697.000000\n",
      "ep 3662: ep_len:500 episode reward: total was -1.790000. running mean: -3.720575\n",
      "ep 3662: ep_len:348 episode reward: total was -15.860000. running mean: -3.841969\n",
      "ep 3662: ep_len:585 episode reward: total was -0.200000. running mean: -3.805549\n",
      "ep 3662: ep_len:525 episode reward: total was 3.910000. running mean: -3.728394\n",
      "ep 3662: ep_len:3 episode reward: total was 0.000000. running mean: -3.691110\n",
      "ep 3662: ep_len:237 episode reward: total was 6.160000. running mean: -3.592599\n",
      "ep 3662: ep_len:281 episode reward: total was 1.700000. running mean: -3.539673\n",
      "epsilon:0.010000 episode_count: 25641. steps_count: 11324176.000000\n",
      "ep 3663: ep_len:590 episode reward: total was -2.380000. running mean: -3.528076\n",
      "ep 3663: ep_len:185 episode reward: total was -5.860000. running mean: -3.551395\n",
      "ep 3663: ep_len:630 episode reward: total was -17.680000. running mean: -3.692681\n",
      "ep 3663: ep_len:505 episode reward: total was 4.970000. running mean: -3.606054\n",
      "ep 3663: ep_len:3 episode reward: total was 0.000000. running mean: -3.569994\n",
      "ep 3663: ep_len:600 episode reward: total was 1.980000. running mean: -3.514494\n",
      "ep 3663: ep_len:333 episode reward: total was -9.760000. running mean: -3.576949\n",
      "epsilon:0.010000 episode_count: 25648. steps_count: 11327022.000000\n",
      "ep 3664: ep_len:560 episode reward: total was -2.060000. running mean: -3.561779\n",
      "ep 3664: ep_len:525 episode reward: total was 3.910000. running mean: -3.487062\n",
      "ep 3664: ep_len:500 episode reward: total was 7.420000. running mean: -3.377991\n",
      "ep 3664: ep_len:111 episode reward: total was 4.590000. running mean: -3.298311\n",
      "ep 3664: ep_len:3 episode reward: total was 0.000000. running mean: -3.265328\n",
      "ep 3664: ep_len:225 episode reward: total was 3.640000. running mean: -3.196275\n",
      "ep 3664: ep_len:500 episode reward: total was -20.590000. running mean: -3.370212\n",
      "epsilon:0.010000 episode_count: 25655. steps_count: 11329446.000000\n",
      "ep 3665: ep_len:500 episode reward: total was 5.920000. running mean: -3.277310\n",
      "ep 3665: ep_len:500 episode reward: total was -1.180000. running mean: -3.256337\n",
      "ep 3665: ep_len:500 episode reward: total was -5.440000. running mean: -3.278173\n",
      "ep 3665: ep_len:555 episode reward: total was 16.050000. running mean: -3.084892\n",
      "ep 3665: ep_len:113 episode reward: total was 6.540000. running mean: -2.988643\n",
      "ep 3665: ep_len:570 episode reward: total was -20.880000. running mean: -3.167556\n",
      "ep 3665: ep_len:500 episode reward: total was -8.570000. running mean: -3.221581\n",
      "epsilon:0.010000 episode_count: 25662. steps_count: 11332684.000000\n",
      "ep 3666: ep_len:126 episode reward: total was 0.050000. running mean: -3.188865\n",
      "ep 3666: ep_len:500 episode reward: total was 11.180000. running mean: -3.045176\n",
      "ep 3666: ep_len:640 episode reward: total was 6.180000. running mean: -2.952925\n",
      "ep 3666: ep_len:500 episode reward: total was -12.030000. running mean: -3.043695\n",
      "ep 3666: ep_len:105 episode reward: total was 6.040000. running mean: -2.952858\n",
      "ep 3666: ep_len:695 episode reward: total was 10.500000. running mean: -2.818330\n",
      "ep 3666: ep_len:590 episode reward: total was -9.580000. running mean: -2.885946\n",
      "epsilon:0.010000 episode_count: 25669. steps_count: 11335840.000000\n",
      "ep 3667: ep_len:535 episode reward: total was -9.900000. running mean: -2.956087\n",
      "ep 3667: ep_len:189 episode reward: total was -7.930000. running mean: -3.005826\n",
      "ep 3667: ep_len:685 episode reward: total was -10.640000. running mean: -3.082168\n",
      "ep 3667: ep_len:170 episode reward: total was 3.120000. running mean: -3.020146\n",
      "ep 3667: ep_len:3 episode reward: total was 0.000000. running mean: -2.989945\n",
      "ep 3667: ep_len:600 episode reward: total was 8.140000. running mean: -2.878645\n",
      "ep 3667: ep_len:505 episode reward: total was -16.540000. running mean: -3.015259\n",
      "epsilon:0.010000 episode_count: 25676. steps_count: 11338527.000000\n",
      "ep 3668: ep_len:645 episode reward: total was -14.240000. running mean: -3.127506\n",
      "ep 3668: ep_len:615 episode reward: total was 3.930000. running mean: -3.056931\n",
      "ep 3668: ep_len:605 episode reward: total was -13.060000. running mean: -3.156962\n",
      "ep 3668: ep_len:550 episode reward: total was -20.050000. running mean: -3.325892\n",
      "ep 3668: ep_len:114 episode reward: total was 8.530000. running mean: -3.207333\n",
      "ep 3668: ep_len:620 episode reward: total was -4.050000. running mean: -3.215760\n",
      "ep 3668: ep_len:500 episode reward: total was -12.540000. running mean: -3.309002\n",
      "epsilon:0.010000 episode_count: 25683. steps_count: 11342176.000000\n",
      "ep 3669: ep_len:251 episode reward: total was 2.120000. running mean: -3.254712\n",
      "ep 3669: ep_len:590 episode reward: total was -4.120000. running mean: -3.263365\n",
      "ep 3669: ep_len:720 episode reward: total was -9.700000. running mean: -3.327732\n",
      "ep 3669: ep_len:500 episode reward: total was -8.020000. running mean: -3.374654\n",
      "ep 3669: ep_len:3 episode reward: total was 0.000000. running mean: -3.340908\n",
      "ep 3669: ep_len:500 episode reward: total was -2.240000. running mean: -3.329899\n",
      "ep 3669: ep_len:510 episode reward: total was -25.440000. running mean: -3.551000\n",
      "epsilon:0.010000 episode_count: 25690. steps_count: 11345250.000000\n",
      "ep 3670: ep_len:615 episode reward: total was -0.890000. running mean: -3.524390\n",
      "ep 3670: ep_len:510 episode reward: total was -14.820000. running mean: -3.637346\n",
      "ep 3670: ep_len:905 episode reward: total was -80.270000. running mean: -4.403672\n",
      "ep 3670: ep_len:500 episode reward: total was -24.110000. running mean: -4.600736\n",
      "ep 3670: ep_len:109 episode reward: total was 7.540000. running mean: -4.479328\n",
      "ep 3670: ep_len:545 episode reward: total was -6.220000. running mean: -4.496735\n",
      "ep 3670: ep_len:515 episode reward: total was -1.590000. running mean: -4.467668\n",
      "epsilon:0.010000 episode_count: 25697. steps_count: 11348949.000000\n",
      "ep 3671: ep_len:610 episode reward: total was -1.420000. running mean: -4.437191\n",
      "ep 3671: ep_len:520 episode reward: total was 8.240000. running mean: -4.310419\n",
      "ep 3671: ep_len:675 episode reward: total was -4.720000. running mean: -4.314515\n",
      "ep 3671: ep_len:639 episode reward: total was -22.350000. running mean: -4.494870\n",
      "ep 3671: ep_len:3 episode reward: total was 0.000000. running mean: -4.449921\n",
      "ep 3671: ep_len:585 episode reward: total was -4.230000. running mean: -4.447722\n",
      "ep 3671: ep_len:615 episode reward: total was -3.060000. running mean: -4.433845\n",
      "epsilon:0.010000 episode_count: 25704. steps_count: 11352596.000000\n",
      "ep 3672: ep_len:540 episode reward: total was -4.990000. running mean: -4.439406\n",
      "ep 3672: ep_len:590 episode reward: total was -7.150000. running mean: -4.466512\n",
      "ep 3672: ep_len:510 episode reward: total was -5.620000. running mean: -4.478047\n",
      "ep 3672: ep_len:500 episode reward: total was -13.550000. running mean: -4.568766\n",
      "ep 3672: ep_len:3 episode reward: total was 0.000000. running mean: -4.523079\n",
      "ep 3672: ep_len:170 episode reward: total was 5.630000. running mean: -4.421548\n",
      "ep 3672: ep_len:500 episode reward: total was -7.460000. running mean: -4.451933\n",
      "epsilon:0.010000 episode_count: 25711. steps_count: 11355409.000000\n",
      "ep 3673: ep_len:750 episode reward: total was -18.650000. running mean: -4.593913\n",
      "ep 3673: ep_len:565 episode reward: total was -0.810000. running mean: -4.556074\n",
      "ep 3673: ep_len:610 episode reward: total was -14.970000. running mean: -4.660213\n",
      "ep 3673: ep_len:500 episode reward: total was 7.400000. running mean: -4.539611\n",
      "ep 3673: ep_len:3 episode reward: total was 0.000000. running mean: -4.494215\n",
      "ep 3673: ep_len:605 episode reward: total was -31.920000. running mean: -4.768473\n",
      "ep 3673: ep_len:515 episode reward: total was -11.330000. running mean: -4.834088\n",
      "epsilon:0.010000 episode_count: 25718. steps_count: 11358957.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3674: ep_len:645 episode reward: total was -8.150000. running mean: -4.867247\n",
      "ep 3674: ep_len:500 episode reward: total was 5.600000. running mean: -4.762575\n",
      "ep 3674: ep_len:580 episode reward: total was -19.800000. running mean: -4.912949\n",
      "ep 3674: ep_len:590 episode reward: total was 9.920000. running mean: -4.764620\n",
      "ep 3674: ep_len:3 episode reward: total was 0.000000. running mean: -4.716973\n",
      "ep 3674: ep_len:500 episode reward: total was -9.740000. running mean: -4.767204\n",
      "ep 3674: ep_len:545 episode reward: total was -19.530000. running mean: -4.914832\n",
      "epsilon:0.010000 episode_count: 25725. steps_count: 11362320.000000\n",
      "ep 3675: ep_len:690 episode reward: total was -13.660000. running mean: -5.002283\n",
      "ep 3675: ep_len:500 episode reward: total was -21.570000. running mean: -5.167960\n",
      "ep 3675: ep_len:500 episode reward: total was -11.840000. running mean: -5.234681\n",
      "ep 3675: ep_len:645 episode reward: total was -32.890000. running mean: -5.511234\n",
      "ep 3675: ep_len:91 episode reward: total was -1.480000. running mean: -5.470922\n",
      "ep 3675: ep_len:530 episode reward: total was 12.220000. running mean: -5.294013\n",
      "ep 3675: ep_len:525 episode reward: total was -14.920000. running mean: -5.390272\n",
      "epsilon:0.010000 episode_count: 25732. steps_count: 11365801.000000\n",
      "ep 3676: ep_len:116 episode reward: total was 2.090000. running mean: -5.315470\n",
      "ep 3676: ep_len:585 episode reward: total was 8.600000. running mean: -5.176315\n",
      "ep 3676: ep_len:635 episode reward: total was -4.740000. running mean: -5.171952\n",
      "ep 3676: ep_len:32 episode reward: total was 2.030000. running mean: -5.099932\n",
      "ep 3676: ep_len:86 episode reward: total was -11.950000. running mean: -5.168433\n",
      "ep 3676: ep_len:500 episode reward: total was -1.220000. running mean: -5.128949\n",
      "ep 3676: ep_len:510 episode reward: total was -9.980000. running mean: -5.177459\n",
      "epsilon:0.010000 episode_count: 25739. steps_count: 11368265.000000\n",
      "ep 3677: ep_len:590 episode reward: total was 3.100000. running mean: -5.094685\n",
      "ep 3677: ep_len:318 episode reward: total was -6.820000. running mean: -5.111938\n",
      "ep 3677: ep_len:565 episode reward: total was 6.460000. running mean: -4.996218\n",
      "ep 3677: ep_len:520 episode reward: total was -14.520000. running mean: -5.091456\n",
      "ep 3677: ep_len:3 episode reward: total was 0.000000. running mean: -5.040542\n",
      "ep 3677: ep_len:252 episode reward: total was 6.160000. running mean: -4.928536\n",
      "ep 3677: ep_len:500 episode reward: total was -29.170000. running mean: -5.170951\n",
      "epsilon:0.010000 episode_count: 25746. steps_count: 11371013.000000\n",
      "ep 3678: ep_len:565 episode reward: total was 3.580000. running mean: -5.083441\n",
      "ep 3678: ep_len:250 episode reward: total was -30.340000. running mean: -5.336007\n",
      "ep 3678: ep_len:535 episode reward: total was -2.580000. running mean: -5.308447\n",
      "ep 3678: ep_len:580 episode reward: total was 8.950000. running mean: -5.165862\n",
      "ep 3678: ep_len:3 episode reward: total was 0.000000. running mean: -5.114204\n",
      "ep 3678: ep_len:580 episode reward: total was 4.610000. running mean: -5.016962\n",
      "ep 3678: ep_len:625 episode reward: total was -4.210000. running mean: -5.008892\n",
      "epsilon:0.010000 episode_count: 25753. steps_count: 11374151.000000\n",
      "ep 3679: ep_len:715 episode reward: total was -20.100000. running mean: -5.159803\n",
      "ep 3679: ep_len:515 episode reward: total was -26.520000. running mean: -5.373405\n",
      "ep 3679: ep_len:525 episode reward: total was 0.960000. running mean: -5.310071\n",
      "ep 3679: ep_len:390 episode reward: total was -27.150000. running mean: -5.528470\n",
      "ep 3679: ep_len:102 episode reward: total was 4.530000. running mean: -5.427886\n",
      "ep 3679: ep_len:520 episode reward: total was -55.590000. running mean: -5.929507\n",
      "ep 3679: ep_len:535 episode reward: total was -4.010000. running mean: -5.910312\n",
      "epsilon:0.010000 episode_count: 25760. steps_count: 11377453.000000\n",
      "ep 3680: ep_len:241 episode reward: total was -21.840000. running mean: -6.069609\n",
      "ep 3680: ep_len:575 episode reward: total was -2.130000. running mean: -6.030213\n",
      "ep 3680: ep_len:590 episode reward: total was -10.400000. running mean: -6.073910\n",
      "ep 3680: ep_len:624 episode reward: total was -40.470000. running mean: -6.417871\n",
      "ep 3680: ep_len:3 episode reward: total was 0.000000. running mean: -6.353693\n",
      "ep 3680: ep_len:540 episode reward: total was 3.430000. running mean: -6.255856\n",
      "ep 3680: ep_len:510 episode reward: total was -35.220000. running mean: -6.545497\n",
      "epsilon:0.010000 episode_count: 25767. steps_count: 11380536.000000\n",
      "ep 3681: ep_len:515 episode reward: total was -0.450000. running mean: -6.484542\n",
      "ep 3681: ep_len:277 episode reward: total was -10.370000. running mean: -6.523397\n",
      "ep 3681: ep_len:79 episode reward: total was 0.040000. running mean: -6.457763\n",
      "ep 3681: ep_len:505 episode reward: total was -18.590000. running mean: -6.579085\n",
      "ep 3681: ep_len:3 episode reward: total was 0.000000. running mean: -6.513294\n",
      "ep 3681: ep_len:530 episode reward: total was -8.560000. running mean: -6.533761\n",
      "ep 3681: ep_len:545 episode reward: total was -26.130000. running mean: -6.729724\n",
      "epsilon:0.010000 episode_count: 25774. steps_count: 11382990.000000\n",
      "ep 3682: ep_len:500 episode reward: total was 6.410000. running mean: -6.598326\n",
      "ep 3682: ep_len:500 episode reward: total was 3.620000. running mean: -6.496143\n",
      "ep 3682: ep_len:500 episode reward: total was -32.480000. running mean: -6.755982\n",
      "ep 3682: ep_len:56 episode reward: total was -2.950000. running mean: -6.717922\n",
      "ep 3682: ep_len:3 episode reward: total was 0.000000. running mean: -6.650743\n",
      "ep 3682: ep_len:550 episode reward: total was -19.740000. running mean: -6.781635\n",
      "ep 3682: ep_len:299 episode reward: total was -8.370000. running mean: -6.797519\n",
      "epsilon:0.010000 episode_count: 25781. steps_count: 11385398.000000\n",
      "ep 3683: ep_len:120 episode reward: total was -1.950000. running mean: -6.749044\n",
      "ep 3683: ep_len:505 episode reward: total was -2.840000. running mean: -6.709953\n",
      "ep 3683: ep_len:335 episode reward: total was 6.170000. running mean: -6.581154\n",
      "ep 3683: ep_len:605 episode reward: total was 2.530000. running mean: -6.490042\n",
      "ep 3683: ep_len:86 episode reward: total was 1.500000. running mean: -6.410142\n",
      "ep 3683: ep_len:160 episode reward: total was 3.090000. running mean: -6.315140\n",
      "ep 3683: ep_len:148 episode reward: total was -4.400000. running mean: -6.295989\n",
      "epsilon:0.010000 episode_count: 25788. steps_count: 11387357.000000\n",
      "ep 3684: ep_len:570 episode reward: total was -54.250000. running mean: -6.775529\n",
      "ep 3684: ep_len:680 episode reward: total was -37.380000. running mean: -7.081574\n",
      "ep 3684: ep_len:610 episode reward: total was -17.460000. running mean: -7.185358\n",
      "ep 3684: ep_len:525 episode reward: total was -20.110000. running mean: -7.314605\n",
      "ep 3684: ep_len:3 episode reward: total was 0.000000. running mean: -7.241458\n",
      "ep 3684: ep_len:535 episode reward: total was -48.610000. running mean: -7.655144\n",
      "ep 3684: ep_len:505 episode reward: total was -17.900000. running mean: -7.757592\n",
      "epsilon:0.010000 episode_count: 25795. steps_count: 11390785.000000\n",
      "ep 3685: ep_len:500 episode reward: total was 0.740000. running mean: -7.672617\n",
      "ep 3685: ep_len:630 episode reward: total was 9.050000. running mean: -7.505390\n",
      "ep 3685: ep_len:570 episode reward: total was -11.030000. running mean: -7.540636\n",
      "ep 3685: ep_len:515 episode reward: total was -6.470000. running mean: -7.529930\n",
      "ep 3685: ep_len:3 episode reward: total was 0.000000. running mean: -7.454631\n",
      "ep 3685: ep_len:625 episode reward: total was -14.910000. running mean: -7.529184\n",
      "ep 3685: ep_len:620 episode reward: total was -9.050000. running mean: -7.544393\n",
      "epsilon:0.010000 episode_count: 25802. steps_count: 11394248.000000\n",
      "ep 3686: ep_len:500 episode reward: total was 12.310000. running mean: -7.345849\n",
      "ep 3686: ep_len:1000 episode reward: total was -73.090000. running mean: -8.003290\n",
      "ep 3686: ep_len:630 episode reward: total was -0.380000. running mean: -7.927057\n",
      "ep 3686: ep_len:505 episode reward: total was -51.700000. running mean: -8.364787\n",
      "ep 3686: ep_len:3 episode reward: total was 0.000000. running mean: -8.281139\n",
      "ep 3686: ep_len:500 episode reward: total was -7.400000. running mean: -8.272327\n",
      "ep 3686: ep_len:545 episode reward: total was -10.560000. running mean: -8.295204\n",
      "epsilon:0.010000 episode_count: 25809. steps_count: 11397931.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3687: ep_len:575 episode reward: total was 2.590000. running mean: -8.186352\n",
      "ep 3687: ep_len:187 episode reward: total was -7.410000. running mean: -8.178589\n",
      "ep 3687: ep_len:640 episode reward: total was -0.450000. running mean: -8.101303\n",
      "ep 3687: ep_len:530 episode reward: total was 11.440000. running mean: -7.905890\n",
      "ep 3687: ep_len:3 episode reward: total was 0.000000. running mean: -7.826831\n",
      "ep 3687: ep_len:500 episode reward: total was -9.220000. running mean: -7.840763\n",
      "ep 3687: ep_len:575 episode reward: total was -6.030000. running mean: -7.822655\n",
      "epsilon:0.010000 episode_count: 25816. steps_count: 11400941.000000\n",
      "ep 3688: ep_len:535 episode reward: total was -16.800000. running mean: -7.912428\n",
      "ep 3688: ep_len:276 episode reward: total was 0.200000. running mean: -7.831304\n",
      "ep 3688: ep_len:655 episode reward: total was -36.810000. running mean: -8.121091\n",
      "ep 3688: ep_len:545 episode reward: total was -19.030000. running mean: -8.230180\n",
      "ep 3688: ep_len:79 episode reward: total was -9.950000. running mean: -8.247378\n",
      "ep 3688: ep_len:650 episode reward: total was -16.210000. running mean: -8.327005\n",
      "ep 3688: ep_len:705 episode reward: total was -38.860000. running mean: -8.632334\n",
      "epsilon:0.010000 episode_count: 25823. steps_count: 11404386.000000\n",
      "ep 3689: ep_len:555 episode reward: total was 0.610000. running mean: -8.539911\n",
      "ep 3689: ep_len:540 episode reward: total was 21.420000. running mean: -8.240312\n",
      "ep 3689: ep_len:620 episode reward: total was 1.180000. running mean: -8.146109\n",
      "ep 3689: ep_len:500 episode reward: total was -9.150000. running mean: -8.156148\n",
      "ep 3689: ep_len:129 episode reward: total was 6.050000. running mean: -8.014086\n",
      "ep 3689: ep_len:545 episode reward: total was -7.050000. running mean: -8.004445\n",
      "ep 3689: ep_len:280 episode reward: total was -0.810000. running mean: -7.932501\n",
      "epsilon:0.010000 episode_count: 25830. steps_count: 11407555.000000\n",
      "ep 3690: ep_len:530 episode reward: total was 11.930000. running mean: -7.733876\n",
      "ep 3690: ep_len:555 episode reward: total was -8.830000. running mean: -7.744837\n",
      "ep 3690: ep_len:540 episode reward: total was -2.680000. running mean: -7.694189\n",
      "ep 3690: ep_len:522 episode reward: total was 5.080000. running mean: -7.566447\n",
      "ep 3690: ep_len:3 episode reward: total was 0.000000. running mean: -7.490783\n",
      "ep 3690: ep_len:655 episode reward: total was 4.390000. running mean: -7.371975\n",
      "ep 3690: ep_len:530 episode reward: total was -18.950000. running mean: -7.487755\n",
      "epsilon:0.010000 episode_count: 25837. steps_count: 11410890.000000\n",
      "ep 3691: ep_len:615 episode reward: total was 12.970000. running mean: -7.283177\n",
      "ep 3691: ep_len:535 episode reward: total was -19.080000. running mean: -7.401146\n",
      "ep 3691: ep_len:655 episode reward: total was -2.130000. running mean: -7.348434\n",
      "ep 3691: ep_len:500 episode reward: total was -19.120000. running mean: -7.466150\n",
      "ep 3691: ep_len:3 episode reward: total was 0.000000. running mean: -7.391488\n",
      "ep 3691: ep_len:500 episode reward: total was 3.900000. running mean: -7.278573\n",
      "ep 3691: ep_len:775 episode reward: total was -51.290000. running mean: -7.718688\n",
      "epsilon:0.010000 episode_count: 25844. steps_count: 11414473.000000\n",
      "ep 3692: ep_len:545 episode reward: total was 2.610000. running mean: -7.615401\n",
      "ep 3692: ep_len:600 episode reward: total was 5.230000. running mean: -7.486947\n",
      "ep 3692: ep_len:540 episode reward: total was -25.390000. running mean: -7.665977\n",
      "ep 3692: ep_len:620 episode reward: total was -10.950000. running mean: -7.698818\n",
      "ep 3692: ep_len:3 episode reward: total was 0.000000. running mean: -7.621829\n",
      "ep 3692: ep_len:149 episode reward: total was 3.580000. running mean: -7.509811\n",
      "ep 3692: ep_len:595 episode reward: total was -3.920000. running mean: -7.473913\n",
      "epsilon:0.010000 episode_count: 25851. steps_count: 11417525.000000\n",
      "ep 3693: ep_len:515 episode reward: total was -10.910000. running mean: -7.508274\n",
      "ep 3693: ep_len:500 episode reward: total was 3.070000. running mean: -7.402491\n",
      "ep 3693: ep_len:397 episode reward: total was 4.210000. running mean: -7.286366\n",
      "ep 3693: ep_len:600 episode reward: total was -27.080000. running mean: -7.484303\n",
      "ep 3693: ep_len:106 episode reward: total was -9.950000. running mean: -7.508960\n",
      "ep 3693: ep_len:620 episode reward: total was 7.350000. running mean: -7.360370\n",
      "ep 3693: ep_len:615 episode reward: total was -4.260000. running mean: -7.329366\n",
      "epsilon:0.010000 episode_count: 25858. steps_count: 11420878.000000\n",
      "ep 3694: ep_len:258 episode reward: total was 0.660000. running mean: -7.249473\n",
      "ep 3694: ep_len:670 episode reward: total was -16.300000. running mean: -7.339978\n",
      "ep 3694: ep_len:625 episode reward: total was 4.650000. running mean: -7.220078\n",
      "ep 3694: ep_len:170 episode reward: total was 4.150000. running mean: -7.106377\n",
      "ep 3694: ep_len:100 episode reward: total was 4.530000. running mean: -6.990014\n",
      "ep 3694: ep_len:670 episode reward: total was -6.170000. running mean: -6.981813\n",
      "ep 3694: ep_len:185 episode reward: total was -8.910000. running mean: -7.001095\n",
      "epsilon:0.010000 episode_count: 25865. steps_count: 11423556.000000\n",
      "ep 3695: ep_len:575 episode reward: total was -53.720000. running mean: -7.468284\n",
      "ep 3695: ep_len:500 episode reward: total was 1.180000. running mean: -7.381801\n",
      "ep 3695: ep_len:700 episode reward: total was -49.300000. running mean: -7.800983\n",
      "ep 3695: ep_len:580 episode reward: total was 3.060000. running mean: -7.692374\n",
      "ep 3695: ep_len:47 episode reward: total was 4.500000. running mean: -7.570450\n",
      "ep 3695: ep_len:505 episode reward: total was 8.510000. running mean: -7.409645\n",
      "ep 3695: ep_len:535 episode reward: total was -13.550000. running mean: -7.471049\n",
      "epsilon:0.010000 episode_count: 25872. steps_count: 11426998.000000\n",
      "ep 3696: ep_len:500 episode reward: total was 10.770000. running mean: -7.288638\n",
      "ep 3696: ep_len:650 episode reward: total was 11.610000. running mean: -7.099652\n",
      "ep 3696: ep_len:550 episode reward: total was -0.150000. running mean: -7.030156\n",
      "ep 3696: ep_len:500 episode reward: total was -12.570000. running mean: -7.085554\n",
      "ep 3696: ep_len:3 episode reward: total was 0.000000. running mean: -7.014698\n",
      "ep 3696: ep_len:535 episode reward: total was -40.530000. running mean: -7.349851\n",
      "ep 3696: ep_len:565 episode reward: total was -14.560000. running mean: -7.421953\n",
      "epsilon:0.010000 episode_count: 25879. steps_count: 11430301.000000\n",
      "ep 3697: ep_len:585 episode reward: total was -16.630000. running mean: -7.514033\n",
      "ep 3697: ep_len:530 episode reward: total was -11.410000. running mean: -7.552993\n",
      "ep 3697: ep_len:580 episode reward: total was 0.310000. running mean: -7.474363\n",
      "ep 3697: ep_len:154 episode reward: total was 1.110000. running mean: -7.388520\n",
      "ep 3697: ep_len:3 episode reward: total was 0.000000. running mean: -7.314634\n",
      "ep 3697: ep_len:510 episode reward: total was -10.090000. running mean: -7.342388\n",
      "ep 3697: ep_len:580 episode reward: total was 0.990000. running mean: -7.259064\n",
      "epsilon:0.010000 episode_count: 25886. steps_count: 11433243.000000\n",
      "ep 3698: ep_len:685 episode reward: total was -0.570000. running mean: -7.192173\n",
      "ep 3698: ep_len:595 episode reward: total was -14.800000. running mean: -7.268252\n",
      "ep 3698: ep_len:540 episode reward: total was -4.410000. running mean: -7.239669\n",
      "ep 3698: ep_len:545 episode reward: total was 13.460000. running mean: -7.032673\n",
      "ep 3698: ep_len:3 episode reward: total was 0.000000. running mean: -6.962346\n",
      "ep 3698: ep_len:655 episode reward: total was -25.690000. running mean: -7.149622\n",
      "ep 3698: ep_len:545 episode reward: total was -11.050000. running mean: -7.188626\n",
      "epsilon:0.010000 episode_count: 25893. steps_count: 11436811.000000\n",
      "ep 3699: ep_len:535 episode reward: total was -20.950000. running mean: -7.326240\n",
      "ep 3699: ep_len:605 episode reward: total was 18.380000. running mean: -7.069177\n",
      "ep 3699: ep_len:575 episode reward: total was -2.750000. running mean: -7.025986\n",
      "ep 3699: ep_len:505 episode reward: total was -9.670000. running mean: -7.052426\n",
      "ep 3699: ep_len:3 episode reward: total was 0.000000. running mean: -6.981902\n",
      "ep 3699: ep_len:505 episode reward: total was -11.780000. running mean: -7.029883\n",
      "ep 3699: ep_len:730 episode reward: total was -52.360000. running mean: -7.483184\n",
      "epsilon:0.010000 episode_count: 25900. steps_count: 11440269.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3700: ep_len:615 episode reward: total was -8.700000. running mean: -7.495352\n",
      "ep 3700: ep_len:555 episode reward: total was -15.350000. running mean: -7.573898\n",
      "ep 3700: ep_len:875 episode reward: total was -158.950000. running mean: -9.087659\n",
      "ep 3700: ep_len:56 episode reward: total was 2.570000. running mean: -8.971083\n",
      "ep 3700: ep_len:3 episode reward: total was 0.000000. running mean: -8.881372\n",
      "ep 3700: ep_len:580 episode reward: total was -2.990000. running mean: -8.822458\n",
      "ep 3700: ep_len:600 episode reward: total was -28.840000. running mean: -9.022634\n",
      "epsilon:0.010000 episode_count: 25907. steps_count: 11443553.000000\n",
      "ep 3701: ep_len:615 episode reward: total was -6.480000. running mean: -8.997207\n",
      "ep 3701: ep_len:515 episode reward: total was 14.720000. running mean: -8.760035\n",
      "ep 3701: ep_len:500 episode reward: total was 3.060000. running mean: -8.641835\n",
      "ep 3701: ep_len:159 episode reward: total was 3.060000. running mean: -8.524817\n",
      "ep 3701: ep_len:84 episode reward: total was 6.020000. running mean: -8.379368\n",
      "ep 3701: ep_len:575 episode reward: total was 7.520000. running mean: -8.220375\n",
      "ep 3701: ep_len:500 episode reward: total was -17.010000. running mean: -8.308271\n",
      "epsilon:0.010000 episode_count: 25914. steps_count: 11446501.000000\n",
      "ep 3702: ep_len:122 episode reward: total was -3.480000. running mean: -8.259988\n",
      "ep 3702: ep_len:680 episode reward: total was -30.360000. running mean: -8.480988\n",
      "ep 3702: ep_len:426 episode reward: total was -15.800000. running mean: -8.554178\n",
      "ep 3702: ep_len:515 episode reward: total was -1.020000. running mean: -8.478837\n",
      "ep 3702: ep_len:107 episode reward: total was 7.050000. running mean: -8.323548\n",
      "ep 3702: ep_len:535 episode reward: total was -12.330000. running mean: -8.363613\n",
      "ep 3702: ep_len:625 episode reward: total was -96.350000. running mean: -9.243477\n",
      "epsilon:0.010000 episode_count: 25921. steps_count: 11449511.000000\n",
      "ep 3703: ep_len:575 episode reward: total was 0.890000. running mean: -9.142142\n",
      "ep 3703: ep_len:500 episode reward: total was 4.630000. running mean: -9.004421\n",
      "ep 3703: ep_len:610 episode reward: total was -10.680000. running mean: -9.021176\n",
      "ep 3703: ep_len:56 episode reward: total was 1.560000. running mean: -8.915365\n",
      "ep 3703: ep_len:3 episode reward: total was 0.000000. running mean: -8.826211\n",
      "ep 3703: ep_len:530 episode reward: total was -37.060000. running mean: -9.108549\n",
      "ep 3703: ep_len:585 episode reward: total was -5.600000. running mean: -9.073463\n",
      "epsilon:0.010000 episode_count: 25928. steps_count: 11452370.000000\n",
      "ep 3704: ep_len:134 episode reward: total was -14.940000. running mean: -9.132129\n",
      "ep 3704: ep_len:565 episode reward: total was 9.620000. running mean: -8.944607\n",
      "ep 3704: ep_len:555 episode reward: total was -12.340000. running mean: -8.978561\n",
      "ep 3704: ep_len:500 episode reward: total was 8.520000. running mean: -8.803576\n",
      "ep 3704: ep_len:38 episode reward: total was 3.500000. running mean: -8.680540\n",
      "ep 3704: ep_len:535 episode reward: total was -8.060000. running mean: -8.674335\n",
      "ep 3704: ep_len:500 episode reward: total was -10.410000. running mean: -8.691691\n",
      "epsilon:0.010000 episode_count: 25935. steps_count: 11455197.000000\n",
      "ep 3705: ep_len:500 episode reward: total was 7.770000. running mean: -8.527074\n",
      "ep 3705: ep_len:640 episode reward: total was 13.050000. running mean: -8.311304\n",
      "ep 3705: ep_len:625 episode reward: total was 4.970000. running mean: -8.178491\n",
      "ep 3705: ep_len:158 episode reward: total was 8.630000. running mean: -8.010406\n",
      "ep 3705: ep_len:3 episode reward: total was 0.000000. running mean: -7.930302\n",
      "ep 3705: ep_len:286 episode reward: total was -2.880000. running mean: -7.879799\n",
      "ep 3705: ep_len:530 episode reward: total was -19.590000. running mean: -7.996901\n",
      "epsilon:0.010000 episode_count: 25942. steps_count: 11457939.000000\n",
      "ep 3706: ep_len:605 episode reward: total was 11.010000. running mean: -7.806832\n",
      "ep 3706: ep_len:261 episode reward: total was -35.370000. running mean: -8.082463\n",
      "ep 3706: ep_len:565 episode reward: total was -33.390000. running mean: -8.335539\n",
      "ep 3706: ep_len:500 episode reward: total was -21.520000. running mean: -8.467383\n",
      "ep 3706: ep_len:3 episode reward: total was 0.000000. running mean: -8.382709\n",
      "ep 3706: ep_len:550 episode reward: total was 8.130000. running mean: -8.217582\n",
      "ep 3706: ep_len:500 episode reward: total was -15.270000. running mean: -8.288106\n",
      "epsilon:0.010000 episode_count: 25949. steps_count: 11460923.000000\n",
      "ep 3707: ep_len:515 episode reward: total was -12.840000. running mean: -8.333625\n",
      "ep 3707: ep_len:645 episode reward: total was 13.650000. running mean: -8.113789\n",
      "ep 3707: ep_len:550 episode reward: total was -5.690000. running mean: -8.089551\n",
      "ep 3707: ep_len:152 episode reward: total was 3.130000. running mean: -7.977356\n",
      "ep 3707: ep_len:3 episode reward: total was 0.000000. running mean: -7.897582\n",
      "ep 3707: ep_len:565 episode reward: total was -6.660000. running mean: -7.885206\n",
      "ep 3707: ep_len:505 episode reward: total was -27.150000. running mean: -8.077854\n",
      "epsilon:0.010000 episode_count: 25956. steps_count: 11463858.000000\n",
      "ep 3708: ep_len:500 episode reward: total was -32.210000. running mean: -8.319176\n",
      "ep 3708: ep_len:560 episode reward: total was 6.240000. running mean: -8.173584\n",
      "ep 3708: ep_len:471 episode reward: total was 4.250000. running mean: -8.049348\n",
      "ep 3708: ep_len:500 episode reward: total was -34.680000. running mean: -8.315655\n",
      "ep 3708: ep_len:3 episode reward: total was 0.000000. running mean: -8.232498\n",
      "ep 3708: ep_len:500 episode reward: total was 8.570000. running mean: -8.064473\n",
      "ep 3708: ep_len:505 episode reward: total was -4.340000. running mean: -8.027228\n",
      "epsilon:0.010000 episode_count: 25963. steps_count: 11466897.000000\n",
      "ep 3709: ep_len:595 episode reward: total was 0.110000. running mean: -7.945856\n",
      "ep 3709: ep_len:167 episode reward: total was -5.430000. running mean: -7.920698\n",
      "ep 3709: ep_len:550 episode reward: total was -2.630000. running mean: -7.867791\n",
      "ep 3709: ep_len:364 episode reward: total was -5.130000. running mean: -7.840413\n",
      "ep 3709: ep_len:3 episode reward: total was 0.000000. running mean: -7.762009\n",
      "ep 3709: ep_len:500 episode reward: total was -5.930000. running mean: -7.743688\n",
      "ep 3709: ep_len:555 episode reward: total was -12.110000. running mean: -7.787352\n",
      "epsilon:0.010000 episode_count: 25970. steps_count: 11469631.000000\n",
      "ep 3710: ep_len:595 episode reward: total was 4.450000. running mean: -7.664978\n",
      "ep 3710: ep_len:286 episode reward: total was -6.370000. running mean: -7.652028\n",
      "ep 3710: ep_len:79 episode reward: total was 0.040000. running mean: -7.575108\n",
      "ep 3710: ep_len:151 episode reward: total was -10.380000. running mean: -7.603157\n",
      "ep 3710: ep_len:86 episode reward: total was 1.530000. running mean: -7.511825\n",
      "ep 3710: ep_len:565 episode reward: total was -5.190000. running mean: -7.488607\n",
      "ep 3710: ep_len:287 episode reward: total was -3.810000. running mean: -7.451821\n",
      "epsilon:0.010000 episode_count: 25977. steps_count: 11471680.000000\n",
      "ep 3711: ep_len:100 episode reward: total was 0.550000. running mean: -7.371803\n",
      "ep 3711: ep_len:500 episode reward: total was 17.210000. running mean: -7.125985\n",
      "ep 3711: ep_len:367 episode reward: total was 1.670000. running mean: -7.038025\n",
      "ep 3711: ep_len:510 episode reward: total was -19.560000. running mean: -7.163245\n",
      "ep 3711: ep_len:103 episode reward: total was 2.540000. running mean: -7.066212\n",
      "ep 3711: ep_len:565 episode reward: total was -19.850000. running mean: -7.194050\n",
      "ep 3711: ep_len:500 episode reward: total was -10.930000. running mean: -7.231410\n",
      "epsilon:0.010000 episode_count: 25984. steps_count: 11474325.000000\n",
      "ep 3712: ep_len:580 episode reward: total was -9.150000. running mean: -7.250596\n",
      "ep 3712: ep_len:500 episode reward: total was 7.700000. running mean: -7.101090\n",
      "ep 3712: ep_len:79 episode reward: total was -5.470000. running mean: -7.084779\n",
      "ep 3712: ep_len:540 episode reward: total was 3.400000. running mean: -6.979931\n",
      "ep 3712: ep_len:88 episode reward: total was 1.010000. running mean: -6.900032\n",
      "ep 3712: ep_len:530 episode reward: total was 0.020000. running mean: -6.830831\n",
      "ep 3712: ep_len:590 episode reward: total was -12.360000. running mean: -6.886123\n",
      "epsilon:0.010000 episode_count: 25991. steps_count: 11477232.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3713: ep_len:655 episode reward: total was -22.180000. running mean: -7.039062\n",
      "ep 3713: ep_len:510 episode reward: total was -1.100000. running mean: -6.979671\n",
      "ep 3713: ep_len:365 episode reward: total was 2.710000. running mean: -6.882774\n",
      "ep 3713: ep_len:530 episode reward: total was -2.570000. running mean: -6.839647\n",
      "ep 3713: ep_len:81 episode reward: total was 2.530000. running mean: -6.745950\n",
      "ep 3713: ep_len:615 episode reward: total was -9.120000. running mean: -6.769691\n",
      "ep 3713: ep_len:565 episode reward: total was -11.080000. running mean: -6.812794\n",
      "epsilon:0.010000 episode_count: 25998. steps_count: 11480553.000000\n",
      "ep 3714: ep_len:500 episode reward: total was 7.260000. running mean: -6.672066\n",
      "ep 3714: ep_len:570 episode reward: total was 20.930000. running mean: -6.396045\n",
      "ep 3714: ep_len:500 episode reward: total was 1.050000. running mean: -6.321585\n",
      "ep 3714: ep_len:520 episode reward: total was -1.110000. running mean: -6.269469\n",
      "ep 3714: ep_len:113 episode reward: total was 6.050000. running mean: -6.146274\n",
      "ep 3714: ep_len:520 episode reward: total was 8.960000. running mean: -5.995211\n",
      "ep 3714: ep_len:525 episode reward: total was -13.610000. running mean: -6.071359\n",
      "epsilon:0.010000 episode_count: 26005. steps_count: 11483801.000000\n",
      "ep 3715: ep_len:505 episode reward: total was -7.330000. running mean: -6.083946\n",
      "ep 3715: ep_len:500 episode reward: total was -16.530000. running mean: -6.188406\n",
      "ep 3715: ep_len:545 episode reward: total was -1.110000. running mean: -6.137622\n",
      "ep 3715: ep_len:575 episode reward: total was -6.570000. running mean: -6.141946\n",
      "ep 3715: ep_len:106 episode reward: total was 4.540000. running mean: -6.035127\n",
      "ep 3715: ep_len:550 episode reward: total was 5.130000. running mean: -5.923475\n",
      "ep 3715: ep_len:540 episode reward: total was -41.220000. running mean: -6.276441\n",
      "epsilon:0.010000 episode_count: 26012. steps_count: 11487122.000000\n",
      "ep 3716: ep_len:595 episode reward: total was -13.790000. running mean: -6.351576\n",
      "ep 3716: ep_len:510 episode reward: total was -5.520000. running mean: -6.343260\n",
      "ep 3716: ep_len:500 episode reward: total was 4.600000. running mean: -6.233828\n",
      "ep 3716: ep_len:505 episode reward: total was 9.920000. running mean: -6.072289\n",
      "ep 3716: ep_len:3 episode reward: total was 0.000000. running mean: -6.011567\n",
      "ep 3716: ep_len:575 episode reward: total was -14.170000. running mean: -6.093151\n",
      "ep 3716: ep_len:505 episode reward: total was -22.980000. running mean: -6.262019\n",
      "epsilon:0.010000 episode_count: 26019. steps_count: 11490315.000000\n",
      "ep 3717: ep_len:505 episode reward: total was -15.500000. running mean: -6.354399\n",
      "ep 3717: ep_len:500 episode reward: total was -7.500000. running mean: -6.365855\n",
      "ep 3717: ep_len:615 episode reward: total was -30.750000. running mean: -6.609697\n",
      "ep 3717: ep_len:505 episode reward: total was -32.190000. running mean: -6.865500\n",
      "ep 3717: ep_len:88 episode reward: total was -3.460000. running mean: -6.831445\n",
      "ep 3717: ep_len:570 episode reward: total was -9.160000. running mean: -6.854730\n",
      "ep 3717: ep_len:500 episode reward: total was -13.900000. running mean: -6.925183\n",
      "epsilon:0.010000 episode_count: 26026. steps_count: 11493598.000000\n",
      "ep 3718: ep_len:610 episode reward: total was 4.650000. running mean: -6.809431\n",
      "ep 3718: ep_len:610 episode reward: total was 13.570000. running mean: -6.605637\n",
      "ep 3718: ep_len:418 episode reward: total was 2.730000. running mean: -6.512280\n",
      "ep 3718: ep_len:500 episode reward: total was -0.090000. running mean: -6.448058\n",
      "ep 3718: ep_len:47 episode reward: total was 4.500000. running mean: -6.338577\n",
      "ep 3718: ep_len:540 episode reward: total was -6.360000. running mean: -6.338791\n",
      "ep 3718: ep_len:505 episode reward: total was -19.430000. running mean: -6.469703\n",
      "epsilon:0.010000 episode_count: 26033. steps_count: 11496828.000000\n",
      "ep 3719: ep_len:520 episode reward: total was -22.450000. running mean: -6.629506\n",
      "ep 3719: ep_len:575 episode reward: total was 2.920000. running mean: -6.534011\n",
      "ep 3719: ep_len:555 episode reward: total was -28.880000. running mean: -6.757471\n",
      "ep 3719: ep_len:505 episode reward: total was 12.080000. running mean: -6.569096\n",
      "ep 3719: ep_len:46 episode reward: total was 4.500000. running mean: -6.458405\n",
      "ep 3719: ep_len:620 episode reward: total was -7.600000. running mean: -6.469821\n",
      "ep 3719: ep_len:590 episode reward: total was -9.360000. running mean: -6.498723\n",
      "epsilon:0.010000 episode_count: 26040. steps_count: 11500239.000000\n",
      "ep 3720: ep_len:500 episode reward: total was -18.490000. running mean: -6.618636\n",
      "ep 3720: ep_len:500 episode reward: total was 12.130000. running mean: -6.431150\n",
      "ep 3720: ep_len:625 episode reward: total was -6.100000. running mean: -6.427838\n",
      "ep 3720: ep_len:500 episode reward: total was -9.580000. running mean: -6.459360\n",
      "ep 3720: ep_len:3 episode reward: total was 0.000000. running mean: -6.394766\n",
      "ep 3720: ep_len:500 episode reward: total was -1.090000. running mean: -6.341718\n",
      "ep 3720: ep_len:315 episode reward: total was -2.760000. running mean: -6.305901\n",
      "epsilon:0.010000 episode_count: 26047. steps_count: 11503182.000000\n",
      "ep 3721: ep_len:605 episode reward: total was 12.500000. running mean: -6.117842\n",
      "ep 3721: ep_len:515 episode reward: total was 5.150000. running mean: -6.005164\n",
      "ep 3721: ep_len:570 episode reward: total was -30.380000. running mean: -6.248912\n",
      "ep 3721: ep_len:515 episode reward: total was 2.910000. running mean: -6.157323\n",
      "ep 3721: ep_len:3 episode reward: total was 0.000000. running mean: -6.095750\n",
      "ep 3721: ep_len:177 episode reward: total was 10.130000. running mean: -5.933492\n",
      "ep 3721: ep_len:334 episode reward: total was -15.330000. running mean: -6.027457\n",
      "epsilon:0.010000 episode_count: 26054. steps_count: 11505901.000000\n",
      "ep 3722: ep_len:120 episode reward: total was -10.920000. running mean: -6.076383\n",
      "ep 3722: ep_len:367 episode reward: total was -48.890000. running mean: -6.504519\n",
      "ep 3722: ep_len:615 episode reward: total was -7.560000. running mean: -6.515074\n",
      "ep 3722: ep_len:510 episode reward: total was -10.500000. running mean: -6.554923\n",
      "ep 3722: ep_len:3 episode reward: total was 0.000000. running mean: -6.489374\n",
      "ep 3722: ep_len:580 episode reward: total was 2.940000. running mean: -6.395080\n",
      "ep 3722: ep_len:201 episode reward: total was -8.390000. running mean: -6.415029\n",
      "epsilon:0.010000 episode_count: 26061. steps_count: 11508297.000000\n",
      "ep 3723: ep_len:630 episode reward: total was 8.640000. running mean: -6.264479\n",
      "ep 3723: ep_len:630 episode reward: total was -61.150000. running mean: -6.813334\n",
      "ep 3723: ep_len:500 episode reward: total was -1.780000. running mean: -6.763001\n",
      "ep 3723: ep_len:148 episode reward: total was 4.620000. running mean: -6.649171\n",
      "ep 3723: ep_len:3 episode reward: total was 0.000000. running mean: -6.582679\n",
      "ep 3723: ep_len:500 episode reward: total was 2.200000. running mean: -6.494852\n",
      "ep 3723: ep_len:500 episode reward: total was -0.700000. running mean: -6.436904\n",
      "epsilon:0.010000 episode_count: 26068. steps_count: 11511208.000000\n",
      "ep 3724: ep_len:520 episode reward: total was -10.510000. running mean: -6.477635\n",
      "ep 3724: ep_len:565 episode reward: total was 3.370000. running mean: -6.379159\n",
      "ep 3724: ep_len:540 episode reward: total was -9.310000. running mean: -6.408467\n",
      "ep 3724: ep_len:560 episode reward: total was 7.440000. running mean: -6.269982\n",
      "ep 3724: ep_len:83 episode reward: total was 5.040000. running mean: -6.156882\n",
      "ep 3724: ep_len:515 episode reward: total was 0.930000. running mean: -6.086014\n",
      "ep 3724: ep_len:575 episode reward: total was -0.480000. running mean: -6.029953\n",
      "epsilon:0.010000 episode_count: 26075. steps_count: 11514566.000000\n",
      "ep 3725: ep_len:635 episode reward: total was 0.630000. running mean: -5.963354\n",
      "ep 3725: ep_len:575 episode reward: total was -2.090000. running mean: -5.924620\n",
      "ep 3725: ep_len:500 episode reward: total was -0.590000. running mean: -5.871274\n",
      "ep 3725: ep_len:500 episode reward: total was 6.370000. running mean: -5.748861\n",
      "ep 3725: ep_len:133 episode reward: total was 7.560000. running mean: -5.615773\n",
      "ep 3725: ep_len:545 episode reward: total was -26.060000. running mean: -5.820215\n",
      "ep 3725: ep_len:590 episode reward: total was -20.440000. running mean: -5.966413\n",
      "epsilon:0.010000 episode_count: 26082. steps_count: 11518044.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3726: ep_len:585 episode reward: total was -29.250000. running mean: -6.199249\n",
      "ep 3726: ep_len:545 episode reward: total was -4.350000. running mean: -6.180756\n",
      "ep 3726: ep_len:555 episode reward: total was -6.640000. running mean: -6.185349\n",
      "ep 3726: ep_len:500 episode reward: total was -22.130000. running mean: -6.344795\n",
      "ep 3726: ep_len:3 episode reward: total was 0.000000. running mean: -6.281347\n",
      "ep 3726: ep_len:565 episode reward: total was -9.650000. running mean: -6.315034\n",
      "ep 3726: ep_len:585 episode reward: total was -10.050000. running mean: -6.352384\n",
      "epsilon:0.010000 episode_count: 26089. steps_count: 11521382.000000\n",
      "ep 3727: ep_len:126 episode reward: total was 3.080000. running mean: -6.258060\n",
      "ep 3727: ep_len:283 episode reward: total was -10.350000. running mean: -6.298979\n",
      "ep 3727: ep_len:590 episode reward: total was -1.070000. running mean: -6.246689\n",
      "ep 3727: ep_len:550 episode reward: total was 6.480000. running mean: -6.119422\n",
      "ep 3727: ep_len:3 episode reward: total was 0.000000. running mean: -6.058228\n",
      "ep 3727: ep_len:570 episode reward: total was 1.390000. running mean: -5.983746\n",
      "ep 3727: ep_len:605 episode reward: total was -8.870000. running mean: -6.012608\n",
      "epsilon:0.010000 episode_count: 26096. steps_count: 11524109.000000\n",
      "ep 3728: ep_len:595 episode reward: total was -22.080000. running mean: -6.173282\n",
      "ep 3728: ep_len:575 episode reward: total was 1.930000. running mean: -6.092250\n",
      "ep 3728: ep_len:500 episode reward: total was 1.360000. running mean: -6.017727\n",
      "ep 3728: ep_len:590 episode reward: total was -45.230000. running mean: -6.409850\n",
      "ep 3728: ep_len:3 episode reward: total was 0.000000. running mean: -6.345751\n",
      "ep 3728: ep_len:186 episode reward: total was 8.130000. running mean: -6.200994\n",
      "ep 3728: ep_len:590 episode reward: total was -22.680000. running mean: -6.365784\n",
      "epsilon:0.010000 episode_count: 26103. steps_count: 11527148.000000\n",
      "ep 3729: ep_len:590 episode reward: total was -6.510000. running mean: -6.367226\n",
      "ep 3729: ep_len:510 episode reward: total was -5.020000. running mean: -6.353754\n",
      "ep 3729: ep_len:550 episode reward: total was -6.960000. running mean: -6.359816\n",
      "ep 3729: ep_len:600 episode reward: total was 8.060000. running mean: -6.215618\n",
      "ep 3729: ep_len:3 episode reward: total was 0.000000. running mean: -6.153462\n",
      "ep 3729: ep_len:540 episode reward: total was -3.120000. running mean: -6.123127\n",
      "ep 3729: ep_len:565 episode reward: total was -12.840000. running mean: -6.190296\n",
      "epsilon:0.010000 episode_count: 26110. steps_count: 11530506.000000\n",
      "ep 3730: ep_len:605 episode reward: total was -1.930000. running mean: -6.147693\n",
      "ep 3730: ep_len:620 episode reward: total was -15.530000. running mean: -6.241516\n",
      "ep 3730: ep_len:640 episode reward: total was -6.390000. running mean: -6.243001\n",
      "ep 3730: ep_len:51 episode reward: total was 2.560000. running mean: -6.154971\n",
      "ep 3730: ep_len:39 episode reward: total was 2.000000. running mean: -6.073421\n",
      "ep 3730: ep_len:525 episode reward: total was -36.460000. running mean: -6.377287\n",
      "ep 3730: ep_len:520 episode reward: total was -21.970000. running mean: -6.533214\n",
      "epsilon:0.010000 episode_count: 26117. steps_count: 11533506.000000\n",
      "ep 3731: ep_len:530 episode reward: total was 6.100000. running mean: -6.406882\n",
      "ep 3731: ep_len:515 episode reward: total was -13.950000. running mean: -6.482313\n",
      "ep 3731: ep_len:473 episode reward: total was -1.780000. running mean: -6.435290\n",
      "ep 3731: ep_len:510 episode reward: total was 5.550000. running mean: -6.315437\n",
      "ep 3731: ep_len:86 episode reward: total was 2.020000. running mean: -6.232083\n",
      "ep 3731: ep_len:610 episode reward: total was 0.470000. running mean: -6.165062\n",
      "ep 3731: ep_len:525 episode reward: total was -4.030000. running mean: -6.143711\n",
      "epsilon:0.010000 episode_count: 26124. steps_count: 11536755.000000\n",
      "ep 3732: ep_len:540 episode reward: total was -20.430000. running mean: -6.286574\n",
      "ep 3732: ep_len:500 episode reward: total was 16.700000. running mean: -6.056708\n",
      "ep 3732: ep_len:625 episode reward: total was -9.920000. running mean: -6.095341\n",
      "ep 3732: ep_len:760 episode reward: total was -56.430000. running mean: -6.598688\n",
      "ep 3732: ep_len:119 episode reward: total was 6.030000. running mean: -6.472401\n",
      "ep 3732: ep_len:500 episode reward: total was 3.300000. running mean: -6.374677\n",
      "ep 3732: ep_len:585 episode reward: total was -13.840000. running mean: -6.449330\n",
      "epsilon:0.010000 episode_count: 26131. steps_count: 11540384.000000\n",
      "ep 3733: ep_len:510 episode reward: total was -26.980000. running mean: -6.654637\n",
      "ep 3733: ep_len:196 episode reward: total was -0.350000. running mean: -6.591591\n",
      "ep 3733: ep_len:580 episode reward: total was 3.980000. running mean: -6.485875\n",
      "ep 3733: ep_len:38 episode reward: total was -2.960000. running mean: -6.450616\n",
      "ep 3733: ep_len:3 episode reward: total was 0.000000. running mean: -6.386110\n",
      "ep 3733: ep_len:500 episode reward: total was 4.250000. running mean: -6.279749\n",
      "ep 3733: ep_len:555 episode reward: total was -30.530000. running mean: -6.522251\n",
      "epsilon:0.010000 episode_count: 26138. steps_count: 11542766.000000\n",
      "ep 3734: ep_len:685 episode reward: total was -22.700000. running mean: -6.684029\n",
      "ep 3734: ep_len:515 episode reward: total was 4.900000. running mean: -6.568188\n",
      "ep 3734: ep_len:630 episode reward: total was -23.300000. running mean: -6.735507\n",
      "ep 3734: ep_len:510 episode reward: total was 8.060000. running mean: -6.587551\n",
      "ep 3734: ep_len:3 episode reward: total was 0.000000. running mean: -6.521676\n",
      "ep 3734: ep_len:625 episode reward: total was -0.030000. running mean: -6.456759\n",
      "ep 3734: ep_len:292 episode reward: total was -9.350000. running mean: -6.485692\n",
      "epsilon:0.010000 episode_count: 26145. steps_count: 11546026.000000\n",
      "ep 3735: ep_len:207 episode reward: total was -5.870000. running mean: -6.479535\n",
      "ep 3735: ep_len:273 episode reward: total was 1.130000. running mean: -6.403439\n",
      "ep 3735: ep_len:655 episode reward: total was -15.740000. running mean: -6.496805\n",
      "ep 3735: ep_len:540 episode reward: total was 17.380000. running mean: -6.258037\n",
      "ep 3735: ep_len:77 episode reward: total was 2.030000. running mean: -6.175157\n",
      "ep 3735: ep_len:500 episode reward: total was 9.800000. running mean: -6.015405\n",
      "ep 3735: ep_len:610 episode reward: total was -20.510000. running mean: -6.160351\n",
      "epsilon:0.010000 episode_count: 26152. steps_count: 11548888.000000\n",
      "ep 3736: ep_len:605 episode reward: total was 5.970000. running mean: -6.039047\n",
      "ep 3736: ep_len:525 episode reward: total was -8.310000. running mean: -6.061757\n",
      "ep 3736: ep_len:725 episode reward: total was -20.170000. running mean: -6.202839\n",
      "ep 3736: ep_len:605 episode reward: total was 17.020000. running mean: -5.970611\n",
      "ep 3736: ep_len:97 episode reward: total was 4.030000. running mean: -5.870605\n",
      "ep 3736: ep_len:550 episode reward: total was 10.500000. running mean: -5.706899\n",
      "ep 3736: ep_len:620 episode reward: total was -12.050000. running mean: -5.770330\n",
      "epsilon:0.010000 episode_count: 26159. steps_count: 11552615.000000\n",
      "ep 3737: ep_len:180 episode reward: total was -5.420000. running mean: -5.766827\n",
      "ep 3737: ep_len:500 episode reward: total was -17.940000. running mean: -5.888558\n",
      "ep 3737: ep_len:453 episode reward: total was 1.730000. running mean: -5.812373\n",
      "ep 3737: ep_len:500 episode reward: total was 3.850000. running mean: -5.715749\n",
      "ep 3737: ep_len:86 episode reward: total was -0.490000. running mean: -5.663491\n",
      "ep 3737: ep_len:550 episode reward: total was -9.950000. running mean: -5.706357\n",
      "ep 3737: ep_len:500 episode reward: total was -25.730000. running mean: -5.906593\n",
      "epsilon:0.010000 episode_count: 26166. steps_count: 11555384.000000\n",
      "ep 3738: ep_len:249 episode reward: total was 2.630000. running mean: -5.821227\n",
      "ep 3738: ep_len:500 episode reward: total was 1.700000. running mean: -5.746015\n",
      "ep 3738: ep_len:379 episode reward: total was -7.840000. running mean: -5.766955\n",
      "ep 3738: ep_len:56 episode reward: total was -0.930000. running mean: -5.718585\n",
      "ep 3738: ep_len:88 episode reward: total was 3.030000. running mean: -5.631099\n",
      "ep 3738: ep_len:625 episode reward: total was -32.350000. running mean: -5.898288\n",
      "ep 3738: ep_len:500 episode reward: total was -10.950000. running mean: -5.948805\n",
      "epsilon:0.010000 episode_count: 26173. steps_count: 11557781.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3739: ep_len:250 episode reward: total was 7.140000. running mean: -5.817917\n",
      "ep 3739: ep_len:530 episode reward: total was 12.870000. running mean: -5.631038\n",
      "ep 3739: ep_len:500 episode reward: total was -21.690000. running mean: -5.791628\n",
      "ep 3739: ep_len:132 episode reward: total was -0.890000. running mean: -5.742611\n",
      "ep 3739: ep_len:49 episode reward: total was 4.500000. running mean: -5.640185\n",
      "ep 3739: ep_len:670 episode reward: total was -8.620000. running mean: -5.669983\n",
      "ep 3739: ep_len:580 episode reward: total was -24.520000. running mean: -5.858484\n",
      "epsilon:0.010000 episode_count: 26180. steps_count: 11560492.000000\n",
      "ep 3740: ep_len:505 episode reward: total was -10.590000. running mean: -5.905799\n",
      "ep 3740: ep_len:590 episode reward: total was 11.240000. running mean: -5.734341\n",
      "ep 3740: ep_len:630 episode reward: total was -28.750000. running mean: -5.964497\n",
      "ep 3740: ep_len:610 episode reward: total was -0.490000. running mean: -5.909752\n",
      "ep 3740: ep_len:3 episode reward: total was 0.000000. running mean: -5.850655\n",
      "ep 3740: ep_len:605 episode reward: total was -3.040000. running mean: -5.822548\n",
      "ep 3740: ep_len:535 episode reward: total was -18.060000. running mean: -5.944923\n",
      "epsilon:0.010000 episode_count: 26187. steps_count: 11563970.000000\n",
      "ep 3741: ep_len:500 episode reward: total was -20.250000. running mean: -6.087974\n",
      "ep 3741: ep_len:590 episode reward: total was 9.650000. running mean: -5.930594\n",
      "ep 3741: ep_len:500 episode reward: total was -9.010000. running mean: -5.961388\n",
      "ep 3741: ep_len:500 episode reward: total was -15.200000. running mean: -6.053774\n",
      "ep 3741: ep_len:3 episode reward: total was 0.000000. running mean: -5.993236\n",
      "ep 3741: ep_len:500 episode reward: total was -14.770000. running mean: -6.081004\n",
      "ep 3741: ep_len:535 episode reward: total was -1.410000. running mean: -6.034294\n",
      "epsilon:0.010000 episode_count: 26194. steps_count: 11567098.000000\n",
      "ep 3742: ep_len:188 episode reward: total was 7.620000. running mean: -5.897751\n",
      "ep 3742: ep_len:620 episode reward: total was -8.020000. running mean: -5.918974\n",
      "ep 3742: ep_len:510 episode reward: total was 1.400000. running mean: -5.845784\n",
      "ep 3742: ep_len:500 episode reward: total was -7.190000. running mean: -5.859226\n",
      "ep 3742: ep_len:89 episode reward: total was -11.460000. running mean: -5.915234\n",
      "ep 3742: ep_len:690 episode reward: total was -18.220000. running mean: -6.038281\n",
      "ep 3742: ep_len:540 episode reward: total was -7.460000. running mean: -6.052499\n",
      "epsilon:0.010000 episode_count: 26201. steps_count: 11570235.000000\n",
      "ep 3743: ep_len:219 episode reward: total was -11.880000. running mean: -6.110774\n",
      "ep 3743: ep_len:550 episode reward: total was 25.880000. running mean: -5.790866\n",
      "ep 3743: ep_len:560 episode reward: total was -1.400000. running mean: -5.746957\n",
      "ep 3743: ep_len:544 episode reward: total was 8.670000. running mean: -5.602788\n",
      "ep 3743: ep_len:84 episode reward: total was 3.540000. running mean: -5.511360\n",
      "ep 3743: ep_len:500 episode reward: total was -0.190000. running mean: -5.458146\n",
      "ep 3743: ep_len:500 episode reward: total was -44.730000. running mean: -5.850865\n",
      "epsilon:0.010000 episode_count: 26208. steps_count: 11573192.000000\n",
      "ep 3744: ep_len:225 episode reward: total was 3.120000. running mean: -5.761156\n",
      "ep 3744: ep_len:362 episode reward: total was -1.310000. running mean: -5.716644\n",
      "ep 3744: ep_len:500 episode reward: total was -0.970000. running mean: -5.669178\n",
      "ep 3744: ep_len:510 episode reward: total was 9.410000. running mean: -5.518386\n",
      "ep 3744: ep_len:3 episode reward: total was 0.000000. running mean: -5.463202\n",
      "ep 3744: ep_len:600 episode reward: total was 15.040000. running mean: -5.258170\n",
      "ep 3744: ep_len:505 episode reward: total was -6.550000. running mean: -5.271089\n",
      "epsilon:0.010000 episode_count: 26215. steps_count: 11575897.000000\n",
      "ep 3745: ep_len:530 episode reward: total was 10.170000. running mean: -5.116678\n",
      "ep 3745: ep_len:500 episode reward: total was -2.760000. running mean: -5.093111\n",
      "ep 3745: ep_len:394 episode reward: total was 4.720000. running mean: -4.994980\n",
      "ep 3745: ep_len:570 episode reward: total was -5.570000. running mean: -5.000730\n",
      "ep 3745: ep_len:53 episode reward: total was 3.500000. running mean: -4.915723\n",
      "ep 3745: ep_len:530 episode reward: total was 3.010000. running mean: -4.836466\n",
      "ep 3745: ep_len:550 episode reward: total was -16.070000. running mean: -4.948801\n",
      "epsilon:0.010000 episode_count: 26222. steps_count: 11579024.000000\n",
      "ep 3746: ep_len:630 episode reward: total was -14.700000. running mean: -5.046313\n",
      "ep 3746: ep_len:186 episode reward: total was -11.860000. running mean: -5.114450\n",
      "ep 3746: ep_len:750 episode reward: total was -23.650000. running mean: -5.299805\n",
      "ep 3746: ep_len:128 episode reward: total was 1.100000. running mean: -5.235807\n",
      "ep 3746: ep_len:92 episode reward: total was 4.540000. running mean: -5.138049\n",
      "ep 3746: ep_len:510 episode reward: total was 3.100000. running mean: -5.055669\n",
      "ep 3746: ep_len:290 episode reward: total was -3.770000. running mean: -5.042812\n",
      "epsilon:0.010000 episode_count: 26229. steps_count: 11581610.000000\n",
      "ep 3747: ep_len:500 episode reward: total was 11.790000. running mean: -4.874484\n",
      "ep 3747: ep_len:575 episode reward: total was -11.790000. running mean: -4.943639\n",
      "ep 3747: ep_len:500 episode reward: total was 4.050000. running mean: -4.853703\n",
      "ep 3747: ep_len:510 episode reward: total was -19.100000. running mean: -4.996166\n",
      "ep 3747: ep_len:87 episode reward: total was 4.040000. running mean: -4.905804\n",
      "ep 3747: ep_len:575 episode reward: total was 4.860000. running mean: -4.808146\n",
      "ep 3747: ep_len:313 episode reward: total was -2.290000. running mean: -4.782964\n",
      "epsilon:0.010000 episode_count: 26236. steps_count: 11584670.000000\n",
      "ep 3748: ep_len:540 episode reward: total was 0.150000. running mean: -4.733635\n",
      "ep 3748: ep_len:565 episode reward: total was -32.530000. running mean: -5.011598\n",
      "ep 3748: ep_len:620 episode reward: total was 5.680000. running mean: -4.904682\n",
      "ep 3748: ep_len:500 episode reward: total was -17.080000. running mean: -5.026436\n",
      "ep 3748: ep_len:80 episode reward: total was 4.030000. running mean: -4.935871\n",
      "ep 3748: ep_len:500 episode reward: total was 7.810000. running mean: -4.808413\n",
      "ep 3748: ep_len:500 episode reward: total was -0.180000. running mean: -4.762128\n",
      "epsilon:0.010000 episode_count: 26243. steps_count: 11587975.000000\n",
      "ep 3749: ep_len:570 episode reward: total was -26.490000. running mean: -4.979407\n",
      "ep 3749: ep_len:525 episode reward: total was 1.730000. running mean: -4.912313\n",
      "ep 3749: ep_len:463 episode reward: total was 4.290000. running mean: -4.820290\n",
      "ep 3749: ep_len:570 episode reward: total was 12.420000. running mean: -4.647887\n",
      "ep 3749: ep_len:3 episode reward: total was 0.000000. running mean: -4.601408\n",
      "ep 3749: ep_len:160 episode reward: total was 7.630000. running mean: -4.479094\n",
      "ep 3749: ep_len:515 episode reward: total was -11.360000. running mean: -4.547903\n",
      "epsilon:0.010000 episode_count: 26250. steps_count: 11590781.000000\n",
      "ep 3750: ep_len:500 episode reward: total was 19.310000. running mean: -4.309324\n",
      "ep 3750: ep_len:650 episode reward: total was 16.630000. running mean: -4.099931\n",
      "ep 3750: ep_len:515 episode reward: total was 8.940000. running mean: -3.969532\n",
      "ep 3750: ep_len:585 episode reward: total was 5.060000. running mean: -3.879236\n",
      "ep 3750: ep_len:48 episode reward: total was 4.500000. running mean: -3.795444\n",
      "ep 3750: ep_len:565 episode reward: total was -2.030000. running mean: -3.777789\n",
      "ep 3750: ep_len:188 episode reward: total was -1.870000. running mean: -3.758712\n",
      "epsilon:0.010000 episode_count: 26257. steps_count: 11593832.000000\n",
      "ep 3751: ep_len:500 episode reward: total was 7.430000. running mean: -3.646824\n",
      "ep 3751: ep_len:500 episode reward: total was 6.390000. running mean: -3.546456\n",
      "ep 3751: ep_len:535 episode reward: total was -0.780000. running mean: -3.518792\n",
      "ep 3751: ep_len:505 episode reward: total was -19.050000. running mean: -3.674104\n",
      "ep 3751: ep_len:3 episode reward: total was 0.000000. running mean: -3.637363\n",
      "ep 3751: ep_len:530 episode reward: total was -0.050000. running mean: -3.601489\n",
      "ep 3751: ep_len:570 episode reward: total was -6.520000. running mean: -3.630674\n",
      "epsilon:0.010000 episode_count: 26264. steps_count: 11596975.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3752: ep_len:575 episode reward: total was -16.230000. running mean: -3.756667\n",
      "ep 3752: ep_len:530 episode reward: total was -19.330000. running mean: -3.912401\n",
      "ep 3752: ep_len:575 episode reward: total was 4.340000. running mean: -3.829877\n",
      "ep 3752: ep_len:122 episode reward: total was 3.600000. running mean: -3.755578\n",
      "ep 3752: ep_len:128 episode reward: total was 7.060000. running mean: -3.647422\n",
      "ep 3752: ep_len:500 episode reward: total was -43.300000. running mean: -4.043948\n",
      "ep 3752: ep_len:500 episode reward: total was -7.710000. running mean: -4.080608\n",
      "epsilon:0.010000 episode_count: 26271. steps_count: 11599905.000000\n",
      "ep 3753: ep_len:595 episode reward: total was 8.430000. running mean: -3.955502\n",
      "ep 3753: ep_len:500 episode reward: total was -5.760000. running mean: -3.973547\n",
      "ep 3753: ep_len:585 episode reward: total was 0.160000. running mean: -3.932212\n",
      "ep 3753: ep_len:500 episode reward: total was -12.050000. running mean: -4.013390\n",
      "ep 3753: ep_len:3 episode reward: total was 0.000000. running mean: -3.973256\n",
      "ep 3753: ep_len:530 episode reward: total was 10.690000. running mean: -3.826623\n",
      "ep 3753: ep_len:650 episode reward: total was -32.530000. running mean: -4.113657\n",
      "epsilon:0.010000 episode_count: 26278. steps_count: 11603268.000000\n",
      "ep 3754: ep_len:231 episode reward: total was 8.140000. running mean: -3.991121\n",
      "ep 3754: ep_len:500 episode reward: total was -6.310000. running mean: -4.014309\n",
      "ep 3754: ep_len:560 episode reward: total was -5.200000. running mean: -4.026166\n",
      "ep 3754: ep_len:505 episode reward: total was 16.470000. running mean: -3.821205\n",
      "ep 3754: ep_len:81 episode reward: total was 5.040000. running mean: -3.732593\n",
      "ep 3754: ep_len:635 episode reward: total was -18.190000. running mean: -3.877167\n",
      "ep 3754: ep_len:560 episode reward: total was -12.470000. running mean: -3.963095\n",
      "epsilon:0.010000 episode_count: 26285. steps_count: 11606340.000000\n",
      "ep 3755: ep_len:540 episode reward: total was 2.020000. running mean: -3.903264\n",
      "ep 3755: ep_len:640 episode reward: total was 7.100000. running mean: -3.793231\n",
      "ep 3755: ep_len:535 episode reward: total was -42.970000. running mean: -4.184999\n",
      "ep 3755: ep_len:500 episode reward: total was -2.480000. running mean: -4.167949\n",
      "ep 3755: ep_len:3 episode reward: total was 0.000000. running mean: -4.126270\n",
      "ep 3755: ep_len:620 episode reward: total was -7.180000. running mean: -4.156807\n",
      "ep 3755: ep_len:565 episode reward: total was -8.890000. running mean: -4.204139\n",
      "epsilon:0.010000 episode_count: 26292. steps_count: 11609743.000000\n",
      "ep 3756: ep_len:510 episode reward: total was 1.760000. running mean: -4.144497\n",
      "ep 3756: ep_len:615 episode reward: total was 6.570000. running mean: -4.037352\n",
      "ep 3756: ep_len:500 episode reward: total was 5.780000. running mean: -3.939179\n",
      "ep 3756: ep_len:505 episode reward: total was -21.140000. running mean: -4.111187\n",
      "ep 3756: ep_len:1 episode reward: total was 0.000000. running mean: -4.070075\n",
      "ep 3756: ep_len:530 episode reward: total was 2.090000. running mean: -4.008474\n",
      "ep 3756: ep_len:550 episode reward: total was -2.480000. running mean: -3.993190\n",
      "epsilon:0.010000 episode_count: 26299. steps_count: 11612954.000000\n",
      "ep 3757: ep_len:500 episode reward: total was -5.160000. running mean: -4.004858\n",
      "ep 3757: ep_len:525 episode reward: total was 10.380000. running mean: -3.861009\n",
      "ep 3757: ep_len:685 episode reward: total was -59.320000. running mean: -4.415599\n",
      "ep 3757: ep_len:528 episode reward: total was -27.390000. running mean: -4.645343\n",
      "ep 3757: ep_len:86 episode reward: total was 0.520000. running mean: -4.593690\n",
      "ep 3757: ep_len:500 episode reward: total was 4.040000. running mean: -4.507353\n",
      "ep 3757: ep_len:338 episode reward: total was -0.230000. running mean: -4.464579\n",
      "epsilon:0.010000 episode_count: 26306. steps_count: 11616116.000000\n",
      "ep 3758: ep_len:510 episode reward: total was 10.470000. running mean: -4.315234\n",
      "ep 3758: ep_len:590 episode reward: total was 13.010000. running mean: -4.141981\n",
      "ep 3758: ep_len:394 episode reward: total was 0.690000. running mean: -4.093661\n",
      "ep 3758: ep_len:550 episode reward: total was 0.880000. running mean: -4.043925\n",
      "ep 3758: ep_len:85 episode reward: total was 4.040000. running mean: -3.963086\n",
      "ep 3758: ep_len:585 episode reward: total was 12.670000. running mean: -3.796755\n",
      "ep 3758: ep_len:500 episode reward: total was -17.130000. running mean: -3.930087\n",
      "epsilon:0.010000 episode_count: 26313. steps_count: 11619330.000000\n",
      "ep 3759: ep_len:505 episode reward: total was 8.990000. running mean: -3.800886\n",
      "ep 3759: ep_len:675 episode reward: total was -28.790000. running mean: -4.050777\n",
      "ep 3759: ep_len:500 episode reward: total was -16.440000. running mean: -4.174670\n",
      "ep 3759: ep_len:510 episode reward: total was -6.130000. running mean: -4.194223\n",
      "ep 3759: ep_len:3 episode reward: total was 0.000000. running mean: -4.152281\n",
      "ep 3759: ep_len:675 episode reward: total was -2.020000. running mean: -4.130958\n",
      "ep 3759: ep_len:515 episode reward: total was -4.230000. running mean: -4.131948\n",
      "epsilon:0.010000 episode_count: 26320. steps_count: 11622713.000000\n",
      "ep 3760: ep_len:565 episode reward: total was 3.990000. running mean: -4.050729\n",
      "ep 3760: ep_len:525 episode reward: total was 0.390000. running mean: -4.006322\n",
      "ep 3760: ep_len:670 episode reward: total was -42.800000. running mean: -4.394258\n",
      "ep 3760: ep_len:505 episode reward: total was -17.060000. running mean: -4.520916\n",
      "ep 3760: ep_len:3 episode reward: total was 0.000000. running mean: -4.475707\n",
      "ep 3760: ep_len:540 episode reward: total was -13.270000. running mean: -4.563650\n",
      "ep 3760: ep_len:525 episode reward: total was -1.470000. running mean: -4.532713\n",
      "epsilon:0.010000 episode_count: 26327. steps_count: 11626046.000000\n",
      "ep 3761: ep_len:555 episode reward: total was -15.230000. running mean: -4.639686\n",
      "ep 3761: ep_len:580 episode reward: total was -19.470000. running mean: -4.787989\n",
      "ep 3761: ep_len:500 episode reward: total was 0.930000. running mean: -4.730809\n",
      "ep 3761: ep_len:580 episode reward: total was 11.980000. running mean: -4.563701\n",
      "ep 3761: ep_len:3 episode reward: total was 0.000000. running mean: -4.518064\n",
      "ep 3761: ep_len:169 episode reward: total was 7.640000. running mean: -4.396483\n",
      "ep 3761: ep_len:510 episode reward: total was -31.150000. running mean: -4.664019\n",
      "epsilon:0.010000 episode_count: 26334. steps_count: 11628943.000000\n",
      "ep 3762: ep_len:630 episode reward: total was -7.720000. running mean: -4.694578\n",
      "ep 3762: ep_len:500 episode reward: total was -6.820000. running mean: -4.715833\n",
      "ep 3762: ep_len:650 episode reward: total was -7.120000. running mean: -4.739874\n",
      "ep 3762: ep_len:651 episode reward: total was -28.810000. running mean: -4.980576\n",
      "ep 3762: ep_len:3 episode reward: total was 0.000000. running mean: -4.930770\n",
      "ep 3762: ep_len:500 episode reward: total was -42.580000. running mean: -5.307262\n",
      "ep 3762: ep_len:535 episode reward: total was -11.390000. running mean: -5.368089\n",
      "epsilon:0.010000 episode_count: 26341. steps_count: 11632412.000000\n",
      "ep 3763: ep_len:500 episode reward: total was 5.960000. running mean: -5.254809\n",
      "ep 3763: ep_len:200 episode reward: total was -5.880000. running mean: -5.261060\n",
      "ep 3763: ep_len:610 episode reward: total was -2.990000. running mean: -5.238350\n",
      "ep 3763: ep_len:500 episode reward: total was -8.690000. running mean: -5.272866\n",
      "ep 3763: ep_len:3 episode reward: total was 0.000000. running mean: -5.220138\n",
      "ep 3763: ep_len:560 episode reward: total was 1.370000. running mean: -5.154236\n",
      "ep 3763: ep_len:284 episode reward: total was -3.750000. running mean: -5.140194\n",
      "epsilon:0.010000 episode_count: 26348. steps_count: 11635069.000000\n",
      "ep 3764: ep_len:620 episode reward: total was -10.220000. running mean: -5.190992\n",
      "ep 3764: ep_len:650 episode reward: total was -17.320000. running mean: -5.312282\n",
      "ep 3764: ep_len:69 episode reward: total was 0.020000. running mean: -5.258959\n",
      "ep 3764: ep_len:145 episode reward: total was 0.640000. running mean: -5.199970\n",
      "ep 3764: ep_len:3 episode reward: total was 0.000000. running mean: -5.147970\n",
      "ep 3764: ep_len:545 episode reward: total was 15.960000. running mean: -4.936890\n",
      "ep 3764: ep_len:645 episode reward: total was -31.500000. running mean: -5.202521\n",
      "epsilon:0.010000 episode_count: 26355. steps_count: 11637746.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3765: ep_len:1155 episode reward: total was -155.270000. running mean: -6.703196\n",
      "ep 3765: ep_len:500 episode reward: total was 11.210000. running mean: -6.524064\n",
      "ep 3765: ep_len:640 episode reward: total was -86.200000. running mean: -7.320824\n",
      "ep 3765: ep_len:500 episode reward: total was 2.320000. running mean: -7.224415\n",
      "ep 3765: ep_len:125 episode reward: total was 3.050000. running mean: -7.121671\n",
      "ep 3765: ep_len:810 episode reward: total was -42.180000. running mean: -7.472254\n",
      "ep 3765: ep_len:550 episode reward: total was -15.850000. running mean: -7.556032\n",
      "epsilon:0.010000 episode_count: 26362. steps_count: 11642026.000000\n",
      "ep 3766: ep_len:570 episode reward: total was -44.170000. running mean: -7.922172\n",
      "ep 3766: ep_len:680 episode reward: total was -18.270000. running mean: -8.025650\n",
      "ep 3766: ep_len:500 episode reward: total was 8.290000. running mean: -7.862493\n",
      "ep 3766: ep_len:355 episode reward: total was 4.830000. running mean: -7.735568\n",
      "ep 3766: ep_len:87 episode reward: total was 4.530000. running mean: -7.612913\n",
      "ep 3766: ep_len:635 episode reward: total was 2.480000. running mean: -7.511984\n",
      "ep 3766: ep_len:515 episode reward: total was -2.270000. running mean: -7.459564\n",
      "epsilon:0.010000 episode_count: 26369. steps_count: 11645368.000000\n",
      "ep 3767: ep_len:115 episode reward: total was 2.080000. running mean: -7.364168\n",
      "ep 3767: ep_len:580 episode reward: total was 17.860000. running mean: -7.111926\n",
      "ep 3767: ep_len:405 episode reward: total was -10.290000. running mean: -7.143707\n",
      "ep 3767: ep_len:500 episode reward: total was -13.610000. running mean: -7.208370\n",
      "ep 3767: ep_len:3 episode reward: total was 0.000000. running mean: -7.136286\n",
      "ep 3767: ep_len:530 episode reward: total was 5.700000. running mean: -7.007924\n",
      "ep 3767: ep_len:500 episode reward: total was -8.470000. running mean: -7.022544\n",
      "epsilon:0.010000 episode_count: 26376. steps_count: 11648001.000000\n",
      "ep 3768: ep_len:505 episode reward: total was -6.770000. running mean: -7.020019\n",
      "ep 3768: ep_len:620 episode reward: total was -20.220000. running mean: -7.152019\n",
      "ep 3768: ep_len:500 episode reward: total was -4.950000. running mean: -7.129999\n",
      "ep 3768: ep_len:125 episode reward: total was 3.110000. running mean: -7.027599\n",
      "ep 3768: ep_len:3 episode reward: total was 0.000000. running mean: -6.957323\n",
      "ep 3768: ep_len:1225 episode reward: total was -156.650000. running mean: -8.454249\n",
      "ep 3768: ep_len:337 episode reward: total was -2.750000. running mean: -8.397207\n",
      "epsilon:0.010000 episode_count: 26383. steps_count: 11651316.000000\n",
      "ep 3769: ep_len:610 episode reward: total was 13.980000. running mean: -8.173435\n",
      "ep 3769: ep_len:605 episode reward: total was 0.390000. running mean: -8.087800\n",
      "ep 3769: ep_len:565 episode reward: total was -15.350000. running mean: -8.160422\n",
      "ep 3769: ep_len:600 episode reward: total was -0.970000. running mean: -8.088518\n",
      "ep 3769: ep_len:3 episode reward: total was 0.000000. running mean: -8.007633\n",
      "ep 3769: ep_len:595 episode reward: total was -14.720000. running mean: -8.074757\n",
      "ep 3769: ep_len:540 episode reward: total was -17.010000. running mean: -8.164109\n",
      "epsilon:0.010000 episode_count: 26390. steps_count: 11654834.000000\n",
      "ep 3770: ep_len:610 episode reward: total was -24.810000. running mean: -8.330568\n",
      "ep 3770: ep_len:530 episode reward: total was 4.710000. running mean: -8.200162\n",
      "ep 3770: ep_len:665 episode reward: total was -13.220000. running mean: -8.250361\n",
      "ep 3770: ep_len:119 episode reward: total was 1.570000. running mean: -8.152157\n",
      "ep 3770: ep_len:3 episode reward: total was 0.000000. running mean: -8.070636\n",
      "ep 3770: ep_len:159 episode reward: total was 4.580000. running mean: -7.944129\n",
      "ep 3770: ep_len:500 episode reward: total was -18.040000. running mean: -8.045088\n",
      "epsilon:0.010000 episode_count: 26397. steps_count: 11657420.000000\n",
      "ep 3771: ep_len:500 episode reward: total was -8.830000. running mean: -8.052937\n",
      "ep 3771: ep_len:720 episode reward: total was -93.570000. running mean: -8.908108\n",
      "ep 3771: ep_len:570 episode reward: total was -3.770000. running mean: -8.856727\n",
      "ep 3771: ep_len:525 episode reward: total was 11.050000. running mean: -8.657659\n",
      "ep 3771: ep_len:44 episode reward: total was 4.000000. running mean: -8.531083\n",
      "ep 3771: ep_len:500 episode reward: total was -8.800000. running mean: -8.533772\n",
      "ep 3771: ep_len:358 episode reward: total was -0.740000. running mean: -8.455834\n",
      "epsilon:0.010000 episode_count: 26404. steps_count: 11660637.000000\n",
      "ep 3772: ep_len:249 episode reward: total was 6.120000. running mean: -8.310076\n",
      "ep 3772: ep_len:505 episode reward: total was 17.220000. running mean: -8.054775\n",
      "ep 3772: ep_len:545 episode reward: total was 4.460000. running mean: -7.929627\n",
      "ep 3772: ep_len:505 episode reward: total was 8.880000. running mean: -7.761531\n",
      "ep 3772: ep_len:117 episode reward: total was 6.060000. running mean: -7.623316\n",
      "ep 3772: ep_len:540 episode reward: total was -31.950000. running mean: -7.866583\n",
      "ep 3772: ep_len:620 episode reward: total was -2.230000. running mean: -7.810217\n",
      "epsilon:0.010000 episode_count: 26411. steps_count: 11663718.000000\n",
      "ep 3773: ep_len:213 episode reward: total was 0.600000. running mean: -7.726115\n",
      "ep 3773: ep_len:520 episode reward: total was -7.340000. running mean: -7.722253\n",
      "ep 3773: ep_len:550 episode reward: total was -6.930000. running mean: -7.714331\n",
      "ep 3773: ep_len:620 episode reward: total was -4.070000. running mean: -7.677888\n",
      "ep 3773: ep_len:101 episode reward: total was 7.040000. running mean: -7.530709\n",
      "ep 3773: ep_len:620 episode reward: total was 3.880000. running mean: -7.416602\n",
      "ep 3773: ep_len:580 episode reward: total was -11.480000. running mean: -7.457236\n",
      "epsilon:0.010000 episode_count: 26418. steps_count: 11666922.000000\n",
      "ep 3774: ep_len:560 episode reward: total was -15.220000. running mean: -7.534863\n",
      "ep 3774: ep_len:590 episode reward: total was 6.770000. running mean: -7.391815\n",
      "ep 3774: ep_len:351 episode reward: total was 4.670000. running mean: -7.271196\n",
      "ep 3774: ep_len:500 episode reward: total was -7.520000. running mean: -7.273685\n",
      "ep 3774: ep_len:103 episode reward: total was -1.460000. running mean: -7.215548\n",
      "ep 3774: ep_len:565 episode reward: total was 1.870000. running mean: -7.124692\n",
      "ep 3774: ep_len:535 episode reward: total was -3.700000. running mean: -7.090445\n",
      "epsilon:0.010000 episode_count: 26425. steps_count: 11670126.000000\n",
      "ep 3775: ep_len:565 episode reward: total was 16.940000. running mean: -6.850141\n",
      "ep 3775: ep_len:600 episode reward: total was -19.300000. running mean: -6.974639\n",
      "ep 3775: ep_len:500 episode reward: total was -1.220000. running mean: -6.917093\n",
      "ep 3775: ep_len:500 episode reward: total was 12.420000. running mean: -6.723722\n",
      "ep 3775: ep_len:3 episode reward: total was 0.000000. running mean: -6.656485\n",
      "ep 3775: ep_len:500 episode reward: total was 4.750000. running mean: -6.542420\n",
      "ep 3775: ep_len:184 episode reward: total was -2.370000. running mean: -6.500696\n",
      "epsilon:0.010000 episode_count: 26432. steps_count: 11672978.000000\n",
      "ep 3776: ep_len:545 episode reward: total was 8.900000. running mean: -6.346689\n",
      "ep 3776: ep_len:595 episode reward: total was -5.830000. running mean: -6.341522\n",
      "ep 3776: ep_len:424 episode reward: total was 1.770000. running mean: -6.260407\n",
      "ep 3776: ep_len:525 episode reward: total was -73.300000. running mean: -6.930803\n",
      "ep 3776: ep_len:115 episode reward: total was -6.430000. running mean: -6.925795\n",
      "ep 3776: ep_len:224 episode reward: total was 8.670000. running mean: -6.769837\n",
      "ep 3776: ep_len:525 episode reward: total was 2.030000. running mean: -6.681838\n",
      "epsilon:0.010000 episode_count: 26439. steps_count: 11675931.000000\n",
      "ep 3777: ep_len:570 episode reward: total was 12.120000. running mean: -6.493820\n",
      "ep 3777: ep_len:575 episode reward: total was -0.740000. running mean: -6.436282\n",
      "ep 3777: ep_len:79 episode reward: total was 0.040000. running mean: -6.371519\n",
      "ep 3777: ep_len:500 episode reward: total was -10.550000. running mean: -6.413304\n",
      "ep 3777: ep_len:2 episode reward: total was 0.000000. running mean: -6.349171\n",
      "ep 3777: ep_len:700 episode reward: total was -26.250000. running mean: -6.548179\n",
      "ep 3777: ep_len:500 episode reward: total was -20.530000. running mean: -6.687997\n",
      "epsilon:0.010000 episode_count: 26446. steps_count: 11678857.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3778: ep_len:211 episode reward: total was -13.880000. running mean: -6.759917\n",
      "ep 3778: ep_len:520 episode reward: total was 8.720000. running mean: -6.605118\n",
      "ep 3778: ep_len:600 episode reward: total was -35.370000. running mean: -6.892767\n",
      "ep 3778: ep_len:575 episode reward: total was 0.570000. running mean: -6.818139\n",
      "ep 3778: ep_len:81 episode reward: total was -4.450000. running mean: -6.794458\n",
      "ep 3778: ep_len:525 episode reward: total was 1.650000. running mean: -6.710013\n",
      "ep 3778: ep_len:605 episode reward: total was -17.900000. running mean: -6.821913\n",
      "epsilon:0.010000 episode_count: 26453. steps_count: 11681974.000000\n",
      "ep 3779: ep_len:500 episode reward: total was 7.970000. running mean: -6.673994\n",
      "ep 3779: ep_len:515 episode reward: total was 20.310000. running mean: -6.404154\n",
      "ep 3779: ep_len:505 episode reward: total was -3.070000. running mean: -6.370813\n",
      "ep 3779: ep_len:525 episode reward: total was -5.560000. running mean: -6.362704\n",
      "ep 3779: ep_len:3 episode reward: total was 0.000000. running mean: -6.299077\n",
      "ep 3779: ep_len:530 episode reward: total was -35.420000. running mean: -6.590287\n",
      "ep 3779: ep_len:287 episode reward: total was -0.800000. running mean: -6.532384\n",
      "epsilon:0.010000 episode_count: 26460. steps_count: 11684839.000000\n",
      "ep 3780: ep_len:500 episode reward: total was 2.950000. running mean: -6.437560\n",
      "ep 3780: ep_len:680 episode reward: total was -24.290000. running mean: -6.616084\n",
      "ep 3780: ep_len:625 episode reward: total was -28.500000. running mean: -6.834923\n",
      "ep 3780: ep_len:125 episode reward: total was 3.600000. running mean: -6.730574\n",
      "ep 3780: ep_len:3 episode reward: total was 0.000000. running mean: -6.663268\n",
      "ep 3780: ep_len:615 episode reward: total was -3.520000. running mean: -6.631836\n",
      "ep 3780: ep_len:510 episode reward: total was -9.570000. running mean: -6.661217\n",
      "epsilon:0.010000 episode_count: 26467. steps_count: 11687897.000000\n",
      "ep 3781: ep_len:500 episode reward: total was 7.110000. running mean: -6.523505\n",
      "ep 3781: ep_len:560 episode reward: total was 1.120000. running mean: -6.447070\n",
      "ep 3781: ep_len:540 episode reward: total was -2.910000. running mean: -6.411699\n",
      "ep 3781: ep_len:585 episode reward: total was 13.950000. running mean: -6.208082\n",
      "ep 3781: ep_len:122 episode reward: total was 3.560000. running mean: -6.110402\n",
      "ep 3781: ep_len:640 episode reward: total was -16.470000. running mean: -6.213998\n",
      "ep 3781: ep_len:334 episode reward: total was -4.310000. running mean: -6.194958\n",
      "epsilon:0.010000 episode_count: 26474. steps_count: 11691178.000000\n",
      "ep 3782: ep_len:600 episode reward: total was 1.110000. running mean: -6.121908\n",
      "ep 3782: ep_len:500 episode reward: total was -21.270000. running mean: -6.273389\n",
      "ep 3782: ep_len:570 episode reward: total was -4.870000. running mean: -6.259355\n",
      "ep 3782: ep_len:500 episode reward: total was -9.050000. running mean: -6.287262\n",
      "ep 3782: ep_len:94 episode reward: total was 6.040000. running mean: -6.163989\n",
      "ep 3782: ep_len:505 episode reward: total was 1.920000. running mean: -6.083149\n",
      "ep 3782: ep_len:206 episode reward: total was -4.860000. running mean: -6.070918\n",
      "epsilon:0.010000 episode_count: 26481. steps_count: 11694153.000000\n",
      "ep 3783: ep_len:600 episode reward: total was 7.660000. running mean: -5.933608\n",
      "ep 3783: ep_len:500 episode reward: total was -9.430000. running mean: -5.968572\n",
      "ep 3783: ep_len:420 episode reward: total was -5.810000. running mean: -5.966987\n",
      "ep 3783: ep_len:500 episode reward: total was -0.490000. running mean: -5.912217\n",
      "ep 3783: ep_len:3 episode reward: total was 0.000000. running mean: -5.853095\n",
      "ep 3783: ep_len:500 episode reward: total was 1.390000. running mean: -5.780664\n",
      "ep 3783: ep_len:575 episode reward: total was -11.800000. running mean: -5.840857\n",
      "epsilon:0.010000 episode_count: 26488. steps_count: 11697251.000000\n",
      "ep 3784: ep_len:515 episode reward: total was 2.440000. running mean: -5.758048\n",
      "ep 3784: ep_len:333 episode reward: total was -34.820000. running mean: -6.048668\n",
      "ep 3784: ep_len:394 episode reward: total was 0.180000. running mean: -5.986381\n",
      "ep 3784: ep_len:128 episode reward: total was 5.110000. running mean: -5.875417\n",
      "ep 3784: ep_len:3 episode reward: total was 0.000000. running mean: -5.816663\n",
      "ep 3784: ep_len:575 episode reward: total was -11.100000. running mean: -5.869497\n",
      "ep 3784: ep_len:575 episode reward: total was -29.450000. running mean: -6.105302\n",
      "epsilon:0.010000 episode_count: 26495. steps_count: 11699774.000000\n",
      "ep 3785: ep_len:560 episode reward: total was 8.090000. running mean: -5.963349\n",
      "ep 3785: ep_len:351 episode reward: total was -0.810000. running mean: -5.911815\n",
      "ep 3785: ep_len:600 episode reward: total was -3.010000. running mean: -5.882797\n",
      "ep 3785: ep_len:123 episode reward: total was 3.080000. running mean: -5.793169\n",
      "ep 3785: ep_len:3 episode reward: total was 0.000000. running mean: -5.735237\n",
      "ep 3785: ep_len:500 episode reward: total was -4.440000. running mean: -5.722285\n",
      "ep 3785: ep_len:283 episode reward: total was -4.360000. running mean: -5.708662\n",
      "epsilon:0.010000 episode_count: 26502. steps_count: 11702194.000000\n",
      "ep 3786: ep_len:600 episode reward: total was 4.940000. running mean: -5.602175\n",
      "ep 3786: ep_len:500 episode reward: total was -12.310000. running mean: -5.669254\n",
      "ep 3786: ep_len:755 episode reward: total was -15.580000. running mean: -5.768361\n",
      "ep 3786: ep_len:505 episode reward: total was 8.360000. running mean: -5.627078\n",
      "ep 3786: ep_len:73 episode reward: total was -1.470000. running mean: -5.585507\n",
      "ep 3786: ep_len:565 episode reward: total was -11.780000. running mean: -5.647452\n",
      "ep 3786: ep_len:540 episode reward: total was -10.320000. running mean: -5.694177\n",
      "epsilon:0.010000 episode_count: 26509. steps_count: 11705732.000000\n",
      "ep 3787: ep_len:615 episode reward: total was -1.370000. running mean: -5.650935\n",
      "ep 3787: ep_len:615 episode reward: total was -58.020000. running mean: -6.174626\n",
      "ep 3787: ep_len:625 episode reward: total was -29.290000. running mean: -6.405780\n",
      "ep 3787: ep_len:44 episode reward: total was 2.050000. running mean: -6.321222\n",
      "ep 3787: ep_len:3 episode reward: total was 0.000000. running mean: -6.258010\n",
      "ep 3787: ep_len:520 episode reward: total was 5.410000. running mean: -6.141330\n",
      "ep 3787: ep_len:565 episode reward: total was -1.520000. running mean: -6.095116\n",
      "epsilon:0.010000 episode_count: 26516. steps_count: 11708719.000000\n",
      "ep 3788: ep_len:505 episode reward: total was 2.820000. running mean: -6.005965\n",
      "ep 3788: ep_len:500 episode reward: total was 0.340000. running mean: -5.942506\n",
      "ep 3788: ep_len:535 episode reward: total was -37.220000. running mean: -6.255281\n",
      "ep 3788: ep_len:520 episode reward: total was 6.400000. running mean: -6.128728\n",
      "ep 3788: ep_len:3 episode reward: total was 0.000000. running mean: -6.067440\n",
      "ep 3788: ep_len:700 episode reward: total was -87.630000. running mean: -6.883066\n",
      "ep 3788: ep_len:337 episode reward: total was -7.820000. running mean: -6.892435\n",
      "epsilon:0.010000 episode_count: 26523. steps_count: 11711819.000000\n",
      "ep 3789: ep_len:251 episode reward: total was 5.120000. running mean: -6.772311\n",
      "ep 3789: ep_len:600 episode reward: total was -3.090000. running mean: -6.735488\n",
      "ep 3789: ep_len:610 episode reward: total was 6.540000. running mean: -6.602733\n",
      "ep 3789: ep_len:168 episode reward: total was 5.630000. running mean: -6.480406\n",
      "ep 3789: ep_len:87 episode reward: total was 1.010000. running mean: -6.405502\n",
      "ep 3789: ep_len:560 episode reward: total was -9.740000. running mean: -6.438847\n",
      "ep 3789: ep_len:500 episode reward: total was -18.590000. running mean: -6.560358\n",
      "epsilon:0.010000 episode_count: 26530. steps_count: 11714595.000000\n",
      "ep 3790: ep_len:106 episode reward: total was -3.450000. running mean: -6.529255\n",
      "ep 3790: ep_len:197 episode reward: total was -4.390000. running mean: -6.507862\n",
      "ep 3790: ep_len:560 episode reward: total was -40.440000. running mean: -6.847183\n",
      "ep 3790: ep_len:530 episode reward: total was 14.620000. running mean: -6.632512\n",
      "ep 3790: ep_len:40 episode reward: total was -3.000000. running mean: -6.596186\n",
      "ep 3790: ep_len:540 episode reward: total was -10.900000. running mean: -6.639225\n",
      "ep 3790: ep_len:570 episode reward: total was -22.470000. running mean: -6.797532\n",
      "epsilon:0.010000 episode_count: 26537. steps_count: 11717138.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3791: ep_len:500 episode reward: total was 7.800000. running mean: -6.651557\n",
      "ep 3791: ep_len:364 episode reward: total was -34.790000. running mean: -6.932941\n",
      "ep 3791: ep_len:575 episode reward: total was 7.000000. running mean: -6.793612\n",
      "ep 3791: ep_len:500 episode reward: total was -21.140000. running mean: -6.937076\n",
      "ep 3791: ep_len:3 episode reward: total was 0.000000. running mean: -6.867705\n",
      "ep 3791: ep_len:560 episode reward: total was 8.090000. running mean: -6.718128\n",
      "ep 3791: ep_len:198 episode reward: total was -5.890000. running mean: -6.709847\n",
      "epsilon:0.010000 episode_count: 26544. steps_count: 11719838.000000\n",
      "ep 3792: ep_len:745 episode reward: total was -29.160000. running mean: -6.934348\n",
      "ep 3792: ep_len:580 episode reward: total was 7.970000. running mean: -6.785305\n",
      "ep 3792: ep_len:550 episode reward: total was -2.860000. running mean: -6.746052\n",
      "ep 3792: ep_len:500 episode reward: total was 2.360000. running mean: -6.654991\n",
      "ep 3792: ep_len:3 episode reward: total was 0.000000. running mean: -6.588441\n",
      "ep 3792: ep_len:505 episode reward: total was -1.370000. running mean: -6.536257\n",
      "ep 3792: ep_len:615 episode reward: total was -1.500000. running mean: -6.485894\n",
      "epsilon:0.010000 episode_count: 26551. steps_count: 11723336.000000\n",
      "ep 3793: ep_len:610 episode reward: total was -9.040000. running mean: -6.511435\n",
      "ep 3793: ep_len:595 episode reward: total was 3.200000. running mean: -6.414321\n",
      "ep 3793: ep_len:500 episode reward: total was -0.970000. running mean: -6.359878\n",
      "ep 3793: ep_len:515 episode reward: total was 12.970000. running mean: -6.166579\n",
      "ep 3793: ep_len:3 episode reward: total was 0.000000. running mean: -6.104913\n",
      "ep 3793: ep_len:640 episode reward: total was -1.290000. running mean: -6.056764\n",
      "ep 3793: ep_len:625 episode reward: total was -17.370000. running mean: -6.169897\n",
      "epsilon:0.010000 episode_count: 26558. steps_count: 11726824.000000\n",
      "ep 3794: ep_len:520 episode reward: total was -11.380000. running mean: -6.221998\n",
      "ep 3794: ep_len:615 episode reward: total was -0.030000. running mean: -6.160078\n",
      "ep 3794: ep_len:685 episode reward: total was -25.730000. running mean: -6.355777\n",
      "ep 3794: ep_len:500 episode reward: total was 11.440000. running mean: -6.177819\n",
      "ep 3794: ep_len:3 episode reward: total was 0.000000. running mean: -6.116041\n",
      "ep 3794: ep_len:238 episode reward: total was 6.160000. running mean: -5.993280\n",
      "ep 3794: ep_len:515 episode reward: total was -4.780000. running mean: -5.981148\n",
      "epsilon:0.010000 episode_count: 26565. steps_count: 11729900.000000\n",
      "ep 3795: ep_len:500 episode reward: total was -2.750000. running mean: -5.948836\n",
      "ep 3795: ep_len:600 episode reward: total was -6.850000. running mean: -5.957848\n",
      "ep 3795: ep_len:755 episode reward: total was -43.220000. running mean: -6.330469\n",
      "ep 3795: ep_len:380 episode reward: total was 0.370000. running mean: -6.263465\n",
      "ep 3795: ep_len:103 episode reward: total was 5.540000. running mean: -6.145430\n",
      "ep 3795: ep_len:545 episode reward: total was 0.160000. running mean: -6.082376\n",
      "ep 3795: ep_len:590 episode reward: total was -7.990000. running mean: -6.101452\n",
      "epsilon:0.010000 episode_count: 26572. steps_count: 11733373.000000\n",
      "ep 3796: ep_len:580 episode reward: total was 5.110000. running mean: -5.989337\n",
      "ep 3796: ep_len:745 episode reward: total was -68.430000. running mean: -6.613744\n",
      "ep 3796: ep_len:500 episode reward: total was 4.950000. running mean: -6.498107\n",
      "ep 3796: ep_len:565 episode reward: total was 9.440000. running mean: -6.338726\n",
      "ep 3796: ep_len:84 episode reward: total was 3.540000. running mean: -6.239938\n",
      "ep 3796: ep_len:660 episode reward: total was -24.250000. running mean: -6.420039\n",
      "ep 3796: ep_len:276 episode reward: total was 2.240000. running mean: -6.333439\n",
      "epsilon:0.010000 episode_count: 26579. steps_count: 11736783.000000\n",
      "ep 3797: ep_len:560 episode reward: total was 1.630000. running mean: -6.253804\n",
      "ep 3797: ep_len:500 episode reward: total was -41.850000. running mean: -6.609766\n",
      "ep 3797: ep_len:645 episode reward: total was -2.830000. running mean: -6.571968\n",
      "ep 3797: ep_len:530 episode reward: total was -2.950000. running mean: -6.535749\n",
      "ep 3797: ep_len:3 episode reward: total was 0.000000. running mean: -6.470391\n",
      "ep 3797: ep_len:310 episode reward: total was -19.330000. running mean: -6.598987\n",
      "ep 3797: ep_len:525 episode reward: total was -4.240000. running mean: -6.575397\n",
      "epsilon:0.010000 episode_count: 26586. steps_count: 11739856.000000\n",
      "ep 3798: ep_len:134 episode reward: total was 1.560000. running mean: -6.494044\n",
      "ep 3798: ep_len:575 episode reward: total was -0.450000. running mean: -6.433603\n",
      "ep 3798: ep_len:530 episode reward: total was -22.810000. running mean: -6.597367\n",
      "ep 3798: ep_len:121 episode reward: total was 2.070000. running mean: -6.510693\n",
      "ep 3798: ep_len:3 episode reward: total was 0.000000. running mean: -6.445586\n",
      "ep 3798: ep_len:670 episode reward: total was -9.140000. running mean: -6.472531\n",
      "ep 3798: ep_len:535 episode reward: total was -16.400000. running mean: -6.571805\n",
      "epsilon:0.010000 episode_count: 26593. steps_count: 11742424.000000\n",
      "ep 3799: ep_len:605 episode reward: total was -1.030000. running mean: -6.516387\n",
      "ep 3799: ep_len:510 episode reward: total was 7.560000. running mean: -6.375623\n",
      "ep 3799: ep_len:500 episode reward: total was 1.430000. running mean: -6.297567\n",
      "ep 3799: ep_len:500 episode reward: total was 16.080000. running mean: -6.073791\n",
      "ep 3799: ep_len:92 episode reward: total was 4.540000. running mean: -5.967654\n",
      "ep 3799: ep_len:655 episode reward: total was -48.650000. running mean: -6.394477\n",
      "ep 3799: ep_len:283 episode reward: total was -2.790000. running mean: -6.358432\n",
      "epsilon:0.010000 episode_count: 26600. steps_count: 11745569.000000\n",
      "ep 3800: ep_len:500 episode reward: total was -0.410000. running mean: -6.298948\n",
      "ep 3800: ep_len:319 episode reward: total was -2.830000. running mean: -6.264258\n",
      "ep 3800: ep_len:500 episode reward: total was -12.460000. running mean: -6.326216\n",
      "ep 3800: ep_len:515 episode reward: total was -17.590000. running mean: -6.438854\n",
      "ep 3800: ep_len:3 episode reward: total was 0.000000. running mean: -6.374465\n",
      "ep 3800: ep_len:500 episode reward: total was 5.110000. running mean: -6.259620\n",
      "ep 3800: ep_len:500 episode reward: total was -0.590000. running mean: -6.202924\n",
      "epsilon:0.010000 episode_count: 26607. steps_count: 11748406.000000\n",
      "ep 3801: ep_len:515 episode reward: total was -0.910000. running mean: -6.149995\n",
      "ep 3801: ep_len:178 episode reward: total was -0.880000. running mean: -6.097295\n",
      "ep 3801: ep_len:725 episode reward: total was -34.760000. running mean: -6.383922\n",
      "ep 3801: ep_len:539 episode reward: total was -25.920000. running mean: -6.579283\n",
      "ep 3801: ep_len:83 episode reward: total was 6.540000. running mean: -6.448090\n",
      "ep 3801: ep_len:510 episode reward: total was 3.150000. running mean: -6.352109\n",
      "ep 3801: ep_len:308 episode reward: total was -1.280000. running mean: -6.301388\n",
      "epsilon:0.010000 episode_count: 26614. steps_count: 11751264.000000\n",
      "ep 3802: ep_len:595 episode reward: total was -21.260000. running mean: -6.450974\n",
      "ep 3802: ep_len:500 episode reward: total was 19.700000. running mean: -6.189464\n",
      "ep 3802: ep_len:580 episode reward: total was 13.520000. running mean: -5.992370\n",
      "ep 3802: ep_len:132 episode reward: total was 5.610000. running mean: -5.876346\n",
      "ep 3802: ep_len:68 episode reward: total was -8.960000. running mean: -5.907183\n",
      "ep 3802: ep_len:590 episode reward: total was -11.730000. running mean: -5.965411\n",
      "ep 3802: ep_len:500 episode reward: total was -2.080000. running mean: -5.926557\n",
      "epsilon:0.010000 episode_count: 26621. steps_count: 11754229.000000\n",
      "ep 3803: ep_len:630 episode reward: total was 11.120000. running mean: -5.756091\n",
      "ep 3803: ep_len:695 episode reward: total was -8.230000. running mean: -5.780830\n",
      "ep 3803: ep_len:500 episode reward: total was -6.230000. running mean: -5.785322\n",
      "ep 3803: ep_len:520 episode reward: total was 18.110000. running mean: -5.546369\n",
      "ep 3803: ep_len:3 episode reward: total was 0.000000. running mean: -5.490905\n",
      "ep 3803: ep_len:500 episode reward: total was -29.550000. running mean: -5.731496\n",
      "ep 3803: ep_len:530 episode reward: total was 2.010000. running mean: -5.654081\n",
      "epsilon:0.010000 episode_count: 26628. steps_count: 11757607.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3804: ep_len:510 episode reward: total was 6.950000. running mean: -5.528040\n",
      "ep 3804: ep_len:500 episode reward: total was -12.850000. running mean: -5.601260\n",
      "ep 3804: ep_len:530 episode reward: total was -28.350000. running mean: -5.828747\n",
      "ep 3804: ep_len:515 episode reward: total was 13.460000. running mean: -5.635860\n",
      "ep 3804: ep_len:3 episode reward: total was 0.000000. running mean: -5.579501\n",
      "ep 3804: ep_len:635 episode reward: total was -6.920000. running mean: -5.592906\n",
      "ep 3804: ep_len:500 episode reward: total was -22.500000. running mean: -5.761977\n",
      "epsilon:0.010000 episode_count: 26635. steps_count: 11760800.000000\n",
      "ep 3805: ep_len:500 episode reward: total was 20.840000. running mean: -5.495957\n",
      "ep 3805: ep_len:575 episode reward: total was -4.860000. running mean: -5.489598\n",
      "ep 3805: ep_len:500 episode reward: total was 3.450000. running mean: -5.400202\n",
      "ep 3805: ep_len:525 episode reward: total was -12.030000. running mean: -5.466500\n",
      "ep 3805: ep_len:3 episode reward: total was 0.000000. running mean: -5.411835\n",
      "ep 3805: ep_len:655 episode reward: total was -7.210000. running mean: -5.429816\n",
      "ep 3805: ep_len:555 episode reward: total was -2.000000. running mean: -5.395518\n",
      "epsilon:0.010000 episode_count: 26642. steps_count: 11764113.000000\n",
      "ep 3806: ep_len:550 episode reward: total was -0.100000. running mean: -5.342563\n",
      "ep 3806: ep_len:925 episode reward: total was -48.020000. running mean: -5.769337\n",
      "ep 3806: ep_len:500 episode reward: total was 5.150000. running mean: -5.660144\n",
      "ep 3806: ep_len:525 episode reward: total was -5.600000. running mean: -5.659543\n",
      "ep 3806: ep_len:3 episode reward: total was 0.000000. running mean: -5.602947\n",
      "ep 3806: ep_len:530 episode reward: total was -6.050000. running mean: -5.607418\n",
      "ep 3806: ep_len:203 episode reward: total was -4.350000. running mean: -5.594844\n",
      "epsilon:0.010000 episode_count: 26649. steps_count: 11767349.000000\n",
      "ep 3807: ep_len:625 episode reward: total was -56.560000. running mean: -6.104495\n",
      "ep 3807: ep_len:585 episode reward: total was 9.120000. running mean: -5.952250\n",
      "ep 3807: ep_len:585 episode reward: total was 2.980000. running mean: -5.862928\n",
      "ep 3807: ep_len:575 episode reward: total was 12.520000. running mean: -5.679098\n",
      "ep 3807: ep_len:78 episode reward: total was -11.970000. running mean: -5.742007\n",
      "ep 3807: ep_len:505 episode reward: total was -58.680000. running mean: -6.271387\n",
      "ep 3807: ep_len:203 episode reward: total was -3.840000. running mean: -6.247073\n",
      "epsilon:0.010000 episode_count: 26656. steps_count: 11770505.000000\n",
      "ep 3808: ep_len:500 episode reward: total was -21.000000. running mean: -6.394603\n",
      "ep 3808: ep_len:790 episode reward: total was -59.870000. running mean: -6.929357\n",
      "ep 3808: ep_len:635 episode reward: total was -11.180000. running mean: -6.971863\n",
      "ep 3808: ep_len:550 episode reward: total was 8.930000. running mean: -6.812844\n",
      "ep 3808: ep_len:3 episode reward: total was 0.000000. running mean: -6.744716\n",
      "ep 3808: ep_len:605 episode reward: total was 6.520000. running mean: -6.612069\n",
      "ep 3808: ep_len:525 episode reward: total was -6.260000. running mean: -6.608548\n",
      "epsilon:0.010000 episode_count: 26663. steps_count: 11774113.000000\n",
      "ep 3809: ep_len:540 episode reward: total was 12.460000. running mean: -6.417863\n",
      "ep 3809: ep_len:500 episode reward: total was -11.850000. running mean: -6.472184\n",
      "ep 3809: ep_len:525 episode reward: total was 8.970000. running mean: -6.317762\n",
      "ep 3809: ep_len:121 episode reward: total was 1.060000. running mean: -6.243985\n",
      "ep 3809: ep_len:3 episode reward: total was 0.000000. running mean: -6.181545\n",
      "ep 3809: ep_len:660 episode reward: total was -5.110000. running mean: -6.170829\n",
      "ep 3809: ep_len:620 episode reward: total was -12.000000. running mean: -6.229121\n",
      "epsilon:0.010000 episode_count: 26670. steps_count: 11777082.000000\n",
      "ep 3810: ep_len:500 episode reward: total was 13.830000. running mean: -6.028530\n",
      "ep 3810: ep_len:615 episode reward: total was 9.990000. running mean: -5.868345\n",
      "ep 3810: ep_len:585 episode reward: total was 2.930000. running mean: -5.780361\n",
      "ep 3810: ep_len:535 episode reward: total was -9.630000. running mean: -5.818857\n",
      "ep 3810: ep_len:3 episode reward: total was 0.000000. running mean: -5.760669\n",
      "ep 3810: ep_len:650 episode reward: total was -16.930000. running mean: -5.872362\n",
      "ep 3810: ep_len:605 episode reward: total was -1.250000. running mean: -5.826139\n",
      "epsilon:0.010000 episode_count: 26677. steps_count: 11780575.000000\n",
      "ep 3811: ep_len:500 episode reward: total was 16.860000. running mean: -5.599277\n",
      "ep 3811: ep_len:515 episode reward: total was -1.870000. running mean: -5.561984\n",
      "ep 3811: ep_len:520 episode reward: total was 2.300000. running mean: -5.483365\n",
      "ep 3811: ep_len:500 episode reward: total was -16.490000. running mean: -5.593431\n",
      "ep 3811: ep_len:3 episode reward: total was 0.000000. running mean: -5.537497\n",
      "ep 3811: ep_len:545 episode reward: total was -1.550000. running mean: -5.497622\n",
      "ep 3811: ep_len:560 episode reward: total was -1.040000. running mean: -5.453045\n",
      "epsilon:0.010000 episode_count: 26684. steps_count: 11783718.000000\n",
      "ep 3812: ep_len:630 episode reward: total was 7.170000. running mean: -5.326815\n",
      "ep 3812: ep_len:500 episode reward: total was -2.500000. running mean: -5.298547\n",
      "ep 3812: ep_len:500 episode reward: total was -10.990000. running mean: -5.355461\n",
      "ep 3812: ep_len:394 episode reward: total was 9.340000. running mean: -5.208507\n",
      "ep 3812: ep_len:3 episode reward: total was 0.000000. running mean: -5.156422\n",
      "ep 3812: ep_len:620 episode reward: total was -2.550000. running mean: -5.130357\n",
      "ep 3812: ep_len:176 episode reward: total was 3.680000. running mean: -5.042254\n",
      "epsilon:0.010000 episode_count: 26691. steps_count: 11786541.000000\n",
      "ep 3813: ep_len:500 episode reward: total was 1.940000. running mean: -4.972431\n",
      "ep 3813: ep_len:580 episode reward: total was -3.870000. running mean: -4.961407\n",
      "ep 3813: ep_len:745 episode reward: total was -27.990000. running mean: -5.191693\n",
      "ep 3813: ep_len:375 episode reward: total was 8.370000. running mean: -5.056076\n",
      "ep 3813: ep_len:127 episode reward: total was 6.570000. running mean: -4.939815\n",
      "ep 3813: ep_len:525 episode reward: total was 5.480000. running mean: -4.835617\n",
      "ep 3813: ep_len:595 episode reward: total was -28.470000. running mean: -5.071961\n",
      "epsilon:0.010000 episode_count: 26698. steps_count: 11789988.000000\n",
      "ep 3814: ep_len:500 episode reward: total was 14.870000. running mean: -4.872541\n",
      "ep 3814: ep_len:615 episode reward: total was 2.230000. running mean: -4.801516\n",
      "ep 3814: ep_len:565 episode reward: total was -27.010000. running mean: -5.023601\n",
      "ep 3814: ep_len:500 episode reward: total was -11.070000. running mean: -5.084065\n",
      "ep 3814: ep_len:3 episode reward: total was 0.000000. running mean: -5.033224\n",
      "ep 3814: ep_len:565 episode reward: total was -1.650000. running mean: -4.999392\n",
      "ep 3814: ep_len:580 episode reward: total was -25.910000. running mean: -5.208498\n",
      "epsilon:0.010000 episode_count: 26705. steps_count: 11793316.000000\n",
      "ep 3815: ep_len:595 episode reward: total was 10.510000. running mean: -5.051313\n",
      "ep 3815: ep_len:530 episode reward: total was -5.300000. running mean: -5.053800\n",
      "ep 3815: ep_len:580 episode reward: total was -8.120000. running mean: -5.084462\n",
      "ep 3815: ep_len:500 episode reward: total was -4.980000. running mean: -5.083417\n",
      "ep 3815: ep_len:3 episode reward: total was 0.000000. running mean: -5.032583\n",
      "ep 3815: ep_len:515 episode reward: total was -2.410000. running mean: -5.006357\n",
      "ep 3815: ep_len:545 episode reward: total was -6.440000. running mean: -5.020694\n",
      "epsilon:0.010000 episode_count: 26712. steps_count: 11796584.000000\n",
      "ep 3816: ep_len:108 episode reward: total was -0.970000. running mean: -4.980187\n",
      "ep 3816: ep_len:500 episode reward: total was 16.200000. running mean: -4.768385\n",
      "ep 3816: ep_len:605 episode reward: total was -34.770000. running mean: -5.068401\n",
      "ep 3816: ep_len:40 episode reward: total was 1.040000. running mean: -5.007317\n",
      "ep 3816: ep_len:53 episode reward: total was 5.000000. running mean: -4.907244\n",
      "ep 3816: ep_len:515 episode reward: total was -8.960000. running mean: -4.947771\n",
      "ep 3816: ep_len:585 episode reward: total was -8.380000. running mean: -4.982094\n",
      "epsilon:0.010000 episode_count: 26719. steps_count: 11798990.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3817: ep_len:580 episode reward: total was 9.660000. running mean: -4.835673\n",
      "ep 3817: ep_len:610 episode reward: total was 10.440000. running mean: -4.682916\n",
      "ep 3817: ep_len:670 episode reward: total was -10.900000. running mean: -4.745087\n",
      "ep 3817: ep_len:625 episode reward: total was -32.070000. running mean: -5.018336\n",
      "ep 3817: ep_len:3 episode reward: total was 0.000000. running mean: -4.968153\n",
      "ep 3817: ep_len:590 episode reward: total was -5.610000. running mean: -4.974571\n",
      "ep 3817: ep_len:585 episode reward: total was -14.640000. running mean: -5.071225\n",
      "epsilon:0.010000 episode_count: 26726. steps_count: 11802653.000000\n",
      "ep 3818: ep_len:117 episode reward: total was 1.560000. running mean: -5.004913\n",
      "ep 3818: ep_len:500 episode reward: total was -7.900000. running mean: -5.033864\n",
      "ep 3818: ep_len:830 episode reward: total was -49.710000. running mean: -5.480625\n",
      "ep 3818: ep_len:510 episode reward: total was -12.510000. running mean: -5.550919\n",
      "ep 3818: ep_len:3 episode reward: total was 0.000000. running mean: -5.495410\n",
      "ep 3818: ep_len:500 episode reward: total was -5.550000. running mean: -5.495956\n",
      "ep 3818: ep_len:500 episode reward: total was -27.060000. running mean: -5.711596\n",
      "epsilon:0.010000 episode_count: 26733. steps_count: 11805613.000000\n",
      "ep 3819: ep_len:645 episode reward: total was -16.660000. running mean: -5.821080\n",
      "ep 3819: ep_len:615 episode reward: total was 3.000000. running mean: -5.732869\n",
      "ep 3819: ep_len:550 episode reward: total was -0.520000. running mean: -5.680741\n",
      "ep 3819: ep_len:394 episode reward: total was 8.910000. running mean: -5.534833\n",
      "ep 3819: ep_len:98 episode reward: total was 6.540000. running mean: -5.414085\n",
      "ep 3819: ep_len:630 episode reward: total was -26.280000. running mean: -5.622744\n",
      "ep 3819: ep_len:575 episode reward: total was -3.000000. running mean: -5.596517\n",
      "epsilon:0.010000 episode_count: 26740. steps_count: 11809120.000000\n",
      "ep 3820: ep_len:124 episode reward: total was 2.580000. running mean: -5.514752\n",
      "ep 3820: ep_len:500 episode reward: total was -10.440000. running mean: -5.564004\n",
      "ep 3820: ep_len:540 episode reward: total was 9.490000. running mean: -5.413464\n",
      "ep 3820: ep_len:160 episode reward: total was 0.610000. running mean: -5.353229\n",
      "ep 3820: ep_len:87 episode reward: total was 2.540000. running mean: -5.274297\n",
      "ep 3820: ep_len:500 episode reward: total was -3.250000. running mean: -5.254054\n",
      "ep 3820: ep_len:348 episode reward: total was -4.800000. running mean: -5.249514\n",
      "epsilon:0.010000 episode_count: 26747. steps_count: 11811379.000000\n",
      "ep 3821: ep_len:605 episode reward: total was 7.490000. running mean: -5.122118\n",
      "ep 3821: ep_len:500 episode reward: total was 11.250000. running mean: -4.958397\n",
      "ep 3821: ep_len:600 episode reward: total was -20.680000. running mean: -5.115613\n",
      "ep 3821: ep_len:505 episode reward: total was -11.100000. running mean: -5.175457\n",
      "ep 3821: ep_len:3 episode reward: total was 0.000000. running mean: -5.123703\n",
      "ep 3821: ep_len:510 episode reward: total was -11.250000. running mean: -5.184966\n",
      "ep 3821: ep_len:725 episode reward: total was -33.820000. running mean: -5.471316\n",
      "epsilon:0.010000 episode_count: 26754. steps_count: 11814827.000000\n",
      "ep 3822: ep_len:545 episode reward: total was -27.540000. running mean: -5.692003\n",
      "ep 3822: ep_len:500 episode reward: total was -2.010000. running mean: -5.655183\n",
      "ep 3822: ep_len:710 episode reward: total was -20.690000. running mean: -5.805531\n",
      "ep 3822: ep_len:413 episode reward: total was 2.370000. running mean: -5.723776\n",
      "ep 3822: ep_len:114 episode reward: total was -9.450000. running mean: -5.761038\n",
      "ep 3822: ep_len:655 episode reward: total was -9.140000. running mean: -5.794827\n",
      "ep 3822: ep_len:770 episode reward: total was -114.820000. running mean: -6.885079\n",
      "epsilon:0.010000 episode_count: 26761. steps_count: 11818534.000000\n",
      "ep 3823: ep_len:540 episode reward: total was -14.340000. running mean: -6.959628\n",
      "ep 3823: ep_len:188 episode reward: total was -2.340000. running mean: -6.913432\n",
      "ep 3823: ep_len:580 episode reward: total was -13.740000. running mean: -6.981698\n",
      "ep 3823: ep_len:116 episode reward: total was -0.400000. running mean: -6.915881\n",
      "ep 3823: ep_len:85 episode reward: total was 4.530000. running mean: -6.801422\n",
      "ep 3823: ep_len:505 episode reward: total was -51.580000. running mean: -7.249208\n",
      "ep 3823: ep_len:510 episode reward: total was -16.570000. running mean: -7.342416\n",
      "epsilon:0.010000 episode_count: 26768. steps_count: 11821058.000000\n",
      "ep 3824: ep_len:580 episode reward: total was -6.120000. running mean: -7.330192\n",
      "ep 3824: ep_len:500 episode reward: total was -18.490000. running mean: -7.441790\n",
      "ep 3824: ep_len:79 episode reward: total was 1.050000. running mean: -7.356872\n",
      "ep 3824: ep_len:520 episode reward: total was -20.610000. running mean: -7.489403\n",
      "ep 3824: ep_len:77 episode reward: total was 2.030000. running mean: -7.394209\n",
      "ep 3824: ep_len:570 episode reward: total was -0.050000. running mean: -7.320767\n",
      "ep 3824: ep_len:505 episode reward: total was -0.520000. running mean: -7.252759\n",
      "epsilon:0.010000 episode_count: 26775. steps_count: 11823889.000000\n",
      "ep 3825: ep_len:500 episode reward: total was -27.310000. running mean: -7.453332\n",
      "ep 3825: ep_len:500 episode reward: total was 4.700000. running mean: -7.331798\n",
      "ep 3825: ep_len:530 episode reward: total was -6.740000. running mean: -7.325880\n",
      "ep 3825: ep_len:510 episode reward: total was -14.530000. running mean: -7.397922\n",
      "ep 3825: ep_len:3 episode reward: total was 0.000000. running mean: -7.323942\n",
      "ep 3825: ep_len:224 episode reward: total was 6.620000. running mean: -7.184503\n",
      "ep 3825: ep_len:555 episode reward: total was 2.730000. running mean: -7.085358\n",
      "epsilon:0.010000 episode_count: 26782. steps_count: 11826711.000000\n",
      "ep 3826: ep_len:107 episode reward: total was -1.950000. running mean: -7.034004\n",
      "ep 3826: ep_len:500 episode reward: total was -30.550000. running mean: -7.269164\n",
      "ep 3826: ep_len:437 episode reward: total was -17.240000. running mean: -7.368873\n",
      "ep 3826: ep_len:600 episode reward: total was 8.670000. running mean: -7.208484\n",
      "ep 3826: ep_len:95 episode reward: total was 6.050000. running mean: -7.075899\n",
      "ep 3826: ep_len:246 episode reward: total was 5.170000. running mean: -6.953440\n",
      "ep 3826: ep_len:183 episode reward: total was -4.880000. running mean: -6.932706\n",
      "epsilon:0.010000 episode_count: 26789. steps_count: 11828879.000000\n",
      "ep 3827: ep_len:600 episode reward: total was 19.540000. running mean: -6.667979\n",
      "ep 3827: ep_len:560 episode reward: total was 3.220000. running mean: -6.569099\n",
      "ep 3827: ep_len:770 episode reward: total was -48.130000. running mean: -6.984708\n",
      "ep 3827: ep_len:48 episode reward: total was 2.550000. running mean: -6.889361\n",
      "ep 3827: ep_len:3 episode reward: total was 0.000000. running mean: -6.820467\n",
      "ep 3827: ep_len:530 episode reward: total was -22.480000. running mean: -6.977062\n",
      "ep 3827: ep_len:590 episode reward: total was -4.480000. running mean: -6.952092\n",
      "epsilon:0.010000 episode_count: 26796. steps_count: 11831980.000000\n",
      "ep 3828: ep_len:585 episode reward: total was -17.190000. running mean: -7.054471\n",
      "ep 3828: ep_len:352 episode reward: total was -37.810000. running mean: -7.362026\n",
      "ep 3828: ep_len:397 episode reward: total was 8.220000. running mean: -7.206206\n",
      "ep 3828: ep_len:126 episode reward: total was -0.410000. running mean: -7.138244\n",
      "ep 3828: ep_len:3 episode reward: total was 0.000000. running mean: -7.066861\n",
      "ep 3828: ep_len:550 episode reward: total was -24.820000. running mean: -7.244393\n",
      "ep 3828: ep_len:500 episode reward: total was -15.480000. running mean: -7.326749\n",
      "epsilon:0.010000 episode_count: 26803. steps_count: 11834493.000000\n",
      "ep 3829: ep_len:500 episode reward: total was -12.340000. running mean: -7.376881\n",
      "ep 3829: ep_len:510 episode reward: total was 8.590000. running mean: -7.217213\n",
      "ep 3829: ep_len:640 episode reward: total was -38.350000. running mean: -7.528540\n",
      "ep 3829: ep_len:610 episode reward: total was 11.570000. running mean: -7.337555\n",
      "ep 3829: ep_len:3 episode reward: total was 0.000000. running mean: -7.264180\n",
      "ep 3829: ep_len:525 episode reward: total was -25.390000. running mean: -7.445438\n",
      "ep 3829: ep_len:610 episode reward: total was -7.330000. running mean: -7.444283\n",
      "epsilon:0.010000 episode_count: 26810. steps_count: 11837891.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3830: ep_len:560 episode reward: total was 10.960000. running mean: -7.260241\n",
      "ep 3830: ep_len:655 episode reward: total was -32.920000. running mean: -7.516838\n",
      "ep 3830: ep_len:640 episode reward: total was 5.650000. running mean: -7.385170\n",
      "ep 3830: ep_len:510 episode reward: total was -11.050000. running mean: -7.421818\n",
      "ep 3830: ep_len:125 episode reward: total was 6.570000. running mean: -7.281900\n",
      "ep 3830: ep_len:635 episode reward: total was -26.200000. running mean: -7.471081\n",
      "ep 3830: ep_len:625 episode reward: total was -16.160000. running mean: -7.557970\n",
      "epsilon:0.010000 episode_count: 26817. steps_count: 11841641.000000\n",
      "ep 3831: ep_len:251 episode reward: total was 1.140000. running mean: -7.470990\n",
      "ep 3831: ep_len:500 episode reward: total was -19.840000. running mean: -7.594680\n",
      "ep 3831: ep_len:640 episode reward: total was -4.100000. running mean: -7.559734\n",
      "ep 3831: ep_len:515 episode reward: total was 15.420000. running mean: -7.329936\n",
      "ep 3831: ep_len:89 episode reward: total was 4.040000. running mean: -7.216237\n",
      "ep 3831: ep_len:500 episode reward: total was -8.820000. running mean: -7.232275\n",
      "ep 3831: ep_len:575 episode reward: total was -4.250000. running mean: -7.202452\n",
      "epsilon:0.010000 episode_count: 26824. steps_count: 11844711.000000\n",
      "ep 3832: ep_len:125 episode reward: total was 2.100000. running mean: -7.109427\n",
      "ep 3832: ep_len:535 episode reward: total was 5.670000. running mean: -6.981633\n",
      "ep 3832: ep_len:75 episode reward: total was -1.460000. running mean: -6.926417\n",
      "ep 3832: ep_len:540 episode reward: total was 15.480000. running mean: -6.702353\n",
      "ep 3832: ep_len:92 episode reward: total was 3.530000. running mean: -6.600029\n",
      "ep 3832: ep_len:319 episode reward: total was -32.280000. running mean: -6.856829\n",
      "ep 3832: ep_len:575 episode reward: total was -17.460000. running mean: -6.962860\n",
      "epsilon:0.010000 episode_count: 26831. steps_count: 11846972.000000\n",
      "ep 3833: ep_len:525 episode reward: total was -17.340000. running mean: -7.066632\n",
      "ep 3833: ep_len:550 episode reward: total was -11.810000. running mean: -7.114066\n",
      "ep 3833: ep_len:635 episode reward: total was -2.340000. running mean: -7.066325\n",
      "ep 3833: ep_len:505 episode reward: total was -8.030000. running mean: -7.075962\n",
      "ep 3833: ep_len:3 episode reward: total was 0.000000. running mean: -7.005202\n",
      "ep 3833: ep_len:650 episode reward: total was -16.190000. running mean: -7.097050\n",
      "ep 3833: ep_len:550 episode reward: total was -4.080000. running mean: -7.066879\n",
      "epsilon:0.010000 episode_count: 26838. steps_count: 11850390.000000\n",
      "ep 3834: ep_len:575 episode reward: total was -17.730000. running mean: -7.173511\n",
      "ep 3834: ep_len:500 episode reward: total was -0.430000. running mean: -7.106076\n",
      "ep 3834: ep_len:500 episode reward: total was -20.940000. running mean: -7.244415\n",
      "ep 3834: ep_len:129 episode reward: total was 5.600000. running mean: -7.115971\n",
      "ep 3834: ep_len:3 episode reward: total was 0.000000. running mean: -7.044811\n",
      "ep 3834: ep_len:500 episode reward: total was -5.240000. running mean: -7.026763\n",
      "ep 3834: ep_len:585 episode reward: total was -14.940000. running mean: -7.105895\n",
      "epsilon:0.010000 episode_count: 26845. steps_count: 11853182.000000\n",
      "ep 3835: ep_len:615 episode reward: total was -8.050000. running mean: -7.115336\n",
      "ep 3835: ep_len:500 episode reward: total was 7.530000. running mean: -6.968883\n",
      "ep 3835: ep_len:730 episode reward: total was -24.720000. running mean: -7.146394\n",
      "ep 3835: ep_len:570 episode reward: total was 8.410000. running mean: -6.990830\n",
      "ep 3835: ep_len:3 episode reward: total was 0.000000. running mean: -6.920922\n",
      "ep 3835: ep_len:530 episode reward: total was -8.130000. running mean: -6.933013\n",
      "ep 3835: ep_len:500 episode reward: total was -22.040000. running mean: -7.084082\n",
      "epsilon:0.010000 episode_count: 26852. steps_count: 11856630.000000\n",
      "ep 3836: ep_len:247 episode reward: total was 2.630000. running mean: -6.986942\n",
      "ep 3836: ep_len:635 episode reward: total was -12.100000. running mean: -7.038072\n",
      "ep 3836: ep_len:580 episode reward: total was 2.510000. running mean: -6.942592\n",
      "ep 3836: ep_len:154 episode reward: total was 7.090000. running mean: -6.802266\n",
      "ep 3836: ep_len:3 episode reward: total was 0.000000. running mean: -6.734243\n",
      "ep 3836: ep_len:510 episode reward: total was -15.020000. running mean: -6.817101\n",
      "ep 3836: ep_len:505 episode reward: total was -8.380000. running mean: -6.832730\n",
      "epsilon:0.010000 episode_count: 26859. steps_count: 11859264.000000\n",
      "ep 3837: ep_len:500 episode reward: total was -18.440000. running mean: -6.948802\n",
      "ep 3837: ep_len:675 episode reward: total was 17.620000. running mean: -6.703114\n",
      "ep 3837: ep_len:387 episode reward: total was 3.730000. running mean: -6.598783\n",
      "ep 3837: ep_len:505 episode reward: total was -43.240000. running mean: -6.965195\n",
      "ep 3837: ep_len:83 episode reward: total was 2.530000. running mean: -6.870243\n",
      "ep 3837: ep_len:565 episode reward: total was -4.540000. running mean: -6.846941\n",
      "ep 3837: ep_len:500 episode reward: total was -6.670000. running mean: -6.845171\n",
      "epsilon:0.010000 episode_count: 26866. steps_count: 11862479.000000\n",
      "ep 3838: ep_len:600 episode reward: total was 14.520000. running mean: -6.631520\n",
      "ep 3838: ep_len:500 episode reward: total was 9.680000. running mean: -6.468405\n",
      "ep 3838: ep_len:545 episode reward: total was -18.800000. running mean: -6.591720\n",
      "ep 3838: ep_len:500 episode reward: total was 12.920000. running mean: -6.396603\n",
      "ep 3838: ep_len:95 episode reward: total was 4.030000. running mean: -6.292337\n",
      "ep 3838: ep_len:293 episode reward: total was 6.160000. running mean: -6.167814\n",
      "ep 3838: ep_len:565 episode reward: total was -3.920000. running mean: -6.145336\n",
      "epsilon:0.010000 episode_count: 26873. steps_count: 11865577.000000\n",
      "ep 3839: ep_len:560 episode reward: total was -1.460000. running mean: -6.098482\n",
      "ep 3839: ep_len:505 episode reward: total was -20.120000. running mean: -6.238698\n",
      "ep 3839: ep_len:500 episode reward: total was -31.450000. running mean: -6.490811\n",
      "ep 3839: ep_len:500 episode reward: total was 3.560000. running mean: -6.390302\n",
      "ep 3839: ep_len:3 episode reward: total was 0.000000. running mean: -6.326399\n",
      "ep 3839: ep_len:540 episode reward: total was -24.890000. running mean: -6.512035\n",
      "ep 3839: ep_len:520 episode reward: total was -18.990000. running mean: -6.636815\n",
      "epsilon:0.010000 episode_count: 26880. steps_count: 11868705.000000\n",
      "ep 3840: ep_len:500 episode reward: total was 7.940000. running mean: -6.491047\n",
      "ep 3840: ep_len:645 episode reward: total was -38.890000. running mean: -6.815036\n",
      "ep 3840: ep_len:500 episode reward: total was -5.410000. running mean: -6.800986\n",
      "ep 3840: ep_len:510 episode reward: total was 3.900000. running mean: -6.693976\n",
      "ep 3840: ep_len:3 episode reward: total was 0.000000. running mean: -6.627036\n",
      "ep 3840: ep_len:515 episode reward: total was -7.720000. running mean: -6.637966\n",
      "ep 3840: ep_len:570 episode reward: total was -27.550000. running mean: -6.847086\n",
      "epsilon:0.010000 episode_count: 26887. steps_count: 11871948.000000\n",
      "ep 3841: ep_len:560 episode reward: total was -7.280000. running mean: -6.851416\n",
      "ep 3841: ep_len:500 episode reward: total was 14.670000. running mean: -6.636201\n",
      "ep 3841: ep_len:710 episode reward: total was -8.080000. running mean: -6.650639\n",
      "ep 3841: ep_len:56 episode reward: total was 2.570000. running mean: -6.558433\n",
      "ep 3841: ep_len:3 episode reward: total was 0.000000. running mean: -6.492849\n",
      "ep 3841: ep_len:660 episode reward: total was -58.150000. running mean: -7.009420\n",
      "ep 3841: ep_len:500 episode reward: total was 4.320000. running mean: -6.896126\n",
      "epsilon:0.010000 episode_count: 26894. steps_count: 11874937.000000\n",
      "ep 3842: ep_len:650 episode reward: total was 14.720000. running mean: -6.679965\n",
      "ep 3842: ep_len:540 episode reward: total was 3.390000. running mean: -6.579265\n",
      "ep 3842: ep_len:342 episode reward: total was -6.810000. running mean: -6.581572\n",
      "ep 3842: ep_len:50 episode reward: total was 1.060000. running mean: -6.505157\n",
      "ep 3842: ep_len:3 episode reward: total was 0.000000. running mean: -6.440105\n",
      "ep 3842: ep_len:625 episode reward: total was -2.540000. running mean: -6.401104\n",
      "ep 3842: ep_len:500 episode reward: total was -24.980000. running mean: -6.586893\n",
      "epsilon:0.010000 episode_count: 26901. steps_count: 11877647.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3843: ep_len:525 episode reward: total was -1.900000. running mean: -6.540024\n",
      "ep 3843: ep_len:515 episode reward: total was -2.760000. running mean: -6.502224\n",
      "ep 3843: ep_len:620 episode reward: total was 3.660000. running mean: -6.400602\n",
      "ep 3843: ep_len:500 episode reward: total was 4.540000. running mean: -6.291196\n",
      "ep 3843: ep_len:3 episode reward: total was 0.000000. running mean: -6.228284\n",
      "ep 3843: ep_len:610 episode reward: total was 3.130000. running mean: -6.134701\n",
      "ep 3843: ep_len:505 episode reward: total was -1.500000. running mean: -6.088354\n",
      "epsilon:0.010000 episode_count: 26908. steps_count: 11880925.000000\n",
      "ep 3844: ep_len:545 episode reward: total was 8.940000. running mean: -5.938070\n",
      "ep 3844: ep_len:595 episode reward: total was 1.670000. running mean: -5.861990\n",
      "ep 3844: ep_len:730 episode reward: total was -4.610000. running mean: -5.849470\n",
      "ep 3844: ep_len:515 episode reward: total was -9.090000. running mean: -5.881875\n",
      "ep 3844: ep_len:110 episode reward: total was 3.030000. running mean: -5.792756\n",
      "ep 3844: ep_len:500 episode reward: total was -0.250000. running mean: -5.737329\n",
      "ep 3844: ep_len:500 episode reward: total was -11.390000. running mean: -5.793855\n",
      "epsilon:0.010000 episode_count: 26915. steps_count: 11884420.000000\n",
      "ep 3845: ep_len:242 episode reward: total was 2.620000. running mean: -5.709717\n",
      "ep 3845: ep_len:515 episode reward: total was -18.920000. running mean: -5.841820\n",
      "ep 3845: ep_len:580 episode reward: total was -0.490000. running mean: -5.788301\n",
      "ep 3845: ep_len:409 episode reward: total was 6.440000. running mean: -5.666018\n",
      "ep 3845: ep_len:3 episode reward: total was 0.000000. running mean: -5.609358\n",
      "ep 3845: ep_len:630 episode reward: total was -25.290000. running mean: -5.806165\n",
      "ep 3845: ep_len:595 episode reward: total was 7.520000. running mean: -5.672903\n",
      "epsilon:0.010000 episode_count: 26922. steps_count: 11887394.000000\n",
      "ep 3846: ep_len:505 episode reward: total was -21.420000. running mean: -5.830374\n",
      "ep 3846: ep_len:525 episode reward: total was -28.100000. running mean: -6.053070\n",
      "ep 3846: ep_len:565 episode reward: total was -3.460000. running mean: -6.027140\n",
      "ep 3846: ep_len:109 episode reward: total was 0.070000. running mean: -5.966168\n",
      "ep 3846: ep_len:3 episode reward: total was 0.000000. running mean: -5.906507\n",
      "ep 3846: ep_len:500 episode reward: total was -1.860000. running mean: -5.866041\n",
      "ep 3846: ep_len:560 episode reward: total was -13.460000. running mean: -5.941981\n",
      "epsilon:0.010000 episode_count: 26929. steps_count: 11890161.000000\n",
      "ep 3847: ep_len:560 episode reward: total was 6.940000. running mean: -5.813161\n",
      "ep 3847: ep_len:505 episode reward: total was -7.310000. running mean: -5.828130\n",
      "ep 3847: ep_len:615 episode reward: total was -7.380000. running mean: -5.843648\n",
      "ep 3847: ep_len:56 episode reward: total was 1.560000. running mean: -5.769612\n",
      "ep 3847: ep_len:3 episode reward: total was 0.000000. running mean: -5.711916\n",
      "ep 3847: ep_len:505 episode reward: total was -22.710000. running mean: -5.881897\n",
      "ep 3847: ep_len:580 episode reward: total was -5.070000. running mean: -5.873778\n",
      "epsilon:0.010000 episode_count: 26936. steps_count: 11892985.000000\n",
      "ep 3848: ep_len:585 episode reward: total was -8.480000. running mean: -5.899840\n",
      "ep 3848: ep_len:535 episode reward: total was -38.470000. running mean: -6.225541\n",
      "ep 3848: ep_len:575 episode reward: total was -0.040000. running mean: -6.163686\n",
      "ep 3848: ep_len:139 episode reward: total was 5.600000. running mean: -6.046049\n",
      "ep 3848: ep_len:3 episode reward: total was 0.000000. running mean: -5.985589\n",
      "ep 3848: ep_len:325 episode reward: total was 5.680000. running mean: -5.868933\n",
      "ep 3848: ep_len:595 episode reward: total was -9.570000. running mean: -5.905943\n",
      "epsilon:0.010000 episode_count: 26943. steps_count: 11895742.000000\n",
      "ep 3849: ep_len:605 episode reward: total was 9.480000. running mean: -5.752084\n",
      "ep 3849: ep_len:615 episode reward: total was -7.120000. running mean: -5.765763\n",
      "ep 3849: ep_len:402 episode reward: total was -1.280000. running mean: -5.720906\n",
      "ep 3849: ep_len:500 episode reward: total was -21.030000. running mean: -5.873996\n",
      "ep 3849: ep_len:3 episode reward: total was 0.000000. running mean: -5.815257\n",
      "ep 3849: ep_len:500 episode reward: total was 2.490000. running mean: -5.732204\n",
      "ep 3849: ep_len:545 episode reward: total was -17.660000. running mean: -5.851482\n",
      "epsilon:0.010000 episode_count: 26950. steps_count: 11898912.000000\n",
      "ep 3850: ep_len:535 episode reward: total was 7.890000. running mean: -5.714067\n",
      "ep 3850: ep_len:535 episode reward: total was 19.360000. running mean: -5.463326\n",
      "ep 3850: ep_len:810 episode reward: total was -49.170000. running mean: -5.900393\n",
      "ep 3850: ep_len:114 episode reward: total was 1.070000. running mean: -5.830689\n",
      "ep 3850: ep_len:3 episode reward: total was 0.000000. running mean: -5.772382\n",
      "ep 3850: ep_len:500 episode reward: total was -9.110000. running mean: -5.805758\n",
      "ep 3850: ep_len:515 episode reward: total was -13.110000. running mean: -5.878801\n",
      "epsilon:0.010000 episode_count: 26957. steps_count: 11901924.000000\n",
      "ep 3851: ep_len:223 episode reward: total was -14.390000. running mean: -5.963913\n",
      "ep 3851: ep_len:500 episode reward: total was 11.160000. running mean: -5.792674\n",
      "ep 3851: ep_len:384 episode reward: total was 9.720000. running mean: -5.637547\n",
      "ep 3851: ep_len:170 episode reward: total was 4.620000. running mean: -5.534972\n",
      "ep 3851: ep_len:98 episode reward: total was 3.540000. running mean: -5.444222\n",
      "ep 3851: ep_len:336 episode reward: total was 4.200000. running mean: -5.347780\n",
      "ep 3851: ep_len:585 episode reward: total was -11.390000. running mean: -5.408202\n",
      "epsilon:0.010000 episode_count: 26964. steps_count: 11904220.000000\n",
      "ep 3852: ep_len:590 episode reward: total was 16.520000. running mean: -5.188920\n",
      "ep 3852: ep_len:580 episode reward: total was -5.420000. running mean: -5.191231\n",
      "ep 3852: ep_len:715 episode reward: total was -20.100000. running mean: -5.340318\n",
      "ep 3852: ep_len:500 episode reward: total was 17.450000. running mean: -5.112415\n",
      "ep 3852: ep_len:86 episode reward: total was 3.520000. running mean: -5.026091\n",
      "ep 3852: ep_len:600 episode reward: total was 0.930000. running mean: -4.966530\n",
      "ep 3852: ep_len:555 episode reward: total was -8.820000. running mean: -5.005065\n",
      "epsilon:0.010000 episode_count: 26971. steps_count: 11907846.000000\n",
      "ep 3853: ep_len:510 episode reward: total was -1.620000. running mean: -4.971214\n",
      "ep 3853: ep_len:505 episode reward: total was 0.630000. running mean: -4.915202\n",
      "ep 3853: ep_len:393 episode reward: total was 3.710000. running mean: -4.828950\n",
      "ep 3853: ep_len:56 episode reward: total was 0.550000. running mean: -4.775160\n",
      "ep 3853: ep_len:3 episode reward: total was 0.000000. running mean: -4.727409\n",
      "ep 3853: ep_len:500 episode reward: total was -31.010000. running mean: -4.990235\n",
      "ep 3853: ep_len:178 episode reward: total was -9.400000. running mean: -5.034332\n",
      "epsilon:0.010000 episode_count: 26978. steps_count: 11909991.000000\n",
      "ep 3854: ep_len:500 episode reward: total was -34.730000. running mean: -5.331289\n",
      "ep 3854: ep_len:565 episode reward: total was 13.910000. running mean: -5.138876\n",
      "ep 3854: ep_len:550 episode reward: total was -3.780000. running mean: -5.125287\n",
      "ep 3854: ep_len:500 episode reward: total was -11.010000. running mean: -5.184135\n",
      "ep 3854: ep_len:3 episode reward: total was 0.000000. running mean: -5.132293\n",
      "ep 3854: ep_len:510 episode reward: total was -20.140000. running mean: -5.282370\n",
      "ep 3854: ep_len:680 episode reward: total was -11.690000. running mean: -5.346447\n",
      "epsilon:0.010000 episode_count: 26985. steps_count: 11913299.000000\n",
      "ep 3855: ep_len:193 episode reward: total was -3.970000. running mean: -5.332682\n",
      "ep 3855: ep_len:575 episode reward: total was -20.870000. running mean: -5.488055\n",
      "ep 3855: ep_len:590 episode reward: total was -5.150000. running mean: -5.484675\n",
      "ep 3855: ep_len:500 episode reward: total was 7.120000. running mean: -5.358628\n",
      "ep 3855: ep_len:53 episode reward: total was 5.000000. running mean: -5.255042\n",
      "ep 3855: ep_len:600 episode reward: total was -19.080000. running mean: -5.393291\n",
      "ep 3855: ep_len:580 episode reward: total was -29.000000. running mean: -5.629358\n",
      "epsilon:0.010000 episode_count: 26992. steps_count: 11916390.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3856: ep_len:500 episode reward: total was -13.640000. running mean: -5.709465\n",
      "ep 3856: ep_len:201 episode reward: total was -5.390000. running mean: -5.706270\n",
      "ep 3856: ep_len:500 episode reward: total was -15.430000. running mean: -5.803507\n",
      "ep 3856: ep_len:500 episode reward: total was -21.690000. running mean: -5.962372\n",
      "ep 3856: ep_len:98 episode reward: total was 5.010000. running mean: -5.852649\n",
      "ep 3856: ep_len:585 episode reward: total was -20.100000. running mean: -5.995122\n",
      "ep 3856: ep_len:585 episode reward: total was -11.550000. running mean: -6.050671\n",
      "epsilon:0.010000 episode_count: 26999. steps_count: 11919359.000000\n",
      "ep 3857: ep_len:585 episode reward: total was -25.470000. running mean: -6.244864\n",
      "ep 3857: ep_len:775 episode reward: total was -37.760000. running mean: -6.560016\n",
      "ep 3857: ep_len:595 episode reward: total was -3.180000. running mean: -6.526215\n",
      "ep 3857: ep_len:537 episode reward: total was -23.880000. running mean: -6.699753\n",
      "ep 3857: ep_len:3 episode reward: total was 0.000000. running mean: -6.632756\n",
      "ep 3857: ep_len:500 episode reward: total was -22.270000. running mean: -6.789128\n",
      "ep 3857: ep_len:605 episode reward: total was -23.630000. running mean: -6.957537\n",
      "epsilon:0.010000 episode_count: 27006. steps_count: 11922959.000000\n",
      "ep 3858: ep_len:505 episode reward: total was -7.130000. running mean: -6.959262\n",
      "ep 3858: ep_len:565 episode reward: total was 10.330000. running mean: -6.786369\n",
      "ep 3858: ep_len:550 episode reward: total was -0.150000. running mean: -6.720005\n",
      "ep 3858: ep_len:505 episode reward: total was 9.020000. running mean: -6.562605\n",
      "ep 3858: ep_len:3 episode reward: total was 0.000000. running mean: -6.496979\n",
      "ep 3858: ep_len:327 episode reward: total was -20.350000. running mean: -6.635509\n",
      "ep 3858: ep_len:332 episode reward: total was -10.860000. running mean: -6.677754\n",
      "epsilon:0.010000 episode_count: 27013. steps_count: 11925746.000000\n",
      "ep 3859: ep_len:500 episode reward: total was 0.710000. running mean: -6.603877\n",
      "ep 3859: ep_len:710 episode reward: total was -98.900000. running mean: -7.526838\n",
      "ep 3859: ep_len:349 episode reward: total was -0.260000. running mean: -7.454170\n",
      "ep 3859: ep_len:615 episode reward: total was -10.000000. running mean: -7.479628\n",
      "ep 3859: ep_len:2 episode reward: total was 0.000000. running mean: -7.404832\n",
      "ep 3859: ep_len:630 episode reward: total was -38.860000. running mean: -7.719383\n",
      "ep 3859: ep_len:500 episode reward: total was -14.720000. running mean: -7.789389\n",
      "epsilon:0.010000 episode_count: 27020. steps_count: 11929052.000000\n",
      "ep 3860: ep_len:575 episode reward: total was -28.810000. running mean: -7.999596\n",
      "ep 3860: ep_len:565 episode reward: total was -14.400000. running mean: -8.063600\n",
      "ep 3860: ep_len:580 episode reward: total was -2.040000. running mean: -8.003364\n",
      "ep 3860: ep_len:56 episode reward: total was 1.560000. running mean: -7.907730\n",
      "ep 3860: ep_len:47 episode reward: total was 4.500000. running mean: -7.783653\n",
      "ep 3860: ep_len:535 episode reward: total was -19.540000. running mean: -7.901216\n",
      "ep 3860: ep_len:620 episode reward: total was -35.530000. running mean: -8.177504\n",
      "epsilon:0.010000 episode_count: 27027. steps_count: 11932030.000000\n",
      "ep 3861: ep_len:605 episode reward: total was -20.700000. running mean: -8.302729\n",
      "ep 3861: ep_len:500 episode reward: total was 3.590000. running mean: -8.183802\n",
      "ep 3861: ep_len:625 episode reward: total was -11.410000. running mean: -8.216064\n",
      "ep 3861: ep_len:500 episode reward: total was -0.540000. running mean: -8.139303\n",
      "ep 3861: ep_len:89 episode reward: total was 3.030000. running mean: -8.027610\n",
      "ep 3861: ep_len:530 episode reward: total was -12.140000. running mean: -8.068734\n",
      "ep 3861: ep_len:535 episode reward: total was -18.150000. running mean: -8.169547\n",
      "epsilon:0.010000 episode_count: 27034. steps_count: 11935414.000000\n",
      "ep 3862: ep_len:600 episode reward: total was -38.310000. running mean: -8.470951\n",
      "ep 3862: ep_len:600 episode reward: total was 27.430000. running mean: -8.111942\n",
      "ep 3862: ep_len:525 episode reward: total was -15.810000. running mean: -8.188922\n",
      "ep 3862: ep_len:169 episode reward: total was 5.100000. running mean: -8.056033\n",
      "ep 3862: ep_len:93 episode reward: total was 1.020000. running mean: -7.965273\n",
      "ep 3862: ep_len:705 episode reward: total was -6.190000. running mean: -7.947520\n",
      "ep 3862: ep_len:620 episode reward: total was -7.770000. running mean: -7.945745\n",
      "epsilon:0.010000 episode_count: 27041. steps_count: 11938726.000000\n",
      "ep 3863: ep_len:760 episode reward: total was -52.760000. running mean: -8.393887\n",
      "ep 3863: ep_len:264 episode reward: total was -13.280000. running mean: -8.442748\n",
      "ep 3863: ep_len:560 episode reward: total was -34.900000. running mean: -8.707321\n",
      "ep 3863: ep_len:131 episode reward: total was 0.590000. running mean: -8.614348\n",
      "ep 3863: ep_len:118 episode reward: total was 4.050000. running mean: -8.487704\n",
      "ep 3863: ep_len:520 episode reward: total was -5.200000. running mean: -8.454827\n",
      "ep 3863: ep_len:282 episode reward: total was -9.870000. running mean: -8.468979\n",
      "epsilon:0.010000 episode_count: 27048. steps_count: 11941361.000000\n",
      "ep 3864: ep_len:224 episode reward: total was 1.080000. running mean: -8.373489\n",
      "ep 3864: ep_len:294 episode reward: total was -21.790000. running mean: -8.507654\n",
      "ep 3864: ep_len:453 episode reward: total was -3.720000. running mean: -8.459778\n",
      "ep 3864: ep_len:105 episode reward: total was -0.880000. running mean: -8.383980\n",
      "ep 3864: ep_len:3 episode reward: total was 0.000000. running mean: -8.300140\n",
      "ep 3864: ep_len:510 episode reward: total was 5.120000. running mean: -8.165939\n",
      "ep 3864: ep_len:530 episode reward: total was -7.260000. running mean: -8.156879\n",
      "epsilon:0.010000 episode_count: 27055. steps_count: 11943480.000000\n",
      "ep 3865: ep_len:665 episode reward: total was -15.180000. running mean: -8.227110\n",
      "ep 3865: ep_len:535 episode reward: total was 14.350000. running mean: -8.001339\n",
      "ep 3865: ep_len:426 episode reward: total was 11.230000. running mean: -7.809026\n",
      "ep 3865: ep_len:515 episode reward: total was 2.030000. running mean: -7.710636\n",
      "ep 3865: ep_len:83 episode reward: total was -3.960000. running mean: -7.673129\n",
      "ep 3865: ep_len:500 episode reward: total was -16.750000. running mean: -7.763898\n",
      "ep 3865: ep_len:500 episode reward: total was -21.660000. running mean: -7.902859\n",
      "epsilon:0.010000 episode_count: 27062. steps_count: 11946704.000000\n",
      "ep 3866: ep_len:500 episode reward: total was -8.860000. running mean: -7.912430\n",
      "ep 3866: ep_len:515 episode reward: total was 1.380000. running mean: -7.819506\n",
      "ep 3866: ep_len:388 episode reward: total was 6.240000. running mean: -7.678911\n",
      "ep 3866: ep_len:510 episode reward: total was 9.930000. running mean: -7.502822\n",
      "ep 3866: ep_len:113 episode reward: total was 7.550000. running mean: -7.352294\n",
      "ep 3866: ep_len:505 episode reward: total was -21.380000. running mean: -7.492571\n",
      "ep 3866: ep_len:590 episode reward: total was -13.910000. running mean: -7.556745\n",
      "epsilon:0.010000 episode_count: 27069. steps_count: 11949825.000000\n",
      "ep 3867: ep_len:515 episode reward: total was -1.890000. running mean: -7.500078\n",
      "ep 3867: ep_len:500 episode reward: total was 1.240000. running mean: -7.412677\n",
      "ep 3867: ep_len:575 episode reward: total was -4.520000. running mean: -7.383750\n",
      "ep 3867: ep_len:132 episode reward: total was 3.590000. running mean: -7.274013\n",
      "ep 3867: ep_len:3 episode reward: total was 0.000000. running mean: -7.201273\n",
      "ep 3867: ep_len:665 episode reward: total was -11.170000. running mean: -7.240960\n",
      "ep 3867: ep_len:520 episode reward: total was -47.250000. running mean: -7.641050\n",
      "epsilon:0.010000 episode_count: 27076. steps_count: 11952735.000000\n",
      "ep 3868: ep_len:655 episode reward: total was 8.180000. running mean: -7.482840\n",
      "ep 3868: ep_len:360 episode reward: total was -9.820000. running mean: -7.506211\n",
      "ep 3868: ep_len:500 episode reward: total was -16.340000. running mean: -7.594549\n",
      "ep 3868: ep_len:505 episode reward: total was -5.460000. running mean: -7.573204\n",
      "ep 3868: ep_len:3 episode reward: total was 0.000000. running mean: -7.497472\n",
      "ep 3868: ep_len:510 episode reward: total was 4.130000. running mean: -7.381197\n",
      "ep 3868: ep_len:332 episode reward: total was -12.840000. running mean: -7.435785\n",
      "epsilon:0.010000 episode_count: 27083. steps_count: 11955600.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3869: ep_len:500 episode reward: total was -12.750000. running mean: -7.488927\n",
      "ep 3869: ep_len:545 episode reward: total was -51.670000. running mean: -7.930738\n",
      "ep 3869: ep_len:600 episode reward: total was 1.890000. running mean: -7.832530\n",
      "ep 3869: ep_len:550 episode reward: total was 13.450000. running mean: -7.619705\n",
      "ep 3869: ep_len:3 episode reward: total was 0.000000. running mean: -7.543508\n",
      "ep 3869: ep_len:500 episode reward: total was -23.280000. running mean: -7.700873\n",
      "ep 3869: ep_len:570 episode reward: total was -32.020000. running mean: -7.944064\n",
      "epsilon:0.010000 episode_count: 27090. steps_count: 11958868.000000\n",
      "ep 3870: ep_len:540 episode reward: total was -35.670000. running mean: -8.221324\n",
      "ep 3870: ep_len:635 episode reward: total was 0.590000. running mean: -8.133210\n",
      "ep 3870: ep_len:555 episode reward: total was 1.130000. running mean: -8.040578\n",
      "ep 3870: ep_len:525 episode reward: total was -47.180000. running mean: -8.431973\n",
      "ep 3870: ep_len:124 episode reward: total was 5.550000. running mean: -8.292153\n",
      "ep 3870: ep_len:765 episode reward: total was -53.350000. running mean: -8.742731\n",
      "ep 3870: ep_len:500 episode reward: total was -18.310000. running mean: -8.838404\n",
      "epsilon:0.010000 episode_count: 27097. steps_count: 11962512.000000\n",
      "ep 3871: ep_len:650 episode reward: total was -30.300000. running mean: -9.053020\n",
      "ep 3871: ep_len:570 episode reward: total was -1.790000. running mean: -8.980390\n",
      "ep 3871: ep_len:600 episode reward: total was 2.510000. running mean: -8.865486\n",
      "ep 3871: ep_len:545 episode reward: total was -27.630000. running mean: -9.053131\n",
      "ep 3871: ep_len:3 episode reward: total was 0.000000. running mean: -8.962600\n",
      "ep 3871: ep_len:570 episode reward: total was -3.430000. running mean: -8.907274\n",
      "ep 3871: ep_len:570 episode reward: total was -8.030000. running mean: -8.898501\n",
      "epsilon:0.010000 episode_count: 27104. steps_count: 11966020.000000\n",
      "ep 3872: ep_len:535 episode reward: total was 10.450000. running mean: -8.705016\n",
      "ep 3872: ep_len:500 episode reward: total was 7.880000. running mean: -8.539166\n",
      "ep 3872: ep_len:615 episode reward: total was -25.490000. running mean: -8.708674\n",
      "ep 3872: ep_len:530 episode reward: total was 9.480000. running mean: -8.526787\n",
      "ep 3872: ep_len:3 episode reward: total was 0.000000. running mean: -8.441519\n",
      "ep 3872: ep_len:585 episode reward: total was 6.440000. running mean: -8.292704\n",
      "ep 3872: ep_len:235 episode reward: total was -16.360000. running mean: -8.373377\n",
      "epsilon:0.010000 episode_count: 27111. steps_count: 11969023.000000\n",
      "ep 3873: ep_len:520 episode reward: total was -11.980000. running mean: -8.409443\n",
      "ep 3873: ep_len:500 episode reward: total was -25.690000. running mean: -8.582249\n",
      "ep 3873: ep_len:625 episode reward: total was 3.180000. running mean: -8.464627\n",
      "ep 3873: ep_len:167 episode reward: total was 4.620000. running mean: -8.333780\n",
      "ep 3873: ep_len:3 episode reward: total was 0.000000. running mean: -8.250442\n",
      "ep 3873: ep_len:1200 episode reward: total was -157.230000. running mean: -9.740238\n",
      "ep 3873: ep_len:540 episode reward: total was -12.050000. running mean: -9.763336\n",
      "epsilon:0.010000 episode_count: 27118. steps_count: 11972578.000000\n",
      "ep 3874: ep_len:525 episode reward: total was 9.410000. running mean: -9.571602\n",
      "ep 3874: ep_len:535 episode reward: total was 12.100000. running mean: -9.354886\n",
      "ep 3874: ep_len:419 episode reward: total was -40.270000. running mean: -9.664037\n",
      "ep 3874: ep_len:600 episode reward: total was 2.620000. running mean: -9.541197\n",
      "ep 3874: ep_len:3 episode reward: total was 0.000000. running mean: -9.445785\n",
      "ep 3874: ep_len:500 episode reward: total was 6.150000. running mean: -9.289827\n",
      "ep 3874: ep_len:500 episode reward: total was -11.820000. running mean: -9.315129\n",
      "epsilon:0.010000 episode_count: 27125. steps_count: 11975660.000000\n",
      "ep 3875: ep_len:205 episode reward: total was -21.860000. running mean: -9.440578\n",
      "ep 3875: ep_len:500 episode reward: total was 0.060000. running mean: -9.345572\n",
      "ep 3875: ep_len:625 episode reward: total was -21.520000. running mean: -9.467316\n",
      "ep 3875: ep_len:153 episode reward: total was 2.120000. running mean: -9.351443\n",
      "ep 3875: ep_len:99 episode reward: total was 2.530000. running mean: -9.232629\n",
      "ep 3875: ep_len:168 episode reward: total was 6.130000. running mean: -9.079002\n",
      "ep 3875: ep_len:500 episode reward: total was -13.490000. running mean: -9.123112\n",
      "epsilon:0.010000 episode_count: 27132. steps_count: 11977910.000000\n",
      "ep 3876: ep_len:510 episode reward: total was 6.430000. running mean: -8.967581\n",
      "ep 3876: ep_len:500 episode reward: total was -13.010000. running mean: -9.008005\n",
      "ep 3876: ep_len:555 episode reward: total was -0.050000. running mean: -8.918425\n",
      "ep 3876: ep_len:515 episode reward: total was 7.950000. running mean: -8.749741\n",
      "ep 3876: ep_len:56 episode reward: total was 5.500000. running mean: -8.607244\n",
      "ep 3876: ep_len:204 episode reward: total was 5.140000. running mean: -8.469771\n",
      "ep 3876: ep_len:500 episode reward: total was -3.830000. running mean: -8.423373\n",
      "epsilon:0.010000 episode_count: 27139. steps_count: 11980750.000000\n",
      "ep 3877: ep_len:106 episode reward: total was -1.950000. running mean: -8.358640\n",
      "ep 3877: ep_len:510 episode reward: total was -17.540000. running mean: -8.450453\n",
      "ep 3877: ep_len:500 episode reward: total was 3.910000. running mean: -8.326849\n",
      "ep 3877: ep_len:505 episode reward: total was -11.030000. running mean: -8.353880\n",
      "ep 3877: ep_len:3 episode reward: total was 0.000000. running mean: -8.270342\n",
      "ep 3877: ep_len:580 episode reward: total was -17.310000. running mean: -8.360738\n",
      "ep 3877: ep_len:555 episode reward: total was -25.440000. running mean: -8.531531\n",
      "epsilon:0.010000 episode_count: 27146. steps_count: 11983509.000000\n",
      "ep 3878: ep_len:500 episode reward: total was 9.270000. running mean: -8.353515\n",
      "ep 3878: ep_len:525 episode reward: total was 12.890000. running mean: -8.141080\n",
      "ep 3878: ep_len:500 episode reward: total was 6.410000. running mean: -7.995569\n",
      "ep 3878: ep_len:535 episode reward: total was 8.920000. running mean: -7.826414\n",
      "ep 3878: ep_len:110 episode reward: total was 3.020000. running mean: -7.717950\n",
      "ep 3878: ep_len:510 episode reward: total was -2.110000. running mean: -7.661870\n",
      "ep 3878: ep_len:935 episode reward: total was -133.250000. running mean: -8.917751\n",
      "epsilon:0.010000 episode_count: 27153. steps_count: 11987124.000000\n",
      "ep 3879: ep_len:500 episode reward: total was 8.320000. running mean: -8.745374\n",
      "ep 3879: ep_len:505 episode reward: total was 9.210000. running mean: -8.565820\n",
      "ep 3879: ep_len:550 episode reward: total was -1.580000. running mean: -8.495962\n",
      "ep 3879: ep_len:56 episode reward: total was 2.570000. running mean: -8.385302\n",
      "ep 3879: ep_len:81 episode reward: total was -8.980000. running mean: -8.391249\n",
      "ep 3879: ep_len:560 episode reward: total was 8.900000. running mean: -8.218337\n",
      "ep 3879: ep_len:303 episode reward: total was -5.330000. running mean: -8.189453\n",
      "epsilon:0.010000 episode_count: 27160. steps_count: 11989679.000000\n",
      "ep 3880: ep_len:510 episode reward: total was 2.390000. running mean: -8.083659\n",
      "ep 3880: ep_len:500 episode reward: total was -4.380000. running mean: -8.046622\n",
      "ep 3880: ep_len:71 episode reward: total was 1.040000. running mean: -7.955756\n",
      "ep 3880: ep_len:550 episode reward: total was 13.990000. running mean: -7.736299\n",
      "ep 3880: ep_len:3 episode reward: total was 0.000000. running mean: -7.658936\n",
      "ep 3880: ep_len:665 episode reward: total was -2.170000. running mean: -7.604046\n",
      "ep 3880: ep_len:525 episode reward: total was -78.270000. running mean: -8.310706\n",
      "epsilon:0.010000 episode_count: 27167. steps_count: 11992503.000000\n",
      "ep 3881: ep_len:530 episode reward: total was -1.170000. running mean: -8.239299\n",
      "ep 3881: ep_len:500 episode reward: total was 4.330000. running mean: -8.113606\n",
      "ep 3881: ep_len:595 episode reward: total was -12.210000. running mean: -8.154570\n",
      "ep 3881: ep_len:500 episode reward: total was 16.460000. running mean: -7.908424\n",
      "ep 3881: ep_len:3 episode reward: total was 0.000000. running mean: -7.829340\n",
      "ep 3881: ep_len:645 episode reward: total was -20.230000. running mean: -7.953346\n",
      "ep 3881: ep_len:615 episode reward: total was -5.980000. running mean: -7.933613\n",
      "epsilon:0.010000 episode_count: 27174. steps_count: 11995891.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3882: ep_len:500 episode reward: total was 8.270000. running mean: -7.771577\n",
      "ep 3882: ep_len:500 episode reward: total was -4.380000. running mean: -7.737661\n",
      "ep 3882: ep_len:590 episode reward: total was -3.910000. running mean: -7.699384\n",
      "ep 3882: ep_len:555 episode reward: total was -11.470000. running mean: -7.737091\n",
      "ep 3882: ep_len:77 episode reward: total was 3.530000. running mean: -7.624420\n",
      "ep 3882: ep_len:570 episode reward: total was -12.230000. running mean: -7.670475\n",
      "ep 3882: ep_len:520 episode reward: total was -8.110000. running mean: -7.674871\n",
      "epsilon:0.010000 episode_count: 27181. steps_count: 11999203.000000\n",
      "ep 3883: ep_len:219 episode reward: total was 1.590000. running mean: -7.582222\n",
      "ep 3883: ep_len:540 episode reward: total was 20.390000. running mean: -7.302500\n",
      "ep 3883: ep_len:655 episode reward: total was -10.350000. running mean: -7.332975\n",
      "ep 3883: ep_len:500 episode reward: total was -9.510000. running mean: -7.354745\n",
      "ep 3883: ep_len:3 episode reward: total was 0.000000. running mean: -7.281198\n",
      "ep 3883: ep_len:1120 episode reward: total was -66.150000. running mean: -7.869886\n",
      "ep 3883: ep_len:505 episode reward: total was -10.920000. running mean: -7.900387\n",
      "epsilon:0.010000 episode_count: 27188. steps_count: 12002745.000000\n",
      "ep 3884: ep_len:510 episode reward: total was -1.930000. running mean: -7.840683\n",
      "ep 3884: ep_len:615 episode reward: total was -1.150000. running mean: -7.773776\n",
      "ep 3884: ep_len:500 episode reward: total was -1.600000. running mean: -7.712038\n",
      "ep 3884: ep_len:56 episode reward: total was 1.560000. running mean: -7.619318\n",
      "ep 3884: ep_len:89 episode reward: total was 5.510000. running mean: -7.488025\n",
      "ep 3884: ep_len:620 episode reward: total was 11.480000. running mean: -7.298344\n",
      "ep 3884: ep_len:274 episode reward: total was -3.330000. running mean: -7.258661\n",
      "epsilon:0.010000 episode_count: 27195. steps_count: 12005409.000000\n",
      "ep 3885: ep_len:211 episode reward: total was -0.400000. running mean: -7.190074\n",
      "ep 3885: ep_len:500 episode reward: total was -11.200000. running mean: -7.230174\n",
      "ep 3885: ep_len:570 episode reward: total was -20.280000. running mean: -7.360672\n",
      "ep 3885: ep_len:530 episode reward: total was -43.810000. running mean: -7.725165\n",
      "ep 3885: ep_len:3 episode reward: total was 0.000000. running mean: -7.647914\n",
      "ep 3885: ep_len:318 episode reward: total was 2.200000. running mean: -7.549434\n",
      "ep 3885: ep_len:530 episode reward: total was -14.330000. running mean: -7.617240\n",
      "epsilon:0.010000 episode_count: 27202. steps_count: 12008071.000000\n",
      "ep 3886: ep_len:595 episode reward: total was 9.960000. running mean: -7.441468\n",
      "ep 3886: ep_len:500 episode reward: total was 1.040000. running mean: -7.356653\n",
      "ep 3886: ep_len:530 episode reward: total was 8.430000. running mean: -7.198786\n",
      "ep 3886: ep_len:56 episode reward: total was 1.560000. running mean: -7.111199\n",
      "ep 3886: ep_len:3 episode reward: total was 0.000000. running mean: -7.040087\n",
      "ep 3886: ep_len:580 episode reward: total was -18.750000. running mean: -7.157186\n",
      "ep 3886: ep_len:600 episode reward: total was -11.090000. running mean: -7.196514\n",
      "epsilon:0.010000 episode_count: 27209. steps_count: 12010935.000000\n",
      "ep 3887: ep_len:134 episode reward: total was 3.580000. running mean: -7.088749\n",
      "ep 3887: ep_len:530 episode reward: total was 14.880000. running mean: -6.869061\n",
      "ep 3887: ep_len:535 episode reward: total was -5.890000. running mean: -6.859271\n",
      "ep 3887: ep_len:559 episode reward: total was -23.840000. running mean: -7.029078\n",
      "ep 3887: ep_len:99 episode reward: total was 1.000000. running mean: -6.948787\n",
      "ep 3887: ep_len:500 episode reward: total was -12.750000. running mean: -7.006799\n",
      "ep 3887: ep_len:505 episode reward: total was -7.600000. running mean: -7.012731\n",
      "epsilon:0.010000 episode_count: 27216. steps_count: 12013797.000000\n",
      "ep 3888: ep_len:600 episode reward: total was 16.050000. running mean: -6.782104\n",
      "ep 3888: ep_len:582 episode reward: total was -50.110000. running mean: -7.215383\n",
      "ep 3888: ep_len:500 episode reward: total was 4.970000. running mean: -7.093529\n",
      "ep 3888: ep_len:605 episode reward: total was 2.170000. running mean: -7.000894\n",
      "ep 3888: ep_len:105 episode reward: total was 1.510000. running mean: -6.915785\n",
      "ep 3888: ep_len:500 episode reward: total was -2.300000. running mean: -6.869627\n",
      "ep 3888: ep_len:535 episode reward: total was -10.080000. running mean: -6.901731\n",
      "epsilon:0.010000 episode_count: 27223. steps_count: 12017224.000000\n",
      "ep 3889: ep_len:600 episode reward: total was -26.220000. running mean: -7.094913\n",
      "ep 3889: ep_len:590 episode reward: total was 2.980000. running mean: -6.994164\n",
      "ep 3889: ep_len:525 episode reward: total was -33.940000. running mean: -7.263623\n",
      "ep 3889: ep_len:725 episode reward: total was -27.380000. running mean: -7.464786\n",
      "ep 3889: ep_len:3 episode reward: total was 0.000000. running mean: -7.390139\n",
      "ep 3889: ep_len:520 episode reward: total was 3.340000. running mean: -7.282837\n",
      "ep 3889: ep_len:500 episode reward: total was -11.470000. running mean: -7.324709\n",
      "epsilon:0.010000 episode_count: 27230. steps_count: 12020687.000000\n",
      "ep 3890: ep_len:500 episode reward: total was 15.360000. running mean: -7.097862\n",
      "ep 3890: ep_len:170 episode reward: total was -4.930000. running mean: -7.076183\n",
      "ep 3890: ep_len:500 episode reward: total was 7.940000. running mean: -6.926021\n",
      "ep 3890: ep_len:505 episode reward: total was -5.540000. running mean: -6.912161\n",
      "ep 3890: ep_len:3 episode reward: total was 0.000000. running mean: -6.843039\n",
      "ep 3890: ep_len:750 episode reward: total was -41.720000. running mean: -7.191809\n",
      "ep 3890: ep_len:560 episode reward: total was -6.310000. running mean: -7.182991\n",
      "epsilon:0.010000 episode_count: 27237. steps_count: 12023675.000000\n",
      "ep 3891: ep_len:190 episode reward: total was 0.630000. running mean: -7.104861\n",
      "ep 3891: ep_len:500 episode reward: total was -5.240000. running mean: -7.086212\n",
      "ep 3891: ep_len:500 episode reward: total was 13.800000. running mean: -6.877350\n",
      "ep 3891: ep_len:705 episode reward: total was -42.430000. running mean: -7.232877\n",
      "ep 3891: ep_len:119 episode reward: total was 8.050000. running mean: -7.080048\n",
      "ep 3891: ep_len:690 episode reward: total was -29.670000. running mean: -7.305948\n",
      "ep 3891: ep_len:600 episode reward: total was 4.120000. running mean: -7.191688\n",
      "epsilon:0.010000 episode_count: 27244. steps_count: 12026979.000000\n",
      "ep 3892: ep_len:218 episode reward: total was 0.590000. running mean: -7.113871\n",
      "ep 3892: ep_len:635 episode reward: total was 5.530000. running mean: -6.987433\n",
      "ep 3892: ep_len:535 episode reward: total was -5.280000. running mean: -6.970358\n",
      "ep 3892: ep_len:545 episode reward: total was 11.910000. running mean: -6.781555\n",
      "ep 3892: ep_len:93 episode reward: total was 4.540000. running mean: -6.668339\n",
      "ep 3892: ep_len:535 episode reward: total was -5.060000. running mean: -6.652256\n",
      "ep 3892: ep_len:505 episode reward: total was -9.360000. running mean: -6.679333\n",
      "epsilon:0.010000 episode_count: 27251. steps_count: 12030045.000000\n",
      "ep 3893: ep_len:265 episode reward: total was 5.150000. running mean: -6.561040\n",
      "ep 3893: ep_len:500 episode reward: total was 14.180000. running mean: -6.353629\n",
      "ep 3893: ep_len:500 episode reward: total was 13.450000. running mean: -6.155593\n",
      "ep 3893: ep_len:630 episode reward: total was 5.070000. running mean: -6.043337\n",
      "ep 3893: ep_len:99 episode reward: total was 4.030000. running mean: -5.942604\n",
      "ep 3893: ep_len:630 episode reward: total was -45.630000. running mean: -6.339478\n",
      "ep 3893: ep_len:565 episode reward: total was 2.000000. running mean: -6.256083\n",
      "epsilon:0.010000 episode_count: 27258. steps_count: 12033234.000000\n",
      "ep 3894: ep_len:550 episode reward: total was 12.980000. running mean: -6.063722\n",
      "ep 3894: ep_len:560 episode reward: total was 1.430000. running mean: -5.988785\n",
      "ep 3894: ep_len:346 episode reward: total was 8.700000. running mean: -5.841897\n",
      "ep 3894: ep_len:590 episode reward: total was -5.530000. running mean: -5.838778\n",
      "ep 3894: ep_len:3 episode reward: total was 0.000000. running mean: -5.780390\n",
      "ep 3894: ep_len:303 episode reward: total was -25.400000. running mean: -5.976586\n",
      "ep 3894: ep_len:324 episode reward: total was -4.790000. running mean: -5.964721\n",
      "epsilon:0.010000 episode_count: 27265. steps_count: 12035910.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3895: ep_len:550 episode reward: total was 11.450000. running mean: -5.790573\n",
      "ep 3895: ep_len:535 episode reward: total was -17.800000. running mean: -5.910668\n",
      "ep 3895: ep_len:615 episode reward: total was -21.140000. running mean: -6.062961\n",
      "ep 3895: ep_len:510 episode reward: total was -7.010000. running mean: -6.072431\n",
      "ep 3895: ep_len:95 episode reward: total was 3.540000. running mean: -5.976307\n",
      "ep 3895: ep_len:570 episode reward: total was -17.620000. running mean: -6.092744\n",
      "ep 3895: ep_len:585 episode reward: total was -2.300000. running mean: -6.054817\n",
      "epsilon:0.010000 episode_count: 27272. steps_count: 12039370.000000\n",
      "ep 3896: ep_len:660 episode reward: total was -22.230000. running mean: -6.216568\n",
      "ep 3896: ep_len:347 episode reward: total was -36.320000. running mean: -6.517603\n",
      "ep 3896: ep_len:500 episode reward: total was 9.930000. running mean: -6.353127\n",
      "ep 3896: ep_len:515 episode reward: total was -13.980000. running mean: -6.429395\n",
      "ep 3896: ep_len:3 episode reward: total was 0.000000. running mean: -6.365101\n",
      "ep 3896: ep_len:650 episode reward: total was -45.100000. running mean: -6.752450\n",
      "ep 3896: ep_len:500 episode reward: total was 1.950000. running mean: -6.665426\n",
      "epsilon:0.010000 episode_count: 27279. steps_count: 12042545.000000\n",
      "ep 3897: ep_len:500 episode reward: total was 10.420000. running mean: -6.494572\n",
      "ep 3897: ep_len:500 episode reward: total was 11.670000. running mean: -6.312926\n",
      "ep 3897: ep_len:570 episode reward: total was -15.490000. running mean: -6.404697\n",
      "ep 3897: ep_len:400 episode reward: total was -24.590000. running mean: -6.586550\n",
      "ep 3897: ep_len:3 episode reward: total was 0.000000. running mean: -6.520684\n",
      "ep 3897: ep_len:520 episode reward: total was -25.170000. running mean: -6.707177\n",
      "ep 3897: ep_len:505 episode reward: total was -17.430000. running mean: -6.814406\n",
      "epsilon:0.010000 episode_count: 27286. steps_count: 12045543.000000\n",
      "ep 3898: ep_len:540 episode reward: total was -14.390000. running mean: -6.890162\n",
      "ep 3898: ep_len:525 episode reward: total was -29.610000. running mean: -7.117360\n",
      "ep 3898: ep_len:67 episode reward: total was 1.550000. running mean: -7.030686\n",
      "ep 3898: ep_len:525 episode reward: total was 9.980000. running mean: -6.860579\n",
      "ep 3898: ep_len:50 episode reward: total was 3.010000. running mean: -6.761874\n",
      "ep 3898: ep_len:595 episode reward: total was 4.410000. running mean: -6.650155\n",
      "ep 3898: ep_len:600 episode reward: total was -29.100000. running mean: -6.874653\n",
      "epsilon:0.010000 episode_count: 27293. steps_count: 12048445.000000\n",
      "ep 3899: ep_len:118 episode reward: total was 3.590000. running mean: -6.770007\n",
      "ep 3899: ep_len:330 episode reward: total was -9.310000. running mean: -6.795407\n",
      "ep 3899: ep_len:595 episode reward: total was -9.450000. running mean: -6.821953\n",
      "ep 3899: ep_len:500 episode reward: total was -5.500000. running mean: -6.808733\n",
      "ep 3899: ep_len:102 episode reward: total was 4.530000. running mean: -6.695346\n",
      "ep 3899: ep_len:630 episode reward: total was -17.530000. running mean: -6.803692\n",
      "ep 3899: ep_len:520 episode reward: total was -8.600000. running mean: -6.821655\n",
      "epsilon:0.010000 episode_count: 27300. steps_count: 12051240.000000\n",
      "ep 3900: ep_len:670 episode reward: total was -12.630000. running mean: -6.879739\n",
      "ep 3900: ep_len:500 episode reward: total was -19.720000. running mean: -7.008142\n",
      "ep 3900: ep_len:374 episode reward: total was 0.760000. running mean: -6.930460\n",
      "ep 3900: ep_len:500 episode reward: total was -3.050000. running mean: -6.891655\n",
      "ep 3900: ep_len:80 episode reward: total was -4.480000. running mean: -6.867539\n",
      "ep 3900: ep_len:685 episode reward: total was -0.110000. running mean: -6.799964\n",
      "ep 3900: ep_len:332 episode reward: total was -0.270000. running mean: -6.734664\n",
      "epsilon:0.010000 episode_count: 27307. steps_count: 12054381.000000\n",
      "ep 3901: ep_len:565 episode reward: total was -17.260000. running mean: -6.839917\n",
      "ep 3901: ep_len:500 episode reward: total was 12.190000. running mean: -6.649618\n",
      "ep 3901: ep_len:635 episode reward: total was -14.920000. running mean: -6.732322\n",
      "ep 3901: ep_len:605 episode reward: total was 11.940000. running mean: -6.545599\n",
      "ep 3901: ep_len:56 episode reward: total was 4.000000. running mean: -6.440143\n",
      "ep 3901: ep_len:160 episode reward: total was 7.620000. running mean: -6.299541\n",
      "ep 3901: ep_len:1334 episode reward: total was -77.300000. running mean: -7.009546\n",
      "epsilon:0.010000 episode_count: 27314. steps_count: 12058236.000000\n",
      "ep 3902: ep_len:585 episode reward: total was 3.100000. running mean: -6.908450\n",
      "ep 3902: ep_len:520 episode reward: total was 3.390000. running mean: -6.805466\n",
      "ep 3902: ep_len:378 episode reward: total was -1.300000. running mean: -6.750411\n",
      "ep 3902: ep_len:552 episode reward: total was -18.450000. running mean: -6.867407\n",
      "ep 3902: ep_len:3 episode reward: total was 0.000000. running mean: -6.798733\n",
      "ep 3902: ep_len:660 episode reward: total was -22.290000. running mean: -6.953646\n",
      "ep 3902: ep_len:500 episode reward: total was -6.100000. running mean: -6.945109\n",
      "epsilon:0.010000 episode_count: 27321. steps_count: 12061434.000000\n",
      "ep 3903: ep_len:615 episode reward: total was 5.610000. running mean: -6.819558\n",
      "ep 3903: ep_len:545 episode reward: total was 17.360000. running mean: -6.577763\n",
      "ep 3903: ep_len:75 episode reward: total was 0.070000. running mean: -6.511285\n",
      "ep 3903: ep_len:500 episode reward: total was -14.050000. running mean: -6.586672\n",
      "ep 3903: ep_len:3 episode reward: total was 0.000000. running mean: -6.520805\n",
      "ep 3903: ep_len:585 episode reward: total was -0.860000. running mean: -6.464197\n",
      "ep 3903: ep_len:500 episode reward: total was -6.830000. running mean: -6.467855\n",
      "epsilon:0.010000 episode_count: 27328. steps_count: 12064257.000000\n",
      "ep 3904: ep_len:655 episode reward: total was 0.610000. running mean: -6.397077\n",
      "ep 3904: ep_len:520 episode reward: total was 8.810000. running mean: -6.245006\n",
      "ep 3904: ep_len:860 episode reward: total was -32.010000. running mean: -6.502656\n",
      "ep 3904: ep_len:500 episode reward: total was -17.590000. running mean: -6.613529\n",
      "ep 3904: ep_len:95 episode reward: total was 6.050000. running mean: -6.486894\n",
      "ep 3904: ep_len:690 episode reward: total was 10.000000. running mean: -6.322025\n",
      "ep 3904: ep_len:615 episode reward: total was -10.780000. running mean: -6.366605\n",
      "epsilon:0.010000 episode_count: 27335. steps_count: 12068192.000000\n",
      "ep 3905: ep_len:224 episode reward: total was 5.130000. running mean: -6.251639\n",
      "ep 3905: ep_len:575 episode reward: total was 0.040000. running mean: -6.188723\n",
      "ep 3905: ep_len:670 episode reward: total was -0.080000. running mean: -6.127635\n",
      "ep 3905: ep_len:500 episode reward: total was -8.470000. running mean: -6.151059\n",
      "ep 3905: ep_len:53 episode reward: total was 2.000000. running mean: -6.069548\n",
      "ep 3905: ep_len:595 episode reward: total was -1.040000. running mean: -6.019253\n",
      "ep 3905: ep_len:580 episode reward: total was -2.960000. running mean: -5.988660\n",
      "epsilon:0.010000 episode_count: 27342. steps_count: 12071389.000000\n",
      "ep 3906: ep_len:575 episode reward: total was -15.190000. running mean: -6.080674\n",
      "ep 3906: ep_len:154 episode reward: total was -11.850000. running mean: -6.138367\n",
      "ep 3906: ep_len:500 episode reward: total was 3.270000. running mean: -6.044283\n",
      "ep 3906: ep_len:555 episode reward: total was -15.000000. running mean: -6.133840\n",
      "ep 3906: ep_len:3 episode reward: total was 0.000000. running mean: -6.072502\n",
      "ep 3906: ep_len:565 episode reward: total was -11.170000. running mean: -6.123477\n",
      "ep 3906: ep_len:302 episode reward: total was -3.280000. running mean: -6.095042\n",
      "epsilon:0.010000 episode_count: 27349. steps_count: 12074043.000000\n",
      "ep 3907: ep_len:505 episode reward: total was 7.430000. running mean: -5.959792\n",
      "ep 3907: ep_len:615 episode reward: total was 14.490000. running mean: -5.755294\n",
      "ep 3907: ep_len:645 episode reward: total was -3.320000. running mean: -5.730941\n",
      "ep 3907: ep_len:515 episode reward: total was 4.400000. running mean: -5.629632\n",
      "ep 3907: ep_len:3 episode reward: total was 0.000000. running mean: -5.573335\n",
      "ep 3907: ep_len:580 episode reward: total was -2.570000. running mean: -5.543302\n",
      "ep 3907: ep_len:575 episode reward: total was -2.020000. running mean: -5.508069\n",
      "epsilon:0.010000 episode_count: 27356. steps_count: 12077481.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3908: ep_len:500 episode reward: total was -9.430000. running mean: -5.547288\n",
      "ep 3908: ep_len:505 episode reward: total was 5.950000. running mean: -5.432315\n",
      "ep 3908: ep_len:505 episode reward: total was 4.260000. running mean: -5.335392\n",
      "ep 3908: ep_len:610 episode reward: total was -13.980000. running mean: -5.421838\n",
      "ep 3908: ep_len:3 episode reward: total was 0.000000. running mean: -5.367620\n",
      "ep 3908: ep_len:645 episode reward: total was 5.590000. running mean: -5.258044\n",
      "ep 3908: ep_len:615 episode reward: total was -12.340000. running mean: -5.328863\n",
      "epsilon:0.010000 episode_count: 27363. steps_count: 12080864.000000\n",
      "ep 3909: ep_len:545 episode reward: total was -16.870000. running mean: -5.444275\n",
      "ep 3909: ep_len:685 episode reward: total was -11.190000. running mean: -5.501732\n",
      "ep 3909: ep_len:625 episode reward: total was -6.680000. running mean: -5.513515\n",
      "ep 3909: ep_len:500 episode reward: total was -2.160000. running mean: -5.479979\n",
      "ep 3909: ep_len:3 episode reward: total was 0.000000. running mean: -5.425180\n",
      "ep 3909: ep_len:715 episode reward: total was -37.740000. running mean: -5.748328\n",
      "ep 3909: ep_len:530 episode reward: total was -15.560000. running mean: -5.846445\n",
      "epsilon:0.010000 episode_count: 27370. steps_count: 12084467.000000\n",
      "ep 3910: ep_len:600 episode reward: total was -19.520000. running mean: -5.983180\n",
      "ep 3910: ep_len:505 episode reward: total was 2.240000. running mean: -5.900948\n",
      "ep 3910: ep_len:73 episode reward: total was 1.040000. running mean: -5.831539\n",
      "ep 3910: ep_len:515 episode reward: total was -13.030000. running mean: -5.903523\n",
      "ep 3910: ep_len:75 episode reward: total was -2.470000. running mean: -5.869188\n",
      "ep 3910: ep_len:580 episode reward: total was -5.200000. running mean: -5.862496\n",
      "ep 3910: ep_len:650 episode reward: total was -23.480000. running mean: -6.038671\n",
      "epsilon:0.010000 episode_count: 27377. steps_count: 12087465.000000\n",
      "ep 3911: ep_len:515 episode reward: total was -9.740000. running mean: -6.075685\n",
      "ep 3911: ep_len:500 episode reward: total was -18.620000. running mean: -6.201128\n",
      "ep 3911: ep_len:565 episode reward: total was -27.300000. running mean: -6.412117\n",
      "ep 3911: ep_len:126 episode reward: total was -0.410000. running mean: -6.352095\n",
      "ep 3911: ep_len:3 episode reward: total was 0.000000. running mean: -6.288574\n",
      "ep 3911: ep_len:500 episode reward: total was 4.280000. running mean: -6.182889\n",
      "ep 3911: ep_len:615 episode reward: total was -20.080000. running mean: -6.321860\n",
      "epsilon:0.010000 episode_count: 27384. steps_count: 12090289.000000\n",
      "ep 3912: ep_len:500 episode reward: total was -7.280000. running mean: -6.331441\n",
      "ep 3912: ep_len:343 episode reward: total was -6.800000. running mean: -6.336127\n",
      "ep 3912: ep_len:610 episode reward: total was 1.970000. running mean: -6.253065\n",
      "ep 3912: ep_len:590 episode reward: total was -10.540000. running mean: -6.295935\n",
      "ep 3912: ep_len:86 episode reward: total was -0.490000. running mean: -6.237875\n",
      "ep 3912: ep_len:303 episode reward: total was 1.130000. running mean: -6.164197\n",
      "ep 3912: ep_len:500 episode reward: total was -16.380000. running mean: -6.266355\n",
      "epsilon:0.010000 episode_count: 27391. steps_count: 12093221.000000\n",
      "ep 3913: ep_len:134 episode reward: total was 6.610000. running mean: -6.137591\n",
      "ep 3913: ep_len:540 episode reward: total was 15.370000. running mean: -5.922515\n",
      "ep 3913: ep_len:79 episode reward: total was -0.450000. running mean: -5.867790\n",
      "ep 3913: ep_len:515 episode reward: total was -6.120000. running mean: -5.870312\n",
      "ep 3913: ep_len:122 episode reward: total was 6.560000. running mean: -5.746009\n",
      "ep 3913: ep_len:580 episode reward: total was 7.640000. running mean: -5.612149\n",
      "ep 3913: ep_len:570 episode reward: total was -26.080000. running mean: -5.816828\n",
      "epsilon:0.010000 episode_count: 27398. steps_count: 12095761.000000\n",
      "ep 3914: ep_len:600 episode reward: total was -4.590000. running mean: -5.804559\n",
      "ep 3914: ep_len:505 episode reward: total was 6.050000. running mean: -5.686014\n",
      "ep 3914: ep_len:500 episode reward: total was 1.880000. running mean: -5.610354\n",
      "ep 3914: ep_len:510 episode reward: total was -21.060000. running mean: -5.764850\n",
      "ep 3914: ep_len:3 episode reward: total was 0.000000. running mean: -5.707201\n",
      "ep 3914: ep_len:605 episode reward: total was -6.040000. running mean: -5.710529\n",
      "ep 3914: ep_len:530 episode reward: total was -14.090000. running mean: -5.794324\n",
      "epsilon:0.010000 episode_count: 27405. steps_count: 12099014.000000\n",
      "ep 3915: ep_len:630 episode reward: total was -9.190000. running mean: -5.828281\n",
      "ep 3915: ep_len:505 episode reward: total was 11.860000. running mean: -5.651398\n",
      "ep 3915: ep_len:555 episode reward: total was -3.260000. running mean: -5.627484\n",
      "ep 3915: ep_len:500 episode reward: total was -16.120000. running mean: -5.732409\n",
      "ep 3915: ep_len:3 episode reward: total was 0.000000. running mean: -5.675085\n",
      "ep 3915: ep_len:620 episode reward: total was 1.860000. running mean: -5.599734\n",
      "ep 3915: ep_len:500 episode reward: total was -11.090000. running mean: -5.654637\n",
      "epsilon:0.010000 episode_count: 27412. steps_count: 12102327.000000\n",
      "ep 3916: ep_len:555 episode reward: total was -13.730000. running mean: -5.735391\n",
      "ep 3916: ep_len:685 episode reward: total was -13.760000. running mean: -5.815637\n",
      "ep 3916: ep_len:57 episode reward: total was 0.520000. running mean: -5.752280\n",
      "ep 3916: ep_len:575 episode reward: total was 9.570000. running mean: -5.599058\n",
      "ep 3916: ep_len:102 episode reward: total was 5.540000. running mean: -5.487667\n",
      "ep 3916: ep_len:645 episode reward: total was -5.030000. running mean: -5.483090\n",
      "ep 3916: ep_len:292 episode reward: total was 0.740000. running mean: -5.420859\n",
      "epsilon:0.010000 episode_count: 27419. steps_count: 12105238.000000\n",
      "ep 3917: ep_len:570 episode reward: total was 4.960000. running mean: -5.317051\n",
      "ep 3917: ep_len:500 episode reward: total was -4.080000. running mean: -5.304680\n",
      "ep 3917: ep_len:530 episode reward: total was -18.370000. running mean: -5.435334\n",
      "ep 3917: ep_len:555 episode reward: total was -4.060000. running mean: -5.421580\n",
      "ep 3917: ep_len:3 episode reward: total was 0.000000. running mean: -5.367364\n",
      "ep 3917: ep_len:500 episode reward: total was -23.600000. running mean: -5.549691\n",
      "ep 3917: ep_len:550 episode reward: total was -43.140000. running mean: -5.925594\n",
      "epsilon:0.010000 episode_count: 27426. steps_count: 12108446.000000\n",
      "ep 3918: ep_len:500 episode reward: total was -1.640000. running mean: -5.882738\n",
      "ep 3918: ep_len:525 episode reward: total was 8.850000. running mean: -5.735411\n",
      "ep 3918: ep_len:505 episode reward: total was -0.130000. running mean: -5.679356\n",
      "ep 3918: ep_len:500 episode reward: total was 9.040000. running mean: -5.532163\n",
      "ep 3918: ep_len:1 episode reward: total was 0.000000. running mean: -5.476841\n",
      "ep 3918: ep_len:655 episode reward: total was -31.840000. running mean: -5.740473\n",
      "ep 3918: ep_len:520 episode reward: total was -7.060000. running mean: -5.753668\n",
      "epsilon:0.010000 episode_count: 27433. steps_count: 12111652.000000\n",
      "ep 3919: ep_len:645 episode reward: total was -16.940000. running mean: -5.865531\n",
      "ep 3919: ep_len:585 episode reward: total was -14.310000. running mean: -5.949976\n",
      "ep 3919: ep_len:436 episode reward: total was -27.310000. running mean: -6.163576\n",
      "ep 3919: ep_len:575 episode reward: total was -2.030000. running mean: -6.122241\n",
      "ep 3919: ep_len:81 episode reward: total was 7.030000. running mean: -5.990718\n",
      "ep 3919: ep_len:550 episode reward: total was 0.460000. running mean: -5.926211\n",
      "ep 3919: ep_len:525 episode reward: total was -15.500000. running mean: -6.021949\n",
      "epsilon:0.010000 episode_count: 27440. steps_count: 12115049.000000\n",
      "ep 3920: ep_len:500 episode reward: total was 11.780000. running mean: -5.843929\n",
      "ep 3920: ep_len:369 episode reward: total was -35.290000. running mean: -6.138390\n",
      "ep 3920: ep_len:590 episode reward: total was -10.770000. running mean: -6.184706\n",
      "ep 3920: ep_len:44 episode reward: total was 1.060000. running mean: -6.112259\n",
      "ep 3920: ep_len:55 episode reward: total was 4.000000. running mean: -6.011137\n",
      "ep 3920: ep_len:810 episode reward: total was -138.350000. running mean: -7.334525\n",
      "ep 3920: ep_len:610 episode reward: total was -6.290000. running mean: -7.324080\n",
      "epsilon:0.010000 episode_count: 27447. steps_count: 12118027.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3921: ep_len:500 episode reward: total was 10.800000. running mean: -7.142839\n",
      "ep 3921: ep_len:545 episode reward: total was -13.530000. running mean: -7.206711\n",
      "ep 3921: ep_len:520 episode reward: total was -2.750000. running mean: -7.162144\n",
      "ep 3921: ep_len:505 episode reward: total was -34.750000. running mean: -7.438022\n",
      "ep 3921: ep_len:3 episode reward: total was 0.000000. running mean: -7.363642\n",
      "ep 3921: ep_len:570 episode reward: total was 5.950000. running mean: -7.230506\n",
      "ep 3921: ep_len:198 episode reward: total was -7.360000. running mean: -7.231800\n",
      "epsilon:0.010000 episode_count: 27454. steps_count: 12120868.000000\n",
      "ep 3922: ep_len:261 episode reward: total was 4.130000. running mean: -7.118182\n",
      "ep 3922: ep_len:500 episode reward: total was 3.070000. running mean: -7.016301\n",
      "ep 3922: ep_len:630 episode reward: total was -12.970000. running mean: -7.075838\n",
      "ep 3922: ep_len:525 episode reward: total was 10.500000. running mean: -6.900079\n",
      "ep 3922: ep_len:3 episode reward: total was 0.000000. running mean: -6.831078\n",
      "ep 3922: ep_len:785 episode reward: total was -49.760000. running mean: -7.260368\n",
      "ep 3922: ep_len:560 episode reward: total was -4.560000. running mean: -7.233364\n",
      "epsilon:0.010000 episode_count: 27461. steps_count: 12124132.000000\n",
      "ep 3923: ep_len:500 episode reward: total was 2.860000. running mean: -7.132430\n",
      "ep 3923: ep_len:515 episode reward: total was -11.050000. running mean: -7.171606\n",
      "ep 3923: ep_len:375 episode reward: total was 5.760000. running mean: -7.042290\n",
      "ep 3923: ep_len:500 episode reward: total was -16.060000. running mean: -7.132467\n",
      "ep 3923: ep_len:3 episode reward: total was 0.000000. running mean: -7.061142\n",
      "ep 3923: ep_len:685 episode reward: total was 3.350000. running mean: -6.957031\n",
      "ep 3923: ep_len:605 episode reward: total was -26.990000. running mean: -7.157361\n",
      "epsilon:0.010000 episode_count: 27468. steps_count: 12127315.000000\n",
      "ep 3924: ep_len:500 episode reward: total was 10.770000. running mean: -6.978087\n",
      "ep 3924: ep_len:276 episode reward: total was -12.320000. running mean: -7.031506\n",
      "ep 3924: ep_len:61 episode reward: total was 3.530000. running mean: -6.925891\n",
      "ep 3924: ep_len:357 episode reward: total was 5.820000. running mean: -6.798432\n",
      "ep 3924: ep_len:3 episode reward: total was 0.000000. running mean: -6.730448\n",
      "ep 3924: ep_len:560 episode reward: total was 1.050000. running mean: -6.652643\n",
      "ep 3924: ep_len:169 episode reward: total was -1.860000. running mean: -6.604717\n",
      "epsilon:0.010000 episode_count: 27475. steps_count: 12129241.000000\n",
      "ep 3925: ep_len:580 episode reward: total was -2.950000. running mean: -6.568170\n",
      "ep 3925: ep_len:640 episode reward: total was -27.360000. running mean: -6.776088\n",
      "ep 3925: ep_len:535 episode reward: total was -20.350000. running mean: -6.911827\n",
      "ep 3925: ep_len:510 episode reward: total was -20.630000. running mean: -7.049009\n",
      "ep 3925: ep_len:130 episode reward: total was 6.060000. running mean: -6.917919\n",
      "ep 3925: ep_len:500 episode reward: total was -19.840000. running mean: -7.047140\n",
      "ep 3925: ep_len:500 episode reward: total was -15.300000. running mean: -7.129668\n",
      "epsilon:0.010000 episode_count: 27482. steps_count: 12132636.000000\n",
      "ep 3926: ep_len:670 episode reward: total was -11.130000. running mean: -7.169672\n",
      "ep 3926: ep_len:500 episode reward: total was -6.110000. running mean: -7.159075\n",
      "ep 3926: ep_len:600 episode reward: total was 10.980000. running mean: -6.977684\n",
      "ep 3926: ep_len:510 episode reward: total was -21.150000. running mean: -7.119407\n",
      "ep 3926: ep_len:3 episode reward: total was 0.000000. running mean: -7.048213\n",
      "ep 3926: ep_len:311 episode reward: total was 3.660000. running mean: -6.941131\n",
      "ep 3926: ep_len:530 episode reward: total was -23.390000. running mean: -7.105620\n",
      "epsilon:0.010000 episode_count: 27489. steps_count: 12135760.000000\n",
      "ep 3927: ep_len:530 episode reward: total was -12.860000. running mean: -7.163164\n",
      "ep 3927: ep_len:535 episode reward: total was 9.180000. running mean: -6.999732\n",
      "ep 3927: ep_len:56 episode reward: total was 0.030000. running mean: -6.929435\n",
      "ep 3927: ep_len:530 episode reward: total was -0.620000. running mean: -6.866340\n",
      "ep 3927: ep_len:3 episode reward: total was 0.000000. running mean: -6.797677\n",
      "ep 3927: ep_len:580 episode reward: total was -2.230000. running mean: -6.752000\n",
      "ep 3927: ep_len:745 episode reward: total was -42.840000. running mean: -7.112880\n",
      "epsilon:0.010000 episode_count: 27496. steps_count: 12138739.000000\n",
      "ep 3928: ep_len:500 episode reward: total was 12.850000. running mean: -6.913251\n",
      "ep 3928: ep_len:500 episode reward: total was -14.880000. running mean: -6.992919\n",
      "ep 3928: ep_len:500 episode reward: total was 3.960000. running mean: -6.883390\n",
      "ep 3928: ep_len:384 episode reward: total was -0.670000. running mean: -6.821256\n",
      "ep 3928: ep_len:3 episode reward: total was 0.000000. running mean: -6.753043\n",
      "ep 3928: ep_len:500 episode reward: total was -29.730000. running mean: -6.982813\n",
      "ep 3928: ep_len:595 episode reward: total was -6.020000. running mean: -6.973185\n",
      "epsilon:0.010000 episode_count: 27503. steps_count: 12141721.000000\n",
      "ep 3929: ep_len:560 episode reward: total was -16.720000. running mean: -7.070653\n",
      "ep 3929: ep_len:555 episode reward: total was 14.910000. running mean: -6.850846\n",
      "ep 3929: ep_len:515 episode reward: total was -16.290000. running mean: -6.945238\n",
      "ep 3929: ep_len:500 episode reward: total was -13.560000. running mean: -7.011385\n",
      "ep 3929: ep_len:3 episode reward: total was 0.000000. running mean: -6.941272\n",
      "ep 3929: ep_len:525 episode reward: total was -20.690000. running mean: -7.078759\n",
      "ep 3929: ep_len:520 episode reward: total was -24.580000. running mean: -7.253771\n",
      "epsilon:0.010000 episode_count: 27510. steps_count: 12144899.000000\n",
      "ep 3930: ep_len:665 episode reward: total was -27.730000. running mean: -7.458534\n",
      "ep 3930: ep_len:345 episode reward: total was -43.850000. running mean: -7.822448\n",
      "ep 3930: ep_len:545 episode reward: total was -11.550000. running mean: -7.859724\n",
      "ep 3930: ep_len:418 episode reward: total was 4.350000. running mean: -7.737626\n",
      "ep 3930: ep_len:3 episode reward: total was 0.000000. running mean: -7.660250\n",
      "ep 3930: ep_len:229 episode reward: total was 8.160000. running mean: -7.502048\n",
      "ep 3930: ep_len:295 episode reward: total was -6.290000. running mean: -7.489927\n",
      "epsilon:0.010000 episode_count: 27517. steps_count: 12147399.000000\n",
      "ep 3931: ep_len:565 episode reward: total was 9.110000. running mean: -7.323928\n",
      "ep 3931: ep_len:520 episode reward: total was 2.610000. running mean: -7.224589\n",
      "ep 3931: ep_len:500 episode reward: total was -7.570000. running mean: -7.228043\n",
      "ep 3931: ep_len:580 episode reward: total was 3.410000. running mean: -7.121662\n",
      "ep 3931: ep_len:50 episode reward: total was 3.500000. running mean: -7.015446\n",
      "ep 3931: ep_len:175 episode reward: total was 7.620000. running mean: -6.869091\n",
      "ep 3931: ep_len:500 episode reward: total was -11.470000. running mean: -6.915100\n",
      "epsilon:0.010000 episode_count: 27524. steps_count: 12150289.000000\n",
      "ep 3932: ep_len:500 episode reward: total was 14.290000. running mean: -6.703049\n",
      "ep 3932: ep_len:615 episode reward: total was -14.950000. running mean: -6.785519\n",
      "ep 3932: ep_len:438 episode reward: total was -12.270000. running mean: -6.840364\n",
      "ep 3932: ep_len:615 episode reward: total was -6.860000. running mean: -6.840560\n",
      "ep 3932: ep_len:3 episode reward: total was 0.000000. running mean: -6.772154\n",
      "ep 3932: ep_len:530 episode reward: total was 2.350000. running mean: -6.680933\n",
      "ep 3932: ep_len:620 episode reward: total was -14.980000. running mean: -6.763924\n",
      "epsilon:0.010000 episode_count: 27531. steps_count: 12153610.000000\n",
      "ep 3933: ep_len:515 episode reward: total was -27.950000. running mean: -6.975784\n",
      "ep 3933: ep_len:670 episode reward: total was 7.600000. running mean: -6.830026\n",
      "ep 3933: ep_len:510 episode reward: total was -4.300000. running mean: -6.804726\n",
      "ep 3933: ep_len:52 episode reward: total was 1.060000. running mean: -6.726079\n",
      "ep 3933: ep_len:105 episode reward: total was 3.530000. running mean: -6.623518\n",
      "ep 3933: ep_len:510 episode reward: total was -14.890000. running mean: -6.706183\n",
      "ep 3933: ep_len:600 episode reward: total was -13.350000. running mean: -6.772621\n",
      "epsilon:0.010000 episode_count: 27538. steps_count: 12156572.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3934: ep_len:500 episode reward: total was 8.920000. running mean: -6.615695\n",
      "ep 3934: ep_len:555 episode reward: total was 4.450000. running mean: -6.505038\n",
      "ep 3934: ep_len:595 episode reward: total was 3.580000. running mean: -6.404188\n",
      "ep 3934: ep_len:90 episode reward: total was -10.930000. running mean: -6.449446\n",
      "ep 3934: ep_len:3 episode reward: total was 0.000000. running mean: -6.384951\n",
      "ep 3934: ep_len:620 episode reward: total was 0.960000. running mean: -6.311502\n",
      "ep 3934: ep_len:208 episode reward: total was -1.830000. running mean: -6.266687\n",
      "epsilon:0.010000 episode_count: 27545. steps_count: 12159143.000000\n",
      "ep 3935: ep_len:525 episode reward: total was -24.810000. running mean: -6.452120\n",
      "ep 3935: ep_len:197 episode reward: total was -3.380000. running mean: -6.421399\n",
      "ep 3935: ep_len:665 episode reward: total was -43.350000. running mean: -6.790685\n",
      "ep 3935: ep_len:535 episode reward: total was -5.690000. running mean: -6.779678\n",
      "ep 3935: ep_len:3 episode reward: total was 0.000000. running mean: -6.711881\n",
      "ep 3935: ep_len:585 episode reward: total was -16.270000. running mean: -6.807462\n",
      "ep 3935: ep_len:525 episode reward: total was -23.100000. running mean: -6.970388\n",
      "epsilon:0.010000 episode_count: 27552. steps_count: 12162178.000000\n",
      "ep 3936: ep_len:635 episode reward: total was -31.300000. running mean: -7.213684\n",
      "ep 3936: ep_len:500 episode reward: total was 0.470000. running mean: -7.136847\n",
      "ep 3936: ep_len:545 episode reward: total was -13.460000. running mean: -7.200078\n",
      "ep 3936: ep_len:500 episode reward: total was 7.370000. running mean: -7.054378\n",
      "ep 3936: ep_len:74 episode reward: total was 4.040000. running mean: -6.943434\n",
      "ep 3936: ep_len:535 episode reward: total was -13.140000. running mean: -7.005400\n",
      "ep 3936: ep_len:500 episode reward: total was -8.090000. running mean: -7.016246\n",
      "epsilon:0.010000 episode_count: 27559. steps_count: 12165467.000000\n",
      "ep 3937: ep_len:525 episode reward: total was 1.100000. running mean: -6.935083\n",
      "ep 3937: ep_len:241 episode reward: total was -18.340000. running mean: -7.049132\n",
      "ep 3937: ep_len:79 episode reward: total was 2.060000. running mean: -6.958041\n",
      "ep 3937: ep_len:510 episode reward: total was 6.090000. running mean: -6.827561\n",
      "ep 3937: ep_len:61 episode reward: total was 2.000000. running mean: -6.739285\n",
      "ep 3937: ep_len:318 episode reward: total was 3.700000. running mean: -6.634892\n",
      "ep 3937: ep_len:605 episode reward: total was -18.360000. running mean: -6.752143\n",
      "epsilon:0.010000 episode_count: 27566. steps_count: 12167806.000000\n",
      "ep 3938: ep_len:530 episode reward: total was -28.500000. running mean: -6.969622\n",
      "ep 3938: ep_len:187 episode reward: total was -18.430000. running mean: -7.084226\n",
      "ep 3938: ep_len:394 episode reward: total was 7.260000. running mean: -6.940783\n",
      "ep 3938: ep_len:505 episode reward: total was -23.670000. running mean: -7.108075\n",
      "ep 3938: ep_len:3 episode reward: total was 0.000000. running mean: -7.036995\n",
      "ep 3938: ep_len:500 episode reward: total was -4.730000. running mean: -7.013925\n",
      "ep 3938: ep_len:530 episode reward: total was -11.030000. running mean: -7.054085\n",
      "epsilon:0.010000 episode_count: 27573. steps_count: 12170455.000000\n",
      "ep 3939: ep_len:580 episode reward: total was 12.510000. running mean: -6.858445\n",
      "ep 3939: ep_len:500 episode reward: total was -3.680000. running mean: -6.826660\n",
      "ep 3939: ep_len:500 episode reward: total was -14.500000. running mean: -6.903394\n",
      "ep 3939: ep_len:525 episode reward: total was -7.440000. running mean: -6.908760\n",
      "ep 3939: ep_len:3 episode reward: total was 0.000000. running mean: -6.839672\n",
      "ep 3939: ep_len:670 episode reward: total was -1.150000. running mean: -6.782775\n",
      "ep 3939: ep_len:575 episode reward: total was -15.280000. running mean: -6.867748\n",
      "epsilon:0.010000 episode_count: 27580. steps_count: 12173808.000000\n",
      "ep 3940: ep_len:550 episode reward: total was 16.440000. running mean: -6.634670\n",
      "ep 3940: ep_len:500 episode reward: total was 14.190000. running mean: -6.426423\n",
      "ep 3940: ep_len:474 episode reward: total was -1.260000. running mean: -6.374759\n",
      "ep 3940: ep_len:500 episode reward: total was 10.400000. running mean: -6.207012\n",
      "ep 3940: ep_len:3 episode reward: total was 0.000000. running mean: -6.144941\n",
      "ep 3940: ep_len:535 episode reward: total was -51.020000. running mean: -6.593692\n",
      "ep 3940: ep_len:605 episode reward: total was 4.810000. running mean: -6.479655\n",
      "epsilon:0.010000 episode_count: 27587. steps_count: 12176975.000000\n",
      "ep 3941: ep_len:238 episode reward: total was -14.350000. running mean: -6.558359\n",
      "ep 3941: ep_len:600 episode reward: total was -24.560000. running mean: -6.738375\n",
      "ep 3941: ep_len:500 episode reward: total was -0.340000. running mean: -6.674391\n",
      "ep 3941: ep_len:122 episode reward: total was 1.600000. running mean: -6.591647\n",
      "ep 3941: ep_len:82 episode reward: total was 5.530000. running mean: -6.470431\n",
      "ep 3941: ep_len:660 episode reward: total was -17.500000. running mean: -6.580727\n",
      "ep 3941: ep_len:540 episode reward: total was -5.010000. running mean: -6.565019\n",
      "epsilon:0.010000 episode_count: 27594. steps_count: 12179717.000000\n",
      "ep 3942: ep_len:535 episode reward: total was 6.400000. running mean: -6.435369\n",
      "ep 3942: ep_len:515 episode reward: total was 14.720000. running mean: -6.223815\n",
      "ep 3942: ep_len:394 episode reward: total was -7.350000. running mean: -6.235077\n",
      "ep 3942: ep_len:550 episode reward: total was 10.970000. running mean: -6.063026\n",
      "ep 3942: ep_len:101 episode reward: total was 6.030000. running mean: -5.942096\n",
      "ep 3942: ep_len:535 episode reward: total was -25.990000. running mean: -6.142575\n",
      "ep 3942: ep_len:555 episode reward: total was -18.500000. running mean: -6.266149\n",
      "epsilon:0.010000 episode_count: 27601. steps_count: 12182902.000000\n",
      "ep 3943: ep_len:585 episode reward: total was -14.070000. running mean: -6.344188\n",
      "ep 3943: ep_len:500 episode reward: total was 0.750000. running mean: -6.273246\n",
      "ep 3943: ep_len:650 episode reward: total was -21.790000. running mean: -6.428414\n",
      "ep 3943: ep_len:530 episode reward: total was 9.940000. running mean: -6.264730\n",
      "ep 3943: ep_len:3 episode reward: total was 0.000000. running mean: -6.202082\n",
      "ep 3943: ep_len:595 episode reward: total was -7.080000. running mean: -6.210861\n",
      "ep 3943: ep_len:525 episode reward: total was -18.480000. running mean: -6.333553\n",
      "epsilon:0.010000 episode_count: 27608. steps_count: 12186290.000000\n",
      "ep 3944: ep_len:585 episode reward: total was -10.150000. running mean: -6.371717\n",
      "ep 3944: ep_len:650 episode reward: total was 15.580000. running mean: -6.152200\n",
      "ep 3944: ep_len:625 episode reward: total was -2.040000. running mean: -6.111078\n",
      "ep 3944: ep_len:555 episode reward: total was 9.480000. running mean: -5.955167\n",
      "ep 3944: ep_len:52 episode reward: total was 5.000000. running mean: -5.845616\n",
      "ep 3944: ep_len:500 episode reward: total was -19.730000. running mean: -5.984459\n",
      "ep 3944: ep_len:580 episode reward: total was -25.540000. running mean: -6.180015\n",
      "epsilon:0.010000 episode_count: 27615. steps_count: 12189837.000000\n",
      "ep 3945: ep_len:555 episode reward: total was -3.920000. running mean: -6.157415\n",
      "ep 3945: ep_len:570 episode reward: total was 21.950000. running mean: -5.876341\n",
      "ep 3945: ep_len:436 episode reward: total was 9.780000. running mean: -5.719777\n",
      "ep 3945: ep_len:515 episode reward: total was -9.540000. running mean: -5.757979\n",
      "ep 3945: ep_len:3 episode reward: total was 0.000000. running mean: -5.700400\n",
      "ep 3945: ep_len:179 episode reward: total was 7.130000. running mean: -5.572096\n",
      "ep 3945: ep_len:545 episode reward: total was -2.980000. running mean: -5.546175\n",
      "epsilon:0.010000 episode_count: 27622. steps_count: 12192640.000000\n",
      "ep 3946: ep_len:550 episode reward: total was 6.170000. running mean: -5.429013\n",
      "ep 3946: ep_len:510 episode reward: total was -3.710000. running mean: -5.411823\n",
      "ep 3946: ep_len:550 episode reward: total was -6.700000. running mean: -5.424705\n",
      "ep 3946: ep_len:500 episode reward: total was 4.050000. running mean: -5.329958\n",
      "ep 3946: ep_len:32 episode reward: total was -3.000000. running mean: -5.306658\n",
      "ep 3946: ep_len:500 episode reward: total was 4.950000. running mean: -5.204091\n",
      "ep 3946: ep_len:565 episode reward: total was -6.270000. running mean: -5.214750\n",
      "epsilon:0.010000 episode_count: 27629. steps_count: 12195847.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3947: ep_len:117 episode reward: total was 0.580000. running mean: -5.156803\n",
      "ep 3947: ep_len:560 episode reward: total was 11.600000. running mean: -4.989235\n",
      "ep 3947: ep_len:550 episode reward: total was -6.380000. running mean: -5.003143\n",
      "ep 3947: ep_len:152 episode reward: total was -11.360000. running mean: -5.066711\n",
      "ep 3947: ep_len:53 episode reward: total was 5.000000. running mean: -4.966044\n",
      "ep 3947: ep_len:540 episode reward: total was -48.010000. running mean: -5.396484\n",
      "ep 3947: ep_len:570 episode reward: total was -41.080000. running mean: -5.753319\n",
      "epsilon:0.010000 episode_count: 27636. steps_count: 12198389.000000\n",
      "ep 3948: ep_len:500 episode reward: total was 16.280000. running mean: -5.532986\n",
      "ep 3948: ep_len:500 episode reward: total was 4.280000. running mean: -5.434856\n",
      "ep 3948: ep_len:610 episode reward: total was -1.270000. running mean: -5.393207\n",
      "ep 3948: ep_len:121 episode reward: total was 2.120000. running mean: -5.318075\n",
      "ep 3948: ep_len:3 episode reward: total was 0.000000. running mean: -5.264894\n",
      "ep 3948: ep_len:530 episode reward: total was -53.570000. running mean: -5.747945\n",
      "ep 3948: ep_len:620 episode reward: total was -1.250000. running mean: -5.702966\n",
      "epsilon:0.010000 episode_count: 27643. steps_count: 12201273.000000\n",
      "ep 3949: ep_len:229 episode reward: total was 6.630000. running mean: -5.579636\n",
      "ep 3949: ep_len:359 episode reward: total was -36.300000. running mean: -5.886840\n",
      "ep 3949: ep_len:560 episode reward: total was 10.490000. running mean: -5.723072\n",
      "ep 3949: ep_len:500 episode reward: total was 11.590000. running mean: -5.549941\n",
      "ep 3949: ep_len:93 episode reward: total was -9.950000. running mean: -5.593941\n",
      "ep 3949: ep_len:560 episode reward: total was -49.540000. running mean: -6.033402\n",
      "ep 3949: ep_len:600 episode reward: total was -14.950000. running mean: -6.122568\n",
      "epsilon:0.010000 episode_count: 27650. steps_count: 12204174.000000\n",
      "ep 3950: ep_len:242 episode reward: total was -10.850000. running mean: -6.169842\n",
      "ep 3950: ep_len:640 episode reward: total was -9.720000. running mean: -6.205344\n",
      "ep 3950: ep_len:580 episode reward: total was -8.250000. running mean: -6.225790\n",
      "ep 3950: ep_len:550 episode reward: total was 12.010000. running mean: -6.043433\n",
      "ep 3950: ep_len:3 episode reward: total was 0.000000. running mean: -5.982998\n",
      "ep 3950: ep_len:525 episode reward: total was -41.980000. running mean: -6.342968\n",
      "ep 3950: ep_len:500 episode reward: total was -17.530000. running mean: -6.454839\n",
      "epsilon:0.010000 episode_count: 27657. steps_count: 12207214.000000\n",
      "ep 3951: ep_len:500 episode reward: total was -3.910000. running mean: -6.429390\n",
      "ep 3951: ep_len:585 episode reward: total was -5.540000. running mean: -6.420496\n",
      "ep 3951: ep_len:665 episode reward: total was -39.560000. running mean: -6.751891\n",
      "ep 3951: ep_len:56 episode reward: total was 1.560000. running mean: -6.668772\n",
      "ep 3951: ep_len:3 episode reward: total was 0.000000. running mean: -6.602085\n",
      "ep 3951: ep_len:680 episode reward: total was -9.670000. running mean: -6.632764\n",
      "ep 3951: ep_len:590 episode reward: total was 0.740000. running mean: -6.559036\n",
      "epsilon:0.010000 episode_count: 27664. steps_count: 12210293.000000\n",
      "ep 3952: ep_len:520 episode reward: total was 6.630000. running mean: -6.427146\n",
      "ep 3952: ep_len:600 episode reward: total was -16.130000. running mean: -6.524174\n",
      "ep 3952: ep_len:645 episode reward: total was -4.080000. running mean: -6.499733\n",
      "ep 3952: ep_len:500 episode reward: total was -27.140000. running mean: -6.706135\n",
      "ep 3952: ep_len:118 episode reward: total was 7.560000. running mean: -6.563474\n",
      "ep 3952: ep_len:500 episode reward: total was -6.250000. running mean: -6.560339\n",
      "ep 3952: ep_len:500 episode reward: total was -1.680000. running mean: -6.511536\n",
      "epsilon:0.010000 episode_count: 27671. steps_count: 12213676.000000\n",
      "ep 3953: ep_len:515 episode reward: total was -8.850000. running mean: -6.534920\n",
      "ep 3953: ep_len:525 episode reward: total was 2.380000. running mean: -6.445771\n",
      "ep 3953: ep_len:605 episode reward: total was -0.390000. running mean: -6.385214\n",
      "ep 3953: ep_len:407 episode reward: total was 7.400000. running mean: -6.247361\n",
      "ep 3953: ep_len:3 episode reward: total was 0.000000. running mean: -6.184888\n",
      "ep 3953: ep_len:570 episode reward: total was 10.190000. running mean: -6.021139\n",
      "ep 3953: ep_len:570 episode reward: total was -4.350000. running mean: -6.004427\n",
      "epsilon:0.010000 episode_count: 27678. steps_count: 12216871.000000\n",
      "ep 3954: ep_len:535 episode reward: total was 5.130000. running mean: -5.893083\n",
      "ep 3954: ep_len:500 episode reward: total was -9.000000. running mean: -5.924152\n",
      "ep 3954: ep_len:366 episode reward: total was 5.220000. running mean: -5.812711\n",
      "ep 3954: ep_len:500 episode reward: total was 9.990000. running mean: -5.654684\n",
      "ep 3954: ep_len:3 episode reward: total was 0.000000. running mean: -5.598137\n",
      "ep 3954: ep_len:500 episode reward: total was -4.380000. running mean: -5.585956\n",
      "ep 3954: ep_len:560 episode reward: total was -13.920000. running mean: -5.669296\n",
      "epsilon:0.010000 episode_count: 27685. steps_count: 12219835.000000\n",
      "ep 3955: ep_len:505 episode reward: total was 3.080000. running mean: -5.581803\n",
      "ep 3955: ep_len:525 episode reward: total was -1.310000. running mean: -5.539085\n",
      "ep 3955: ep_len:79 episode reward: total was 0.040000. running mean: -5.483294\n",
      "ep 3955: ep_len:565 episode reward: total was 9.500000. running mean: -5.333461\n",
      "ep 3955: ep_len:3 episode reward: total was 0.000000. running mean: -5.280127\n",
      "ep 3955: ep_len:600 episode reward: total was -8.680000. running mean: -5.314125\n",
      "ep 3955: ep_len:610 episode reward: total was -11.860000. running mean: -5.379584\n",
      "epsilon:0.010000 episode_count: 27692. steps_count: 12222722.000000\n",
      "ep 3956: ep_len:700 episode reward: total was -26.250000. running mean: -5.588288\n",
      "ep 3956: ep_len:500 episode reward: total was -6.020000. running mean: -5.592605\n",
      "ep 3956: ep_len:560 episode reward: total was -1.100000. running mean: -5.547679\n",
      "ep 3956: ep_len:105 episode reward: total was 2.610000. running mean: -5.466103\n",
      "ep 3956: ep_len:3 episode reward: total was 0.000000. running mean: -5.411441\n",
      "ep 3956: ep_len:545 episode reward: total was -1.850000. running mean: -5.375827\n",
      "ep 3956: ep_len:304 episode reward: total was -4.320000. running mean: -5.365269\n",
      "epsilon:0.010000 episode_count: 27699. steps_count: 12225439.000000\n",
      "ep 3957: ep_len:90 episode reward: total was 2.550000. running mean: -5.286116\n",
      "ep 3957: ep_len:595 episode reward: total was -8.910000. running mean: -5.322355\n",
      "ep 3957: ep_len:354 episode reward: total was -2.810000. running mean: -5.297231\n",
      "ep 3957: ep_len:545 episode reward: total was 7.960000. running mean: -5.164659\n",
      "ep 3957: ep_len:98 episode reward: total was -0.440000. running mean: -5.117412\n",
      "ep 3957: ep_len:500 episode reward: total was -17.530000. running mean: -5.241538\n",
      "ep 3957: ep_len:565 episode reward: total was -27.960000. running mean: -5.468723\n",
      "epsilon:0.010000 episode_count: 27706. steps_count: 12228186.000000\n",
      "ep 3958: ep_len:555 episode reward: total was -12.200000. running mean: -5.536036\n",
      "ep 3958: ep_len:590 episode reward: total was -37.960000. running mean: -5.860275\n",
      "ep 3958: ep_len:1259 episode reward: total was -98.820000. running mean: -6.789873\n",
      "ep 3958: ep_len:500 episode reward: total was -15.560000. running mean: -6.877574\n",
      "ep 3958: ep_len:96 episode reward: total was 5.530000. running mean: -6.753498\n",
      "ep 3958: ep_len:500 episode reward: total was -18.750000. running mean: -6.873463\n",
      "ep 3958: ep_len:600 episode reward: total was -16.080000. running mean: -6.965529\n",
      "epsilon:0.010000 episode_count: 27713. steps_count: 12232286.000000\n",
      "ep 3959: ep_len:550 episode reward: total was 7.840000. running mean: -6.817473\n",
      "ep 3959: ep_len:500 episode reward: total was 12.720000. running mean: -6.622099\n",
      "ep 3959: ep_len:610 episode reward: total was -2.450000. running mean: -6.580378\n",
      "ep 3959: ep_len:510 episode reward: total was -7.000000. running mean: -6.584574\n",
      "ep 3959: ep_len:103 episode reward: total was 4.040000. running mean: -6.478328\n",
      "ep 3959: ep_len:635 episode reward: total was 2.300000. running mean: -6.390545\n",
      "ep 3959: ep_len:545 episode reward: total was -19.510000. running mean: -6.521739\n",
      "epsilon:0.010000 episode_count: 27720. steps_count: 12235739.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3960: ep_len:525 episode reward: total was -3.030000. running mean: -6.486822\n",
      "ep 3960: ep_len:520 episode reward: total was 0.880000. running mean: -6.413154\n",
      "ep 3960: ep_len:545 episode reward: total was -2.780000. running mean: -6.376822\n",
      "ep 3960: ep_len:533 episode reward: total was 9.140000. running mean: -6.221654\n",
      "ep 3960: ep_len:3 episode reward: total was 0.000000. running mean: -6.159437\n",
      "ep 3960: ep_len:680 episode reward: total was -18.240000. running mean: -6.280243\n",
      "ep 3960: ep_len:610 episode reward: total was -6.980000. running mean: -6.287241\n",
      "epsilon:0.010000 episode_count: 27727. steps_count: 12239155.000000\n",
      "ep 3961: ep_len:570 episode reward: total was 12.500000. running mean: -6.099368\n",
      "ep 3961: ep_len:299 episode reward: total was -36.830000. running mean: -6.406675\n",
      "ep 3961: ep_len:570 episode reward: total was -2.560000. running mean: -6.368208\n",
      "ep 3961: ep_len:510 episode reward: total was -2.090000. running mean: -6.325426\n",
      "ep 3961: ep_len:49 episode reward: total was -3.000000. running mean: -6.292171\n",
      "ep 3961: ep_len:620 episode reward: total was -1.900000. running mean: -6.248250\n",
      "ep 3961: ep_len:555 episode reward: total was -20.570000. running mean: -6.391467\n",
      "epsilon:0.010000 episode_count: 27734. steps_count: 12242328.000000\n",
      "ep 3962: ep_len:640 episode reward: total was -26.770000. running mean: -6.595253\n",
      "ep 3962: ep_len:530 episode reward: total was -7.900000. running mean: -6.608300\n",
      "ep 3962: ep_len:530 episode reward: total was -15.240000. running mean: -6.694617\n",
      "ep 3962: ep_len:155 episode reward: total was 4.150000. running mean: -6.586171\n",
      "ep 3962: ep_len:3 episode reward: total was 0.000000. running mean: -6.520309\n",
      "ep 3962: ep_len:635 episode reward: total was -4.460000. running mean: -6.499706\n",
      "ep 3962: ep_len:575 episode reward: total was -46.100000. running mean: -6.895709\n",
      "epsilon:0.010000 episode_count: 27741. steps_count: 12245396.000000\n",
      "ep 3963: ep_len:560 episode reward: total was 1.420000. running mean: -6.812552\n",
      "ep 3963: ep_len:510 episode reward: total was -5.340000. running mean: -6.797826\n",
      "ep 3963: ep_len:605 episode reward: total was -7.130000. running mean: -6.801148\n",
      "ep 3963: ep_len:500 episode reward: total was 4.920000. running mean: -6.683937\n",
      "ep 3963: ep_len:89 episode reward: total was 6.520000. running mean: -6.551897\n",
      "ep 3963: ep_len:520 episode reward: total was -14.780000. running mean: -6.634178\n",
      "ep 3963: ep_len:585 episode reward: total was -13.870000. running mean: -6.706537\n",
      "epsilon:0.010000 episode_count: 27748. steps_count: 12248765.000000\n",
      "ep 3964: ep_len:630 episode reward: total was 2.120000. running mean: -6.618271\n",
      "ep 3964: ep_len:625 episode reward: total was 12.510000. running mean: -6.426988\n",
      "ep 3964: ep_len:605 episode reward: total was -1.920000. running mean: -6.381919\n",
      "ep 3964: ep_len:500 episode reward: total was 1.310000. running mean: -6.304999\n",
      "ep 3964: ep_len:3 episode reward: total was 0.000000. running mean: -6.241949\n",
      "ep 3964: ep_len:525 episode reward: total was 0.520000. running mean: -6.174330\n",
      "ep 3964: ep_len:605 episode reward: total was -10.550000. running mean: -6.218087\n",
      "epsilon:0.010000 episode_count: 27755. steps_count: 12252258.000000\n",
      "ep 3965: ep_len:540 episode reward: total was -29.660000. running mean: -6.452506\n",
      "ep 3965: ep_len:185 episode reward: total was -3.350000. running mean: -6.421481\n",
      "ep 3965: ep_len:570 episode reward: total was -43.490000. running mean: -6.792166\n",
      "ep 3965: ep_len:545 episode reward: total was -14.680000. running mean: -6.871044\n",
      "ep 3965: ep_len:114 episode reward: total was 7.550000. running mean: -6.726834\n",
      "ep 3965: ep_len:625 episode reward: total was -27.770000. running mean: -6.937265\n",
      "ep 3965: ep_len:620 episode reward: total was -14.320000. running mean: -7.011093\n",
      "epsilon:0.010000 episode_count: 27762. steps_count: 12255457.000000\n",
      "ep 3966: ep_len:515 episode reward: total was -25.920000. running mean: -7.200182\n",
      "ep 3966: ep_len:174 episode reward: total was -6.400000. running mean: -7.192180\n",
      "ep 3966: ep_len:640 episode reward: total was -1.930000. running mean: -7.139558\n",
      "ep 3966: ep_len:500 episode reward: total was -15.080000. running mean: -7.218963\n",
      "ep 3966: ep_len:3 episode reward: total was 0.000000. running mean: -7.146773\n",
      "ep 3966: ep_len:650 episode reward: total was -23.510000. running mean: -7.310405\n",
      "ep 3966: ep_len:500 episode reward: total was -8.910000. running mean: -7.326401\n",
      "epsilon:0.010000 episode_count: 27769. steps_count: 12258439.000000\n",
      "ep 3967: ep_len:520 episode reward: total was -5.950000. running mean: -7.312637\n",
      "ep 3967: ep_len:500 episode reward: total was -14.420000. running mean: -7.383711\n",
      "ep 3967: ep_len:540 episode reward: total was -2.580000. running mean: -7.335674\n",
      "ep 3967: ep_len:520 episode reward: total was -10.020000. running mean: -7.362517\n",
      "ep 3967: ep_len:3 episode reward: total was 0.000000. running mean: -7.288892\n",
      "ep 3967: ep_len:520 episode reward: total was 0.110000. running mean: -7.214903\n",
      "ep 3967: ep_len:555 episode reward: total was -7.060000. running mean: -7.213354\n",
      "epsilon:0.010000 episode_count: 27776. steps_count: 12261597.000000\n",
      "ep 3968: ep_len:214 episode reward: total was 2.620000. running mean: -7.115020\n",
      "ep 3968: ep_len:510 episode reward: total was 13.220000. running mean: -6.911670\n",
      "ep 3968: ep_len:600 episode reward: total was -47.710000. running mean: -7.319653\n",
      "ep 3968: ep_len:500 episode reward: total was -1.620000. running mean: -7.262657\n",
      "ep 3968: ep_len:54 episode reward: total was 3.500000. running mean: -7.155030\n",
      "ep 3968: ep_len:680 episode reward: total was -45.340000. running mean: -7.536880\n",
      "ep 3968: ep_len:555 episode reward: total was -19.000000. running mean: -7.651511\n",
      "epsilon:0.010000 episode_count: 27783. steps_count: 12264710.000000\n",
      "ep 3969: ep_len:540 episode reward: total was -6.080000. running mean: -7.635796\n",
      "ep 3969: ep_len:347 episode reward: total was -8.290000. running mean: -7.642338\n",
      "ep 3969: ep_len:500 episode reward: total was -3.020000. running mean: -7.596115\n",
      "ep 3969: ep_len:570 episode reward: total was -7.510000. running mean: -7.595254\n",
      "ep 3969: ep_len:97 episode reward: total was 6.540000. running mean: -7.453901\n",
      "ep 3969: ep_len:515 episode reward: total was -7.720000. running mean: -7.456562\n",
      "ep 3969: ep_len:505 episode reward: total was -14.550000. running mean: -7.527496\n",
      "epsilon:0.010000 episode_count: 27790. steps_count: 12267784.000000\n",
      "ep 3970: ep_len:525 episode reward: total was -2.420000. running mean: -7.476421\n",
      "ep 3970: ep_len:560 episode reward: total was 13.840000. running mean: -7.263257\n",
      "ep 3970: ep_len:580 episode reward: total was -36.880000. running mean: -7.559425\n",
      "ep 3970: ep_len:359 episode reward: total was 6.310000. running mean: -7.420730\n",
      "ep 3970: ep_len:3 episode reward: total was 0.000000. running mean: -7.346523\n",
      "ep 3970: ep_len:303 episode reward: total was 1.190000. running mean: -7.261158\n",
      "ep 3970: ep_len:500 episode reward: total was -12.130000. running mean: -7.309846\n",
      "epsilon:0.010000 episode_count: 27797. steps_count: 12270614.000000\n",
      "ep 3971: ep_len:111 episode reward: total was 4.090000. running mean: -7.195848\n",
      "ep 3971: ep_len:570 episode reward: total was -27.400000. running mean: -7.397889\n",
      "ep 3971: ep_len:590 episode reward: total was -25.740000. running mean: -7.581311\n",
      "ep 3971: ep_len:585 episode reward: total was 11.960000. running mean: -7.385897\n",
      "ep 3971: ep_len:3 episode reward: total was 0.000000. running mean: -7.312038\n",
      "ep 3971: ep_len:635 episode reward: total was -3.660000. running mean: -7.275518\n",
      "ep 3971: ep_len:570 episode reward: total was -16.990000. running mean: -7.372663\n",
      "epsilon:0.010000 episode_count: 27804. steps_count: 12273678.000000\n",
      "ep 3972: ep_len:252 episode reward: total was 0.130000. running mean: -7.297636\n",
      "ep 3972: ep_len:136 episode reward: total was -13.400000. running mean: -7.358660\n",
      "ep 3972: ep_len:79 episode reward: total was 0.040000. running mean: -7.284673\n",
      "ep 3972: ep_len:545 episode reward: total was -17.020000. running mean: -7.382027\n",
      "ep 3972: ep_len:48 episode reward: total was 4.500000. running mean: -7.263206\n",
      "ep 3972: ep_len:590 episode reward: total was -23.810000. running mean: -7.428674\n",
      "ep 3972: ep_len:500 episode reward: total was -23.970000. running mean: -7.594087\n",
      "epsilon:0.010000 episode_count: 27811. steps_count: 12275828.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3973: ep_len:580 episode reward: total was -24.530000. running mean: -7.763447\n",
      "ep 3973: ep_len:500 episode reward: total was -9.400000. running mean: -7.779812\n",
      "ep 3973: ep_len:695 episode reward: total was -12.670000. running mean: -7.828714\n",
      "ep 3973: ep_len:500 episode reward: total was 9.540000. running mean: -7.655027\n",
      "ep 3973: ep_len:3 episode reward: total was 0.000000. running mean: -7.578477\n",
      "ep 3973: ep_len:535 episode reward: total was -11.960000. running mean: -7.622292\n",
      "ep 3973: ep_len:615 episode reward: total was -19.870000. running mean: -7.744769\n",
      "epsilon:0.010000 episode_count: 27818. steps_count: 12279256.000000\n",
      "ep 3974: ep_len:500 episode reward: total was 5.810000. running mean: -7.609221\n",
      "ep 3974: ep_len:595 episode reward: total was 2.960000. running mean: -7.503529\n",
      "ep 3974: ep_len:555 episode reward: total was -3.950000. running mean: -7.467994\n",
      "ep 3974: ep_len:56 episode reward: total was 1.560000. running mean: -7.377714\n",
      "ep 3974: ep_len:51 episode reward: total was 5.000000. running mean: -7.253937\n",
      "ep 3974: ep_len:575 episode reward: total was -3.020000. running mean: -7.211597\n",
      "ep 3974: ep_len:575 episode reward: total was -7.820000. running mean: -7.217681\n",
      "epsilon:0.010000 episode_count: 27825. steps_count: 12282163.000000\n",
      "ep 3975: ep_len:535 episode reward: total was -34.430000. running mean: -7.489804\n",
      "ep 3975: ep_len:520 episode reward: total was 16.880000. running mean: -7.246106\n",
      "ep 3975: ep_len:535 episode reward: total was -17.550000. running mean: -7.349145\n",
      "ep 3975: ep_len:122 episode reward: total was 5.590000. running mean: -7.219754\n",
      "ep 3975: ep_len:118 episode reward: total was -8.430000. running mean: -7.231856\n",
      "ep 3975: ep_len:515 episode reward: total was -0.450000. running mean: -7.164038\n",
      "ep 3975: ep_len:530 episode reward: total was -7.540000. running mean: -7.167797\n",
      "epsilon:0.010000 episode_count: 27832. steps_count: 12285038.000000\n",
      "ep 3976: ep_len:710 episode reward: total was -95.150000. running mean: -8.047619\n",
      "ep 3976: ep_len:585 episode reward: total was -63.260000. running mean: -8.599743\n",
      "ep 3976: ep_len:368 episode reward: total was 6.750000. running mean: -8.446246\n",
      "ep 3976: ep_len:395 episode reward: total was 2.850000. running mean: -8.333283\n",
      "ep 3976: ep_len:110 episode reward: total was 6.050000. running mean: -8.189451\n",
      "ep 3976: ep_len:242 episode reward: total was 7.120000. running mean: -8.036356\n",
      "ep 3976: ep_len:540 episode reward: total was -35.920000. running mean: -8.315192\n",
      "epsilon:0.010000 episode_count: 27839. steps_count: 12287988.000000\n",
      "ep 3977: ep_len:530 episode reward: total was -30.570000. running mean: -8.537741\n",
      "ep 3977: ep_len:500 episode reward: total was -4.170000. running mean: -8.494063\n",
      "ep 3977: ep_len:545 episode reward: total was -16.780000. running mean: -8.576923\n",
      "ep 3977: ep_len:500 episode reward: total was -8.120000. running mean: -8.572353\n",
      "ep 3977: ep_len:3 episode reward: total was 0.000000. running mean: -8.486630\n",
      "ep 3977: ep_len:585 episode reward: total was -18.510000. running mean: -8.586863\n",
      "ep 3977: ep_len:500 episode reward: total was -5.010000. running mean: -8.551095\n",
      "epsilon:0.010000 episode_count: 27846. steps_count: 12291151.000000\n",
      "ep 3978: ep_len:535 episode reward: total was -7.140000. running mean: -8.536984\n",
      "ep 3978: ep_len:505 episode reward: total was -15.570000. running mean: -8.607314\n",
      "ep 3978: ep_len:555 episode reward: total was 6.930000. running mean: -8.451941\n",
      "ep 3978: ep_len:500 episode reward: total was -18.610000. running mean: -8.553522\n",
      "ep 3978: ep_len:3 episode reward: total was 0.000000. running mean: -8.467986\n",
      "ep 3978: ep_len:500 episode reward: total was -24.750000. running mean: -8.630806\n",
      "ep 3978: ep_len:580 episode reward: total was -13.960000. running mean: -8.684098\n",
      "epsilon:0.010000 episode_count: 27853. steps_count: 12294329.000000\n",
      "ep 3979: ep_len:525 episode reward: total was -23.390000. running mean: -8.831157\n",
      "ep 3979: ep_len:515 episode reward: total was 19.330000. running mean: -8.549546\n",
      "ep 3979: ep_len:500 episode reward: total was 5.890000. running mean: -8.405150\n",
      "ep 3979: ep_len:550 episode reward: total was 11.020000. running mean: -8.210899\n",
      "ep 3979: ep_len:3 episode reward: total was 0.000000. running mean: -8.128790\n",
      "ep 3979: ep_len:510 episode reward: total was -7.500000. running mean: -8.122502\n",
      "ep 3979: ep_len:510 episode reward: total was -15.140000. running mean: -8.192677\n",
      "epsilon:0.010000 episode_count: 27860. steps_count: 12297442.000000\n",
      "ep 3980: ep_len:545 episode reward: total was -20.820000. running mean: -8.318950\n",
      "ep 3980: ep_len:600 episode reward: total was -9.390000. running mean: -8.329661\n",
      "ep 3980: ep_len:655 episode reward: total was -11.700000. running mean: -8.363364\n",
      "ep 3980: ep_len:600 episode reward: total was -29.480000. running mean: -8.574530\n",
      "ep 3980: ep_len:3 episode reward: total was 0.000000. running mean: -8.488785\n",
      "ep 3980: ep_len:500 episode reward: total was -23.280000. running mean: -8.636697\n",
      "ep 3980: ep_len:570 episode reward: total was 1.160000. running mean: -8.538730\n",
      "epsilon:0.010000 episode_count: 27867. steps_count: 12300915.000000\n",
      "ep 3981: ep_len:500 episode reward: total was 1.330000. running mean: -8.440043\n",
      "ep 3981: ep_len:515 episode reward: total was -0.450000. running mean: -8.360143\n",
      "ep 3981: ep_len:443 episode reward: total was 1.220000. running mean: -8.264341\n",
      "ep 3981: ep_len:585 episode reward: total was 2.520000. running mean: -8.156498\n",
      "ep 3981: ep_len:3 episode reward: total was 0.000000. running mean: -8.074933\n",
      "ep 3981: ep_len:500 episode reward: total was 7.540000. running mean: -7.918783\n",
      "ep 3981: ep_len:600 episode reward: total was -17.150000. running mean: -8.011096\n",
      "epsilon:0.010000 episode_count: 27874. steps_count: 12304061.000000\n",
      "ep 3982: ep_len:535 episode reward: total was -24.940000. running mean: -8.180385\n",
      "ep 3982: ep_len:500 episode reward: total was -2.010000. running mean: -8.118681\n",
      "ep 3982: ep_len:510 episode reward: total was 8.430000. running mean: -7.953194\n",
      "ep 3982: ep_len:354 episode reward: total was -28.210000. running mean: -8.155762\n",
      "ep 3982: ep_len:3 episode reward: total was 0.000000. running mean: -8.074204\n",
      "ep 3982: ep_len:580 episode reward: total was -3.080000. running mean: -8.024262\n",
      "ep 3982: ep_len:515 episode reward: total was -21.630000. running mean: -8.160320\n",
      "epsilon:0.010000 episode_count: 27881. steps_count: 12307058.000000\n",
      "ep 3983: ep_len:99 episode reward: total was -1.940000. running mean: -8.098117\n",
      "ep 3983: ep_len:535 episode reward: total was -21.420000. running mean: -8.231335\n",
      "ep 3983: ep_len:580 episode reward: total was -15.290000. running mean: -8.301922\n",
      "ep 3983: ep_len:170 episode reward: total was 3.630000. running mean: -8.182603\n",
      "ep 3983: ep_len:3 episode reward: total was 0.000000. running mean: -8.100777\n",
      "ep 3983: ep_len:261 episode reward: total was 9.180000. running mean: -7.927969\n",
      "ep 3983: ep_len:515 episode reward: total was -4.590000. running mean: -7.894589\n",
      "epsilon:0.010000 episode_count: 27888. steps_count: 12309221.000000\n",
      "ep 3984: ep_len:500 episode reward: total was 7.830000. running mean: -7.737343\n",
      "ep 3984: ep_len:595 episode reward: total was 20.420000. running mean: -7.455770\n",
      "ep 3984: ep_len:555 episode reward: total was -14.880000. running mean: -7.530012\n",
      "ep 3984: ep_len:53 episode reward: total was 1.060000. running mean: -7.444112\n",
      "ep 3984: ep_len:72 episode reward: total was -6.950000. running mean: -7.439171\n",
      "ep 3984: ep_len:565 episode reward: total was -11.190000. running mean: -7.476679\n",
      "ep 3984: ep_len:327 episode reward: total was -6.370000. running mean: -7.465613\n",
      "epsilon:0.010000 episode_count: 27895. steps_count: 12311888.000000\n",
      "ep 3985: ep_len:555 episode reward: total was 5.350000. running mean: -7.337456\n",
      "ep 3985: ep_len:510 episode reward: total was -1.330000. running mean: -7.277382\n",
      "ep 3985: ep_len:570 episode reward: total was -11.270000. running mean: -7.317308\n",
      "ep 3985: ep_len:500 episode reward: total was -4.010000. running mean: -7.284235\n",
      "ep 3985: ep_len:3 episode reward: total was 0.000000. running mean: -7.211393\n",
      "ep 3985: ep_len:555 episode reward: total was 0.930000. running mean: -7.129979\n",
      "ep 3985: ep_len:610 episode reward: total was -23.380000. running mean: -7.292479\n",
      "epsilon:0.010000 episode_count: 27902. steps_count: 12315191.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3986: ep_len:520 episode reward: total was 6.280000. running mean: -7.156754\n",
      "ep 3986: ep_len:530 episode reward: total was -13.320000. running mean: -7.218387\n",
      "ep 3986: ep_len:454 episode reward: total was 1.760000. running mean: -7.128603\n",
      "ep 3986: ep_len:605 episode reward: total was 5.040000. running mean: -7.006917\n",
      "ep 3986: ep_len:3 episode reward: total was 0.000000. running mean: -6.936848\n",
      "ep 3986: ep_len:530 episode reward: total was -3.480000. running mean: -6.902279\n",
      "ep 3986: ep_len:500 episode reward: total was -12.400000. running mean: -6.957256\n",
      "epsilon:0.010000 episode_count: 27909. steps_count: 12318333.000000\n",
      "ep 3987: ep_len:225 episode reward: total was 0.140000. running mean: -6.886284\n",
      "ep 3987: ep_len:640 episode reward: total was -12.100000. running mean: -6.938421\n",
      "ep 3987: ep_len:620 episode reward: total was -21.150000. running mean: -7.080537\n",
      "ep 3987: ep_len:570 episode reward: total was 6.010000. running mean: -6.949631\n",
      "ep 3987: ep_len:3 episode reward: total was 0.000000. running mean: -6.880135\n",
      "ep 3987: ep_len:154 episode reward: total was 1.610000. running mean: -6.795234\n",
      "ep 3987: ep_len:170 episode reward: total was -7.410000. running mean: -6.801381\n",
      "epsilon:0.010000 episode_count: 27916. steps_count: 12320715.000000\n",
      "ep 3988: ep_len:615 episode reward: total was -60.970000. running mean: -7.343067\n",
      "ep 3988: ep_len:550 episode reward: total was 11.900000. running mean: -7.150637\n",
      "ep 3988: ep_len:445 episode reward: total was 1.720000. running mean: -7.061930\n",
      "ep 3988: ep_len:505 episode reward: total was 14.950000. running mean: -6.841811\n",
      "ep 3988: ep_len:3 episode reward: total was 0.000000. running mean: -6.773393\n",
      "ep 3988: ep_len:625 episode reward: total was -9.090000. running mean: -6.796559\n",
      "ep 3988: ep_len:565 episode reward: total was -18.650000. running mean: -6.915093\n",
      "epsilon:0.010000 episode_count: 27923. steps_count: 12324023.000000\n",
      "ep 3989: ep_len:645 episode reward: total was 3.130000. running mean: -6.814643\n",
      "ep 3989: ep_len:565 episode reward: total was 4.430000. running mean: -6.702196\n",
      "ep 3989: ep_len:68 episode reward: total was -4.970000. running mean: -6.684874\n",
      "ep 3989: ep_len:525 episode reward: total was -4.980000. running mean: -6.667825\n",
      "ep 3989: ep_len:87 episode reward: total was -3.460000. running mean: -6.635747\n",
      "ep 3989: ep_len:640 episode reward: total was -2.840000. running mean: -6.597790\n",
      "ep 3989: ep_len:525 episode reward: total was -9.110000. running mean: -6.622912\n",
      "epsilon:0.010000 episode_count: 27930. steps_count: 12327078.000000\n",
      "ep 3990: ep_len:565 episode reward: total was -1.090000. running mean: -6.567583\n",
      "ep 3990: ep_len:500 episode reward: total was -4.650000. running mean: -6.548407\n",
      "ep 3990: ep_len:362 episode reward: total was 7.230000. running mean: -6.410623\n",
      "ep 3990: ep_len:432 episode reward: total was 5.870000. running mean: -6.287817\n",
      "ep 3990: ep_len:3 episode reward: total was 0.000000. running mean: -6.224938\n",
      "ep 3990: ep_len:670 episode reward: total was 3.410000. running mean: -6.128589\n",
      "ep 3990: ep_len:570 episode reward: total was -2.790000. running mean: -6.095203\n",
      "epsilon:0.010000 episode_count: 27937. steps_count: 12330180.000000\n",
      "ep 3991: ep_len:605 episode reward: total was -4.430000. running mean: -6.078551\n",
      "ep 3991: ep_len:515 episode reward: total was 7.920000. running mean: -5.938566\n",
      "ep 3991: ep_len:530 episode reward: total was -33.620000. running mean: -6.215380\n",
      "ep 3991: ep_len:500 episode reward: total was 15.760000. running mean: -5.995626\n",
      "ep 3991: ep_len:3 episode reward: total was 0.000000. running mean: -5.935670\n",
      "ep 3991: ep_len:530 episode reward: total was -2.470000. running mean: -5.901013\n",
      "ep 3991: ep_len:500 episode reward: total was -8.750000. running mean: -5.929503\n",
      "epsilon:0.010000 episode_count: 27944. steps_count: 12333363.000000\n",
      "ep 3992: ep_len:500 episode reward: total was 6.470000. running mean: -5.805508\n",
      "ep 3992: ep_len:291 episode reward: total was -2.350000. running mean: -5.770953\n",
      "ep 3992: ep_len:540 episode reward: total was 1.930000. running mean: -5.693943\n",
      "ep 3992: ep_len:595 episode reward: total was 16.490000. running mean: -5.472104\n",
      "ep 3992: ep_len:3 episode reward: total was 0.000000. running mean: -5.417383\n",
      "ep 3992: ep_len:505 episode reward: total was -2.150000. running mean: -5.384709\n",
      "ep 3992: ep_len:630 episode reward: total was -13.950000. running mean: -5.470362\n",
      "epsilon:0.010000 episode_count: 27951. steps_count: 12336427.000000\n",
      "ep 3993: ep_len:510 episode reward: total was -1.430000. running mean: -5.429958\n",
      "ep 3993: ep_len:570 episode reward: total was 21.820000. running mean: -5.157459\n",
      "ep 3993: ep_len:535 episode reward: total was 1.960000. running mean: -5.086284\n",
      "ep 3993: ep_len:525 episode reward: total was 12.580000. running mean: -4.909621\n",
      "ep 3993: ep_len:3 episode reward: total was 0.000000. running mean: -4.860525\n",
      "ep 3993: ep_len:510 episode reward: total was -2.900000. running mean: -4.840920\n",
      "ep 3993: ep_len:500 episode reward: total was -13.970000. running mean: -4.932211\n",
      "epsilon:0.010000 episode_count: 27958. steps_count: 12339580.000000\n",
      "ep 3994: ep_len:550 episode reward: total was 1.940000. running mean: -4.863489\n",
      "ep 3994: ep_len:500 episode reward: total was -11.360000. running mean: -4.928454\n",
      "ep 3994: ep_len:462 episode reward: total was 5.300000. running mean: -4.826169\n",
      "ep 3994: ep_len:565 episode reward: total was -3.040000. running mean: -4.808307\n",
      "ep 3994: ep_len:3 episode reward: total was 0.000000. running mean: -4.760224\n",
      "ep 3994: ep_len:580 episode reward: total was -23.280000. running mean: -4.945422\n",
      "ep 3994: ep_len:530 episode reward: total was -3.860000. running mean: -4.934568\n",
      "epsilon:0.010000 episode_count: 27965. steps_count: 12342770.000000\n",
      "ep 3995: ep_len:705 episode reward: total was -23.240000. running mean: -5.117622\n",
      "ep 3995: ep_len:520 episode reward: total was 11.610000. running mean: -4.950346\n",
      "ep 3995: ep_len:500 episode reward: total was 1.910000. running mean: -4.881743\n",
      "ep 3995: ep_len:56 episode reward: total was 2.570000. running mean: -4.807225\n",
      "ep 3995: ep_len:55 episode reward: total was 4.000000. running mean: -4.719153\n",
      "ep 3995: ep_len:545 episode reward: total was -22.830000. running mean: -4.900261\n",
      "ep 3995: ep_len:580 episode reward: total was -24.600000. running mean: -5.097259\n",
      "epsilon:0.010000 episode_count: 27972. steps_count: 12345731.000000\n",
      "ep 3996: ep_len:635 episode reward: total was -6.670000. running mean: -5.112986\n",
      "ep 3996: ep_len:302 episode reward: total was -6.860000. running mean: -5.130456\n",
      "ep 3996: ep_len:550 episode reward: total was -4.750000. running mean: -5.126652\n",
      "ep 3996: ep_len:515 episode reward: total was 4.950000. running mean: -5.025885\n",
      "ep 3996: ep_len:3 episode reward: total was 0.000000. running mean: -4.975626\n",
      "ep 3996: ep_len:700 episode reward: total was -30.780000. running mean: -5.233670\n",
      "ep 3996: ep_len:565 episode reward: total was -6.530000. running mean: -5.246633\n",
      "epsilon:0.010000 episode_count: 27979. steps_count: 12349001.000000\n",
      "ep 3997: ep_len:198 episode reward: total was -6.370000. running mean: -5.257867\n",
      "ep 3997: ep_len:248 episode reward: total was -11.870000. running mean: -5.323988\n",
      "ep 3997: ep_len:590 episode reward: total was -15.240000. running mean: -5.423149\n",
      "ep 3997: ep_len:600 episode reward: total was 5.550000. running mean: -5.313417\n",
      "ep 3997: ep_len:50 episode reward: total was 4.510000. running mean: -5.215183\n",
      "ep 3997: ep_len:510 episode reward: total was -9.030000. running mean: -5.253331\n",
      "ep 3997: ep_len:500 episode reward: total was -0.240000. running mean: -5.203198\n",
      "epsilon:0.010000 episode_count: 27986. steps_count: 12351697.000000\n",
      "ep 3998: ep_len:525 episode reward: total was 9.940000. running mean: -5.051766\n",
      "ep 3998: ep_len:565 episode reward: total was 1.550000. running mean: -4.985748\n",
      "ep 3998: ep_len:350 episode reward: total was -18.360000. running mean: -5.119491\n",
      "ep 3998: ep_len:500 episode reward: total was 5.940000. running mean: -5.008896\n",
      "ep 3998: ep_len:123 episode reward: total was 4.050000. running mean: -4.918307\n",
      "ep 3998: ep_len:1120 episode reward: total was -139.720000. running mean: -6.266324\n",
      "ep 3998: ep_len:500 episode reward: total was -1.000000. running mean: -6.213660\n",
      "epsilon:0.010000 episode_count: 27993. steps_count: 12355380.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3999: ep_len:605 episode reward: total was 6.400000. running mean: -6.087524\n",
      "ep 3999: ep_len:181 episode reward: total was 0.600000. running mean: -6.020649\n",
      "ep 3999: ep_len:580 episode reward: total was -13.770000. running mean: -6.098142\n",
      "ep 3999: ep_len:625 episode reward: total was -11.990000. running mean: -6.157061\n",
      "ep 3999: ep_len:3 episode reward: total was 0.000000. running mean: -6.095490\n",
      "ep 3999: ep_len:186 episode reward: total was 8.650000. running mean: -5.948035\n",
      "ep 3999: ep_len:525 episode reward: total was -13.060000. running mean: -6.019155\n",
      "epsilon:0.010000 episode_count: 28000. steps_count: 12358085.000000\n",
      "ep 4000: ep_len:605 episode reward: total was -18.790000. running mean: -6.146863\n",
      "ep 4000: ep_len:500 episode reward: total was -22.860000. running mean: -6.313995\n",
      "ep 4000: ep_len:585 episode reward: total was -10.960000. running mean: -6.360455\n",
      "ep 4000: ep_len:515 episode reward: total was -13.520000. running mean: -6.432050\n",
      "ep 4000: ep_len:3 episode reward: total was 0.000000. running mean: -6.367730\n",
      "ep 4000: ep_len:625 episode reward: total was 6.490000. running mean: -6.239152\n",
      "ep 4000: ep_len:500 episode reward: total was -7.110000. running mean: -6.247861\n",
      "epsilon:0.010000 episode_count: 28007. steps_count: 12361418.000000\n",
      "ep 4001: ep_len:565 episode reward: total was -10.190000. running mean: -6.287282\n",
      "ep 4001: ep_len:321 episode reward: total was -49.890000. running mean: -6.723309\n",
      "ep 4001: ep_len:560 episode reward: total was -2.700000. running mean: -6.683076\n",
      "ep 4001: ep_len:515 episode reward: total was 1.980000. running mean: -6.596446\n",
      "ep 4001: ep_len:3 episode reward: total was 0.000000. running mean: -6.530481\n",
      "ep 4001: ep_len:625 episode reward: total was -22.180000. running mean: -6.686976\n",
      "ep 4001: ep_len:590 episode reward: total was -7.800000. running mean: -6.698107\n",
      "epsilon:0.010000 episode_count: 28014. steps_count: 12364597.000000\n",
      "ep 4002: ep_len:565 episode reward: total was -14.720000. running mean: -6.778325\n",
      "ep 4002: ep_len:615 episode reward: total was -1.260000. running mean: -6.723142\n",
      "ep 4002: ep_len:515 episode reward: total was 6.390000. running mean: -6.592011\n",
      "ep 4002: ep_len:510 episode reward: total was -21.610000. running mean: -6.742191\n",
      "ep 4002: ep_len:3 episode reward: total was 0.000000. running mean: -6.674769\n",
      "ep 4002: ep_len:595 episode reward: total was -0.090000. running mean: -6.608921\n",
      "ep 4002: ep_len:500 episode reward: total was -5.580000. running mean: -6.598632\n",
      "epsilon:0.010000 episode_count: 28021. steps_count: 12367900.000000\n",
      "ep 4003: ep_len:500 episode reward: total was 14.350000. running mean: -6.389146\n",
      "ep 4003: ep_len:500 episode reward: total was 13.860000. running mean: -6.186654\n",
      "ep 4003: ep_len:500 episode reward: total was -4.470000. running mean: -6.169488\n",
      "ep 4003: ep_len:605 episode reward: total was -11.970000. running mean: -6.227493\n",
      "ep 4003: ep_len:3 episode reward: total was 0.000000. running mean: -6.165218\n",
      "ep 4003: ep_len:585 episode reward: total was -5.130000. running mean: -6.154866\n",
      "ep 4003: ep_len:530 episode reward: total was -12.860000. running mean: -6.221917\n",
      "epsilon:0.010000 episode_count: 28028. steps_count: 12371123.000000\n",
      "ep 4004: ep_len:620 episode reward: total was 12.660000. running mean: -6.033098\n",
      "ep 4004: ep_len:510 episode reward: total was 6.200000. running mean: -5.910767\n",
      "ep 4004: ep_len:555 episode reward: total was -15.660000. running mean: -6.008259\n",
      "ep 4004: ep_len:520 episode reward: total was -25.130000. running mean: -6.199477\n",
      "ep 4004: ep_len:3 episode reward: total was 0.000000. running mean: -6.137482\n",
      "ep 4004: ep_len:675 episode reward: total was 3.910000. running mean: -6.037007\n",
      "ep 4004: ep_len:515 episode reward: total was -10.490000. running mean: -6.081537\n",
      "epsilon:0.010000 episode_count: 28035. steps_count: 12374521.000000\n",
      "ep 4005: ep_len:580 episode reward: total was -12.150000. running mean: -6.142221\n",
      "ep 4005: ep_len:550 episode reward: total was 6.830000. running mean: -6.012499\n",
      "ep 4005: ep_len:79 episode reward: total was 0.040000. running mean: -5.951974\n",
      "ep 4005: ep_len:114 episode reward: total was 2.100000. running mean: -5.871455\n",
      "ep 4005: ep_len:105 episode reward: total was 1.550000. running mean: -5.797240\n",
      "ep 4005: ep_len:630 episode reward: total was -12.560000. running mean: -5.864868\n",
      "ep 4005: ep_len:344 episode reward: total was -6.310000. running mean: -5.869319\n",
      "epsilon:0.010000 episode_count: 28042. steps_count: 12376923.000000\n",
      "ep 4006: ep_len:595 episode reward: total was -18.040000. running mean: -5.991026\n",
      "ep 4006: ep_len:550 episode reward: total was 9.140000. running mean: -5.839715\n",
      "ep 4006: ep_len:500 episode reward: total was -13.210000. running mean: -5.913418\n",
      "ep 4006: ep_len:500 episode reward: total was -1.620000. running mean: -5.870484\n",
      "ep 4006: ep_len:3 episode reward: total was 0.000000. running mean: -5.811779\n",
      "ep 4006: ep_len:500 episode reward: total was 2.800000. running mean: -5.725661\n",
      "ep 4006: ep_len:310 episode reward: total was -15.860000. running mean: -5.827005\n",
      "epsilon:0.010000 episode_count: 28049. steps_count: 12379881.000000\n",
      "ep 4007: ep_len:250 episode reward: total was 5.650000. running mean: -5.712235\n",
      "ep 4007: ep_len:525 episode reward: total was -4.110000. running mean: -5.696212\n",
      "ep 4007: ep_len:565 episode reward: total was -14.310000. running mean: -5.782350\n",
      "ep 4007: ep_len:478 episode reward: total was 9.550000. running mean: -5.629027\n",
      "ep 4007: ep_len:3 episode reward: total was 0.000000. running mean: -5.572737\n",
      "ep 4007: ep_len:500 episode reward: total was -5.070000. running mean: -5.567709\n",
      "ep 4007: ep_len:600 episode reward: total was -3.520000. running mean: -5.547232\n",
      "epsilon:0.010000 episode_count: 28056. steps_count: 12382802.000000\n",
      "ep 4008: ep_len:575 episode reward: total was -10.170000. running mean: -5.593460\n",
      "ep 4008: ep_len:505 episode reward: total was 1.090000. running mean: -5.526625\n",
      "ep 4008: ep_len:515 episode reward: total was -6.360000. running mean: -5.534959\n",
      "ep 4008: ep_len:150 episode reward: total was 2.150000. running mean: -5.458109\n",
      "ep 4008: ep_len:50 episode reward: total was 4.510000. running mean: -5.358428\n",
      "ep 4008: ep_len:625 episode reward: total was -13.070000. running mean: -5.435544\n",
      "ep 4008: ep_len:520 episode reward: total was -2.020000. running mean: -5.401389\n",
      "epsilon:0.010000 episode_count: 28063. steps_count: 12385742.000000\n",
      "ep 4009: ep_len:265 episode reward: total was -1.830000. running mean: -5.365675\n",
      "ep 4009: ep_len:500 episode reward: total was 15.220000. running mean: -5.159818\n",
      "ep 4009: ep_len:960 episode reward: total was -96.720000. running mean: -6.075420\n",
      "ep 4009: ep_len:560 episode reward: total was 3.920000. running mean: -5.975466\n",
      "ep 4009: ep_len:3 episode reward: total was 0.000000. running mean: -5.915711\n",
      "ep 4009: ep_len:585 episode reward: total was -4.410000. running mean: -5.900654\n",
      "ep 4009: ep_len:630 episode reward: total was -14.930000. running mean: -5.990947\n",
      "epsilon:0.010000 episode_count: 28070. steps_count: 12389245.000000\n",
      "ep 4010: ep_len:545 episode reward: total was 5.040000. running mean: -5.880638\n",
      "ep 4010: ep_len:356 episode reward: total was -14.820000. running mean: -5.970031\n",
      "ep 4010: ep_len:570 episode reward: total was -7.400000. running mean: -5.984331\n",
      "ep 4010: ep_len:510 episode reward: total was 10.450000. running mean: -5.819988\n",
      "ep 4010: ep_len:3 episode reward: total was 0.000000. running mean: -5.761788\n",
      "ep 4010: ep_len:630 episode reward: total was -1.490000. running mean: -5.719070\n",
      "ep 4010: ep_len:570 episode reward: total was -13.900000. running mean: -5.800879\n",
      "epsilon:0.010000 episode_count: 28077. steps_count: 12392429.000000\n",
      "ep 4011: ep_len:555 episode reward: total was -1.670000. running mean: -5.759571\n",
      "ep 4011: ep_len:585 episode reward: total was -16.330000. running mean: -5.865275\n",
      "ep 4011: ep_len:505 episode reward: total was -10.860000. running mean: -5.915222\n",
      "ep 4011: ep_len:620 episode reward: total was 8.940000. running mean: -5.766670\n",
      "ep 4011: ep_len:3 episode reward: total was 0.000000. running mean: -5.709003\n",
      "ep 4011: ep_len:665 episode reward: total was 7.930000. running mean: -5.572613\n",
      "ep 4011: ep_len:535 episode reward: total was 0.550000. running mean: -5.511387\n",
      "epsilon:0.010000 episode_count: 28084. steps_count: 12395897.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4012: ep_len:500 episode reward: total was 16.450000. running mean: -5.291773\n",
      "ep 4012: ep_len:500 episode reward: total was 23.640000. running mean: -5.002455\n",
      "ep 4012: ep_len:995 episode reward: total was -96.160000. running mean: -5.914031\n",
      "ep 4012: ep_len:530 episode reward: total was -2.620000. running mean: -5.881091\n",
      "ep 4012: ep_len:104 episode reward: total was 7.040000. running mean: -5.751880\n",
      "ep 4012: ep_len:695 episode reward: total was 8.390000. running mean: -5.610461\n",
      "ep 4012: ep_len:515 episode reward: total was -5.030000. running mean: -5.604656\n",
      "epsilon:0.010000 episode_count: 28091. steps_count: 12399736.000000\n",
      "ep 4013: ep_len:570 episode reward: total was 0.550000. running mean: -5.543110\n",
      "ep 4013: ep_len:585 episode reward: total was 14.560000. running mean: -5.342079\n",
      "ep 4013: ep_len:840 episode reward: total was -94.400000. running mean: -6.232658\n",
      "ep 4013: ep_len:500 episode reward: total was -1.690000. running mean: -6.187231\n",
      "ep 4013: ep_len:81 episode reward: total was -9.450000. running mean: -6.219859\n",
      "ep 4013: ep_len:505 episode reward: total was 6.390000. running mean: -6.093760\n",
      "ep 4013: ep_len:615 episode reward: total was -6.800000. running mean: -6.100823\n",
      "epsilon:0.010000 episode_count: 28098. steps_count: 12403432.000000\n",
      "ep 4014: ep_len:243 episode reward: total was -13.310000. running mean: -6.172914\n",
      "ep 4014: ep_len:525 episode reward: total was -20.000000. running mean: -6.311185\n",
      "ep 4014: ep_len:535 episode reward: total was -8.860000. running mean: -6.336673\n",
      "ep 4014: ep_len:595 episode reward: total was -15.510000. running mean: -6.428407\n",
      "ep 4014: ep_len:3 episode reward: total was 0.000000. running mean: -6.364123\n",
      "ep 4014: ep_len:520 episode reward: total was 1.640000. running mean: -6.284081\n",
      "ep 4014: ep_len:291 episode reward: total was -11.860000. running mean: -6.339841\n",
      "epsilon:0.010000 episode_count: 28105. steps_count: 12406144.000000\n",
      "ep 4015: ep_len:510 episode reward: total was -11.890000. running mean: -6.395342\n",
      "ep 4015: ep_len:625 episode reward: total was -4.950000. running mean: -6.380889\n",
      "ep 4015: ep_len:625 episode reward: total was -8.200000. running mean: -6.399080\n",
      "ep 4015: ep_len:500 episode reward: total was -33.240000. running mean: -6.667489\n",
      "ep 4015: ep_len:3 episode reward: total was 0.000000. running mean: -6.600814\n",
      "ep 4015: ep_len:660 episode reward: total was -20.780000. running mean: -6.742606\n",
      "ep 4015: ep_len:640 episode reward: total was -15.410000. running mean: -6.829280\n",
      "epsilon:0.010000 episode_count: 28112. steps_count: 12409707.000000\n",
      "ep 4016: ep_len:525 episode reward: total was -18.290000. running mean: -6.943887\n",
      "ep 4016: ep_len:500 episode reward: total was -13.010000. running mean: -7.004548\n",
      "ep 4016: ep_len:590 episode reward: total was 3.030000. running mean: -6.904203\n",
      "ep 4016: ep_len:500 episode reward: total was 0.040000. running mean: -6.834761\n",
      "ep 4016: ep_len:3 episode reward: total was 0.000000. running mean: -6.766413\n",
      "ep 4016: ep_len:580 episode reward: total was 1.950000. running mean: -6.679249\n",
      "ep 4016: ep_len:705 episode reward: total was -49.410000. running mean: -7.106557\n",
      "epsilon:0.010000 episode_count: 28119. steps_count: 12413110.000000\n",
      "ep 4017: ep_len:134 episode reward: total was 4.590000. running mean: -6.989591\n",
      "ep 4017: ep_len:515 episode reward: total was -15.980000. running mean: -7.079495\n",
      "ep 4017: ep_len:438 episode reward: total was -7.240000. running mean: -7.081100\n",
      "ep 4017: ep_len:540 episode reward: total was 3.850000. running mean: -6.971789\n",
      "ep 4017: ep_len:3 episode reward: total was 0.000000. running mean: -6.902071\n",
      "ep 4017: ep_len:545 episode reward: total was -0.480000. running mean: -6.837851\n",
      "ep 4017: ep_len:500 episode reward: total was -15.020000. running mean: -6.919672\n",
      "epsilon:0.010000 episode_count: 28126. steps_count: 12415785.000000\n",
      "ep 4018: ep_len:500 episode reward: total was 10.340000. running mean: -6.747075\n",
      "ep 4018: ep_len:540 episode reward: total was 0.420000. running mean: -6.675405\n",
      "ep 4018: ep_len:540 episode reward: total was -11.190000. running mean: -6.720551\n",
      "ep 4018: ep_len:165 episode reward: total was 5.150000. running mean: -6.601845\n",
      "ep 4018: ep_len:3 episode reward: total was 0.000000. running mean: -6.535827\n",
      "ep 4018: ep_len:630 episode reward: total was 1.830000. running mean: -6.452168\n",
      "ep 4018: ep_len:500 episode reward: total was -2.380000. running mean: -6.411447\n",
      "epsilon:0.010000 episode_count: 28133. steps_count: 12418663.000000\n",
      "ep 4019: ep_len:520 episode reward: total was 5.490000. running mean: -6.292432\n",
      "ep 4019: ep_len:500 episode reward: total was -8.850000. running mean: -6.318008\n",
      "ep 4019: ep_len:500 episode reward: total was 9.930000. running mean: -6.155528\n",
      "ep 4019: ep_len:515 episode reward: total was 13.510000. running mean: -5.958872\n",
      "ep 4019: ep_len:3 episode reward: total was 0.000000. running mean: -5.899284\n",
      "ep 4019: ep_len:164 episode reward: total was 8.110000. running mean: -5.759191\n",
      "ep 4019: ep_len:535 episode reward: total was 0.250000. running mean: -5.699099\n",
      "epsilon:0.010000 episode_count: 28140. steps_count: 12421400.000000\n",
      "ep 4020: ep_len:525 episode reward: total was 14.970000. running mean: -5.492408\n",
      "ep 4020: ep_len:201 episode reward: total was 0.670000. running mean: -5.430784\n",
      "ep 4020: ep_len:406 episode reward: total was 2.220000. running mean: -5.354276\n",
      "ep 4020: ep_len:510 episode reward: total was -33.780000. running mean: -5.638533\n",
      "ep 4020: ep_len:3 episode reward: total was 0.000000. running mean: -5.582148\n",
      "ep 4020: ep_len:575 episode reward: total was -28.290000. running mean: -5.809227\n",
      "ep 4020: ep_len:600 episode reward: total was -63.110000. running mean: -6.382234\n",
      "epsilon:0.010000 episode_count: 28147. steps_count: 12424220.000000\n",
      "ep 4021: ep_len:500 episode reward: total was 15.330000. running mean: -6.165112\n",
      "ep 4021: ep_len:500 episode reward: total was 15.160000. running mean: -5.951861\n",
      "ep 4021: ep_len:535 episode reward: total was -34.240000. running mean: -6.234742\n",
      "ep 4021: ep_len:500 episode reward: total was 10.350000. running mean: -6.068895\n",
      "ep 4021: ep_len:49 episode reward: total was 4.500000. running mean: -5.963206\n",
      "ep 4021: ep_len:595 episode reward: total was -7.990000. running mean: -5.983474\n",
      "ep 4021: ep_len:610 episode reward: total was -8.770000. running mean: -6.011339\n",
      "epsilon:0.010000 episode_count: 28154. steps_count: 12427509.000000\n",
      "ep 4022: ep_len:224 episode reward: total was 9.130000. running mean: -5.859926\n",
      "ep 4022: ep_len:690 episode reward: total was 0.520000. running mean: -5.796126\n",
      "ep 4022: ep_len:570 episode reward: total was 0.350000. running mean: -5.734665\n",
      "ep 4022: ep_len:147 episode reward: total was 2.610000. running mean: -5.651218\n",
      "ep 4022: ep_len:3 episode reward: total was 0.000000. running mean: -5.594706\n",
      "ep 4022: ep_len:510 episode reward: total was -5.080000. running mean: -5.589559\n",
      "ep 4022: ep_len:154 episode reward: total was -6.440000. running mean: -5.598064\n",
      "epsilon:0.010000 episode_count: 28161. steps_count: 12429807.000000\n",
      "ep 4023: ep_len:510 episode reward: total was 0.130000. running mean: -5.540783\n",
      "ep 4023: ep_len:500 episode reward: total was 0.330000. running mean: -5.482075\n",
      "ep 4023: ep_len:500 episode reward: total was 8.460000. running mean: -5.342654\n",
      "ep 4023: ep_len:530 episode reward: total was -17.550000. running mean: -5.464728\n",
      "ep 4023: ep_len:3 episode reward: total was 0.000000. running mean: -5.410081\n",
      "ep 4023: ep_len:635 episode reward: total was 7.930000. running mean: -5.276680\n",
      "ep 4023: ep_len:540 episode reward: total was -1.350000. running mean: -5.237413\n",
      "epsilon:0.010000 episode_count: 28168. steps_count: 12433025.000000\n",
      "ep 4024: ep_len:615 episode reward: total was -31.580000. running mean: -5.500839\n",
      "ep 4024: ep_len:510 episode reward: total was -27.360000. running mean: -5.719430\n",
      "ep 4024: ep_len:595 episode reward: total was -0.720000. running mean: -5.669436\n",
      "ep 4024: ep_len:500 episode reward: total was -5.450000. running mean: -5.667242\n",
      "ep 4024: ep_len:3 episode reward: total was 0.000000. running mean: -5.610569\n",
      "ep 4024: ep_len:632 episode reward: total was -57.980000. running mean: -6.134264\n",
      "ep 4024: ep_len:545 episode reward: total was -0.930000. running mean: -6.082221\n",
      "epsilon:0.010000 episode_count: 28175. steps_count: 12436425.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4025: ep_len:208 episode reward: total was 0.620000. running mean: -6.015199\n",
      "ep 4025: ep_len:585 episode reward: total was -11.320000. running mean: -6.068247\n",
      "ep 4025: ep_len:615 episode reward: total was -28.340000. running mean: -6.290964\n",
      "ep 4025: ep_len:500 episode reward: total was 1.570000. running mean: -6.212355\n",
      "ep 4025: ep_len:112 episode reward: total was 5.040000. running mean: -6.099831\n",
      "ep 4025: ep_len:159 episode reward: total was 2.590000. running mean: -6.012933\n",
      "ep 4025: ep_len:346 episode reward: total was -5.810000. running mean: -6.010904\n",
      "epsilon:0.010000 episode_count: 28182. steps_count: 12438950.000000\n",
      "ep 4026: ep_len:620 episode reward: total was -5.980000. running mean: -6.010595\n",
      "ep 4026: ep_len:610 episode reward: total was 12.560000. running mean: -5.824889\n",
      "ep 4026: ep_len:565 episode reward: total was -5.680000. running mean: -5.823440\n",
      "ep 4026: ep_len:120 episode reward: total was 3.130000. running mean: -5.733905\n",
      "ep 4026: ep_len:3 episode reward: total was 0.000000. running mean: -5.676566\n",
      "ep 4026: ep_len:500 episode reward: total was -4.030000. running mean: -5.660101\n",
      "ep 4026: ep_len:342 episode reward: total was -11.820000. running mean: -5.721700\n",
      "epsilon:0.010000 episode_count: 28189. steps_count: 12441710.000000\n",
      "ep 4027: ep_len:251 episode reward: total was 6.650000. running mean: -5.597983\n",
      "ep 4027: ep_len:505 episode reward: total was -28.520000. running mean: -5.827203\n",
      "ep 4027: ep_len:565 episode reward: total was -8.800000. running mean: -5.856931\n",
      "ep 4027: ep_len:45 episode reward: total was -0.430000. running mean: -5.802661\n",
      "ep 4027: ep_len:98 episode reward: total was 5.040000. running mean: -5.694235\n",
      "ep 4027: ep_len:254 episode reward: total was 1.630000. running mean: -5.620992\n",
      "ep 4027: ep_len:336 episode reward: total was -8.800000. running mean: -5.652783\n",
      "epsilon:0.010000 episode_count: 28196. steps_count: 12443764.000000\n",
      "ep 4028: ep_len:580 episode reward: total was 1.580000. running mean: -5.580455\n",
      "ep 4028: ep_len:500 episode reward: total was 5.230000. running mean: -5.472350\n",
      "ep 4028: ep_len:525 episode reward: total was -9.750000. running mean: -5.515127\n",
      "ep 4028: ep_len:535 episode reward: total was 6.900000. running mean: -5.390975\n",
      "ep 4028: ep_len:3 episode reward: total was 0.000000. running mean: -5.337066\n",
      "ep 4028: ep_len:695 episode reward: total was 7.440000. running mean: -5.209295\n",
      "ep 4028: ep_len:545 episode reward: total was -6.580000. running mean: -5.223002\n",
      "epsilon:0.010000 episode_count: 28203. steps_count: 12447147.000000\n",
      "ep 4029: ep_len:92 episode reward: total was 2.550000. running mean: -5.145272\n",
      "ep 4029: ep_len:500 episode reward: total was -5.860000. running mean: -5.152419\n",
      "ep 4029: ep_len:555 episode reward: total was -12.780000. running mean: -5.228695\n",
      "ep 4029: ep_len:500 episode reward: total was 11.900000. running mean: -5.057408\n",
      "ep 4029: ep_len:3 episode reward: total was 0.000000. running mean: -5.006834\n",
      "ep 4029: ep_len:620 episode reward: total was -5.430000. running mean: -5.011066\n",
      "ep 4029: ep_len:510 episode reward: total was -0.570000. running mean: -4.966655\n",
      "epsilon:0.010000 episode_count: 28210. steps_count: 12449927.000000\n",
      "ep 4030: ep_len:550 episode reward: total was -25.340000. running mean: -5.170389\n",
      "ep 4030: ep_len:505 episode reward: total was -11.900000. running mean: -5.237685\n",
      "ep 4030: ep_len:645 episode reward: total was -44.280000. running mean: -5.628108\n",
      "ep 4030: ep_len:500 episode reward: total was 11.840000. running mean: -5.453427\n",
      "ep 4030: ep_len:3 episode reward: total was 0.000000. running mean: -5.398892\n",
      "ep 4030: ep_len:500 episode reward: total was 5.750000. running mean: -5.287404\n",
      "ep 4030: ep_len:535 episode reward: total was 0.030000. running mean: -5.234229\n",
      "epsilon:0.010000 episode_count: 28217. steps_count: 12453165.000000\n",
      "ep 4031: ep_len:244 episode reward: total was 5.140000. running mean: -5.130487\n",
      "ep 4031: ep_len:182 episode reward: total was -1.390000. running mean: -5.093082\n",
      "ep 4031: ep_len:545 episode reward: total was 2.350000. running mean: -5.018651\n",
      "ep 4031: ep_len:500 episode reward: total was 5.890000. running mean: -4.909565\n",
      "ep 4031: ep_len:98 episode reward: total was 2.540000. running mean: -4.835069\n",
      "ep 4031: ep_len:525 episode reward: total was 3.640000. running mean: -4.750319\n",
      "ep 4031: ep_len:530 episode reward: total was -11.910000. running mean: -4.821915\n",
      "epsilon:0.010000 episode_count: 28224. steps_count: 12455789.000000\n",
      "ep 4032: ep_len:500 episode reward: total was -26.970000. running mean: -5.043396\n",
      "ep 4032: ep_len:625 episode reward: total was 6.850000. running mean: -4.924462\n",
      "ep 4032: ep_len:565 episode reward: total was -1.560000. running mean: -4.890818\n",
      "ep 4032: ep_len:530 episode reward: total was 7.610000. running mean: -4.765810\n",
      "ep 4032: ep_len:3 episode reward: total was 0.000000. running mean: -4.718151\n",
      "ep 4032: ep_len:500 episode reward: total was 2.340000. running mean: -4.647570\n",
      "ep 4032: ep_len:550 episode reward: total was -50.180000. running mean: -5.102894\n",
      "epsilon:0.010000 episode_count: 28231. steps_count: 12459062.000000\n",
      "ep 4033: ep_len:500 episode reward: total was 7.250000. running mean: -4.979365\n",
      "ep 4033: ep_len:188 episode reward: total was -3.400000. running mean: -4.963572\n",
      "ep 4033: ep_len:417 episode reward: total was 7.730000. running mean: -4.836636\n",
      "ep 4033: ep_len:575 episode reward: total was 8.450000. running mean: -4.703770\n",
      "ep 4033: ep_len:3 episode reward: total was 0.000000. running mean: -4.656732\n",
      "ep 4033: ep_len:755 episode reward: total was -16.130000. running mean: -4.771465\n",
      "ep 4033: ep_len:595 episode reward: total was 0.950000. running mean: -4.714250\n",
      "epsilon:0.010000 episode_count: 28238. steps_count: 12462095.000000\n",
      "ep 4034: ep_len:600 episode reward: total was -19.180000. running mean: -4.858907\n",
      "ep 4034: ep_len:595 episode reward: total was -38.490000. running mean: -5.195218\n",
      "ep 4034: ep_len:540 episode reward: total was -0.200000. running mean: -5.145266\n",
      "ep 4034: ep_len:525 episode reward: total was 2.340000. running mean: -5.070413\n",
      "ep 4034: ep_len:3 episode reward: total was 0.000000. running mean: -5.019709\n",
      "ep 4034: ep_len:764 episode reward: total was -60.330000. running mean: -5.572812\n",
      "ep 4034: ep_len:555 episode reward: total was -15.830000. running mean: -5.675384\n",
      "epsilon:0.010000 episode_count: 28245. steps_count: 12465677.000000\n",
      "ep 4035: ep_len:500 episode reward: total was -24.750000. running mean: -5.866130\n",
      "ep 4035: ep_len:500 episode reward: total was 1.240000. running mean: -5.795069\n",
      "ep 4035: ep_len:545 episode reward: total was 6.160000. running mean: -5.675518\n",
      "ep 4035: ep_len:575 episode reward: total was -38.280000. running mean: -6.001563\n",
      "ep 4035: ep_len:3 episode reward: total was 0.000000. running mean: -5.941547\n",
      "ep 4035: ep_len:515 episode reward: total was 2.000000. running mean: -5.862132\n",
      "ep 4035: ep_len:615 episode reward: total was -4.040000. running mean: -5.843911\n",
      "epsilon:0.010000 episode_count: 28252. steps_count: 12468930.000000\n",
      "ep 4036: ep_len:205 episode reward: total was -9.390000. running mean: -5.879372\n",
      "ep 4036: ep_len:595 episode reward: total was -26.390000. running mean: -6.084478\n",
      "ep 4036: ep_len:59 episode reward: total was -3.460000. running mean: -6.058233\n",
      "ep 4036: ep_len:500 episode reward: total was -15.600000. running mean: -6.153651\n",
      "ep 4036: ep_len:3 episode reward: total was 0.000000. running mean: -6.092114\n",
      "ep 4036: ep_len:625 episode reward: total was 8.170000. running mean: -5.949493\n",
      "ep 4036: ep_len:515 episode reward: total was -4.050000. running mean: -5.930498\n",
      "epsilon:0.010000 episode_count: 28259. steps_count: 12471432.000000\n",
      "ep 4037: ep_len:575 episode reward: total was 7.960000. running mean: -5.791593\n",
      "ep 4037: ep_len:560 episode reward: total was 4.120000. running mean: -5.692477\n",
      "ep 4037: ep_len:74 episode reward: total was 1.040000. running mean: -5.625152\n",
      "ep 4037: ep_len:500 episode reward: total was -8.010000. running mean: -5.649001\n",
      "ep 4037: ep_len:3 episode reward: total was 0.000000. running mean: -5.592511\n",
      "ep 4037: ep_len:525 episode reward: total was 6.150000. running mean: -5.475086\n",
      "ep 4037: ep_len:187 episode reward: total was -5.420000. running mean: -5.474535\n",
      "epsilon:0.010000 episode_count: 28266. steps_count: 12473856.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4038: ep_len:525 episode reward: total was 6.890000. running mean: -5.350890\n",
      "ep 4038: ep_len:500 episode reward: total was 3.620000. running mean: -5.261181\n",
      "ep 4038: ep_len:600 episode reward: total was 0.480000. running mean: -5.203769\n",
      "ep 4038: ep_len:56 episode reward: total was -0.460000. running mean: -5.156331\n",
      "ep 4038: ep_len:68 episode reward: total was -6.940000. running mean: -5.174168\n",
      "ep 4038: ep_len:605 episode reward: total was -17.510000. running mean: -5.297526\n",
      "ep 4038: ep_len:560 episode reward: total was -9.470000. running mean: -5.339251\n",
      "epsilon:0.010000 episode_count: 28273. steps_count: 12476770.000000\n",
      "ep 4039: ep_len:226 episode reward: total was 0.630000. running mean: -5.279558\n",
      "ep 4039: ep_len:610 episode reward: total was -1.560000. running mean: -5.242363\n",
      "ep 4039: ep_len:515 episode reward: total was -9.680000. running mean: -5.286739\n",
      "ep 4039: ep_len:520 episode reward: total was -21.130000. running mean: -5.445172\n",
      "ep 4039: ep_len:3 episode reward: total was 0.000000. running mean: -5.390720\n",
      "ep 4039: ep_len:625 episode reward: total was 5.140000. running mean: -5.285413\n",
      "ep 4039: ep_len:625 episode reward: total was -75.640000. running mean: -5.988959\n",
      "epsilon:0.010000 episode_count: 28280. steps_count: 12479894.000000\n",
      "ep 4040: ep_len:665 episode reward: total was -35.810000. running mean: -6.287169\n",
      "ep 4040: ep_len:500 episode reward: total was 4.620000. running mean: -6.178098\n",
      "ep 4040: ep_len:500 episode reward: total was 1.630000. running mean: -6.100017\n",
      "ep 4040: ep_len:635 episode reward: total was 19.010000. running mean: -5.848916\n",
      "ep 4040: ep_len:3 episode reward: total was 0.000000. running mean: -5.790427\n",
      "ep 4040: ep_len:540 episode reward: total was 3.150000. running mean: -5.701023\n",
      "ep 4040: ep_len:570 episode reward: total was -9.830000. running mean: -5.742313\n",
      "epsilon:0.010000 episode_count: 28287. steps_count: 12483307.000000\n",
      "ep 4041: ep_len:635 episode reward: total was -8.950000. running mean: -5.774390\n",
      "ep 4041: ep_len:540 episode reward: total was -8.450000. running mean: -5.801146\n",
      "ep 4041: ep_len:645 episode reward: total was -1.630000. running mean: -5.759434\n",
      "ep 4041: ep_len:520 episode reward: total was -12.470000. running mean: -5.826540\n",
      "ep 4041: ep_len:2 episode reward: total was 0.000000. running mean: -5.768275\n",
      "ep 4041: ep_len:500 episode reward: total was -38.220000. running mean: -6.092792\n",
      "ep 4041: ep_len:540 episode reward: total was -10.880000. running mean: -6.140664\n",
      "epsilon:0.010000 episode_count: 28294. steps_count: 12486689.000000\n",
      "ep 4042: ep_len:630 episode reward: total was -15.710000. running mean: -6.236357\n",
      "ep 4042: ep_len:505 episode reward: total was 7.520000. running mean: -6.098794\n",
      "ep 4042: ep_len:580 episode reward: total was 0.460000. running mean: -6.033206\n",
      "ep 4042: ep_len:520 episode reward: total was 1.840000. running mean: -5.954474\n",
      "ep 4042: ep_len:105 episode reward: total was 7.050000. running mean: -5.824429\n",
      "ep 4042: ep_len:705 episode reward: total was 0.970000. running mean: -5.756485\n",
      "ep 4042: ep_len:590 episode reward: total was -15.110000. running mean: -5.850020\n",
      "epsilon:0.010000 episode_count: 28301. steps_count: 12490324.000000\n",
      "ep 4043: ep_len:640 episode reward: total was -1.410000. running mean: -5.805620\n",
      "ep 4043: ep_len:530 episode reward: total was 9.420000. running mean: -5.653363\n",
      "ep 4043: ep_len:590 episode reward: total was -1.050000. running mean: -5.607330\n",
      "ep 4043: ep_len:530 episode reward: total was 7.350000. running mean: -5.477756\n",
      "ep 4043: ep_len:50 episode reward: total was 4.510000. running mean: -5.377879\n",
      "ep 4043: ep_len:605 episode reward: total was -0.040000. running mean: -5.324500\n",
      "ep 4043: ep_len:500 episode reward: total was -20.470000. running mean: -5.475955\n",
      "epsilon:0.010000 episode_count: 28308. steps_count: 12493769.000000\n",
      "ep 4044: ep_len:615 episode reward: total was -0.530000. running mean: -5.426496\n",
      "ep 4044: ep_len:620 episode reward: total was -5.640000. running mean: -5.428631\n",
      "ep 4044: ep_len:351 episode reward: total was 7.180000. running mean: -5.302544\n",
      "ep 4044: ep_len:500 episode reward: total was -14.990000. running mean: -5.399419\n",
      "ep 4044: ep_len:107 episode reward: total was 6.040000. running mean: -5.285025\n",
      "ep 4044: ep_len:520 episode reward: total was -5.690000. running mean: -5.289074\n",
      "ep 4044: ep_len:570 episode reward: total was -13.570000. running mean: -5.371884\n",
      "epsilon:0.010000 episode_count: 28315. steps_count: 12497052.000000\n",
      "ep 4045: ep_len:228 episode reward: total was 3.630000. running mean: -5.281865\n",
      "ep 4045: ep_len:505 episode reward: total was -2.130000. running mean: -5.250346\n",
      "ep 4045: ep_len:376 episode reward: total was 0.680000. running mean: -5.191043\n",
      "ep 4045: ep_len:500 episode reward: total was -15.050000. running mean: -5.289632\n",
      "ep 4045: ep_len:85 episode reward: total was 5.020000. running mean: -5.186536\n",
      "ep 4045: ep_len:530 episode reward: total was -2.800000. running mean: -5.162671\n",
      "ep 4045: ep_len:500 episode reward: total was -7.590000. running mean: -5.186944\n",
      "epsilon:0.010000 episode_count: 28322. steps_count: 12499776.000000\n",
      "ep 4046: ep_len:515 episode reward: total was -1.400000. running mean: -5.149074\n",
      "ep 4046: ep_len:277 episode reward: total was -3.820000. running mean: -5.135784\n",
      "ep 4046: ep_len:500 episode reward: total was 1.600000. running mean: -5.068426\n",
      "ep 4046: ep_len:676 episode reward: total was -54.630000. running mean: -5.564042\n",
      "ep 4046: ep_len:3 episode reward: total was 0.000000. running mean: -5.508401\n",
      "ep 4046: ep_len:540 episode reward: total was -14.900000. running mean: -5.602317\n",
      "ep 4046: ep_len:347 episode reward: total was -10.340000. running mean: -5.649694\n",
      "epsilon:0.010000 episode_count: 28329. steps_count: 12502634.000000\n",
      "ep 4047: ep_len:535 episode reward: total was 7.930000. running mean: -5.513897\n",
      "ep 4047: ep_len:535 episode reward: total was -0.050000. running mean: -5.459258\n",
      "ep 4047: ep_len:670 episode reward: total was -38.230000. running mean: -5.786966\n",
      "ep 4047: ep_len:610 episode reward: total was 11.590000. running mean: -5.613196\n",
      "ep 4047: ep_len:128 episode reward: total was 8.070000. running mean: -5.476364\n",
      "ep 4047: ep_len:239 episode reward: total was 8.150000. running mean: -5.340100\n",
      "ep 4047: ep_len:605 episode reward: total was -2.970000. running mean: -5.316399\n",
      "epsilon:0.010000 episode_count: 28336. steps_count: 12505956.000000\n",
      "ep 4048: ep_len:520 episode reward: total was 5.650000. running mean: -5.206735\n",
      "ep 4048: ep_len:560 episode reward: total was 24.860000. running mean: -4.906068\n",
      "ep 4048: ep_len:500 episode reward: total was 1.710000. running mean: -4.839907\n",
      "ep 4048: ep_len:555 episode reward: total was 4.060000. running mean: -4.750908\n",
      "ep 4048: ep_len:3 episode reward: total was 0.000000. running mean: -4.703399\n",
      "ep 4048: ep_len:520 episode reward: total was -4.220000. running mean: -4.698565\n",
      "ep 4048: ep_len:540 episode reward: total was -7.550000. running mean: -4.727079\n",
      "epsilon:0.010000 episode_count: 28343. steps_count: 12509154.000000\n",
      "ep 4049: ep_len:500 episode reward: total was 1.090000. running mean: -4.668909\n",
      "ep 4049: ep_len:500 episode reward: total was 3.060000. running mean: -4.591620\n",
      "ep 4049: ep_len:560 episode reward: total was 0.100000. running mean: -4.544703\n",
      "ep 4049: ep_len:625 episode reward: total was 8.110000. running mean: -4.418156\n",
      "ep 4049: ep_len:3 episode reward: total was 0.000000. running mean: -4.373975\n",
      "ep 4049: ep_len:610 episode reward: total was 13.700000. running mean: -4.193235\n",
      "ep 4049: ep_len:185 episode reward: total was -10.920000. running mean: -4.260503\n",
      "epsilon:0.010000 episode_count: 28350. steps_count: 12512137.000000\n",
      "ep 4050: ep_len:206 episode reward: total was -5.440000. running mean: -4.272298\n",
      "ep 4050: ep_len:570 episode reward: total was 9.520000. running mean: -4.134375\n",
      "ep 4050: ep_len:555 episode reward: total was 3.450000. running mean: -4.058531\n",
      "ep 4050: ep_len:535 episode reward: total was 1.360000. running mean: -4.004346\n",
      "ep 4050: ep_len:3 episode reward: total was 0.000000. running mean: -3.964302\n",
      "ep 4050: ep_len:535 episode reward: total was 5.190000. running mean: -3.872759\n",
      "ep 4050: ep_len:570 episode reward: total was -21.600000. running mean: -4.050032\n",
      "epsilon:0.010000 episode_count: 28357. steps_count: 12515111.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4051: ep_len:515 episode reward: total was 8.290000. running mean: -3.926631\n",
      "ep 4051: ep_len:500 episode reward: total was -19.610000. running mean: -4.083465\n",
      "ep 4051: ep_len:610 episode reward: total was -11.250000. running mean: -4.155130\n",
      "ep 4051: ep_len:505 episode reward: total was -7.510000. running mean: -4.188679\n",
      "ep 4051: ep_len:3 episode reward: total was 0.000000. running mean: -4.146792\n",
      "ep 4051: ep_len:520 episode reward: total was -0.490000. running mean: -4.110224\n",
      "ep 4051: ep_len:590 episode reward: total was -31.110000. running mean: -4.380222\n",
      "epsilon:0.010000 episode_count: 28364. steps_count: 12518354.000000\n",
      "ep 4052: ep_len:545 episode reward: total was -22.370000. running mean: -4.560120\n",
      "ep 4052: ep_len:500 episode reward: total was 6.560000. running mean: -4.448919\n",
      "ep 4052: ep_len:565 episode reward: total was -23.380000. running mean: -4.638229\n",
      "ep 4052: ep_len:500 episode reward: total was -14.040000. running mean: -4.732247\n",
      "ep 4052: ep_len:40 episode reward: total was 2.500000. running mean: -4.659925\n",
      "ep 4052: ep_len:500 episode reward: total was -15.250000. running mean: -4.765825\n",
      "ep 4052: ep_len:500 episode reward: total was -20.450000. running mean: -4.922667\n",
      "epsilon:0.010000 episode_count: 28371. steps_count: 12521504.000000\n",
      "ep 4053: ep_len:580 episode reward: total was 5.910000. running mean: -4.814340\n",
      "ep 4053: ep_len:500 episode reward: total was 10.600000. running mean: -4.660197\n",
      "ep 4053: ep_len:79 episode reward: total was 0.040000. running mean: -4.613195\n",
      "ep 4053: ep_len:500 episode reward: total was -38.830000. running mean: -4.955363\n",
      "ep 4053: ep_len:80 episode reward: total was 5.070000. running mean: -4.855110\n",
      "ep 4053: ep_len:580 episode reward: total was -14.020000. running mean: -4.946758\n",
      "ep 4053: ep_len:685 episode reward: total was -16.640000. running mean: -5.063691\n",
      "epsilon:0.010000 episode_count: 28378. steps_count: 12524508.000000\n",
      "ep 4054: ep_len:595 episode reward: total was 2.990000. running mean: -4.983154\n",
      "ep 4054: ep_len:500 episode reward: total was 10.560000. running mean: -4.827722\n",
      "ep 4054: ep_len:555 episode reward: total was 3.960000. running mean: -4.739845\n",
      "ep 4054: ep_len:555 episode reward: total was 9.030000. running mean: -4.602147\n",
      "ep 4054: ep_len:3 episode reward: total was 0.000000. running mean: -4.556125\n",
      "ep 4054: ep_len:500 episode reward: total was 1.300000. running mean: -4.497564\n",
      "ep 4054: ep_len:510 episode reward: total was -7.060000. running mean: -4.523188\n",
      "epsilon:0.010000 episode_count: 28385. steps_count: 12527726.000000\n",
      "ep 4055: ep_len:600 episode reward: total was 8.120000. running mean: -4.396756\n",
      "ep 4055: ep_len:600 episode reward: total was -17.520000. running mean: -4.527989\n",
      "ep 4055: ep_len:590 episode reward: total was 5.400000. running mean: -4.428709\n",
      "ep 4055: ep_len:51 episode reward: total was -1.430000. running mean: -4.398722\n",
      "ep 4055: ep_len:38 episode reward: total was -2.500000. running mean: -4.379735\n",
      "ep 4055: ep_len:530 episode reward: total was -10.910000. running mean: -4.445037\n",
      "ep 4055: ep_len:354 episode reward: total was -8.310000. running mean: -4.483687\n",
      "epsilon:0.010000 episode_count: 28392. steps_count: 12530489.000000\n",
      "ep 4056: ep_len:520 episode reward: total was 0.570000. running mean: -4.433150\n",
      "ep 4056: ep_len:500 episode reward: total was 1.590000. running mean: -4.372919\n",
      "ep 4056: ep_len:650 episode reward: total was -9.140000. running mean: -4.420589\n",
      "ep 4056: ep_len:515 episode reward: total was -3.030000. running mean: -4.406684\n",
      "ep 4056: ep_len:113 episode reward: total was 2.540000. running mean: -4.337217\n",
      "ep 4056: ep_len:530 episode reward: total was -6.740000. running mean: -4.361245\n",
      "ep 4056: ep_len:510 episode reward: total was -15.860000. running mean: -4.476232\n",
      "epsilon:0.010000 episode_count: 28399. steps_count: 12533827.000000\n",
      "ep 4057: ep_len:560 episode reward: total was 4.890000. running mean: -4.382570\n",
      "ep 4057: ep_len:580 episode reward: total was -29.490000. running mean: -4.633644\n",
      "ep 4057: ep_len:595 episode reward: total was -28.730000. running mean: -4.874608\n",
      "ep 4057: ep_len:600 episode reward: total was 11.100000. running mean: -4.714862\n",
      "ep 4057: ep_len:100 episode reward: total was 6.030000. running mean: -4.607413\n",
      "ep 4057: ep_len:515 episode reward: total was 3.600000. running mean: -4.525339\n",
      "ep 4057: ep_len:193 episode reward: total was -12.910000. running mean: -4.609185\n",
      "epsilon:0.010000 episode_count: 28406. steps_count: 12536970.000000\n",
      "ep 4058: ep_len:500 episode reward: total was 5.400000. running mean: -4.509094\n",
      "ep 4058: ep_len:625 episode reward: total was -1.030000. running mean: -4.474303\n",
      "ep 4058: ep_len:391 episode reward: total was 3.710000. running mean: -4.392460\n",
      "ep 4058: ep_len:56 episode reward: total was -0.930000. running mean: -4.357835\n",
      "ep 4058: ep_len:75 episode reward: total was 2.520000. running mean: -4.289057\n",
      "ep 4058: ep_len:505 episode reward: total was -10.470000. running mean: -4.350866\n",
      "ep 4058: ep_len:500 episode reward: total was -46.680000. running mean: -4.774157\n",
      "epsilon:0.010000 episode_count: 28413. steps_count: 12539622.000000\n",
      "ep 4059: ep_len:630 episode reward: total was 3.620000. running mean: -4.690216\n",
      "ep 4059: ep_len:585 episode reward: total was -0.060000. running mean: -4.643914\n",
      "ep 4059: ep_len:500 episode reward: total was -0.420000. running mean: -4.601675\n",
      "ep 4059: ep_len:56 episode reward: total was 2.570000. running mean: -4.529958\n",
      "ep 4059: ep_len:3 episode reward: total was 0.000000. running mean: -4.484658\n",
      "ep 4059: ep_len:565 episode reward: total was -22.190000. running mean: -4.661712\n",
      "ep 4059: ep_len:336 episode reward: total was -2.280000. running mean: -4.637895\n",
      "epsilon:0.010000 episode_count: 28420. steps_count: 12542297.000000\n",
      "ep 4060: ep_len:510 episode reward: total was -21.340000. running mean: -4.804916\n",
      "ep 4060: ep_len:500 episode reward: total was -27.580000. running mean: -5.032666\n",
      "ep 4060: ep_len:665 episode reward: total was -28.510000. running mean: -5.267440\n",
      "ep 4060: ep_len:510 episode reward: total was -8.970000. running mean: -5.304465\n",
      "ep 4060: ep_len:3 episode reward: total was 0.000000. running mean: -5.251421\n",
      "ep 4060: ep_len:525 episode reward: total was -17.540000. running mean: -5.374307\n",
      "ep 4060: ep_len:195 episode reward: total was -9.900000. running mean: -5.419563\n",
      "epsilon:0.010000 episode_count: 28427. steps_count: 12545205.000000\n",
      "ep 4061: ep_len:500 episode reward: total was 12.300000. running mean: -5.242368\n",
      "ep 4061: ep_len:600 episode reward: total was 17.910000. running mean: -5.010844\n",
      "ep 4061: ep_len:620 episode reward: total was 6.980000. running mean: -4.890936\n",
      "ep 4061: ep_len:605 episode reward: total was 3.020000. running mean: -4.811826\n",
      "ep 4061: ep_len:3 episode reward: total was 0.000000. running mean: -4.763708\n",
      "ep 4061: ep_len:230 episode reward: total was 7.650000. running mean: -4.639571\n",
      "ep 4061: ep_len:585 episode reward: total was -10.080000. running mean: -4.693975\n",
      "epsilon:0.010000 episode_count: 28434. steps_count: 12548348.000000\n",
      "ep 4062: ep_len:118 episode reward: total was 3.090000. running mean: -4.616136\n",
      "ep 4062: ep_len:570 episode reward: total was 5.970000. running mean: -4.510274\n",
      "ep 4062: ep_len:565 episode reward: total was -6.630000. running mean: -4.531471\n",
      "ep 4062: ep_len:535 episode reward: total was -19.070000. running mean: -4.676857\n",
      "ep 4062: ep_len:3 episode reward: total was 0.000000. running mean: -4.630088\n",
      "ep 4062: ep_len:565 episode reward: total was -8.650000. running mean: -4.670287\n",
      "ep 4062: ep_len:610 episode reward: total was -20.560000. running mean: -4.829184\n",
      "epsilon:0.010000 episode_count: 28441. steps_count: 12551314.000000\n",
      "ep 4063: ep_len:540 episode reward: total was -12.750000. running mean: -4.908393\n",
      "ep 4063: ep_len:560 episode reward: total was -21.830000. running mean: -5.077609\n",
      "ep 4063: ep_len:396 episode reward: total was -5.780000. running mean: -5.084633\n",
      "ep 4063: ep_len:500 episode reward: total was 11.350000. running mean: -4.920286\n",
      "ep 4063: ep_len:3 episode reward: total was 0.000000. running mean: -4.871083\n",
      "ep 4063: ep_len:515 episode reward: total was -8.930000. running mean: -4.911673\n",
      "ep 4063: ep_len:500 episode reward: total was -15.470000. running mean: -5.017256\n",
      "epsilon:0.010000 episode_count: 28448. steps_count: 12554328.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4064: ep_len:590 episode reward: total was 6.450000. running mean: -4.902583\n",
      "ep 4064: ep_len:201 episode reward: total was 1.650000. running mean: -4.837057\n",
      "ep 4064: ep_len:625 episode reward: total was 0.470000. running mean: -4.783987\n",
      "ep 4064: ep_len:530 episode reward: total was 4.950000. running mean: -4.686647\n",
      "ep 4064: ep_len:3 episode reward: total was 0.000000. running mean: -4.639780\n",
      "ep 4064: ep_len:585 episode reward: total was 0.330000. running mean: -4.590083\n",
      "ep 4064: ep_len:585 episode reward: total was -21.020000. running mean: -4.754382\n",
      "epsilon:0.010000 episode_count: 28455. steps_count: 12557447.000000\n",
      "ep 4065: ep_len:500 episode reward: total was -26.250000. running mean: -4.969338\n",
      "ep 4065: ep_len:500 episode reward: total was 17.710000. running mean: -4.742545\n",
      "ep 4065: ep_len:79 episode reward: total was -0.970000. running mean: -4.704819\n",
      "ep 4065: ep_len:525 episode reward: total was -9.030000. running mean: -4.748071\n",
      "ep 4065: ep_len:3 episode reward: total was 0.000000. running mean: -4.700590\n",
      "ep 4065: ep_len:610 episode reward: total was -18.670000. running mean: -4.840284\n",
      "ep 4065: ep_len:510 episode reward: total was -1.550000. running mean: -4.807382\n",
      "epsilon:0.010000 episode_count: 28462. steps_count: 12560174.000000\n",
      "ep 4066: ep_len:117 episode reward: total was -3.490000. running mean: -4.794208\n",
      "ep 4066: ep_len:172 episode reward: total was -11.910000. running mean: -4.865366\n",
      "ep 4066: ep_len:675 episode reward: total was -29.500000. running mean: -5.111712\n",
      "ep 4066: ep_len:149 episode reward: total was 7.110000. running mean: -4.989495\n",
      "ep 4066: ep_len:1 episode reward: total was 0.000000. running mean: -4.939600\n",
      "ep 4066: ep_len:540 episode reward: total was -34.910000. running mean: -5.239304\n",
      "ep 4066: ep_len:211 episode reward: total was -6.380000. running mean: -5.250711\n",
      "epsilon:0.010000 episode_count: 28469. steps_count: 12562039.000000\n",
      "ep 4067: ep_len:500 episode reward: total was 9.440000. running mean: -5.103804\n",
      "ep 4067: ep_len:500 episode reward: total was 12.780000. running mean: -4.924966\n",
      "ep 4067: ep_len:70 episode reward: total was -0.950000. running mean: -4.885216\n",
      "ep 4067: ep_len:575 episode reward: total was -35.590000. running mean: -5.192264\n",
      "ep 4067: ep_len:3 episode reward: total was 0.000000. running mean: -5.140341\n",
      "ep 4067: ep_len:500 episode reward: total was -24.810000. running mean: -5.337038\n",
      "ep 4067: ep_len:342 episode reward: total was -10.750000. running mean: -5.391168\n",
      "epsilon:0.010000 episode_count: 28476. steps_count: 12564529.000000\n",
      "ep 4068: ep_len:530 episode reward: total was -5.350000. running mean: -5.390756\n",
      "ep 4068: ep_len:262 episode reward: total was 3.650000. running mean: -5.300348\n",
      "ep 4068: ep_len:615 episode reward: total was -26.810000. running mean: -5.515445\n",
      "ep 4068: ep_len:404 episode reward: total was -29.640000. running mean: -5.756690\n",
      "ep 4068: ep_len:3 episode reward: total was 0.000000. running mean: -5.699123\n",
      "ep 4068: ep_len:530 episode reward: total was 0.550000. running mean: -5.636632\n",
      "ep 4068: ep_len:290 episode reward: total was -6.290000. running mean: -5.643166\n",
      "epsilon:0.010000 episode_count: 28483. steps_count: 12567163.000000\n",
      "ep 4069: ep_len:500 episode reward: total was 10.480000. running mean: -5.481934\n",
      "ep 4069: ep_len:515 episode reward: total was -23.480000. running mean: -5.661915\n",
      "ep 4069: ep_len:605 episode reward: total was 1.510000. running mean: -5.590196\n",
      "ep 4069: ep_len:500 episode reward: total was 14.840000. running mean: -5.385894\n",
      "ep 4069: ep_len:3 episode reward: total was 0.000000. running mean: -5.332035\n",
      "ep 4069: ep_len:570 episode reward: total was 2.460000. running mean: -5.254114\n",
      "ep 4069: ep_len:575 episode reward: total was -34.650000. running mean: -5.548073\n",
      "epsilon:0.010000 episode_count: 28490. steps_count: 12570431.000000\n",
      "ep 4070: ep_len:565 episode reward: total was 6.460000. running mean: -5.427993\n",
      "ep 4070: ep_len:525 episode reward: total was -26.950000. running mean: -5.643213\n",
      "ep 4070: ep_len:615 episode reward: total was -30.830000. running mean: -5.895081\n",
      "ep 4070: ep_len:575 episode reward: total was 1.610000. running mean: -5.820030\n",
      "ep 4070: ep_len:3 episode reward: total was 0.000000. running mean: -5.761829\n",
      "ep 4070: ep_len:500 episode reward: total was -10.720000. running mean: -5.811411\n",
      "ep 4070: ep_len:520 episode reward: total was -9.060000. running mean: -5.843897\n",
      "epsilon:0.010000 episode_count: 28497. steps_count: 12573734.000000\n",
      "ep 4071: ep_len:565 episode reward: total was -20.750000. running mean: -5.992958\n",
      "ep 4071: ep_len:500 episode reward: total was 2.670000. running mean: -5.906328\n",
      "ep 4071: ep_len:590 episode reward: total was 2.120000. running mean: -5.826065\n",
      "ep 4071: ep_len:56 episode reward: total was 1.560000. running mean: -5.752205\n",
      "ep 4071: ep_len:26 episode reward: total was -5.000000. running mean: -5.744683\n",
      "ep 4071: ep_len:510 episode reward: total was -0.920000. running mean: -5.696436\n",
      "ep 4071: ep_len:560 episode reward: total was -9.440000. running mean: -5.733871\n",
      "epsilon:0.010000 episode_count: 28504. steps_count: 12576541.000000\n",
      "ep 4072: ep_len:630 episode reward: total was -31.470000. running mean: -5.991233\n",
      "ep 4072: ep_len:500 episode reward: total was -12.460000. running mean: -6.055920\n",
      "ep 4072: ep_len:505 episode reward: total was -0.300000. running mean: -5.998361\n",
      "ep 4072: ep_len:708 episode reward: total was -31.960000. running mean: -6.257977\n",
      "ep 4072: ep_len:80 episode reward: total was 3.540000. running mean: -6.159998\n",
      "ep 4072: ep_len:525 episode reward: total was -5.950000. running mean: -6.157898\n",
      "ep 4072: ep_len:500 episode reward: total was -9.560000. running mean: -6.191919\n",
      "epsilon:0.010000 episode_count: 28511. steps_count: 12579989.000000\n",
      "ep 4073: ep_len:134 episode reward: total was 2.570000. running mean: -6.104300\n",
      "ep 4073: ep_len:500 episode reward: total was 18.220000. running mean: -5.861057\n",
      "ep 4073: ep_len:540 episode reward: total was -7.810000. running mean: -5.880546\n",
      "ep 4073: ep_len:132 episode reward: total was 1.550000. running mean: -5.806241\n",
      "ep 4073: ep_len:3 episode reward: total was 0.000000. running mean: -5.748178\n",
      "ep 4073: ep_len:575 episode reward: total was 1.890000. running mean: -5.671796\n",
      "ep 4073: ep_len:635 episode reward: total was -9.890000. running mean: -5.713978\n",
      "epsilon:0.010000 episode_count: 28518. steps_count: 12582508.000000\n",
      "ep 4074: ep_len:500 episode reward: total was 10.420000. running mean: -5.552639\n",
      "ep 4074: ep_len:585 episode reward: total was -37.060000. running mean: -5.867712\n",
      "ep 4074: ep_len:467 episode reward: total was -7.710000. running mean: -5.886135\n",
      "ep 4074: ep_len:505 episode reward: total was -21.620000. running mean: -6.043474\n",
      "ep 4074: ep_len:3 episode reward: total was 0.000000. running mean: -5.983039\n",
      "ep 4074: ep_len:635 episode reward: total was -5.980000. running mean: -5.983009\n",
      "ep 4074: ep_len:515 episode reward: total was -16.380000. running mean: -6.086979\n",
      "epsilon:0.010000 episode_count: 28525. steps_count: 12585718.000000\n",
      "ep 4075: ep_len:234 episode reward: total was 1.590000. running mean: -6.010209\n",
      "ep 4075: ep_len:500 episode reward: total was 7.540000. running mean: -5.874707\n",
      "ep 4075: ep_len:570 episode reward: total was -26.850000. running mean: -6.084460\n",
      "ep 4075: ep_len:570 episode reward: total was 4.490000. running mean: -5.978715\n",
      "ep 4075: ep_len:1 episode reward: total was 0.000000. running mean: -5.918928\n",
      "ep 4075: ep_len:520 episode reward: total was -5.460000. running mean: -5.914339\n",
      "ep 4075: ep_len:540 episode reward: total was -15.900000. running mean: -6.014195\n",
      "epsilon:0.010000 episode_count: 28532. steps_count: 12588653.000000\n",
      "ep 4076: ep_len:585 episode reward: total was -27.140000. running mean: -6.225453\n",
      "ep 4076: ep_len:670 episode reward: total was 6.570000. running mean: -6.097499\n",
      "ep 4076: ep_len:540 episode reward: total was -15.980000. running mean: -6.196324\n",
      "ep 4076: ep_len:527 episode reward: total was -29.470000. running mean: -6.429060\n",
      "ep 4076: ep_len:3 episode reward: total was 0.000000. running mean: -6.364770\n",
      "ep 4076: ep_len:510 episode reward: total was 11.940000. running mean: -6.181722\n",
      "ep 4076: ep_len:500 episode reward: total was 0.970000. running mean: -6.110205\n",
      "epsilon:0.010000 episode_count: 28539. steps_count: 12591988.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4077: ep_len:535 episode reward: total was 16.000000. running mean: -5.889103\n",
      "ep 4077: ep_len:520 episode reward: total was 9.760000. running mean: -5.732612\n",
      "ep 4077: ep_len:615 episode reward: total was -0.010000. running mean: -5.675386\n",
      "ep 4077: ep_len:605 episode reward: total was 2.560000. running mean: -5.593032\n",
      "ep 4077: ep_len:3 episode reward: total was 0.000000. running mean: -5.537102\n",
      "ep 4077: ep_len:500 episode reward: total was 7.940000. running mean: -5.402331\n",
      "ep 4077: ep_len:590 episode reward: total was -9.050000. running mean: -5.438807\n",
      "epsilon:0.010000 episode_count: 28546. steps_count: 12595356.000000\n",
      "ep 4078: ep_len:740 episode reward: total was -35.310000. running mean: -5.737519\n",
      "ep 4078: ep_len:565 episode reward: total was -11.320000. running mean: -5.793344\n",
      "ep 4078: ep_len:74 episode reward: total was -1.470000. running mean: -5.750111\n",
      "ep 4078: ep_len:500 episode reward: total was 10.890000. running mean: -5.583709\n",
      "ep 4078: ep_len:92 episode reward: total was -0.970000. running mean: -5.537572\n",
      "ep 4078: ep_len:505 episode reward: total was 3.970000. running mean: -5.442497\n",
      "ep 4078: ep_len:515 episode reward: total was -1.070000. running mean: -5.398772\n",
      "epsilon:0.010000 episode_count: 28553. steps_count: 12598347.000000\n",
      "ep 4079: ep_len:565 episode reward: total was 8.950000. running mean: -5.255284\n",
      "ep 4079: ep_len:505 episode reward: total was 17.860000. running mean: -5.024131\n",
      "ep 4079: ep_len:347 episode reward: total was -1.830000. running mean: -4.992190\n",
      "ep 4079: ep_len:391 episode reward: total was -27.150000. running mean: -5.213768\n",
      "ep 4079: ep_len:83 episode reward: total was -0.960000. running mean: -5.171230\n",
      "ep 4079: ep_len:580 episode reward: total was 15.000000. running mean: -4.969518\n",
      "ep 4079: ep_len:500 episode reward: total was -10.090000. running mean: -5.020723\n",
      "epsilon:0.010000 episode_count: 28560. steps_count: 12601318.000000\n",
      "ep 4080: ep_len:610 episode reward: total was -6.390000. running mean: -5.034416\n",
      "ep 4080: ep_len:347 episode reward: total was -0.290000. running mean: -4.986971\n",
      "ep 4080: ep_len:530 episode reward: total was -8.270000. running mean: -5.019802\n",
      "ep 4080: ep_len:565 episode reward: total was -24.880000. running mean: -5.218404\n",
      "ep 4080: ep_len:3 episode reward: total was 0.000000. running mean: -5.166220\n",
      "ep 4080: ep_len:545 episode reward: total was -11.940000. running mean: -5.233957\n",
      "ep 4080: ep_len:525 episode reward: total was -0.010000. running mean: -5.181718\n",
      "epsilon:0.010000 episode_count: 28567. steps_count: 12604443.000000\n",
      "ep 4081: ep_len:500 episode reward: total was 7.420000. running mean: -5.055701\n",
      "ep 4081: ep_len:500 episode reward: total was -26.640000. running mean: -5.271544\n",
      "ep 4081: ep_len:422 episode reward: total was -2.760000. running mean: -5.246428\n",
      "ep 4081: ep_len:627 episode reward: total was -24.910000. running mean: -5.443064\n",
      "ep 4081: ep_len:3 episode reward: total was 0.000000. running mean: -5.388633\n",
      "ep 4081: ep_len:535 episode reward: total was 1.320000. running mean: -5.321547\n",
      "ep 4081: ep_len:500 episode reward: total was -18.650000. running mean: -5.454831\n",
      "epsilon:0.010000 episode_count: 28574. steps_count: 12607530.000000\n",
      "ep 4082: ep_len:565 episode reward: total was -41.110000. running mean: -5.811383\n",
      "ep 4082: ep_len:500 episode reward: total was -32.380000. running mean: -6.077069\n",
      "ep 4082: ep_len:690 episode reward: total was -1.200000. running mean: -6.028299\n",
      "ep 4082: ep_len:685 episode reward: total was -50.110000. running mean: -6.469116\n",
      "ep 4082: ep_len:3 episode reward: total was 0.000000. running mean: -6.404424\n",
      "ep 4082: ep_len:550 episode reward: total was -11.550000. running mean: -6.455880\n",
      "ep 4082: ep_len:204 episode reward: total was 0.180000. running mean: -6.389521\n",
      "epsilon:0.010000 episode_count: 28581. steps_count: 12610727.000000\n",
      "ep 4083: ep_len:106 episode reward: total was -9.940000. running mean: -6.425026\n",
      "ep 4083: ep_len:500 episode reward: total was 5.090000. running mean: -6.309876\n",
      "ep 4083: ep_len:462 episode reward: total was 5.760000. running mean: -6.189177\n",
      "ep 4083: ep_len:545 episode reward: total was -99.960000. running mean: -7.126885\n",
      "ep 4083: ep_len:3 episode reward: total was 0.000000. running mean: -7.055617\n",
      "ep 4083: ep_len:520 episode reward: total was -7.900000. running mean: -7.064060\n",
      "ep 4083: ep_len:525 episode reward: total was -2.760000. running mean: -7.021020\n",
      "epsilon:0.010000 episode_count: 28588. steps_count: 12613388.000000\n",
      "ep 4084: ep_len:525 episode reward: total was -20.920000. running mean: -7.160010\n",
      "ep 4084: ep_len:272 episode reward: total was -4.870000. running mean: -7.137110\n",
      "ep 4084: ep_len:58 episode reward: total was -0.980000. running mean: -7.075538\n",
      "ep 4084: ep_len:34 episode reward: total was -0.950000. running mean: -7.014283\n",
      "ep 4084: ep_len:88 episode reward: total was -1.960000. running mean: -6.963740\n",
      "ep 4084: ep_len:525 episode reward: total was -11.680000. running mean: -7.010903\n",
      "ep 4084: ep_len:580 episode reward: total was -7.550000. running mean: -7.016294\n",
      "epsilon:0.010000 episode_count: 28595. steps_count: 12615470.000000\n",
      "ep 4085: ep_len:123 episode reward: total was 1.570000. running mean: -6.930431\n",
      "ep 4085: ep_len:510 episode reward: total was 11.210000. running mean: -6.749027\n",
      "ep 4085: ep_len:625 episode reward: total was -4.400000. running mean: -6.725536\n",
      "ep 4085: ep_len:515 episode reward: total was -28.120000. running mean: -6.939481\n",
      "ep 4085: ep_len:34 episode reward: total was -1.500000. running mean: -6.885086\n",
      "ep 4085: ep_len:590 episode reward: total was -14.660000. running mean: -6.962835\n",
      "ep 4085: ep_len:500 episode reward: total was -23.160000. running mean: -7.124807\n",
      "epsilon:0.010000 episode_count: 28602. steps_count: 12618367.000000\n",
      "ep 4086: ep_len:575 episode reward: total was 7.690000. running mean: -6.976659\n",
      "ep 4086: ep_len:535 episode reward: total was 18.410000. running mean: -6.722792\n",
      "ep 4086: ep_len:530 episode reward: total was -14.040000. running mean: -6.795964\n",
      "ep 4086: ep_len:500 episode reward: total was 6.590000. running mean: -6.662105\n",
      "ep 4086: ep_len:3 episode reward: total was 0.000000. running mean: -6.595484\n",
      "ep 4086: ep_len:500 episode reward: total was -7.420000. running mean: -6.603729\n",
      "ep 4086: ep_len:500 episode reward: total was -12.510000. running mean: -6.662791\n",
      "epsilon:0.010000 episode_count: 28609. steps_count: 12621510.000000\n",
      "ep 4087: ep_len:560 episode reward: total was 0.620000. running mean: -6.589964\n",
      "ep 4087: ep_len:550 episode reward: total was 15.400000. running mean: -6.370064\n",
      "ep 4087: ep_len:645 episode reward: total was -9.770000. running mean: -6.404063\n",
      "ep 4087: ep_len:408 episode reward: total was 7.390000. running mean: -6.266123\n",
      "ep 4087: ep_len:3 episode reward: total was 0.000000. running mean: -6.203461\n",
      "ep 4087: ep_len:750 episode reward: total was -55.340000. running mean: -6.694827\n",
      "ep 4087: ep_len:600 episode reward: total was -29.570000. running mean: -6.923579\n",
      "epsilon:0.010000 episode_count: 28616. steps_count: 12625026.000000\n",
      "ep 4088: ep_len:500 episode reward: total was 12.820000. running mean: -6.726143\n",
      "ep 4088: ep_len:585 episode reward: total was 9.020000. running mean: -6.568681\n",
      "ep 4088: ep_len:630 episode reward: total was -44.020000. running mean: -6.943195\n",
      "ep 4088: ep_len:500 episode reward: total was 7.080000. running mean: -6.802963\n",
      "ep 4088: ep_len:3 episode reward: total was 0.000000. running mean: -6.734933\n",
      "ep 4088: ep_len:605 episode reward: total was -95.330000. running mean: -7.620884\n",
      "ep 4088: ep_len:590 episode reward: total was -18.390000. running mean: -7.728575\n",
      "epsilon:0.010000 episode_count: 28623. steps_count: 12628439.000000\n",
      "ep 4089: ep_len:118 episode reward: total was -12.950000. running mean: -7.780789\n",
      "ep 4089: ep_len:500 episode reward: total was 0.440000. running mean: -7.698581\n",
      "ep 4089: ep_len:550 episode reward: total was -4.030000. running mean: -7.661895\n",
      "ep 4089: ep_len:635 episode reward: total was 14.940000. running mean: -7.435876\n",
      "ep 4089: ep_len:3 episode reward: total was 0.000000. running mean: -7.361518\n",
      "ep 4089: ep_len:245 episode reward: total was 6.150000. running mean: -7.226402\n",
      "ep 4089: ep_len:620 episode reward: total was -7.770000. running mean: -7.231838\n",
      "epsilon:0.010000 episode_count: 28630. steps_count: 12631110.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4090: ep_len:545 episode reward: total was -1.400000. running mean: -7.173520\n",
      "ep 4090: ep_len:570 episode reward: total was -18.900000. running mean: -7.290785\n",
      "ep 4090: ep_len:580 episode reward: total was 10.510000. running mean: -7.112777\n",
      "ep 4090: ep_len:500 episode reward: total was 10.480000. running mean: -6.936849\n",
      "ep 4090: ep_len:3 episode reward: total was 0.000000. running mean: -6.867481\n",
      "ep 4090: ep_len:500 episode reward: total was -3.110000. running mean: -6.829906\n",
      "ep 4090: ep_len:545 episode reward: total was -10.410000. running mean: -6.865707\n",
      "epsilon:0.010000 episode_count: 28637. steps_count: 12634353.000000\n",
      "ep 4091: ep_len:665 episode reward: total was -25.800000. running mean: -7.055050\n",
      "ep 4091: ep_len:505 episode reward: total was -1.130000. running mean: -6.995799\n",
      "ep 4091: ep_len:356 episode reward: total was -41.450000. running mean: -7.340341\n",
      "ep 4091: ep_len:500 episode reward: total was -34.610000. running mean: -7.613038\n",
      "ep 4091: ep_len:3 episode reward: total was 0.000000. running mean: -7.536908\n",
      "ep 4091: ep_len:545 episode reward: total was 7.450000. running mean: -7.387038\n",
      "ep 4091: ep_len:600 episode reward: total was -43.000000. running mean: -7.743168\n",
      "epsilon:0.010000 episode_count: 28644. steps_count: 12637527.000000\n",
      "ep 4092: ep_len:570 episode reward: total was 3.970000. running mean: -7.626036\n",
      "ep 4092: ep_len:500 episode reward: total was 5.060000. running mean: -7.499176\n",
      "ep 4092: ep_len:520 episode reward: total was 0.910000. running mean: -7.415084\n",
      "ep 4092: ep_len:500 episode reward: total was 3.930000. running mean: -7.301633\n",
      "ep 4092: ep_len:94 episode reward: total was 4.540000. running mean: -7.183217\n",
      "ep 4092: ep_len:245 episode reward: total was 4.160000. running mean: -7.069785\n",
      "ep 4092: ep_len:585 episode reward: total was -10.950000. running mean: -7.108587\n",
      "epsilon:0.010000 episode_count: 28651. steps_count: 12640541.000000\n",
      "ep 4093: ep_len:630 episode reward: total was -23.820000. running mean: -7.275701\n",
      "ep 4093: ep_len:530 episode reward: total was 12.410000. running mean: -7.078844\n",
      "ep 4093: ep_len:555 episode reward: total was -4.610000. running mean: -7.054156\n",
      "ep 4093: ep_len:500 episode reward: total was -12.110000. running mean: -7.104714\n",
      "ep 4093: ep_len:100 episode reward: total was 6.550000. running mean: -6.968167\n",
      "ep 4093: ep_len:530 episode reward: total was 3.620000. running mean: -6.862285\n",
      "ep 4093: ep_len:515 episode reward: total was -7.570000. running mean: -6.869363\n",
      "epsilon:0.010000 episode_count: 28658. steps_count: 12643901.000000\n",
      "ep 4094: ep_len:575 episode reward: total was -0.880000. running mean: -6.809469\n",
      "ep 4094: ep_len:500 episode reward: total was 19.720000. running mean: -6.544174\n",
      "ep 4094: ep_len:560 episode reward: total was -1.970000. running mean: -6.498432\n",
      "ep 4094: ep_len:690 episode reward: total was -63.130000. running mean: -7.064748\n",
      "ep 4094: ep_len:3 episode reward: total was 0.000000. running mean: -6.994101\n",
      "ep 4094: ep_len:585 episode reward: total was -21.290000. running mean: -7.137060\n",
      "ep 4094: ep_len:312 episode reward: total was -3.280000. running mean: -7.098489\n",
      "epsilon:0.010000 episode_count: 28665. steps_count: 12647126.000000\n",
      "ep 4095: ep_len:575 episode reward: total was 8.520000. running mean: -6.942304\n",
      "ep 4095: ep_len:570 episode reward: total was 2.980000. running mean: -6.843081\n",
      "ep 4095: ep_len:625 episode reward: total was 5.640000. running mean: -6.718250\n",
      "ep 4095: ep_len:515 episode reward: total was 3.360000. running mean: -6.617468\n",
      "ep 4095: ep_len:3 episode reward: total was 0.000000. running mean: -6.551293\n",
      "ep 4095: ep_len:500 episode reward: total was 6.930000. running mean: -6.416480\n",
      "ep 4095: ep_len:500 episode reward: total was -18.600000. running mean: -6.538315\n",
      "epsilon:0.010000 episode_count: 28672. steps_count: 12650414.000000\n",
      "ep 4096: ep_len:580 episode reward: total was -29.810000. running mean: -6.771032\n",
      "ep 4096: ep_len:500 episode reward: total was -35.580000. running mean: -7.059122\n",
      "ep 4096: ep_len:510 episode reward: total was 0.090000. running mean: -6.987631\n",
      "ep 4096: ep_len:615 episode reward: total was -7.910000. running mean: -6.996854\n",
      "ep 4096: ep_len:92 episode reward: total was 3.530000. running mean: -6.891586\n",
      "ep 4096: ep_len:505 episode reward: total was -7.000000. running mean: -6.892670\n",
      "ep 4096: ep_len:580 episode reward: total was 5.030000. running mean: -6.773443\n",
      "epsilon:0.010000 episode_count: 28679. steps_count: 12653796.000000\n",
      "ep 4097: ep_len:500 episode reward: total was -52.370000. running mean: -7.229409\n",
      "ep 4097: ep_len:500 episode reward: total was 2.550000. running mean: -7.131615\n",
      "ep 4097: ep_len:695 episode reward: total was -36.740000. running mean: -7.427699\n",
      "ep 4097: ep_len:555 episode reward: total was -5.570000. running mean: -7.409122\n",
      "ep 4097: ep_len:3 episode reward: total was 0.000000. running mean: -7.335030\n",
      "ep 4097: ep_len:580 episode reward: total was 0.460000. running mean: -7.257080\n",
      "ep 4097: ep_len:520 episode reward: total was -5.510000. running mean: -7.239609\n",
      "epsilon:0.010000 episode_count: 28686. steps_count: 12657149.000000\n",
      "ep 4098: ep_len:660 episode reward: total was -11.730000. running mean: -7.284513\n",
      "ep 4098: ep_len:630 episode reward: total was 17.110000. running mean: -7.040568\n",
      "ep 4098: ep_len:575 episode reward: total was 0.770000. running mean: -6.962462\n",
      "ep 4098: ep_len:550 episode reward: total was 7.530000. running mean: -6.817538\n",
      "ep 4098: ep_len:55 episode reward: total was 5.010000. running mean: -6.699262\n",
      "ep 4098: ep_len:700 episode reward: total was -1.110000. running mean: -6.643370\n",
      "ep 4098: ep_len:251 episode reward: total was -6.380000. running mean: -6.640736\n",
      "epsilon:0.010000 episode_count: 28693. steps_count: 12660570.000000\n",
      "ep 4099: ep_len:133 episode reward: total was 5.600000. running mean: -6.518329\n",
      "ep 4099: ep_len:500 episode reward: total was 2.190000. running mean: -6.431245\n",
      "ep 4099: ep_len:520 episode reward: total was -0.100000. running mean: -6.367933\n",
      "ep 4099: ep_len:530 episode reward: total was 8.940000. running mean: -6.214854\n",
      "ep 4099: ep_len:3 episode reward: total was 0.000000. running mean: -6.152705\n",
      "ep 4099: ep_len:500 episode reward: total was 2.950000. running mean: -6.061678\n",
      "ep 4099: ep_len:535 episode reward: total was 1.040000. running mean: -5.990661\n",
      "epsilon:0.010000 episode_count: 28700. steps_count: 12663291.000000\n",
      "ep 4100: ep_len:128 episode reward: total was 2.590000. running mean: -5.904855\n",
      "ep 4100: ep_len:520 episode reward: total was 19.880000. running mean: -5.647006\n",
      "ep 4100: ep_len:500 episode reward: total was 1.420000. running mean: -5.576336\n",
      "ep 4100: ep_len:520 episode reward: total was 5.420000. running mean: -5.466373\n",
      "ep 4100: ep_len:52 episode reward: total was 3.500000. running mean: -5.376709\n",
      "ep 4100: ep_len:665 episode reward: total was 4.380000. running mean: -5.279142\n",
      "ep 4100: ep_len:615 episode reward: total was -10.840000. running mean: -5.334750\n",
      "epsilon:0.010000 episode_count: 28707. steps_count: 12666291.000000\n",
      "ep 4101: ep_len:670 episode reward: total was -17.800000. running mean: -5.459403\n",
      "ep 4101: ep_len:570 episode reward: total was -3.420000. running mean: -5.439009\n",
      "ep 4101: ep_len:525 episode reward: total was -7.470000. running mean: -5.459319\n",
      "ep 4101: ep_len:560 episode reward: total was -41.680000. running mean: -5.821526\n",
      "ep 4101: ep_len:3 episode reward: total was 0.000000. running mean: -5.763310\n",
      "ep 4101: ep_len:505 episode reward: total was -24.680000. running mean: -5.952477\n",
      "ep 4101: ep_len:545 episode reward: total was -1.480000. running mean: -5.907753\n",
      "epsilon:0.010000 episode_count: 28714. steps_count: 12669669.000000\n",
      "ep 4102: ep_len:515 episode reward: total was 5.350000. running mean: -5.795175\n",
      "ep 4102: ep_len:505 episode reward: total was 0.270000. running mean: -5.734523\n",
      "ep 4102: ep_len:560 episode reward: total was -2.900000. running mean: -5.706178\n",
      "ep 4102: ep_len:515 episode reward: total was 5.090000. running mean: -5.598216\n",
      "ep 4102: ep_len:96 episode reward: total was 6.540000. running mean: -5.476834\n",
      "ep 4102: ep_len:500 episode reward: total was 1.650000. running mean: -5.405566\n",
      "ep 4102: ep_len:505 episode reward: total was -16.510000. running mean: -5.516610\n",
      "epsilon:0.010000 episode_count: 28721. steps_count: 12672865.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4103: ep_len:510 episode reward: total was 4.160000. running mean: -5.419844\n",
      "ep 4103: ep_len:615 episode reward: total was 11.620000. running mean: -5.249446\n",
      "ep 4103: ep_len:397 episode reward: total was -8.340000. running mean: -5.280351\n",
      "ep 4103: ep_len:520 episode reward: total was -9.070000. running mean: -5.318248\n",
      "ep 4103: ep_len:111 episode reward: total was 3.020000. running mean: -5.234865\n",
      "ep 4103: ep_len:500 episode reward: total was -9.790000. running mean: -5.280416\n",
      "ep 4103: ep_len:540 episode reward: total was -45.620000. running mean: -5.683812\n",
      "epsilon:0.010000 episode_count: 28728. steps_count: 12676058.000000\n",
      "ep 4104: ep_len:575 episode reward: total was -11.080000. running mean: -5.737774\n",
      "ep 4104: ep_len:590 episode reward: total was 16.870000. running mean: -5.511696\n",
      "ep 4104: ep_len:64 episode reward: total was -1.460000. running mean: -5.471179\n",
      "ep 4104: ep_len:505 episode reward: total was -18.530000. running mean: -5.601768\n",
      "ep 4104: ep_len:3 episode reward: total was 0.000000. running mean: -5.545750\n",
      "ep 4104: ep_len:575 episode reward: total was 8.530000. running mean: -5.404992\n",
      "ep 4104: ep_len:565 episode reward: total was -3.980000. running mean: -5.390743\n",
      "epsilon:0.010000 episode_count: 28735. steps_count: 12678935.000000\n",
      "ep 4105: ep_len:219 episode reward: total was -8.910000. running mean: -5.425935\n",
      "ep 4105: ep_len:635 episode reward: total was 12.060000. running mean: -5.251076\n",
      "ep 4105: ep_len:625 episode reward: total was -8.030000. running mean: -5.278865\n",
      "ep 4105: ep_len:530 episode reward: total was 9.990000. running mean: -5.126176\n",
      "ep 4105: ep_len:107 episode reward: total was 6.040000. running mean: -5.014515\n",
      "ep 4105: ep_len:500 episode reward: total was -48.780000. running mean: -5.452169\n",
      "ep 4105: ep_len:500 episode reward: total was -17.470000. running mean: -5.572348\n",
      "epsilon:0.010000 episode_count: 28742. steps_count: 12682051.000000\n",
      "ep 4106: ep_len:600 episode reward: total was 3.520000. running mean: -5.481424\n",
      "ep 4106: ep_len:580 episode reward: total was 8.770000. running mean: -5.338910\n",
      "ep 4106: ep_len:500 episode reward: total was 8.950000. running mean: -5.196021\n",
      "ep 4106: ep_len:500 episode reward: total was 3.850000. running mean: -5.105561\n",
      "ep 4106: ep_len:96 episode reward: total was 3.020000. running mean: -5.024305\n",
      "ep 4106: ep_len:520 episode reward: total was -2.580000. running mean: -4.999862\n",
      "ep 4106: ep_len:580 episode reward: total was 1.510000. running mean: -4.934763\n",
      "epsilon:0.010000 episode_count: 28749. steps_count: 12685427.000000\n",
      "ep 4107: ep_len:610 episode reward: total was 1.590000. running mean: -4.869516\n",
      "ep 4107: ep_len:535 episode reward: total was -14.840000. running mean: -4.969221\n",
      "ep 4107: ep_len:368 episode reward: total was 0.720000. running mean: -4.912328\n",
      "ep 4107: ep_len:520 episode reward: total was 2.390000. running mean: -4.839305\n",
      "ep 4107: ep_len:91 episode reward: total was -2.960000. running mean: -4.820512\n",
      "ep 4107: ep_len:500 episode reward: total was 4.760000. running mean: -4.724707\n",
      "ep 4107: ep_len:500 episode reward: total was -2.470000. running mean: -4.702160\n",
      "epsilon:0.010000 episode_count: 28756. steps_count: 12688551.000000\n",
      "ep 4108: ep_len:500 episode reward: total was 13.370000. running mean: -4.521438\n",
      "ep 4108: ep_len:500 episode reward: total was -42.370000. running mean: -4.899924\n",
      "ep 4108: ep_len:620 episode reward: total was -24.850000. running mean: -5.099425\n",
      "ep 4108: ep_len:33 episode reward: total was 0.030000. running mean: -5.048130\n",
      "ep 4108: ep_len:113 episode reward: total was 6.540000. running mean: -4.932249\n",
      "ep 4108: ep_len:620 episode reward: total was -16.560000. running mean: -5.048527\n",
      "ep 4108: ep_len:560 episode reward: total was -16.080000. running mean: -5.158841\n",
      "epsilon:0.010000 episode_count: 28763. steps_count: 12691497.000000\n",
      "ep 4109: ep_len:600 episode reward: total was -23.330000. running mean: -5.340553\n",
      "ep 4109: ep_len:500 episode reward: total was -6.300000. running mean: -5.350147\n",
      "ep 4109: ep_len:525 episode reward: total was -2.560000. running mean: -5.322246\n",
      "ep 4109: ep_len:520 episode reward: total was -6.500000. running mean: -5.334024\n",
      "ep 4109: ep_len:3 episode reward: total was 0.000000. running mean: -5.280683\n",
      "ep 4109: ep_len:585 episode reward: total was -17.020000. running mean: -5.398076\n",
      "ep 4109: ep_len:620 episode reward: total was -11.380000. running mean: -5.457896\n",
      "epsilon:0.010000 episode_count: 28770. steps_count: 12694850.000000\n",
      "ep 4110: ep_len:590 episode reward: total was 1.410000. running mean: -5.389217\n",
      "ep 4110: ep_len:595 episode reward: total was -4.350000. running mean: -5.378825\n",
      "ep 4110: ep_len:500 episode reward: total was -3.990000. running mean: -5.364936\n",
      "ep 4110: ep_len:625 episode reward: total was -31.970000. running mean: -5.630987\n",
      "ep 4110: ep_len:3 episode reward: total was 0.000000. running mean: -5.574677\n",
      "ep 4110: ep_len:585 episode reward: total was -2.650000. running mean: -5.545430\n",
      "ep 4110: ep_len:615 episode reward: total was -6.360000. running mean: -5.553576\n",
      "epsilon:0.010000 episode_count: 28777. steps_count: 12698363.000000\n",
      "ep 4111: ep_len:525 episode reward: total was 8.970000. running mean: -5.408340\n",
      "ep 4111: ep_len:500 episode reward: total was -1.560000. running mean: -5.369857\n",
      "ep 4111: ep_len:580 episode reward: total was -2.370000. running mean: -5.339858\n",
      "ep 4111: ep_len:610 episode reward: total was 4.580000. running mean: -5.240660\n",
      "ep 4111: ep_len:3 episode reward: total was 0.000000. running mean: -5.188253\n",
      "ep 4111: ep_len:565 episode reward: total was -12.030000. running mean: -5.256671\n",
      "ep 4111: ep_len:500 episode reward: total was -5.440000. running mean: -5.258504\n",
      "epsilon:0.010000 episode_count: 28784. steps_count: 12701646.000000\n",
      "ep 4112: ep_len:265 episode reward: total was 6.680000. running mean: -5.139119\n",
      "ep 4112: ep_len:292 episode reward: total was -37.850000. running mean: -5.466228\n",
      "ep 4112: ep_len:625 episode reward: total was -1.180000. running mean: -5.423365\n",
      "ep 4112: ep_len:119 episode reward: total was 1.080000. running mean: -5.358332\n",
      "ep 4112: ep_len:119 episode reward: total was 4.560000. running mean: -5.259148\n",
      "ep 4112: ep_len:530 episode reward: total was -2.670000. running mean: -5.233257\n",
      "ep 4112: ep_len:324 episode reward: total was -3.210000. running mean: -5.213024\n",
      "epsilon:0.010000 episode_count: 28791. steps_count: 12703920.000000\n",
      "ep 4113: ep_len:585 episode reward: total was 13.090000. running mean: -5.029994\n",
      "ep 4113: ep_len:615 episode reward: total was -5.680000. running mean: -5.036494\n",
      "ep 4113: ep_len:404 episode reward: total was 9.760000. running mean: -4.888529\n",
      "ep 4113: ep_len:510 episode reward: total was -31.250000. running mean: -5.152144\n",
      "ep 4113: ep_len:45 episode reward: total was 2.510000. running mean: -5.075522\n",
      "ep 4113: ep_len:505 episode reward: total was 1.290000. running mean: -5.011867\n",
      "ep 4113: ep_len:292 episode reward: total was -0.210000. running mean: -4.963849\n",
      "epsilon:0.010000 episode_count: 28798. steps_count: 12706876.000000\n",
      "ep 4114: ep_len:505 episode reward: total was 3.140000. running mean: -4.882810\n",
      "ep 4114: ep_len:500 episode reward: total was -34.050000. running mean: -5.174482\n",
      "ep 4114: ep_len:575 episode reward: total was 7.460000. running mean: -5.048137\n",
      "ep 4114: ep_len:620 episode reward: total was 9.430000. running mean: -4.903356\n",
      "ep 4114: ep_len:38 episode reward: total was 2.000000. running mean: -4.834322\n",
      "ep 4114: ep_len:500 episode reward: total was 0.120000. running mean: -4.784779\n",
      "ep 4114: ep_len:310 episode reward: total was -4.750000. running mean: -4.784431\n",
      "epsilon:0.010000 episode_count: 28805. steps_count: 12709924.000000\n",
      "ep 4115: ep_len:590 episode reward: total was 9.960000. running mean: -4.636987\n",
      "ep 4115: ep_len:500 episode reward: total was 13.180000. running mean: -4.458817\n",
      "ep 4115: ep_len:500 episode reward: total was -13.010000. running mean: -4.544329\n",
      "ep 4115: ep_len:500 episode reward: total was -10.550000. running mean: -4.604386\n",
      "ep 4115: ep_len:55 episode reward: total was 5.010000. running mean: -4.508242\n",
      "ep 4115: ep_len:610 episode reward: total was -1.190000. running mean: -4.475059\n",
      "ep 4115: ep_len:580 episode reward: total was -14.450000. running mean: -4.574809\n",
      "epsilon:0.010000 episode_count: 28812. steps_count: 12713259.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4116: ep_len:545 episode reward: total was 10.720000. running mean: -4.421861\n",
      "ep 4116: ep_len:168 episode reward: total was -1.380000. running mean: -4.391442\n",
      "ep 4116: ep_len:79 episode reward: total was 3.070000. running mean: -4.316828\n",
      "ep 4116: ep_len:525 episode reward: total was 3.520000. running mean: -4.238459\n",
      "ep 4116: ep_len:98 episode reward: total was -8.470000. running mean: -4.280775\n",
      "ep 4116: ep_len:560 episode reward: total was 1.310000. running mean: -4.224867\n",
      "ep 4116: ep_len:500 episode reward: total was -15.570000. running mean: -4.338318\n",
      "epsilon:0.010000 episode_count: 28819. steps_count: 12715734.000000\n",
      "ep 4117: ep_len:126 episode reward: total was 6.110000. running mean: -4.233835\n",
      "ep 4117: ep_len:500 episode reward: total was 9.580000. running mean: -4.095697\n",
      "ep 4117: ep_len:625 episode reward: total was 6.230000. running mean: -3.992440\n",
      "ep 4117: ep_len:510 episode reward: total was 12.440000. running mean: -3.828115\n",
      "ep 4117: ep_len:3 episode reward: total was 0.000000. running mean: -3.789834\n",
      "ep 4117: ep_len:243 episode reward: total was 9.660000. running mean: -3.655336\n",
      "ep 4117: ep_len:605 episode reward: total was 2.000000. running mean: -3.598783\n",
      "epsilon:0.010000 episode_count: 28826. steps_count: 12718346.000000\n",
      "ep 4118: ep_len:570 episode reward: total was -13.070000. running mean: -3.693495\n",
      "ep 4118: ep_len:585 episode reward: total was 6.260000. running mean: -3.593960\n",
      "ep 4118: ep_len:745 episode reward: total was -15.080000. running mean: -3.708820\n",
      "ep 4118: ep_len:161 episode reward: total was 0.640000. running mean: -3.665332\n",
      "ep 4118: ep_len:3 episode reward: total was 0.000000. running mean: -3.628679\n",
      "ep 4118: ep_len:500 episode reward: total was -28.240000. running mean: -3.874792\n",
      "ep 4118: ep_len:560 episode reward: total was -31.980000. running mean: -4.155844\n",
      "epsilon:0.010000 episode_count: 28833. steps_count: 12721470.000000\n",
      "ep 4119: ep_len:620 episode reward: total was -17.260000. running mean: -4.286886\n",
      "ep 4119: ep_len:600 episode reward: total was 18.580000. running mean: -4.058217\n",
      "ep 4119: ep_len:690 episode reward: total was -17.730000. running mean: -4.194935\n",
      "ep 4119: ep_len:550 episode reward: total was -18.000000. running mean: -4.332985\n",
      "ep 4119: ep_len:93 episode reward: total was 2.520000. running mean: -4.264455\n",
      "ep 4119: ep_len:525 episode reward: total was -1.010000. running mean: -4.231911\n",
      "ep 4119: ep_len:189 episode reward: total was 4.710000. running mean: -4.142492\n",
      "epsilon:0.010000 episode_count: 28840. steps_count: 12724737.000000\n",
      "ep 4120: ep_len:500 episode reward: total was -25.420000. running mean: -4.355267\n",
      "ep 4120: ep_len:500 episode reward: total was 2.350000. running mean: -4.288214\n",
      "ep 4120: ep_len:615 episode reward: total was -13.980000. running mean: -4.385132\n",
      "ep 4120: ep_len:510 episode reward: total was 4.430000. running mean: -4.296981\n",
      "ep 4120: ep_len:111 episode reward: total was -6.940000. running mean: -4.323411\n",
      "ep 4120: ep_len:635 episode reward: total was -4.710000. running mean: -4.327277\n",
      "ep 4120: ep_len:505 episode reward: total was -3.030000. running mean: -4.314304\n",
      "epsilon:0.010000 episode_count: 28847. steps_count: 12728113.000000\n",
      "ep 4121: ep_len:214 episode reward: total was 2.130000. running mean: -4.249861\n",
      "ep 4121: ep_len:615 episode reward: total was 5.530000. running mean: -4.152062\n",
      "ep 4121: ep_len:640 episode reward: total was -3.760000. running mean: -4.148142\n",
      "ep 4121: ep_len:500 episode reward: total was -14.730000. running mean: -4.253960\n",
      "ep 4121: ep_len:90 episode reward: total was 0.530000. running mean: -4.206121\n",
      "ep 4121: ep_len:675 episode reward: total was -56.390000. running mean: -4.727959\n",
      "ep 4121: ep_len:580 episode reward: total was -2.390000. running mean: -4.704580\n",
      "epsilon:0.010000 episode_count: 28854. steps_count: 12731427.000000\n",
      "ep 4122: ep_len:525 episode reward: total was -15.780000. running mean: -4.815334\n",
      "ep 4122: ep_len:500 episode reward: total was 14.650000. running mean: -4.620681\n",
      "ep 4122: ep_len:500 episode reward: total was 12.960000. running mean: -4.444874\n",
      "ep 4122: ep_len:56 episode reward: total was -0.930000. running mean: -4.409725\n",
      "ep 4122: ep_len:3 episode reward: total was 0.000000. running mean: -4.365628\n",
      "ep 4122: ep_len:515 episode reward: total was 4.140000. running mean: -4.280572\n",
      "ep 4122: ep_len:306 episode reward: total was 3.770000. running mean: -4.200066\n",
      "epsilon:0.010000 episode_count: 28861. steps_count: 12733832.000000\n",
      "ep 4123: ep_len:610 episode reward: total was -4.440000. running mean: -4.202465\n",
      "ep 4123: ep_len:530 episode reward: total was -0.250000. running mean: -4.162941\n",
      "ep 4123: ep_len:550 episode reward: total was -20.320000. running mean: -4.324511\n",
      "ep 4123: ep_len:525 episode reward: total was 13.970000. running mean: -4.141566\n",
      "ep 4123: ep_len:3 episode reward: total was 0.000000. running mean: -4.100150\n",
      "ep 4123: ep_len:535 episode reward: total was -3.870000. running mean: -4.097849\n",
      "ep 4123: ep_len:555 episode reward: total was -42.100000. running mean: -4.477870\n",
      "epsilon:0.010000 episode_count: 28868. steps_count: 12737140.000000\n",
      "ep 4124: ep_len:560 episode reward: total was -25.550000. running mean: -4.688592\n",
      "ep 4124: ep_len:277 episode reward: total was -33.840000. running mean: -4.980106\n",
      "ep 4124: ep_len:545 episode reward: total was -8.500000. running mean: -5.015305\n",
      "ep 4124: ep_len:575 episode reward: total was 8.480000. running mean: -4.880352\n",
      "ep 4124: ep_len:48 episode reward: total was 4.500000. running mean: -4.786548\n",
      "ep 4124: ep_len:500 episode reward: total was -0.460000. running mean: -4.743283\n",
      "ep 4124: ep_len:165 episode reward: total was -0.900000. running mean: -4.704850\n",
      "epsilon:0.010000 episode_count: 28875. steps_count: 12739810.000000\n",
      "ep 4125: ep_len:505 episode reward: total was 9.950000. running mean: -4.558301\n",
      "ep 4125: ep_len:515 episode reward: total was 2.260000. running mean: -4.490118\n",
      "ep 4125: ep_len:595 episode reward: total was 0.170000. running mean: -4.443517\n",
      "ep 4125: ep_len:500 episode reward: total was -15.140000. running mean: -4.550482\n",
      "ep 4125: ep_len:3 episode reward: total was 0.000000. running mean: -4.504977\n",
      "ep 4125: ep_len:530 episode reward: total was -3.220000. running mean: -4.492127\n",
      "ep 4125: ep_len:540 episode reward: total was -6.540000. running mean: -4.512606\n",
      "epsilon:0.010000 episode_count: 28882. steps_count: 12742998.000000\n",
      "ep 4126: ep_len:550 episode reward: total was 14.020000. running mean: -4.327280\n",
      "ep 4126: ep_len:545 episode reward: total was 20.430000. running mean: -4.079707\n",
      "ep 4126: ep_len:505 episode reward: total was -13.430000. running mean: -4.173210\n",
      "ep 4126: ep_len:535 episode reward: total was 10.440000. running mean: -4.027078\n",
      "ep 4126: ep_len:3 episode reward: total was 0.000000. running mean: -3.986807\n",
      "ep 4126: ep_len:615 episode reward: total was 12.670000. running mean: -3.820239\n",
      "ep 4126: ep_len:525 episode reward: total was -3.860000. running mean: -3.820637\n",
      "epsilon:0.010000 episode_count: 28889. steps_count: 12746276.000000\n",
      "ep 4127: ep_len:545 episode reward: total was 9.100000. running mean: -3.691430\n",
      "ep 4127: ep_len:535 episode reward: total was -0.830000. running mean: -3.662816\n",
      "ep 4127: ep_len:580 episode reward: total was 0.980000. running mean: -3.616388\n",
      "ep 4127: ep_len:620 episode reward: total was 10.210000. running mean: -3.478124\n",
      "ep 4127: ep_len:3 episode reward: total was 0.000000. running mean: -3.443343\n",
      "ep 4127: ep_len:535 episode reward: total was 3.670000. running mean: -3.372209\n",
      "ep 4127: ep_len:505 episode reward: total was -13.020000. running mean: -3.468687\n",
      "epsilon:0.010000 episode_count: 28896. steps_count: 12749599.000000\n",
      "ep 4128: ep_len:525 episode reward: total was -10.730000. running mean: -3.541300\n",
      "ep 4128: ep_len:580 episode reward: total was -8.020000. running mean: -3.586087\n",
      "ep 4128: ep_len:745 episode reward: total was -61.850000. running mean: -4.168727\n",
      "ep 4128: ep_len:156 episode reward: total was 4.130000. running mean: -4.085739\n",
      "ep 4128: ep_len:50 episode reward: total was 4.510000. running mean: -3.999782\n",
      "ep 4128: ep_len:500 episode reward: total was 9.610000. running mean: -3.863684\n",
      "ep 4128: ep_len:585 episode reward: total was -13.490000. running mean: -3.959947\n",
      "epsilon:0.010000 episode_count: 28903. steps_count: 12752740.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4129: ep_len:505 episode reward: total was 1.120000. running mean: -3.909148\n",
      "ep 4129: ep_len:555 episode reward: total was -49.550000. running mean: -4.365556\n",
      "ep 4129: ep_len:510 episode reward: total was -2.090000. running mean: -4.342801\n",
      "ep 4129: ep_len:570 episode reward: total was 8.930000. running mean: -4.210073\n",
      "ep 4129: ep_len:127 episode reward: total was 7.060000. running mean: -4.097372\n",
      "ep 4129: ep_len:500 episode reward: total was 6.290000. running mean: -3.993498\n",
      "ep 4129: ep_len:525 episode reward: total was -3.400000. running mean: -3.987563\n",
      "epsilon:0.010000 episode_count: 28910. steps_count: 12756032.000000\n",
      "ep 4130: ep_len:585 episode reward: total was 15.040000. running mean: -3.797288\n",
      "ep 4130: ep_len:525 episode reward: total was 5.410000. running mean: -3.705215\n",
      "ep 4130: ep_len:530 episode reward: total was 1.970000. running mean: -3.648463\n",
      "ep 4130: ep_len:132 episode reward: total was 4.110000. running mean: -3.570878\n",
      "ep 4130: ep_len:98 episode reward: total was -10.460000. running mean: -3.639769\n",
      "ep 4130: ep_len:540 episode reward: total was -14.870000. running mean: -3.752072\n",
      "ep 4130: ep_len:292 episode reward: total was 3.250000. running mean: -3.682051\n",
      "epsilon:0.010000 episode_count: 28917. steps_count: 12758734.000000\n",
      "ep 4131: ep_len:134 episode reward: total was 4.590000. running mean: -3.599330\n",
      "ep 4131: ep_len:590 episode reward: total was -2.850000. running mean: -3.591837\n",
      "ep 4131: ep_len:451 episode reward: total was -8.720000. running mean: -3.643119\n",
      "ep 4131: ep_len:500 episode reward: total was -13.460000. running mean: -3.741287\n",
      "ep 4131: ep_len:115 episode reward: total was 4.020000. running mean: -3.663675\n",
      "ep 4131: ep_len:515 episode reward: total was 9.930000. running mean: -3.527738\n",
      "ep 4131: ep_len:505 episode reward: total was -4.970000. running mean: -3.542160\n",
      "epsilon:0.010000 episode_count: 28924. steps_count: 12761544.000000\n",
      "ep 4132: ep_len:500 episode reward: total was 11.980000. running mean: -3.386939\n",
      "ep 4132: ep_len:500 episode reward: total was 7.130000. running mean: -3.281770\n",
      "ep 4132: ep_len:605 episode reward: total was 7.530000. running mean: -3.173652\n",
      "ep 4132: ep_len:500 episode reward: total was -11.100000. running mean: -3.252915\n",
      "ep 4132: ep_len:3 episode reward: total was 0.000000. running mean: -3.220386\n",
      "ep 4132: ep_len:211 episode reward: total was 3.640000. running mean: -3.151782\n",
      "ep 4132: ep_len:182 episode reward: total was 0.170000. running mean: -3.118564\n",
      "epsilon:0.010000 episode_count: 28931. steps_count: 12764045.000000\n",
      "ep 4133: ep_len:500 episode reward: total was 12.330000. running mean: -2.964079\n",
      "ep 4133: ep_len:595 episode reward: total was -1.470000. running mean: -2.949138\n",
      "ep 4133: ep_len:361 episode reward: total was 0.680000. running mean: -2.912847\n",
      "ep 4133: ep_len:613 episode reward: total was -16.860000. running mean: -3.052318\n",
      "ep 4133: ep_len:91 episode reward: total was -8.940000. running mean: -3.111195\n",
      "ep 4133: ep_len:530 episode reward: total was -3.130000. running mean: -3.111383\n",
      "ep 4133: ep_len:625 episode reward: total was -20.200000. running mean: -3.282269\n",
      "epsilon:0.010000 episode_count: 28938. steps_count: 12767360.000000\n",
      "ep 4134: ep_len:500 episode reward: total was -11.340000. running mean: -3.362847\n",
      "ep 4134: ep_len:500 episode reward: total was 21.220000. running mean: -3.117018\n",
      "ep 4134: ep_len:79 episode reward: total was 4.080000. running mean: -3.045048\n",
      "ep 4134: ep_len:505 episode reward: total was 14.930000. running mean: -2.865297\n",
      "ep 4134: ep_len:127 episode reward: total was 4.550000. running mean: -2.791144\n",
      "ep 4134: ep_len:530 episode reward: total was 1.600000. running mean: -2.747233\n",
      "ep 4134: ep_len:500 episode reward: total was 4.000000. running mean: -2.679761\n",
      "epsilon:0.010000 episode_count: 28945. steps_count: 12770101.000000\n",
      "ep 4135: ep_len:525 episode reward: total was -2.090000. running mean: -2.673863\n",
      "ep 4135: ep_len:795 episode reward: total was -100.580000. running mean: -3.652924\n",
      "ep 4135: ep_len:620 episode reward: total was -28.720000. running mean: -3.903595\n",
      "ep 4135: ep_len:610 episode reward: total was 16.510000. running mean: -3.699459\n",
      "ep 4135: ep_len:62 episode reward: total was -5.970000. running mean: -3.722165\n",
      "ep 4135: ep_len:550 episode reward: total was 1.520000. running mean: -3.669743\n",
      "ep 4135: ep_len:595 episode reward: total was -1.930000. running mean: -3.652346\n",
      "epsilon:0.010000 episode_count: 28952. steps_count: 12773858.000000\n",
      "ep 4136: ep_len:545 episode reward: total was 18.500000. running mean: -3.430822\n",
      "ep 4136: ep_len:520 episode reward: total was -2.350000. running mean: -3.420014\n",
      "ep 4136: ep_len:630 episode reward: total was -21.200000. running mean: -3.597814\n",
      "ep 4136: ep_len:56 episode reward: total was 0.550000. running mean: -3.556336\n",
      "ep 4136: ep_len:3 episode reward: total was 0.000000. running mean: -3.520772\n",
      "ep 4136: ep_len:500 episode reward: total was -1.920000. running mean: -3.504765\n",
      "ep 4136: ep_len:585 episode reward: total was -31.080000. running mean: -3.780517\n",
      "epsilon:0.010000 episode_count: 28959. steps_count: 12776697.000000\n",
      "ep 4137: ep_len:575 episode reward: total was -23.420000. running mean: -3.976912\n",
      "ep 4137: ep_len:610 episode reward: total was 3.930000. running mean: -3.897843\n",
      "ep 4137: ep_len:630 episode reward: total was 8.560000. running mean: -3.773264\n",
      "ep 4137: ep_len:515 episode reward: total was -8.960000. running mean: -3.825132\n",
      "ep 4137: ep_len:3 episode reward: total was 0.000000. running mean: -3.786880\n",
      "ep 4137: ep_len:500 episode reward: total was -10.380000. running mean: -3.852811\n",
      "ep 4137: ep_len:555 episode reward: total was 8.560000. running mean: -3.728683\n",
      "epsilon:0.010000 episode_count: 28966. steps_count: 12780085.000000\n",
      "ep 4138: ep_len:218 episode reward: total was -4.380000. running mean: -3.735196\n",
      "ep 4138: ep_len:500 episode reward: total was 13.670000. running mean: -3.561144\n",
      "ep 4138: ep_len:500 episode reward: total was -33.930000. running mean: -3.864833\n",
      "ep 4138: ep_len:104 episode reward: total was 5.090000. running mean: -3.775285\n",
      "ep 4138: ep_len:3 episode reward: total was 0.000000. running mean: -3.737532\n",
      "ep 4138: ep_len:675 episode reward: total was 5.440000. running mean: -3.645757\n",
      "ep 4138: ep_len:560 episode reward: total was -16.010000. running mean: -3.769399\n",
      "epsilon:0.010000 episode_count: 28973. steps_count: 12782645.000000\n",
      "ep 4139: ep_len:500 episode reward: total was -10.850000. running mean: -3.840205\n",
      "ep 4139: ep_len:500 episode reward: total was -1.200000. running mean: -3.813803\n",
      "ep 4139: ep_len:620 episode reward: total was -37.290000. running mean: -4.148565\n",
      "ep 4139: ep_len:575 episode reward: total was 21.000000. running mean: -3.897079\n",
      "ep 4139: ep_len:109 episode reward: total was 6.040000. running mean: -3.797708\n",
      "ep 4139: ep_len:570 episode reward: total was -2.020000. running mean: -3.779931\n",
      "ep 4139: ep_len:555 episode reward: total was -1.860000. running mean: -3.760732\n",
      "epsilon:0.010000 episode_count: 28980. steps_count: 12786074.000000\n",
      "ep 4140: ep_len:570 episode reward: total was -9.660000. running mean: -3.819725\n",
      "ep 4140: ep_len:373 episode reward: total was -15.330000. running mean: -3.934828\n",
      "ep 4140: ep_len:625 episode reward: total was 7.080000. running mean: -3.824679\n",
      "ep 4140: ep_len:560 episode reward: total was 2.450000. running mean: -3.761932\n",
      "ep 4140: ep_len:127 episode reward: total was 6.050000. running mean: -3.663813\n",
      "ep 4140: ep_len:640 episode reward: total was -16.460000. running mean: -3.791775\n",
      "ep 4140: ep_len:600 episode reward: total was -20.920000. running mean: -3.963057\n",
      "epsilon:0.010000 episode_count: 28987. steps_count: 12789569.000000\n",
      "ep 4141: ep_len:216 episode reward: total was -6.400000. running mean: -3.987427\n",
      "ep 4141: ep_len:525 episode reward: total was -33.990000. running mean: -4.287452\n",
      "ep 4141: ep_len:605 episode reward: total was -2.530000. running mean: -4.269878\n",
      "ep 4141: ep_len:585 episode reward: total was 1.960000. running mean: -4.207579\n",
      "ep 4141: ep_len:3 episode reward: total was 0.000000. running mean: -4.165503\n",
      "ep 4141: ep_len:329 episode reward: total was 4.240000. running mean: -4.081448\n",
      "ep 4141: ep_len:595 episode reward: total was -16.500000. running mean: -4.205634\n",
      "epsilon:0.010000 episode_count: 28994. steps_count: 12792427.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4142: ep_len:520 episode reward: total was 7.640000. running mean: -4.087177\n",
      "ep 4142: ep_len:520 episode reward: total was 20.960000. running mean: -3.836706\n",
      "ep 4142: ep_len:570 episode reward: total was -27.820000. running mean: -4.076539\n",
      "ep 4142: ep_len:515 episode reward: total was 14.120000. running mean: -3.894573\n",
      "ep 4142: ep_len:3 episode reward: total was 0.000000. running mean: -3.855627\n",
      "ep 4142: ep_len:615 episode reward: total was -37.850000. running mean: -4.195571\n",
      "ep 4142: ep_len:515 episode reward: total was -3.410000. running mean: -4.187716\n",
      "epsilon:0.010000 episode_count: 29001. steps_count: 12795685.000000\n",
      "ep 4143: ep_len:179 episode reward: total was -2.930000. running mean: -4.175138\n",
      "ep 4143: ep_len:545 episode reward: total was 0.210000. running mean: -4.131287\n",
      "ep 4143: ep_len:393 episode reward: total was -1.780000. running mean: -4.107774\n",
      "ep 4143: ep_len:500 episode reward: total was 6.880000. running mean: -3.997896\n",
      "ep 4143: ep_len:3 episode reward: total was 0.000000. running mean: -3.957917\n",
      "ep 4143: ep_len:224 episode reward: total was 8.150000. running mean: -3.836838\n",
      "ep 4143: ep_len:540 episode reward: total was -8.020000. running mean: -3.878670\n",
      "epsilon:0.010000 episode_count: 29008. steps_count: 12798069.000000\n",
      "ep 4144: ep_len:540 episode reward: total was 18.030000. running mean: -3.659583\n",
      "ep 4144: ep_len:530 episode reward: total was 5.260000. running mean: -3.570387\n",
      "ep 4144: ep_len:500 episode reward: total was 12.090000. running mean: -3.413783\n",
      "ep 4144: ep_len:500 episode reward: total was -10.520000. running mean: -3.484846\n",
      "ep 4144: ep_len:3 episode reward: total was 0.000000. running mean: -3.449997\n",
      "ep 4144: ep_len:236 episode reward: total was 9.190000. running mean: -3.323597\n",
      "ep 4144: ep_len:182 episode reward: total was 2.160000. running mean: -3.268761\n",
      "epsilon:0.010000 episode_count: 29015. steps_count: 12800560.000000\n",
      "ep 4145: ep_len:500 episode reward: total was 16.370000. running mean: -3.072374\n",
      "ep 4145: ep_len:350 episode reward: total was -11.340000. running mean: -3.155050\n",
      "ep 4145: ep_len:500 episode reward: total was 0.370000. running mean: -3.119799\n",
      "ep 4145: ep_len:641 episode reward: total was -27.360000. running mean: -3.362201\n",
      "ep 4145: ep_len:3 episode reward: total was 0.000000. running mean: -3.328579\n",
      "ep 4145: ep_len:590 episode reward: total was -30.120000. running mean: -3.596494\n",
      "ep 4145: ep_len:615 episode reward: total was -15.310000. running mean: -3.713629\n",
      "epsilon:0.010000 episode_count: 29022. steps_count: 12803759.000000\n",
      "ep 4146: ep_len:132 episode reward: total was -0.430000. running mean: -3.680792\n",
      "ep 4146: ep_len:520 episode reward: total was 26.960000. running mean: -3.374384\n",
      "ep 4146: ep_len:565 episode reward: total was -16.250000. running mean: -3.503141\n",
      "ep 4146: ep_len:500 episode reward: total was -15.980000. running mean: -3.627909\n",
      "ep 4146: ep_len:3 episode reward: total was 0.000000. running mean: -3.591630\n",
      "ep 4146: ep_len:605 episode reward: total was 0.460000. running mean: -3.551114\n",
      "ep 4146: ep_len:505 episode reward: total was -8.840000. running mean: -3.604003\n",
      "epsilon:0.010000 episode_count: 29029. steps_count: 12806589.000000\n",
      "ep 4147: ep_len:505 episode reward: total was -12.410000. running mean: -3.692063\n",
      "ep 4147: ep_len:500 episode reward: total was 12.690000. running mean: -3.528242\n",
      "ep 4147: ep_len:630 episode reward: total was -11.070000. running mean: -3.603660\n",
      "ep 4147: ep_len:515 episode reward: total was -9.480000. running mean: -3.662423\n",
      "ep 4147: ep_len:3 episode reward: total was 0.000000. running mean: -3.625799\n",
      "ep 4147: ep_len:515 episode reward: total was -13.260000. running mean: -3.722141\n",
      "ep 4147: ep_len:500 episode reward: total was -37.530000. running mean: -4.060219\n",
      "epsilon:0.010000 episode_count: 29036. steps_count: 12809757.000000\n",
      "ep 4148: ep_len:545 episode reward: total was 2.030000. running mean: -3.999317\n",
      "ep 4148: ep_len:560 episode reward: total was 22.910000. running mean: -3.730224\n",
      "ep 4148: ep_len:451 episode reward: total was 9.290000. running mean: -3.600022\n",
      "ep 4148: ep_len:580 episode reward: total was -47.160000. running mean: -4.035622\n",
      "ep 4148: ep_len:3 episode reward: total was 0.000000. running mean: -3.995265\n",
      "ep 4148: ep_len:186 episode reward: total was 6.140000. running mean: -3.893913\n",
      "ep 4148: ep_len:610 episode reward: total was -10.850000. running mean: -3.963474\n",
      "epsilon:0.010000 episode_count: 29043. steps_count: 12812692.000000\n",
      "ep 4149: ep_len:216 episode reward: total was 3.610000. running mean: -3.887739\n",
      "ep 4149: ep_len:580 episode reward: total was -2.370000. running mean: -3.872561\n",
      "ep 4149: ep_len:750 episode reward: total was -26.170000. running mean: -4.095536\n",
      "ep 4149: ep_len:509 episode reward: total was -27.950000. running mean: -4.334080\n",
      "ep 4149: ep_len:3 episode reward: total was 0.000000. running mean: -4.290740\n",
      "ep 4149: ep_len:570 episode reward: total was 7.590000. running mean: -4.171932\n",
      "ep 4149: ep_len:199 episode reward: total was -4.360000. running mean: -4.173813\n",
      "epsilon:0.010000 episode_count: 29050. steps_count: 12815519.000000\n",
      "ep 4150: ep_len:265 episode reward: total was 5.640000. running mean: -4.075675\n",
      "ep 4150: ep_len:500 episode reward: total was 13.690000. running mean: -3.898018\n",
      "ep 4150: ep_len:605 episode reward: total was -4.520000. running mean: -3.904238\n",
      "ep 4150: ep_len:510 episode reward: total was -32.700000. running mean: -4.192195\n",
      "ep 4150: ep_len:80 episode reward: total was 0.050000. running mean: -4.149774\n",
      "ep 4150: ep_len:575 episode reward: total was -1.860000. running mean: -4.126876\n",
      "ep 4150: ep_len:645 episode reward: total was -41.890000. running mean: -4.504507\n",
      "epsilon:0.010000 episode_count: 29057. steps_count: 12818699.000000\n",
      "ep 4151: ep_len:655 episode reward: total was -5.620000. running mean: -4.515662\n",
      "ep 4151: ep_len:760 episode reward: total was -24.630000. running mean: -4.716805\n",
      "ep 4151: ep_len:645 episode reward: total was -3.160000. running mean: -4.701237\n",
      "ep 4151: ep_len:860 episode reward: total was -71.320000. running mean: -5.367425\n",
      "ep 4151: ep_len:3 episode reward: total was 0.000000. running mean: -5.313751\n",
      "ep 4151: ep_len:670 episode reward: total was 5.400000. running mean: -5.206613\n",
      "ep 4151: ep_len:261 episode reward: total was -8.340000. running mean: -5.237947\n",
      "epsilon:0.010000 episode_count: 29064. steps_count: 12822553.000000\n",
      "ep 4152: ep_len:510 episode reward: total was -20.150000. running mean: -5.387068\n",
      "ep 4152: ep_len:500 episode reward: total was 17.210000. running mean: -5.161097\n",
      "ep 4152: ep_len:595 episode reward: total was -30.800000. running mean: -5.417486\n",
      "ep 4152: ep_len:520 episode reward: total was -7.970000. running mean: -5.443011\n",
      "ep 4152: ep_len:95 episode reward: total was 2.530000. running mean: -5.363281\n",
      "ep 4152: ep_len:645 episode reward: total was 3.920000. running mean: -5.270448\n",
      "ep 4152: ep_len:525 episode reward: total was 3.550000. running mean: -5.182244\n",
      "epsilon:0.010000 episode_count: 29071. steps_count: 12825943.000000\n",
      "ep 4153: ep_len:635 episode reward: total was -3.060000. running mean: -5.161021\n",
      "ep 4153: ep_len:575 episode reward: total was 12.890000. running mean: -4.980511\n",
      "ep 4153: ep_len:565 episode reward: total was -13.510000. running mean: -5.065806\n",
      "ep 4153: ep_len:500 episode reward: total was 10.510000. running mean: -4.910048\n",
      "ep 4153: ep_len:3 episode reward: total was 0.000000. running mean: -4.860947\n",
      "ep 4153: ep_len:515 episode reward: total was -4.050000. running mean: -4.852838\n",
      "ep 4153: ep_len:610 episode reward: total was -3.500000. running mean: -4.839309\n",
      "epsilon:0.010000 episode_count: 29078. steps_count: 12829346.000000\n",
      "ep 4154: ep_len:640 episode reward: total was 4.690000. running mean: -4.744016\n",
      "ep 4154: ep_len:505 episode reward: total was 2.900000. running mean: -4.667576\n",
      "ep 4154: ep_len:500 episode reward: total was -10.350000. running mean: -4.724400\n",
      "ep 4154: ep_len:565 episode reward: total was 16.420000. running mean: -4.512956\n",
      "ep 4154: ep_len:56 episode reward: total was 5.500000. running mean: -4.412827\n",
      "ep 4154: ep_len:500 episode reward: total was -3.290000. running mean: -4.401599\n",
      "ep 4154: ep_len:610 episode reward: total was -16.460000. running mean: -4.522183\n",
      "epsilon:0.010000 episode_count: 29085. steps_count: 12832722.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4155: ep_len:535 episode reward: total was 13.950000. running mean: -4.337461\n",
      "ep 4155: ep_len:525 episode reward: total was 6.720000. running mean: -4.226886\n",
      "ep 4155: ep_len:660 episode reward: total was -8.120000. running mean: -4.265817\n",
      "ep 4155: ep_len:516 episode reward: total was -0.390000. running mean: -4.227059\n",
      "ep 4155: ep_len:3 episode reward: total was 0.000000. running mean: -4.184789\n",
      "ep 4155: ep_len:680 episode reward: total was 13.440000. running mean: -4.008541\n",
      "ep 4155: ep_len:500 episode reward: total was -51.290000. running mean: -4.481355\n",
      "epsilon:0.010000 episode_count: 29092. steps_count: 12836141.000000\n",
      "ep 4156: ep_len:725 episode reward: total was -20.600000. running mean: -4.642542\n",
      "ep 4156: ep_len:610 episode reward: total was -12.240000. running mean: -4.718516\n",
      "ep 4156: ep_len:620 episode reward: total was -1.340000. running mean: -4.684731\n",
      "ep 4156: ep_len:550 episode reward: total was 6.930000. running mean: -4.568584\n",
      "ep 4156: ep_len:3 episode reward: total was 0.000000. running mean: -4.522898\n",
      "ep 4156: ep_len:505 episode reward: total was -33.980000. running mean: -4.817469\n",
      "ep 4156: ep_len:500 episode reward: total was -21.340000. running mean: -4.982694\n",
      "epsilon:0.010000 episode_count: 29099. steps_count: 12839654.000000\n",
      "ep 4157: ep_len:545 episode reward: total was -3.650000. running mean: -4.969367\n",
      "ep 4157: ep_len:585 episode reward: total was 26.430000. running mean: -4.655374\n",
      "ep 4157: ep_len:570 episode reward: total was 4.420000. running mean: -4.564620\n",
      "ep 4157: ep_len:500 episode reward: total was 6.880000. running mean: -4.450174\n",
      "ep 4157: ep_len:3 episode reward: total was 0.000000. running mean: -4.405672\n",
      "ep 4157: ep_len:500 episode reward: total was -1.780000. running mean: -4.379415\n",
      "ep 4157: ep_len:535 episode reward: total was -30.480000. running mean: -4.640421\n",
      "epsilon:0.010000 episode_count: 29106. steps_count: 12842892.000000\n",
      "ep 4158: ep_len:555 episode reward: total was 0.150000. running mean: -4.592517\n",
      "ep 4158: ep_len:605 episode reward: total was 15.670000. running mean: -4.389892\n",
      "ep 4158: ep_len:650 episode reward: total was 6.720000. running mean: -4.278793\n",
      "ep 4158: ep_len:154 episode reward: total was -11.360000. running mean: -4.349605\n",
      "ep 4158: ep_len:3 episode reward: total was 0.000000. running mean: -4.306109\n",
      "ep 4158: ep_len:224 episode reward: total was 3.620000. running mean: -4.226848\n",
      "ep 4158: ep_len:640 episode reward: total was -6.750000. running mean: -4.252079\n",
      "epsilon:0.010000 episode_count: 29113. steps_count: 12845723.000000\n",
      "ep 4159: ep_len:605 episode reward: total was -111.930000. running mean: -5.328859\n",
      "ep 4159: ep_len:810 episode reward: total was -86.140000. running mean: -6.136970\n",
      "ep 4159: ep_len:600 episode reward: total was -1.020000. running mean: -6.085800\n",
      "ep 4159: ep_len:500 episode reward: total was 13.400000. running mean: -5.890942\n",
      "ep 4159: ep_len:3 episode reward: total was 0.000000. running mean: -5.832033\n",
      "ep 4159: ep_len:630 episode reward: total was -10.060000. running mean: -5.874312\n",
      "ep 4159: ep_len:500 episode reward: total was -41.740000. running mean: -6.232969\n",
      "epsilon:0.010000 episode_count: 29120. steps_count: 12849371.000000\n",
      "ep 4160: ep_len:500 episode reward: total was 7.800000. running mean: -6.092640\n",
      "ep 4160: ep_len:500 episode reward: total was 14.210000. running mean: -5.889613\n",
      "ep 4160: ep_len:595 episode reward: total was -10.840000. running mean: -5.939117\n",
      "ep 4160: ep_len:114 episode reward: total was -0.900000. running mean: -5.888726\n",
      "ep 4160: ep_len:3 episode reward: total was 0.000000. running mean: -5.829839\n",
      "ep 4160: ep_len:660 episode reward: total was 0.380000. running mean: -5.767740\n",
      "ep 4160: ep_len:505 episode reward: total was -27.590000. running mean: -5.985963\n",
      "epsilon:0.010000 episode_count: 29127. steps_count: 12852248.000000\n",
      "ep 4161: ep_len:500 episode reward: total was 5.290000. running mean: -5.873203\n",
      "ep 4161: ep_len:605 episode reward: total was 27.920000. running mean: -5.535271\n",
      "ep 4161: ep_len:530 episode reward: total was -9.850000. running mean: -5.578419\n",
      "ep 4161: ep_len:500 episode reward: total was -8.040000. running mean: -5.603034\n",
      "ep 4161: ep_len:129 episode reward: total was 6.570000. running mean: -5.481304\n",
      "ep 4161: ep_len:505 episode reward: total was -12.290000. running mean: -5.549391\n",
      "ep 4161: ep_len:211 episode reward: total was -5.860000. running mean: -5.552497\n",
      "epsilon:0.010000 episode_count: 29134. steps_count: 12855228.000000\n",
      "ep 4162: ep_len:525 episode reward: total was -10.820000. running mean: -5.605172\n",
      "ep 4162: ep_len:500 episode reward: total was 3.070000. running mean: -5.518420\n",
      "ep 4162: ep_len:405 episode reward: total was -9.300000. running mean: -5.556236\n",
      "ep 4162: ep_len:133 episode reward: total was 2.600000. running mean: -5.474674\n",
      "ep 4162: ep_len:55 episode reward: total was -8.980000. running mean: -5.509727\n",
      "ep 4162: ep_len:241 episode reward: total was 8.130000. running mean: -5.373330\n",
      "ep 4162: ep_len:197 episode reward: total was -0.350000. running mean: -5.323097\n",
      "epsilon:0.010000 episode_count: 29141. steps_count: 12857284.000000\n",
      "ep 4163: ep_len:640 episode reward: total was 3.670000. running mean: -5.233166\n",
      "ep 4163: ep_len:600 episode reward: total was -22.300000. running mean: -5.403834\n",
      "ep 4163: ep_len:79 episode reward: total was 0.040000. running mean: -5.349396\n",
      "ep 4163: ep_len:500 episode reward: total was 9.010000. running mean: -5.205802\n",
      "ep 4163: ep_len:119 episode reward: total was 5.050000. running mean: -5.103244\n",
      "ep 4163: ep_len:525 episode reward: total was -2.880000. running mean: -5.081011\n",
      "ep 4163: ep_len:580 episode reward: total was -4.790000. running mean: -5.078101\n",
      "epsilon:0.010000 episode_count: 29148. steps_count: 12860327.000000\n",
      "ep 4164: ep_len:248 episode reward: total was 6.640000. running mean: -4.960920\n",
      "ep 4164: ep_len:350 episode reward: total was -4.820000. running mean: -4.959511\n",
      "ep 4164: ep_len:630 episode reward: total was 2.920000. running mean: -4.880716\n",
      "ep 4164: ep_len:126 episode reward: total was 3.580000. running mean: -4.796109\n",
      "ep 4164: ep_len:112 episode reward: total was 7.550000. running mean: -4.672647\n",
      "ep 4164: ep_len:245 episode reward: total was 7.620000. running mean: -4.549721\n",
      "ep 4164: ep_len:500 episode reward: total was -26.620000. running mean: -4.770424\n",
      "epsilon:0.010000 episode_count: 29155. steps_count: 12862538.000000\n",
      "ep 4165: ep_len:200 episode reward: total was -2.380000. running mean: -4.746520\n",
      "ep 4165: ep_len:540 episode reward: total was 20.940000. running mean: -4.489654\n",
      "ep 4165: ep_len:67 episode reward: total was -0.960000. running mean: -4.454358\n",
      "ep 4165: ep_len:560 episode reward: total was 5.360000. running mean: -4.356214\n",
      "ep 4165: ep_len:3 episode reward: total was 0.000000. running mean: -4.312652\n",
      "ep 4165: ep_len:141 episode reward: total was 4.610000. running mean: -4.223426\n",
      "ep 4165: ep_len:545 episode reward: total was -6.590000. running mean: -4.247091\n",
      "epsilon:0.010000 episode_count: 29162. steps_count: 12864594.000000\n",
      "ep 4166: ep_len:500 episode reward: total was 14.000000. running mean: -4.064620\n",
      "ep 4166: ep_len:620 episode reward: total was -8.810000. running mean: -4.112074\n",
      "ep 4166: ep_len:500 episode reward: total was 8.920000. running mean: -3.981753\n",
      "ep 4166: ep_len:521 episode reward: total was 4.070000. running mean: -3.901236\n",
      "ep 4166: ep_len:74 episode reward: total was -8.950000. running mean: -3.951724\n",
      "ep 4166: ep_len:640 episode reward: total was -2.050000. running mean: -3.932706\n",
      "ep 4166: ep_len:525 episode reward: total was -6.320000. running mean: -3.956579\n",
      "epsilon:0.010000 episode_count: 29169. steps_count: 12867974.000000\n",
      "ep 4167: ep_len:515 episode reward: total was 1.690000. running mean: -3.900113\n",
      "ep 4167: ep_len:650 episode reward: total was -65.110000. running mean: -4.512212\n",
      "ep 4167: ep_len:565 episode reward: total was 3.950000. running mean: -4.427590\n",
      "ep 4167: ep_len:115 episode reward: total was 3.130000. running mean: -4.352014\n",
      "ep 4167: ep_len:94 episode reward: total was 4.540000. running mean: -4.263094\n",
      "ep 4167: ep_len:500 episode reward: total was -29.810000. running mean: -4.518563\n",
      "ep 4167: ep_len:500 episode reward: total was -10.230000. running mean: -4.575678\n",
      "epsilon:0.010000 episode_count: 29176. steps_count: 12870913.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4168: ep_len:555 episode reward: total was -0.080000. running mean: -4.530721\n",
      "ep 4168: ep_len:515 episode reward: total was 20.370000. running mean: -4.281714\n",
      "ep 4168: ep_len:74 episode reward: total was -0.460000. running mean: -4.243496\n",
      "ep 4168: ep_len:525 episode reward: total was 10.890000. running mean: -4.092162\n",
      "ep 4168: ep_len:3 episode reward: total was 0.000000. running mean: -4.051240\n",
      "ep 4168: ep_len:525 episode reward: total was -6.580000. running mean: -4.076527\n",
      "ep 4168: ep_len:575 episode reward: total was -3.270000. running mean: -4.068462\n",
      "epsilon:0.010000 episode_count: 29183. steps_count: 12873685.000000\n",
      "ep 4169: ep_len:545 episode reward: total was -25.700000. running mean: -4.284778\n",
      "ep 4169: ep_len:500 episode reward: total was -15.490000. running mean: -4.396830\n",
      "ep 4169: ep_len:620 episode reward: total was 0.710000. running mean: -4.345762\n",
      "ep 4169: ep_len:530 episode reward: total was 4.040000. running mean: -4.261904\n",
      "ep 4169: ep_len:3 episode reward: total was 0.000000. running mean: -4.219285\n",
      "ep 4169: ep_len:710 episode reward: total was -4.770000. running mean: -4.224792\n",
      "ep 4169: ep_len:600 episode reward: total was -17.390000. running mean: -4.356444\n",
      "epsilon:0.010000 episode_count: 29190. steps_count: 12877193.000000\n",
      "ep 4170: ep_len:555 episode reward: total was 4.910000. running mean: -4.263780\n",
      "ep 4170: ep_len:500 episode reward: total was -9.320000. running mean: -4.314342\n",
      "ep 4170: ep_len:575 episode reward: total was -10.340000. running mean: -4.374598\n",
      "ep 4170: ep_len:520 episode reward: total was -20.660000. running mean: -4.537452\n",
      "ep 4170: ep_len:99 episode reward: total was 5.040000. running mean: -4.441678\n",
      "ep 4170: ep_len:500 episode reward: total was -24.690000. running mean: -4.644161\n",
      "ep 4170: ep_len:620 episode reward: total was -17.550000. running mean: -4.773220\n",
      "epsilon:0.010000 episode_count: 29197. steps_count: 12880562.000000\n",
      "ep 4171: ep_len:540 episode reward: total was -10.930000. running mean: -4.834787\n",
      "ep 4171: ep_len:610 episode reward: total was -1.790000. running mean: -4.804339\n",
      "ep 4171: ep_len:590 episode reward: total was -2.070000. running mean: -4.776996\n",
      "ep 4171: ep_len:525 episode reward: total was -10.990000. running mean: -4.839126\n",
      "ep 4171: ep_len:131 episode reward: total was 6.060000. running mean: -4.730135\n",
      "ep 4171: ep_len:564 episode reward: total was -39.520000. running mean: -5.078034\n",
      "ep 4171: ep_len:540 episode reward: total was -9.400000. running mean: -5.121253\n",
      "epsilon:0.010000 episode_count: 29204. steps_count: 12884062.000000\n",
      "ep 4172: ep_len:545 episode reward: total was 11.620000. running mean: -4.953841\n",
      "ep 4172: ep_len:500 episode reward: total was -2.650000. running mean: -4.930802\n",
      "ep 4172: ep_len:845 episode reward: total was -106.130000. running mean: -5.942794\n",
      "ep 4172: ep_len:500 episode reward: total was -10.500000. running mean: -5.988366\n",
      "ep 4172: ep_len:3 episode reward: total was 0.000000. running mean: -5.928483\n",
      "ep 4172: ep_len:505 episode reward: total was -2.290000. running mean: -5.892098\n",
      "ep 4172: ep_len:500 episode reward: total was 2.820000. running mean: -5.804977\n",
      "epsilon:0.010000 episode_count: 29211. steps_count: 12887460.000000\n",
      "ep 4173: ep_len:545 episode reward: total was 5.150000. running mean: -5.695427\n",
      "ep 4173: ep_len:530 episode reward: total was -8.790000. running mean: -5.726373\n",
      "ep 4173: ep_len:645 episode reward: total was -37.970000. running mean: -6.048809\n",
      "ep 4173: ep_len:56 episode reward: total was -0.930000. running mean: -5.997621\n",
      "ep 4173: ep_len:3 episode reward: total was 0.000000. running mean: -5.937645\n",
      "ep 4173: ep_len:580 episode reward: total was -2.550000. running mean: -5.903768\n",
      "ep 4173: ep_len:510 episode reward: total was -29.080000. running mean: -6.135531\n",
      "epsilon:0.010000 episode_count: 29218. steps_count: 12890329.000000\n",
      "ep 4174: ep_len:500 episode reward: total was 8.920000. running mean: -5.984975\n",
      "ep 4174: ep_len:605 episode reward: total was -58.600000. running mean: -6.511126\n",
      "ep 4174: ep_len:620 episode reward: total was 1.020000. running mean: -6.435814\n",
      "ep 4174: ep_len:515 episode reward: total was 7.600000. running mean: -6.295456\n",
      "ep 4174: ep_len:97 episode reward: total was 5.040000. running mean: -6.182102\n",
      "ep 4174: ep_len:615 episode reward: total was -5.530000. running mean: -6.175581\n",
      "ep 4174: ep_len:590 episode reward: total was -17.500000. running mean: -6.288825\n",
      "epsilon:0.010000 episode_count: 29225. steps_count: 12893871.000000\n",
      "ep 4175: ep_len:585 episode reward: total was 13.970000. running mean: -6.086237\n",
      "ep 4175: ep_len:865 episode reward: total was -69.870000. running mean: -6.724074\n",
      "ep 4175: ep_len:79 episode reward: total was 0.040000. running mean: -6.656433\n",
      "ep 4175: ep_len:505 episode reward: total was 9.950000. running mean: -6.490369\n",
      "ep 4175: ep_len:3 episode reward: total was 0.000000. running mean: -6.425465\n",
      "ep 4175: ep_len:515 episode reward: total was -6.420000. running mean: -6.425411\n",
      "ep 4175: ep_len:590 episode reward: total was -4.970000. running mean: -6.410857\n",
      "epsilon:0.010000 episode_count: 29232. steps_count: 12897013.000000\n",
      "ep 4176: ep_len:590 episode reward: total was 19.470000. running mean: -6.152048\n",
      "ep 4176: ep_len:520 episode reward: total was -8.060000. running mean: -6.171128\n",
      "ep 4176: ep_len:500 episode reward: total was 7.250000. running mean: -6.036916\n",
      "ep 4176: ep_len:56 episode reward: total was -0.930000. running mean: -5.985847\n",
      "ep 4176: ep_len:82 episode reward: total was -0.470000. running mean: -5.930689\n",
      "ep 4176: ep_len:640 episode reward: total was -1.470000. running mean: -5.886082\n",
      "ep 4176: ep_len:540 episode reward: total was -6.940000. running mean: -5.896621\n",
      "epsilon:0.010000 episode_count: 29239. steps_count: 12899941.000000\n",
      "ep 4177: ep_len:600 episode reward: total was -24.500000. running mean: -6.082655\n",
      "ep 4177: ep_len:620 episode reward: total was 7.980000. running mean: -5.942028\n",
      "ep 4177: ep_len:755 episode reward: total was -38.810000. running mean: -6.270708\n",
      "ep 4177: ep_len:615 episode reward: total was 14.960000. running mean: -6.058401\n",
      "ep 4177: ep_len:3 episode reward: total was 0.000000. running mean: -5.997817\n",
      "ep 4177: ep_len:500 episode reward: total was -11.970000. running mean: -6.057539\n",
      "ep 4177: ep_len:575 episode reward: total was -5.400000. running mean: -6.050963\n",
      "epsilon:0.010000 episode_count: 29246. steps_count: 12903609.000000\n",
      "ep 4178: ep_len:530 episode reward: total was -11.760000. running mean: -6.108054\n",
      "ep 4178: ep_len:560 episode reward: total was 23.920000. running mean: -5.807773\n",
      "ep 4178: ep_len:605 episode reward: total was 9.520000. running mean: -5.654495\n",
      "ep 4178: ep_len:500 episode reward: total was 3.890000. running mean: -5.559050\n",
      "ep 4178: ep_len:78 episode reward: total was -8.970000. running mean: -5.593160\n",
      "ep 4178: ep_len:700 episode reward: total was -38.360000. running mean: -5.920828\n",
      "ep 4178: ep_len:590 episode reward: total was 0.010000. running mean: -5.861520\n",
      "epsilon:0.010000 episode_count: 29253. steps_count: 12907172.000000\n",
      "ep 4179: ep_len:500 episode reward: total was 10.310000. running mean: -5.699805\n",
      "ep 4179: ep_len:570 episode reward: total was -27.030000. running mean: -5.913107\n",
      "ep 4179: ep_len:535 episode reward: total was 4.980000. running mean: -5.804176\n",
      "ep 4179: ep_len:500 episode reward: total was 9.040000. running mean: -5.655734\n",
      "ep 4179: ep_len:3 episode reward: total was 0.000000. running mean: -5.599177\n",
      "ep 4179: ep_len:500 episode reward: total was -2.230000. running mean: -5.565485\n",
      "ep 4179: ep_len:520 episode reward: total was -2.020000. running mean: -5.530030\n",
      "epsilon:0.010000 episode_count: 29260. steps_count: 12910300.000000\n",
      "ep 4180: ep_len:560 episode reward: total was 1.460000. running mean: -5.460130\n",
      "ep 4180: ep_len:580 episode reward: total was 20.400000. running mean: -5.201528\n",
      "ep 4180: ep_len:570 episode reward: total was -19.760000. running mean: -5.347113\n",
      "ep 4180: ep_len:530 episode reward: total was 13.950000. running mean: -5.154142\n",
      "ep 4180: ep_len:3 episode reward: total was 0.000000. running mean: -5.102601\n",
      "ep 4180: ep_len:312 episode reward: total was 4.150000. running mean: -5.010075\n",
      "ep 4180: ep_len:324 episode reward: total was -50.380000. running mean: -5.463774\n",
      "epsilon:0.010000 episode_count: 29267. steps_count: 12913179.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4181: ep_len:580 episode reward: total was -5.010000. running mean: -5.459236\n",
      "ep 4181: ep_len:183 episode reward: total was -2.890000. running mean: -5.433544\n",
      "ep 4181: ep_len:580 episode reward: total was 1.990000. running mean: -5.359308\n",
      "ep 4181: ep_len:419 episode reward: total was 2.890000. running mean: -5.276815\n",
      "ep 4181: ep_len:3 episode reward: total was 0.000000. running mean: -5.224047\n",
      "ep 4181: ep_len:520 episode reward: total was -0.210000. running mean: -5.173907\n",
      "ep 4181: ep_len:540 episode reward: total was -2.530000. running mean: -5.147468\n",
      "epsilon:0.010000 episode_count: 29274. steps_count: 12916004.000000\n",
      "ep 4182: ep_len:580 episode reward: total was -46.110000. running mean: -5.557093\n",
      "ep 4182: ep_len:270 episode reward: total was -35.870000. running mean: -5.860222\n",
      "ep 4182: ep_len:463 episode reward: total was 2.300000. running mean: -5.778620\n",
      "ep 4182: ep_len:615 episode reward: total was -27.660000. running mean: -5.997434\n",
      "ep 4182: ep_len:3 episode reward: total was 0.000000. running mean: -5.937459\n",
      "ep 4182: ep_len:505 episode reward: total was -25.940000. running mean: -6.137485\n",
      "ep 4182: ep_len:615 episode reward: total was -24.390000. running mean: -6.320010\n",
      "epsilon:0.010000 episode_count: 29281. steps_count: 12919055.000000\n",
      "ep 4183: ep_len:585 episode reward: total was -71.710000. running mean: -6.973910\n",
      "ep 4183: ep_len:540 episode reward: total was -1.900000. running mean: -6.923171\n",
      "ep 4183: ep_len:321 episode reward: total was -1.740000. running mean: -6.871339\n",
      "ep 4183: ep_len:530 episode reward: total was -19.600000. running mean: -6.998625\n",
      "ep 4183: ep_len:3 episode reward: total was 0.000000. running mean: -6.928639\n",
      "ep 4183: ep_len:620 episode reward: total was 1.460000. running mean: -6.844753\n",
      "ep 4183: ep_len:575 episode reward: total was -16.920000. running mean: -6.945505\n",
      "epsilon:0.010000 episode_count: 29288. steps_count: 12922229.000000\n",
      "ep 4184: ep_len:575 episode reward: total was 7.980000. running mean: -6.796250\n",
      "ep 4184: ep_len:500 episode reward: total was 12.190000. running mean: -6.606388\n",
      "ep 4184: ep_len:378 episode reward: total was -12.760000. running mean: -6.667924\n",
      "ep 4184: ep_len:520 episode reward: total was -17.580000. running mean: -6.777045\n",
      "ep 4184: ep_len:3 episode reward: total was 0.000000. running mean: -6.709274\n",
      "ep 4184: ep_len:500 episode reward: total was -29.740000. running mean: -6.939581\n",
      "ep 4184: ep_len:278 episode reward: total was -2.290000. running mean: -6.893086\n",
      "epsilon:0.010000 episode_count: 29295. steps_count: 12924983.000000\n",
      "ep 4185: ep_len:505 episode reward: total was -30.400000. running mean: -7.128155\n",
      "ep 4185: ep_len:370 episode reward: total was -5.790000. running mean: -7.114773\n",
      "ep 4185: ep_len:62 episode reward: total was 3.530000. running mean: -7.008325\n",
      "ep 4185: ep_len:500 episode reward: total was -13.060000. running mean: -7.068842\n",
      "ep 4185: ep_len:3 episode reward: total was 0.000000. running mean: -6.998154\n",
      "ep 4185: ep_len:620 episode reward: total was -25.510000. running mean: -7.183272\n",
      "ep 4185: ep_len:500 episode reward: total was 4.400000. running mean: -7.067440\n",
      "epsilon:0.010000 episode_count: 29302. steps_count: 12927543.000000\n",
      "ep 4186: ep_len:134 episode reward: total was 4.590000. running mean: -6.950865\n",
      "ep 4186: ep_len:525 episode reward: total was 21.890000. running mean: -6.662456\n",
      "ep 4186: ep_len:690 episode reward: total was -5.580000. running mean: -6.651632\n",
      "ep 4186: ep_len:550 episode reward: total was -28.080000. running mean: -6.865916\n",
      "ep 4186: ep_len:3 episode reward: total was 0.000000. running mean: -6.797256\n",
      "ep 4186: ep_len:510 episode reward: total was -6.080000. running mean: -6.790084\n",
      "ep 4186: ep_len:555 episode reward: total was 1.520000. running mean: -6.706983\n",
      "epsilon:0.010000 episode_count: 29309. steps_count: 12930510.000000\n",
      "ep 4187: ep_len:500 episode reward: total was 8.790000. running mean: -6.552013\n",
      "ep 4187: ep_len:500 episode reward: total was -0.510000. running mean: -6.491593\n",
      "ep 4187: ep_len:505 episode reward: total was -4.940000. running mean: -6.476077\n",
      "ep 4187: ep_len:610 episode reward: total was 13.510000. running mean: -6.276216\n",
      "ep 4187: ep_len:124 episode reward: total was 6.560000. running mean: -6.147854\n",
      "ep 4187: ep_len:244 episode reward: total was -8.800000. running mean: -6.174376\n",
      "ep 4187: ep_len:520 episode reward: total was -18.020000. running mean: -6.292832\n",
      "epsilon:0.010000 episode_count: 29316. steps_count: 12933513.000000\n",
      "ep 4188: ep_len:500 episode reward: total was -12.400000. running mean: -6.353904\n",
      "ep 4188: ep_len:610 episode reward: total was -2.600000. running mean: -6.316365\n",
      "ep 4188: ep_len:570 episode reward: total was -32.400000. running mean: -6.577201\n",
      "ep 4188: ep_len:532 episode reward: total was 8.590000. running mean: -6.425529\n",
      "ep 4188: ep_len:3 episode reward: total was 0.000000. running mean: -6.361274\n",
      "ep 4188: ep_len:530 episode reward: total was -4.200000. running mean: -6.339661\n",
      "ep 4188: ep_len:590 episode reward: total was -20.900000. running mean: -6.485264\n",
      "epsilon:0.010000 episode_count: 29323. steps_count: 12936848.000000\n",
      "ep 4189: ep_len:625 episode reward: total was -13.240000. running mean: -6.552812\n",
      "ep 4189: ep_len:200 episode reward: total was 1.160000. running mean: -6.475684\n",
      "ep 4189: ep_len:500 episode reward: total was -8.510000. running mean: -6.496027\n",
      "ep 4189: ep_len:500 episode reward: total was -1.580000. running mean: -6.446866\n",
      "ep 4189: ep_len:3 episode reward: total was 0.000000. running mean: -6.382398\n",
      "ep 4189: ep_len:510 episode reward: total was -55.260000. running mean: -6.871174\n",
      "ep 4189: ep_len:585 episode reward: total was -8.960000. running mean: -6.892062\n",
      "epsilon:0.010000 episode_count: 29330. steps_count: 12939771.000000\n",
      "ep 4190: ep_len:650 episode reward: total was -13.970000. running mean: -6.962841\n",
      "ep 4190: ep_len:500 episode reward: total was 1.300000. running mean: -6.880213\n",
      "ep 4190: ep_len:920 episode reward: total was -29.760000. running mean: -7.109011\n",
      "ep 4190: ep_len:522 episode reward: total was -4.390000. running mean: -7.081821\n",
      "ep 4190: ep_len:81 episode reward: total was -8.960000. running mean: -7.100603\n",
      "ep 4190: ep_len:515 episode reward: total was -5.210000. running mean: -7.081697\n",
      "ep 4190: ep_len:282 episode reward: total was -6.830000. running mean: -7.079180\n",
      "epsilon:0.010000 episode_count: 29337. steps_count: 12943241.000000\n",
      "ep 4191: ep_len:206 episode reward: total was -16.840000. running mean: -7.176788\n",
      "ep 4191: ep_len:500 episode reward: total was -30.060000. running mean: -7.405620\n",
      "ep 4191: ep_len:605 episode reward: total was -36.800000. running mean: -7.699564\n",
      "ep 4191: ep_len:48 episode reward: total was -0.950000. running mean: -7.632068\n",
      "ep 4191: ep_len:3 episode reward: total was 0.000000. running mean: -7.555747\n",
      "ep 4191: ep_len:500 episode reward: total was -3.220000. running mean: -7.512390\n",
      "ep 4191: ep_len:279 episode reward: total was -40.910000. running mean: -7.846366\n",
      "epsilon:0.010000 episode_count: 29344. steps_count: 12945382.000000\n",
      "ep 4192: ep_len:510 episode reward: total was -8.940000. running mean: -7.857302\n",
      "ep 4192: ep_len:530 episode reward: total was 14.390000. running mean: -7.634829\n",
      "ep 4192: ep_len:570 episode reward: total was -7.700000. running mean: -7.635481\n",
      "ep 4192: ep_len:36 episode reward: total was -0.970000. running mean: -7.568826\n",
      "ep 4192: ep_len:3 episode reward: total was 0.000000. running mean: -7.493138\n",
      "ep 4192: ep_len:235 episode reward: total was 4.170000. running mean: -7.376507\n",
      "ep 4192: ep_len:565 episode reward: total was -7.880000. running mean: -7.381541\n",
      "epsilon:0.010000 episode_count: 29351. steps_count: 12947831.000000\n",
      "ep 4193: ep_len:620 episode reward: total was 3.630000. running mean: -7.271426\n",
      "ep 4193: ep_len:565 episode reward: total was 0.390000. running mean: -7.194812\n",
      "ep 4193: ep_len:570 episode reward: total was 1.680000. running mean: -7.106064\n",
      "ep 4193: ep_len:595 episode reward: total was 14.490000. running mean: -6.890103\n",
      "ep 4193: ep_len:3 episode reward: total was 0.000000. running mean: -6.821202\n",
      "ep 4193: ep_len:525 episode reward: total was 3.640000. running mean: -6.716590\n",
      "ep 4193: ep_len:192 episode reward: total was -4.370000. running mean: -6.693124\n",
      "epsilon:0.010000 episode_count: 29358. steps_count: 12950901.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4194: ep_len:131 episode reward: total was 3.090000. running mean: -6.595293\n",
      "ep 4194: ep_len:515 episode reward: total was -4.470000. running mean: -6.574040\n",
      "ep 4194: ep_len:765 episode reward: total was -42.660000. running mean: -6.934900\n",
      "ep 4194: ep_len:510 episode reward: total was 12.960000. running mean: -6.735951\n",
      "ep 4194: ep_len:47 episode reward: total was 3.000000. running mean: -6.638591\n",
      "ep 4194: ep_len:530 episode reward: total was 1.140000. running mean: -6.560805\n",
      "ep 4194: ep_len:695 episode reward: total was -44.870000. running mean: -6.943897\n",
      "epsilon:0.010000 episode_count: 29365. steps_count: 12954094.000000\n",
      "ep 4195: ep_len:890 episode reward: total was -92.920000. running mean: -7.803658\n",
      "ep 4195: ep_len:165 episode reward: total was -6.380000. running mean: -7.789422\n",
      "ep 4195: ep_len:650 episode reward: total was -15.180000. running mean: -7.863327\n",
      "ep 4195: ep_len:500 episode reward: total was -18.090000. running mean: -7.965594\n",
      "ep 4195: ep_len:53 episode reward: total was 5.000000. running mean: -7.835938\n",
      "ep 4195: ep_len:530 episode reward: total was -36.500000. running mean: -8.122579\n",
      "ep 4195: ep_len:550 episode reward: total was -5.060000. running mean: -8.091953\n",
      "epsilon:0.010000 episode_count: 29372. steps_count: 12957432.000000\n",
      "ep 4196: ep_len:665 episode reward: total was -11.140000. running mean: -8.122433\n",
      "ep 4196: ep_len:505 episode reward: total was 2.210000. running mean: -8.019109\n",
      "ep 4196: ep_len:525 episode reward: total was 3.940000. running mean: -7.899518\n",
      "ep 4196: ep_len:500 episode reward: total was 11.090000. running mean: -7.709623\n",
      "ep 4196: ep_len:3 episode reward: total was 0.000000. running mean: -7.632527\n",
      "ep 4196: ep_len:570 episode reward: total was 4.990000. running mean: -7.506301\n",
      "ep 4196: ep_len:272 episode reward: total was -14.800000. running mean: -7.579238\n",
      "epsilon:0.010000 episode_count: 29379. steps_count: 12960472.000000\n",
      "ep 4197: ep_len:218 episode reward: total was 5.600000. running mean: -7.447446\n",
      "ep 4197: ep_len:605 episode reward: total was 14.380000. running mean: -7.229171\n",
      "ep 4197: ep_len:620 episode reward: total was 5.550000. running mean: -7.101380\n",
      "ep 4197: ep_len:505 episode reward: total was 8.070000. running mean: -6.949666\n",
      "ep 4197: ep_len:3 episode reward: total was 0.000000. running mean: -6.880169\n",
      "ep 4197: ep_len:640 episode reward: total was -18.460000. running mean: -6.995968\n",
      "ep 4197: ep_len:515 episode reward: total was -12.970000. running mean: -7.055708\n",
      "epsilon:0.010000 episode_count: 29386. steps_count: 12963578.000000\n",
      "ep 4198: ep_len:500 episode reward: total was -32.790000. running mean: -7.313051\n",
      "ep 4198: ep_len:500 episode reward: total was -1.930000. running mean: -7.259220\n",
      "ep 4198: ep_len:560 episode reward: total was -7.740000. running mean: -7.264028\n",
      "ep 4198: ep_len:530 episode reward: total was -11.520000. running mean: -7.306588\n",
      "ep 4198: ep_len:103 episode reward: total was 6.550000. running mean: -7.168022\n",
      "ep 4198: ep_len:515 episode reward: total was -6.410000. running mean: -7.160442\n",
      "ep 4198: ep_len:285 episode reward: total was -9.290000. running mean: -7.181737\n",
      "epsilon:0.010000 episode_count: 29393. steps_count: 12966571.000000\n",
      "ep 4199: ep_len:520 episode reward: total was -15.010000. running mean: -7.260020\n",
      "ep 4199: ep_len:600 episode reward: total was 0.730000. running mean: -7.180120\n",
      "ep 4199: ep_len:500 episode reward: total was -15.000000. running mean: -7.258319\n",
      "ep 4199: ep_len:56 episode reward: total was -1.940000. running mean: -7.205135\n",
      "ep 4199: ep_len:38 episode reward: total was 2.500000. running mean: -7.108084\n",
      "ep 4199: ep_len:500 episode reward: total was 4.270000. running mean: -6.994303\n",
      "ep 4199: ep_len:500 episode reward: total was -14.450000. running mean: -7.068860\n",
      "epsilon:0.010000 episode_count: 29400. steps_count: 12969285.000000\n",
      "ep 4200: ep_len:102 episode reward: total was 0.590000. running mean: -6.992272\n",
      "ep 4200: ep_len:575 episode reward: total was 14.730000. running mean: -6.775049\n",
      "ep 4200: ep_len:575 episode reward: total was -7.220000. running mean: -6.779498\n",
      "ep 4200: ep_len:132 episode reward: total was 2.110000. running mean: -6.690603\n",
      "ep 4200: ep_len:3 episode reward: total was 0.000000. running mean: -6.623697\n",
      "ep 4200: ep_len:640 episode reward: total was -0.510000. running mean: -6.562560\n",
      "ep 4200: ep_len:339 episode reward: total was -4.210000. running mean: -6.539035\n",
      "epsilon:0.010000 episode_count: 29407. steps_count: 12971651.000000\n",
      "ep 4201: ep_len:500 episode reward: total was -13.060000. running mean: -6.604244\n",
      "ep 4201: ep_len:500 episode reward: total was -0.890000. running mean: -6.547102\n",
      "ep 4201: ep_len:595 episode reward: total was 5.080000. running mean: -6.430831\n",
      "ep 4201: ep_len:585 episode reward: total was 2.940000. running mean: -6.337123\n",
      "ep 4201: ep_len:3 episode reward: total was 0.000000. running mean: -6.273751\n",
      "ep 4201: ep_len:334 episode reward: total was 2.690000. running mean: -6.184114\n",
      "ep 4201: ep_len:500 episode reward: total was -24.270000. running mean: -6.364973\n",
      "epsilon:0.010000 episode_count: 29414. steps_count: 12974668.000000\n",
      "ep 4202: ep_len:630 episode reward: total was -9.280000. running mean: -6.394123\n",
      "ep 4202: ep_len:695 episode reward: total was -5.170000. running mean: -6.381882\n",
      "ep 4202: ep_len:350 episode reward: total was 0.210000. running mean: -6.315963\n",
      "ep 4202: ep_len:610 episode reward: total was 14.170000. running mean: -6.111103\n",
      "ep 4202: ep_len:3 episode reward: total was 0.000000. running mean: -6.049992\n",
      "ep 4202: ep_len:500 episode reward: total was -1.920000. running mean: -6.008692\n",
      "ep 4202: ep_len:605 episode reward: total was -14.870000. running mean: -6.097305\n",
      "epsilon:0.010000 episode_count: 29421. steps_count: 12978061.000000\n",
      "ep 4203: ep_len:570 episode reward: total was 5.920000. running mean: -5.977132\n",
      "ep 4203: ep_len:500 episode reward: total was 3.580000. running mean: -5.881561\n",
      "ep 4203: ep_len:440 episode reward: total was -0.720000. running mean: -5.829945\n",
      "ep 4203: ep_len:500 episode reward: total was -5.520000. running mean: -5.826846\n",
      "ep 4203: ep_len:81 episode reward: total was 0.510000. running mean: -5.763478\n",
      "ep 4203: ep_len:605 episode reward: total was 10.650000. running mean: -5.599343\n",
      "ep 4203: ep_len:350 episode reward: total was -48.400000. running mean: -6.027349\n",
      "epsilon:0.010000 episode_count: 29428. steps_count: 12981107.000000\n",
      "ep 4204: ep_len:665 episode reward: total was -13.160000. running mean: -6.098676\n",
      "ep 4204: ep_len:535 episode reward: total was 3.570000. running mean: -6.001989\n",
      "ep 4204: ep_len:545 episode reward: total was -0.190000. running mean: -5.943869\n",
      "ep 4204: ep_len:540 episode reward: total was -16.990000. running mean: -6.054331\n",
      "ep 4204: ep_len:3 episode reward: total was 0.000000. running mean: -5.993787\n",
      "ep 4204: ep_len:227 episode reward: total was 7.120000. running mean: -5.862649\n",
      "ep 4204: ep_len:510 episode reward: total was -18.440000. running mean: -5.988423\n",
      "epsilon:0.010000 episode_count: 29435. steps_count: 12984132.000000\n",
      "ep 4205: ep_len:500 episode reward: total was -12.080000. running mean: -6.049339\n",
      "ep 4205: ep_len:570 episode reward: total was -0.550000. running mean: -5.994345\n",
      "ep 4205: ep_len:505 episode reward: total was -10.300000. running mean: -6.037402\n",
      "ep 4205: ep_len:505 episode reward: total was -0.970000. running mean: -5.986728\n",
      "ep 4205: ep_len:3 episode reward: total was 0.000000. running mean: -5.926860\n",
      "ep 4205: ep_len:540 episode reward: total was -12.350000. running mean: -5.991092\n",
      "ep 4205: ep_len:565 episode reward: total was -16.180000. running mean: -6.092981\n",
      "epsilon:0.010000 episode_count: 29442. steps_count: 12987320.000000\n",
      "ep 4206: ep_len:265 episode reward: total was -6.390000. running mean: -6.095951\n",
      "ep 4206: ep_len:525 episode reward: total was -15.840000. running mean: -6.193392\n",
      "ep 4206: ep_len:560 episode reward: total was -10.190000. running mean: -6.233358\n",
      "ep 4206: ep_len:129 episode reward: total was 0.110000. running mean: -6.169924\n",
      "ep 4206: ep_len:3 episode reward: total was 0.000000. running mean: -6.108225\n",
      "ep 4206: ep_len:525 episode reward: total was -53.240000. running mean: -6.579543\n",
      "ep 4206: ep_len:630 episode reward: total was -19.130000. running mean: -6.705047\n",
      "epsilon:0.010000 episode_count: 29449. steps_count: 12989957.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4207: ep_len:560 episode reward: total was -20.960000. running mean: -6.847597\n",
      "ep 4207: ep_len:324 episode reward: total was -35.820000. running mean: -7.137321\n",
      "ep 4207: ep_len:630 episode reward: total was -15.270000. running mean: -7.218648\n",
      "ep 4207: ep_len:515 episode reward: total was 9.040000. running mean: -7.056061\n",
      "ep 4207: ep_len:87 episode reward: total was -12.500000. running mean: -7.110500\n",
      "ep 4207: ep_len:645 episode reward: total was -1.850000. running mean: -7.057895\n",
      "ep 4207: ep_len:630 episode reward: total was -15.580000. running mean: -7.143117\n",
      "epsilon:0.010000 episode_count: 29456. steps_count: 12993348.000000\n",
      "ep 4208: ep_len:715 episode reward: total was -23.220000. running mean: -7.303885\n",
      "ep 4208: ep_len:555 episode reward: total was 1.500000. running mean: -7.215847\n",
      "ep 4208: ep_len:605 episode reward: total was -8.550000. running mean: -7.229188\n",
      "ep 4208: ep_len:500 episode reward: total was -16.720000. running mean: -7.324096\n",
      "ep 4208: ep_len:69 episode reward: total was 4.010000. running mean: -7.210755\n",
      "ep 4208: ep_len:311 episode reward: total was 1.180000. running mean: -7.126848\n",
      "ep 4208: ep_len:600 episode reward: total was -56.100000. running mean: -7.616579\n",
      "epsilon:0.010000 episode_count: 29463. steps_count: 12996703.000000\n",
      "ep 4209: ep_len:500 episode reward: total was -3.280000. running mean: -7.573213\n",
      "ep 4209: ep_len:530 episode reward: total was -15.820000. running mean: -7.655681\n",
      "ep 4209: ep_len:630 episode reward: total was -6.880000. running mean: -7.647924\n",
      "ep 4209: ep_len:132 episode reward: total was 4.110000. running mean: -7.530345\n",
      "ep 4209: ep_len:110 episode reward: total was 3.020000. running mean: -7.424842\n",
      "ep 4209: ep_len:610 episode reward: total was -16.250000. running mean: -7.513093\n",
      "ep 4209: ep_len:515 episode reward: total was -9.590000. running mean: -7.533862\n",
      "epsilon:0.010000 episode_count: 29470. steps_count: 12999730.000000\n",
      "ep 4210: ep_len:115 episode reward: total was 0.550000. running mean: -7.453024\n",
      "ep 4210: ep_len:268 episode reward: total was -8.850000. running mean: -7.466994\n",
      "ep 4210: ep_len:540 episode reward: total was -8.970000. running mean: -7.482024\n",
      "ep 4210: ep_len:535 episode reward: total was -10.500000. running mean: -7.512203\n",
      "ep 4210: ep_len:3 episode reward: total was 0.000000. running mean: -7.437081\n",
      "ep 4210: ep_len:500 episode reward: total was -26.790000. running mean: -7.630611\n",
      "ep 4210: ep_len:292 episode reward: total was -3.300000. running mean: -7.587304\n",
      "epsilon:0.010000 episode_count: 29477. steps_count: 13001983.000000\n",
      "ep 4211: ep_len:195 episode reward: total was -18.360000. running mean: -7.695031\n",
      "ep 4211: ep_len:500 episode reward: total was -39.070000. running mean: -8.008781\n",
      "ep 4211: ep_len:535 episode reward: total was -8.260000. running mean: -8.011293\n",
      "ep 4211: ep_len:570 episode reward: total was 13.490000. running mean: -7.796280\n",
      "ep 4211: ep_len:48 episode reward: total was 4.500000. running mean: -7.673318\n",
      "ep 4211: ep_len:675 episode reward: total was -25.720000. running mean: -7.853784\n",
      "ep 4211: ep_len:535 episode reward: total was -11.840000. running mean: -7.893646\n",
      "epsilon:0.010000 episode_count: 29484. steps_count: 13005041.000000\n",
      "ep 4212: ep_len:510 episode reward: total was 5.420000. running mean: -7.760510\n",
      "ep 4212: ep_len:600 episode reward: total was 1.470000. running mean: -7.668205\n",
      "ep 4212: ep_len:555 episode reward: total was -22.060000. running mean: -7.812123\n",
      "ep 4212: ep_len:164 episode reward: total was 5.140000. running mean: -7.682602\n",
      "ep 4212: ep_len:89 episode reward: total was 2.020000. running mean: -7.585576\n",
      "ep 4212: ep_len:525 episode reward: total was -6.430000. running mean: -7.574020\n",
      "ep 4212: ep_len:336 episode reward: total was -0.720000. running mean: -7.505480\n",
      "epsilon:0.010000 episode_count: 29491. steps_count: 13007820.000000\n",
      "ep 4213: ep_len:265 episode reward: total was -2.870000. running mean: -7.459125\n",
      "ep 4213: ep_len:500 episode reward: total was -18.520000. running mean: -7.569734\n",
      "ep 4213: ep_len:550 episode reward: total was -45.510000. running mean: -7.949136\n",
      "ep 4213: ep_len:55 episode reward: total was -0.930000. running mean: -7.878945\n",
      "ep 4213: ep_len:3 episode reward: total was 0.000000. running mean: -7.800155\n",
      "ep 4213: ep_len:560 episode reward: total was 3.530000. running mean: -7.686854\n",
      "ep 4213: ep_len:500 episode reward: total was -13.090000. running mean: -7.740885\n",
      "epsilon:0.010000 episode_count: 29498. steps_count: 13010253.000000\n",
      "ep 4214: ep_len:975 episode reward: total was -84.690000. running mean: -8.510377\n",
      "ep 4214: ep_len:284 episode reward: total was -8.370000. running mean: -8.508973\n",
      "ep 4214: ep_len:555 episode reward: total was -11.430000. running mean: -8.538183\n",
      "ep 4214: ep_len:580 episode reward: total was 11.920000. running mean: -8.333601\n",
      "ep 4214: ep_len:120 episode reward: total was 6.050000. running mean: -8.189765\n",
      "ep 4214: ep_len:590 episode reward: total was -27.770000. running mean: -8.385568\n",
      "ep 4214: ep_len:600 episode reward: total was -17.120000. running mean: -8.472912\n",
      "epsilon:0.010000 episode_count: 29505. steps_count: 13013957.000000\n",
      "ep 4215: ep_len:530 episode reward: total was -14.080000. running mean: -8.528983\n",
      "ep 4215: ep_len:550 episode reward: total was -11.780000. running mean: -8.561493\n",
      "ep 4215: ep_len:540 episode reward: total was -17.420000. running mean: -8.650078\n",
      "ep 4215: ep_len:500 episode reward: total was -24.670000. running mean: -8.810277\n",
      "ep 4215: ep_len:3 episode reward: total was 0.000000. running mean: -8.722174\n",
      "ep 4215: ep_len:545 episode reward: total was -17.290000. running mean: -8.807853\n",
      "ep 4215: ep_len:575 episode reward: total was -6.060000. running mean: -8.780374\n",
      "epsilon:0.010000 episode_count: 29512. steps_count: 13017200.000000\n",
      "ep 4216: ep_len:116 episode reward: total was 1.050000. running mean: -8.682070\n",
      "ep 4216: ep_len:500 episode reward: total was -25.660000. running mean: -8.851850\n",
      "ep 4216: ep_len:570 episode reward: total was -13.760000. running mean: -8.900931\n",
      "ep 4216: ep_len:500 episode reward: total was -10.030000. running mean: -8.912222\n",
      "ep 4216: ep_len:3 episode reward: total was 0.000000. running mean: -8.823100\n",
      "ep 4216: ep_len:580 episode reward: total was -8.230000. running mean: -8.817169\n",
      "ep 4216: ep_len:575 episode reward: total was -23.580000. running mean: -8.964797\n",
      "epsilon:0.010000 episode_count: 29519. steps_count: 13020044.000000\n",
      "ep 4217: ep_len:550 episode reward: total was -10.080000. running mean: -8.975949\n",
      "ep 4217: ep_len:600 episode reward: total was -19.850000. running mean: -9.084690\n",
      "ep 4217: ep_len:575 episode reward: total was -1.370000. running mean: -9.007543\n",
      "ep 4217: ep_len:500 episode reward: total was -3.680000. running mean: -8.954267\n",
      "ep 4217: ep_len:3 episode reward: total was 0.000000. running mean: -8.864725\n",
      "ep 4217: ep_len:615 episode reward: total was -9.570000. running mean: -8.871777\n",
      "ep 4217: ep_len:600 episode reward: total was -9.550000. running mean: -8.878560\n",
      "epsilon:0.010000 episode_count: 29526. steps_count: 13023487.000000\n",
      "ep 4218: ep_len:195 episode reward: total was -0.900000. running mean: -8.798774\n",
      "ep 4218: ep_len:530 episode reward: total was -6.950000. running mean: -8.780286\n",
      "ep 4218: ep_len:560 episode reward: total was -5.180000. running mean: -8.744283\n",
      "ep 4218: ep_len:510 episode reward: total was -0.520000. running mean: -8.662041\n",
      "ep 4218: ep_len:3 episode reward: total was 0.000000. running mean: -8.575420\n",
      "ep 4218: ep_len:550 episode reward: total was -40.690000. running mean: -8.896566\n",
      "ep 4218: ep_len:555 episode reward: total was -10.600000. running mean: -8.913600\n",
      "epsilon:0.010000 episode_count: 29533. steps_count: 13026390.000000\n",
      "ep 4219: ep_len:227 episode reward: total was 1.610000. running mean: -8.808364\n",
      "ep 4219: ep_len:585 episode reward: total was -6.640000. running mean: -8.786681\n",
      "ep 4219: ep_len:79 episode reward: total was 1.050000. running mean: -8.688314\n",
      "ep 4219: ep_len:500 episode reward: total was 6.570000. running mean: -8.535731\n",
      "ep 4219: ep_len:102 episode reward: total was 1.500000. running mean: -8.435373\n",
      "ep 4219: ep_len:620 episode reward: total was -0.620000. running mean: -8.357220\n",
      "ep 4219: ep_len:705 episode reward: total was -43.900000. running mean: -8.712647\n",
      "epsilon:0.010000 episode_count: 29540. steps_count: 13029208.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4220: ep_len:510 episode reward: total was -3.450000. running mean: -8.660021\n",
      "ep 4220: ep_len:515 episode reward: total was 3.670000. running mean: -8.536721\n",
      "ep 4220: ep_len:545 episode reward: total was 0.330000. running mean: -8.448054\n",
      "ep 4220: ep_len:500 episode reward: total was -14.510000. running mean: -8.508673\n",
      "ep 4220: ep_len:82 episode reward: total was 3.020000. running mean: -8.393386\n",
      "ep 4220: ep_len:500 episode reward: total was -9.780000. running mean: -8.407252\n",
      "ep 4220: ep_len:540 episode reward: total was -6.590000. running mean: -8.389080\n",
      "epsilon:0.010000 episode_count: 29547. steps_count: 13032400.000000\n",
      "ep 4221: ep_len:625 episode reward: total was -13.980000. running mean: -8.444989\n",
      "ep 4221: ep_len:770 episode reward: total was -63.440000. running mean: -8.994939\n",
      "ep 4221: ep_len:615 episode reward: total was -3.930000. running mean: -8.944290\n",
      "ep 4221: ep_len:535 episode reward: total was 6.590000. running mean: -8.788947\n",
      "ep 4221: ep_len:3 episode reward: total was 0.000000. running mean: -8.701057\n",
      "ep 4221: ep_len:186 episode reward: total was 9.140000. running mean: -8.522647\n",
      "ep 4221: ep_len:500 episode reward: total was -4.590000. running mean: -8.483320\n",
      "epsilon:0.010000 episode_count: 29554. steps_count: 13035634.000000\n",
      "ep 4222: ep_len:645 episode reward: total was -19.260000. running mean: -8.591087\n",
      "ep 4222: ep_len:650 episode reward: total was -26.300000. running mean: -8.768176\n",
      "ep 4222: ep_len:595 episode reward: total was -8.490000. running mean: -8.765395\n",
      "ep 4222: ep_len:590 episode reward: total was 4.030000. running mean: -8.637441\n",
      "ep 4222: ep_len:3 episode reward: total was 0.000000. running mean: -8.551066\n",
      "ep 4222: ep_len:705 episode reward: total was -30.770000. running mean: -8.773256\n",
      "ep 4222: ep_len:500 episode reward: total was -7.350000. running mean: -8.759023\n",
      "epsilon:0.010000 episode_count: 29561. steps_count: 13039322.000000\n",
      "ep 4223: ep_len:500 episode reward: total was -12.090000. running mean: -8.792333\n",
      "ep 4223: ep_len:570 episode reward: total was 10.550000. running mean: -8.598909\n",
      "ep 4223: ep_len:450 episode reward: total was -15.730000. running mean: -8.670220\n",
      "ep 4223: ep_len:520 episode reward: total was -1.140000. running mean: -8.594918\n",
      "ep 4223: ep_len:71 episode reward: total was 4.540000. running mean: -8.463569\n",
      "ep 4223: ep_len:176 episode reward: total was 6.150000. running mean: -8.317433\n",
      "ep 4223: ep_len:625 episode reward: total was -1.760000. running mean: -8.251859\n",
      "epsilon:0.010000 episode_count: 29568. steps_count: 13042234.000000\n",
      "ep 4224: ep_len:500 episode reward: total was -19.910000. running mean: -8.368440\n",
      "ep 4224: ep_len:500 episode reward: total was -3.550000. running mean: -8.320256\n",
      "ep 4224: ep_len:655 episode reward: total was -3.140000. running mean: -8.268453\n",
      "ep 4224: ep_len:500 episode reward: total was -10.000000. running mean: -8.285769\n",
      "ep 4224: ep_len:3 episode reward: total was 0.000000. running mean: -8.202911\n",
      "ep 4224: ep_len:253 episode reward: total was 5.670000. running mean: -8.064182\n",
      "ep 4224: ep_len:500 episode reward: total was -28.180000. running mean: -8.265340\n",
      "epsilon:0.010000 episode_count: 29575. steps_count: 13045145.000000\n",
      "ep 4225: ep_len:500 episode reward: total was -2.260000. running mean: -8.205287\n",
      "ep 4225: ep_len:685 episode reward: total was -14.310000. running mean: -8.266334\n",
      "ep 4225: ep_len:625 episode reward: total was -5.100000. running mean: -8.234671\n",
      "ep 4225: ep_len:570 episode reward: total was 17.410000. running mean: -7.978224\n",
      "ep 4225: ep_len:3 episode reward: total was 0.000000. running mean: -7.898442\n",
      "ep 4225: ep_len:525 episode reward: total was -8.910000. running mean: -7.908557\n",
      "ep 4225: ep_len:580 episode reward: total was -1.790000. running mean: -7.847372\n",
      "epsilon:0.010000 episode_count: 29582. steps_count: 13048633.000000\n",
      "ep 4226: ep_len:500 episode reward: total was 1.350000. running mean: -7.755398\n",
      "ep 4226: ep_len:580 episode reward: total was 12.620000. running mean: -7.551644\n",
      "ep 4226: ep_len:585 episode reward: total was -25.820000. running mean: -7.734328\n",
      "ep 4226: ep_len:371 episode reward: total was 7.370000. running mean: -7.583284\n",
      "ep 4226: ep_len:104 episode reward: total was 5.540000. running mean: -7.452051\n",
      "ep 4226: ep_len:520 episode reward: total was -6.730000. running mean: -7.444831\n",
      "ep 4226: ep_len:565 episode reward: total was -26.150000. running mean: -7.631883\n",
      "epsilon:0.010000 episode_count: 29589. steps_count: 13051858.000000\n",
      "ep 4227: ep_len:585 episode reward: total was 14.960000. running mean: -7.405964\n",
      "ep 4227: ep_len:500 episode reward: total was -24.020000. running mean: -7.572104\n",
      "ep 4227: ep_len:500 episode reward: total was -14.610000. running mean: -7.642483\n",
      "ep 4227: ep_len:510 episode reward: total was 1.500000. running mean: -7.551058\n",
      "ep 4227: ep_len:104 episode reward: total was 4.530000. running mean: -7.430248\n",
      "ep 4227: ep_len:160 episode reward: total was 5.630000. running mean: -7.299645\n",
      "ep 4227: ep_len:505 episode reward: total was -10.100000. running mean: -7.327649\n",
      "epsilon:0.010000 episode_count: 29596. steps_count: 13054722.000000\n",
      "ep 4228: ep_len:565 episode reward: total was -27.600000. running mean: -7.530372\n",
      "ep 4228: ep_len:500 episode reward: total was 8.570000. running mean: -7.369369\n",
      "ep 4228: ep_len:405 episode reward: total was -11.840000. running mean: -7.414075\n",
      "ep 4228: ep_len:575 episode reward: total was 3.920000. running mean: -7.300734\n",
      "ep 4228: ep_len:3 episode reward: total was 0.000000. running mean: -7.227727\n",
      "ep 4228: ep_len:610 episode reward: total was 1.120000. running mean: -7.144249\n",
      "ep 4228: ep_len:565 episode reward: total was -2.390000. running mean: -7.096707\n",
      "epsilon:0.010000 episode_count: 29603. steps_count: 13057945.000000\n",
      "ep 4229: ep_len:510 episode reward: total was 13.440000. running mean: -6.891340\n",
      "ep 4229: ep_len:645 episode reward: total was -17.240000. running mean: -6.994827\n",
      "ep 4229: ep_len:520 episode reward: total was -10.770000. running mean: -7.032578\n",
      "ep 4229: ep_len:500 episode reward: total was 10.630000. running mean: -6.855952\n",
      "ep 4229: ep_len:87 episode reward: total was 2.020000. running mean: -6.767193\n",
      "ep 4229: ep_len:520 episode reward: total was -2.390000. running mean: -6.723421\n",
      "ep 4229: ep_len:312 episode reward: total was -19.240000. running mean: -6.848587\n",
      "epsilon:0.010000 episode_count: 29610. steps_count: 13061039.000000\n",
      "ep 4230: ep_len:134 episode reward: total was 3.580000. running mean: -6.744301\n",
      "ep 4230: ep_len:175 episode reward: total was -7.400000. running mean: -6.750858\n",
      "ep 4230: ep_len:343 episode reward: total was -6.860000. running mean: -6.751949\n",
      "ep 4230: ep_len:510 episode reward: total was 6.870000. running mean: -6.615730\n",
      "ep 4230: ep_len:77 episode reward: total was 5.030000. running mean: -6.499273\n",
      "ep 4230: ep_len:605 episode reward: total was -17.630000. running mean: -6.610580\n",
      "ep 4230: ep_len:585 episode reward: total was -12.800000. running mean: -6.672474\n",
      "epsilon:0.010000 episode_count: 29617. steps_count: 13063468.000000\n",
      "ep 4231: ep_len:595 episode reward: total was -8.220000. running mean: -6.687949\n",
      "ep 4231: ep_len:555 episode reward: total was 5.720000. running mean: -6.563870\n",
      "ep 4231: ep_len:615 episode reward: total was -29.800000. running mean: -6.796231\n",
      "ep 4231: ep_len:500 episode reward: total was 15.930000. running mean: -6.568969\n",
      "ep 4231: ep_len:95 episode reward: total was 0.540000. running mean: -6.497879\n",
      "ep 4231: ep_len:540 episode reward: total was -2.530000. running mean: -6.458200\n",
      "ep 4231: ep_len:200 episode reward: total was -6.350000. running mean: -6.457118\n",
      "epsilon:0.010000 episode_count: 29624. steps_count: 13066568.000000\n",
      "ep 4232: ep_len:520 episode reward: total was -7.110000. running mean: -6.463647\n",
      "ep 4232: ep_len:1190 episode reward: total was -153.900000. running mean: -7.938011\n",
      "ep 4232: ep_len:373 episode reward: total was 5.730000. running mean: -7.801331\n",
      "ep 4232: ep_len:630 episode reward: total was 8.120000. running mean: -7.642117\n",
      "ep 4232: ep_len:98 episode reward: total was 4.030000. running mean: -7.525396\n",
      "ep 4232: ep_len:695 episode reward: total was -5.200000. running mean: -7.502142\n",
      "ep 4232: ep_len:570 episode reward: total was -6.990000. running mean: -7.497021\n",
      "epsilon:0.010000 episode_count: 29631. steps_count: 13070644.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4233: ep_len:500 episode reward: total was -16.240000. running mean: -7.584450\n",
      "ep 4233: ep_len:620 episode reward: total was 12.680000. running mean: -7.381806\n",
      "ep 4233: ep_len:580 episode reward: total was -9.200000. running mean: -7.399988\n",
      "ep 4233: ep_len:500 episode reward: total was -7.580000. running mean: -7.401788\n",
      "ep 4233: ep_len:106 episode reward: total was 7.540000. running mean: -7.252370\n",
      "ep 4233: ep_len:314 episode reward: total was 0.630000. running mean: -7.173546\n",
      "ep 4233: ep_len:272 episode reward: total was 0.700000. running mean: -7.094811\n",
      "epsilon:0.010000 episode_count: 29638. steps_count: 13073536.000000\n",
      "ep 4234: ep_len:555 episode reward: total was -11.680000. running mean: -7.140663\n",
      "ep 4234: ep_len:610 episode reward: total was 8.590000. running mean: -6.983356\n",
      "ep 4234: ep_len:500 episode reward: total was 8.920000. running mean: -6.824323\n",
      "ep 4234: ep_len:56 episode reward: total was 2.570000. running mean: -6.730379\n",
      "ep 4234: ep_len:70 episode reward: total was 2.050000. running mean: -6.642576\n",
      "ep 4234: ep_len:515 episode reward: total was -2.560000. running mean: -6.601750\n",
      "ep 4234: ep_len:269 episode reward: total was 1.180000. running mean: -6.523932\n",
      "epsilon:0.010000 episode_count: 29645. steps_count: 13076111.000000\n",
      "ep 4235: ep_len:580 episode reward: total was -9.150000. running mean: -6.550193\n",
      "ep 4235: ep_len:379 episode reward: total was -9.790000. running mean: -6.582591\n",
      "ep 4235: ep_len:500 episode reward: total was -0.490000. running mean: -6.521665\n",
      "ep 4235: ep_len:570 episode reward: total was 8.600000. running mean: -6.370449\n",
      "ep 4235: ep_len:3 episode reward: total was 0.000000. running mean: -6.306744\n",
      "ep 4235: ep_len:605 episode reward: total was -8.100000. running mean: -6.324677\n",
      "ep 4235: ep_len:500 episode reward: total was -6.700000. running mean: -6.328430\n",
      "epsilon:0.010000 episode_count: 29652. steps_count: 13079248.000000\n",
      "ep 4236: ep_len:560 episode reward: total was -13.720000. running mean: -6.402346\n",
      "ep 4236: ep_len:530 episode reward: total was -1.750000. running mean: -6.355822\n",
      "ep 4236: ep_len:500 episode reward: total was -18.540000. running mean: -6.477664\n",
      "ep 4236: ep_len:565 episode reward: total was 8.920000. running mean: -6.323687\n",
      "ep 4236: ep_len:119 episode reward: total was 6.060000. running mean: -6.199850\n",
      "ep 4236: ep_len:500 episode reward: total was -6.320000. running mean: -6.201052\n",
      "ep 4236: ep_len:347 episode reward: total was -0.730000. running mean: -6.146341\n",
      "epsilon:0.010000 episode_count: 29659. steps_count: 13082369.000000\n",
      "ep 4237: ep_len:108 episode reward: total was 4.050000. running mean: -6.044378\n",
      "ep 4237: ep_len:500 episode reward: total was 0.080000. running mean: -5.983134\n",
      "ep 4237: ep_len:560 episode reward: total was -5.870000. running mean: -5.982003\n",
      "ep 4237: ep_len:505 episode reward: total was 13.950000. running mean: -5.782683\n",
      "ep 4237: ep_len:3 episode reward: total was 0.000000. running mean: -5.724856\n",
      "ep 4237: ep_len:555 episode reward: total was -8.850000. running mean: -5.756107\n",
      "ep 4237: ep_len:630 episode reward: total was -15.940000. running mean: -5.857946\n",
      "epsilon:0.010000 episode_count: 29666. steps_count: 13085230.000000\n",
      "ep 4238: ep_len:134 episode reward: total was -12.920000. running mean: -5.928567\n",
      "ep 4238: ep_len:265 episode reward: total was -7.350000. running mean: -5.942781\n",
      "ep 4238: ep_len:630 episode reward: total was 4.370000. running mean: -5.839653\n",
      "ep 4238: ep_len:510 episode reward: total was 9.030000. running mean: -5.690957\n",
      "ep 4238: ep_len:55 episode reward: total was 4.000000. running mean: -5.594047\n",
      "ep 4238: ep_len:550 episode reward: total was -21.090000. running mean: -5.749007\n",
      "ep 4238: ep_len:570 episode reward: total was -6.970000. running mean: -5.761217\n",
      "epsilon:0.010000 episode_count: 29673. steps_count: 13087944.000000\n",
      "ep 4239: ep_len:555 episode reward: total was 13.970000. running mean: -5.563905\n",
      "ep 4239: ep_len:585 episode reward: total was 18.890000. running mean: -5.319366\n",
      "ep 4239: ep_len:431 episode reward: total was 7.780000. running mean: -5.188372\n",
      "ep 4239: ep_len:550 episode reward: total was 8.430000. running mean: -5.052188\n",
      "ep 4239: ep_len:3 episode reward: total was 0.000000. running mean: -5.001666\n",
      "ep 4239: ep_len:645 episode reward: total was 4.340000. running mean: -4.908250\n",
      "ep 4239: ep_len:330 episode reward: total was -0.730000. running mean: -4.866467\n",
      "epsilon:0.010000 episode_count: 29680. steps_count: 13091043.000000\n",
      "ep 4240: ep_len:640 episode reward: total was 9.240000. running mean: -4.725402\n",
      "ep 4240: ep_len:361 episode reward: total was -5.730000. running mean: -4.735448\n",
      "ep 4240: ep_len:500 episode reward: total was -13.150000. running mean: -4.819594\n",
      "ep 4240: ep_len:500 episode reward: total was 12.900000. running mean: -4.642398\n",
      "ep 4240: ep_len:3 episode reward: total was 0.000000. running mean: -4.595974\n",
      "ep 4240: ep_len:500 episode reward: total was 0.640000. running mean: -4.543614\n",
      "ep 4240: ep_len:500 episode reward: total was -11.850000. running mean: -4.616678\n",
      "epsilon:0.010000 episode_count: 29687. steps_count: 13094047.000000\n",
      "ep 4241: ep_len:570 episode reward: total was -5.650000. running mean: -4.627011\n",
      "ep 4241: ep_len:500 episode reward: total was -0.360000. running mean: -4.584341\n",
      "ep 4241: ep_len:650 episode reward: total was 3.200000. running mean: -4.506498\n",
      "ep 4241: ep_len:610 episode reward: total was 12.630000. running mean: -4.335133\n",
      "ep 4241: ep_len:38 episode reward: total was 3.500000. running mean: -4.256782\n",
      "ep 4241: ep_len:540 episode reward: total was 0.030000. running mean: -4.213914\n",
      "ep 4241: ep_len:349 episode reward: total was 4.810000. running mean: -4.123675\n",
      "epsilon:0.010000 episode_count: 29694. steps_count: 13097304.000000\n",
      "ep 4242: ep_len:665 episode reward: total was 6.690000. running mean: -4.015538\n",
      "ep 4242: ep_len:630 episode reward: total was 17.110000. running mean: -3.804282\n",
      "ep 4242: ep_len:545 episode reward: total was -0.820000. running mean: -3.774440\n",
      "ep 4242: ep_len:398 episode reward: total was 7.880000. running mean: -3.657895\n",
      "ep 4242: ep_len:3 episode reward: total was 0.000000. running mean: -3.621316\n",
      "ep 4242: ep_len:570 episode reward: total was -15.160000. running mean: -3.736703\n",
      "ep 4242: ep_len:690 episode reward: total was -54.890000. running mean: -4.248236\n",
      "epsilon:0.010000 episode_count: 29701. steps_count: 13100805.000000\n",
      "ep 4243: ep_len:226 episode reward: total was 7.610000. running mean: -4.129654\n",
      "ep 4243: ep_len:580 episode reward: total was 26.370000. running mean: -3.824657\n",
      "ep 4243: ep_len:60 episode reward: total was -0.970000. running mean: -3.796111\n",
      "ep 4243: ep_len:425 episode reward: total was 6.930000. running mean: -3.688850\n",
      "ep 4243: ep_len:3 episode reward: total was 0.000000. running mean: -3.651961\n",
      "ep 4243: ep_len:555 episode reward: total was 4.970000. running mean: -3.565741\n",
      "ep 4243: ep_len:575 episode reward: total was -2.510000. running mean: -3.555184\n",
      "epsilon:0.010000 episode_count: 29708. steps_count: 13103229.000000\n",
      "ep 4244: ep_len:625 episode reward: total was 1.130000. running mean: -3.508332\n",
      "ep 4244: ep_len:650 episode reward: total was 3.100000. running mean: -3.442249\n",
      "ep 4244: ep_len:79 episode reward: total was 0.040000. running mean: -3.407426\n",
      "ep 4244: ep_len:505 episode reward: total was -8.950000. running mean: -3.462852\n",
      "ep 4244: ep_len:3 episode reward: total was 0.000000. running mean: -3.428224\n",
      "ep 4244: ep_len:505 episode reward: total was 1.610000. running mean: -3.377841\n",
      "ep 4244: ep_len:555 episode reward: total was -7.890000. running mean: -3.422963\n",
      "epsilon:0.010000 episode_count: 29715. steps_count: 13106151.000000\n",
      "ep 4245: ep_len:580 episode reward: total was 12.000000. running mean: -3.268733\n",
      "ep 4245: ep_len:655 episode reward: total was 2.650000. running mean: -3.209546\n",
      "ep 4245: ep_len:396 episode reward: total was -2.270000. running mean: -3.200151\n",
      "ep 4245: ep_len:140 episode reward: total was 5.600000. running mean: -3.112149\n",
      "ep 4245: ep_len:3 episode reward: total was 0.000000. running mean: -3.081028\n",
      "ep 4245: ep_len:530 episode reward: total was -1.370000. running mean: -3.063917\n",
      "ep 4245: ep_len:505 episode reward: total was -4.280000. running mean: -3.076078\n",
      "epsilon:0.010000 episode_count: 29722. steps_count: 13108960.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4246: ep_len:575 episode reward: total was 8.150000. running mean: -2.963817\n",
      "ep 4246: ep_len:560 episode reward: total was 5.090000. running mean: -2.883279\n",
      "ep 4246: ep_len:655 episode reward: total was 4.130000. running mean: -2.813146\n",
      "ep 4246: ep_len:570 episode reward: total was -2.040000. running mean: -2.805415\n",
      "ep 4246: ep_len:106 episode reward: total was 6.530000. running mean: -2.712061\n",
      "ep 4246: ep_len:1126 episode reward: total was -114.760000. running mean: -3.832540\n",
      "ep 4246: ep_len:284 episode reward: total was -1.790000. running mean: -3.812115\n",
      "epsilon:0.010000 episode_count: 29729. steps_count: 13112836.000000\n",
      "ep 4247: ep_len:500 episode reward: total was -0.200000. running mean: -3.775994\n",
      "ep 4247: ep_len:2268 episode reward: total was -194.030000. running mean: -5.678534\n",
      "ep 4247: ep_len:650 episode reward: total was -8.890000. running mean: -5.710648\n",
      "ep 4247: ep_len:500 episode reward: total was 10.140000. running mean: -5.552142\n",
      "ep 4247: ep_len:3 episode reward: total was 0.000000. running mean: -5.496620\n",
      "ep 4247: ep_len:500 episode reward: total was 8.570000. running mean: -5.355954\n",
      "ep 4247: ep_len:555 episode reward: total was 0.730000. running mean: -5.295095\n",
      "epsilon:0.010000 episode_count: 29736. steps_count: 13117812.000000\n",
      "ep 4248: ep_len:515 episode reward: total was 3.590000. running mean: -5.206244\n",
      "ep 4248: ep_len:505 episode reward: total was -7.050000. running mean: -5.224681\n",
      "ep 4248: ep_len:690 episode reward: total was -21.190000. running mean: -5.384334\n",
      "ep 4248: ep_len:539 episode reward: total was -24.400000. running mean: -5.574491\n",
      "ep 4248: ep_len:3 episode reward: total was 0.000000. running mean: -5.518746\n",
      "ep 4248: ep_len:295 episode reward: total was 6.690000. running mean: -5.396659\n",
      "ep 4248: ep_len:595 episode reward: total was -2.860000. running mean: -5.371292\n",
      "epsilon:0.010000 episode_count: 29743. steps_count: 13120954.000000\n",
      "ep 4249: ep_len:500 episode reward: total was -10.390000. running mean: -5.421479\n",
      "ep 4249: ep_len:605 episode reward: total was -4.140000. running mean: -5.408664\n",
      "ep 4249: ep_len:670 episode reward: total was -71.450000. running mean: -6.069078\n",
      "ep 4249: ep_len:500 episode reward: total was 9.880000. running mean: -5.909587\n",
      "ep 4249: ep_len:3 episode reward: total was 0.000000. running mean: -5.850491\n",
      "ep 4249: ep_len:560 episode reward: total was 1.340000. running mean: -5.778586\n",
      "ep 4249: ep_len:575 episode reward: total was -22.030000. running mean: -5.941100\n",
      "epsilon:0.010000 episode_count: 29750. steps_count: 13124367.000000\n",
      "ep 4250: ep_len:500 episode reward: total was -18.460000. running mean: -6.066289\n",
      "ep 4250: ep_len:570 episode reward: total was 22.800000. running mean: -5.777626\n",
      "ep 4250: ep_len:500 episode reward: total was 0.010000. running mean: -5.719750\n",
      "ep 4250: ep_len:500 episode reward: total was -32.710000. running mean: -5.989653\n",
      "ep 4250: ep_len:3 episode reward: total was 0.000000. running mean: -5.929756\n",
      "ep 4250: ep_len:500 episode reward: total was -4.230000. running mean: -5.912759\n",
      "ep 4250: ep_len:535 episode reward: total was -16.480000. running mean: -6.018431\n",
      "epsilon:0.010000 episode_count: 29757. steps_count: 13127475.000000\n",
      "ep 4251: ep_len:215 episode reward: total was -4.900000. running mean: -6.007247\n",
      "ep 4251: ep_len:585 episode reward: total was -2.620000. running mean: -5.973374\n",
      "ep 4251: ep_len:655 episode reward: total was -33.810000. running mean: -6.251740\n",
      "ep 4251: ep_len:500 episode reward: total was -21.170000. running mean: -6.400923\n",
      "ep 4251: ep_len:95 episode reward: total was -12.450000. running mean: -6.461414\n",
      "ep 4251: ep_len:247 episode reward: total was 7.680000. running mean: -6.320000\n",
      "ep 4251: ep_len:300 episode reward: total was -5.870000. running mean: -6.315500\n",
      "epsilon:0.010000 episode_count: 29764. steps_count: 13130072.000000\n",
      "ep 4252: ep_len:660 episode reward: total was -11.700000. running mean: -6.369345\n",
      "ep 4252: ep_len:500 episode reward: total was 5.050000. running mean: -6.255151\n",
      "ep 4252: ep_len:540 episode reward: total was -4.840000. running mean: -6.241000\n",
      "ep 4252: ep_len:42 episode reward: total was 0.520000. running mean: -6.173390\n",
      "ep 4252: ep_len:3 episode reward: total was 0.000000. running mean: -6.111656\n",
      "ep 4252: ep_len:545 episode reward: total was -8.760000. running mean: -6.138139\n",
      "ep 4252: ep_len:271 episode reward: total was -0.310000. running mean: -6.079858\n",
      "epsilon:0.010000 episode_count: 29771. steps_count: 13132633.000000\n",
      "ep 4253: ep_len:196 episode reward: total was 1.580000. running mean: -6.003259\n",
      "ep 4253: ep_len:540 episode reward: total was 20.880000. running mean: -5.734427\n",
      "ep 4253: ep_len:550 episode reward: total was -5.720000. running mean: -5.734282\n",
      "ep 4253: ep_len:500 episode reward: total was -12.570000. running mean: -5.802640\n",
      "ep 4253: ep_len:3 episode reward: total was 0.000000. running mean: -5.744613\n",
      "ep 4253: ep_len:595 episode reward: total was -0.150000. running mean: -5.688667\n",
      "ep 4253: ep_len:570 episode reward: total was -23.050000. running mean: -5.862280\n",
      "epsilon:0.010000 episode_count: 29778. steps_count: 13135587.000000\n",
      "ep 4254: ep_len:580 episode reward: total was 11.480000. running mean: -5.688858\n",
      "ep 4254: ep_len:505 episode reward: total was -8.680000. running mean: -5.718769\n",
      "ep 4254: ep_len:535 episode reward: total was -31.400000. running mean: -5.975581\n",
      "ep 4254: ep_len:170 episode reward: total was 5.630000. running mean: -5.859526\n",
      "ep 4254: ep_len:3 episode reward: total was 0.000000. running mean: -5.800930\n",
      "ep 4254: ep_len:595 episode reward: total was -2.600000. running mean: -5.768921\n",
      "ep 4254: ep_len:595 episode reward: total was -22.480000. running mean: -5.936032\n",
      "epsilon:0.010000 episode_count: 29785. steps_count: 13138570.000000\n",
      "ep 4255: ep_len:208 episode reward: total was -9.420000. running mean: -5.970871\n",
      "ep 4255: ep_len:500 episode reward: total was -20.940000. running mean: -6.120563\n",
      "ep 4255: ep_len:560 episode reward: total was -1.100000. running mean: -6.070357\n",
      "ep 4255: ep_len:590 episode reward: total was 4.550000. running mean: -5.964154\n",
      "ep 4255: ep_len:3 episode reward: total was 0.000000. running mean: -5.904512\n",
      "ep 4255: ep_len:615 episode reward: total was -26.760000. running mean: -6.113067\n",
      "ep 4255: ep_len:600 episode reward: total was 0.020000. running mean: -6.051736\n",
      "epsilon:0.010000 episode_count: 29792. steps_count: 13141646.000000\n",
      "ep 4256: ep_len:182 episode reward: total was -6.950000. running mean: -6.060719\n",
      "ep 4256: ep_len:530 episode reward: total was 5.220000. running mean: -5.947912\n",
      "ep 4256: ep_len:645 episode reward: total was -25.430000. running mean: -6.142733\n",
      "ep 4256: ep_len:500 episode reward: total was 11.870000. running mean: -5.962605\n",
      "ep 4256: ep_len:3 episode reward: total was 0.000000. running mean: -5.902979\n",
      "ep 4256: ep_len:500 episode reward: total was 7.500000. running mean: -5.768949\n",
      "ep 4256: ep_len:286 episode reward: total was -6.860000. running mean: -5.779860\n",
      "epsilon:0.010000 episode_count: 29799. steps_count: 13144292.000000\n",
      "ep 4257: ep_len:550 episode reward: total was 1.580000. running mean: -5.706261\n",
      "ep 4257: ep_len:285 episode reward: total was -7.840000. running mean: -5.727599\n",
      "ep 4257: ep_len:845 episode reward: total was -55.180000. running mean: -6.222123\n",
      "ep 4257: ep_len:515 episode reward: total was -21.600000. running mean: -6.375901\n",
      "ep 4257: ep_len:3 episode reward: total was 0.000000. running mean: -6.312142\n",
      "ep 4257: ep_len:500 episode reward: total was -0.850000. running mean: -6.257521\n",
      "ep 4257: ep_len:326 episode reward: total was -3.830000. running mean: -6.233246\n",
      "epsilon:0.010000 episode_count: 29806. steps_count: 13147316.000000\n",
      "ep 4258: ep_len:535 episode reward: total was -13.040000. running mean: -6.301313\n",
      "ep 4258: ep_len:665 episode reward: total was -34.830000. running mean: -6.586600\n",
      "ep 4258: ep_len:530 episode reward: total was 2.920000. running mean: -6.491534\n",
      "ep 4258: ep_len:500 episode reward: total was -33.170000. running mean: -6.758319\n",
      "ep 4258: ep_len:3 episode reward: total was 0.000000. running mean: -6.690736\n",
      "ep 4258: ep_len:510 episode reward: total was -5.410000. running mean: -6.677928\n",
      "ep 4258: ep_len:565 episode reward: total was -3.020000. running mean: -6.641349\n",
      "epsilon:0.010000 episode_count: 29813. steps_count: 13150624.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4259: ep_len:248 episode reward: total was 2.640000. running mean: -6.548536\n",
      "ep 4259: ep_len:590 episode reward: total was -5.380000. running mean: -6.536850\n",
      "ep 4259: ep_len:570 episode reward: total was -22.530000. running mean: -6.696782\n",
      "ep 4259: ep_len:580 episode reward: total was 9.070000. running mean: -6.539114\n",
      "ep 4259: ep_len:94 episode reward: total was -6.950000. running mean: -6.543223\n",
      "ep 4259: ep_len:232 episode reward: total was 8.140000. running mean: -6.396391\n",
      "ep 4259: ep_len:585 episode reward: total was -2.520000. running mean: -6.357627\n",
      "epsilon:0.010000 episode_count: 29820. steps_count: 13153523.000000\n",
      "ep 4260: ep_len:265 episode reward: total was -0.360000. running mean: -6.297650\n",
      "ep 4260: ep_len:525 episode reward: total was -12.010000. running mean: -6.354774\n",
      "ep 4260: ep_len:625 episode reward: total was -0.660000. running mean: -6.297826\n",
      "ep 4260: ep_len:505 episode reward: total was -10.540000. running mean: -6.340248\n",
      "ep 4260: ep_len:3 episode reward: total was 0.000000. running mean: -6.276845\n",
      "ep 4260: ep_len:162 episode reward: total was 8.150000. running mean: -6.132577\n",
      "ep 4260: ep_len:560 episode reward: total was -22.570000. running mean: -6.296951\n",
      "epsilon:0.010000 episode_count: 29827. steps_count: 13156168.000000\n",
      "ep 4261: ep_len:615 episode reward: total was -22.180000. running mean: -6.455782\n",
      "ep 4261: ep_len:655 episode reward: total was -32.400000. running mean: -6.715224\n",
      "ep 4261: ep_len:66 episode reward: total was -1.970000. running mean: -6.667772\n",
      "ep 4261: ep_len:615 episode reward: total was 11.930000. running mean: -6.481794\n",
      "ep 4261: ep_len:90 episode reward: total was -9.950000. running mean: -6.516476\n",
      "ep 4261: ep_len:500 episode reward: total was -38.770000. running mean: -6.839011\n",
      "ep 4261: ep_len:520 episode reward: total was -21.890000. running mean: -6.989521\n",
      "epsilon:0.010000 episode_count: 29834. steps_count: 13159229.000000\n",
      "ep 4262: ep_len:615 episode reward: total was 7.460000. running mean: -6.845026\n",
      "ep 4262: ep_len:287 episode reward: total was -3.340000. running mean: -6.809976\n",
      "ep 4262: ep_len:500 episode reward: total was -7.370000. running mean: -6.815576\n",
      "ep 4262: ep_len:159 episode reward: total was 8.140000. running mean: -6.666020\n",
      "ep 4262: ep_len:3 episode reward: total was 0.000000. running mean: -6.599360\n",
      "ep 4262: ep_len:500 episode reward: total was -6.110000. running mean: -6.594466\n",
      "ep 4262: ep_len:540 episode reward: total was 0.290000. running mean: -6.525622\n",
      "epsilon:0.010000 episode_count: 29841. steps_count: 13161833.000000\n",
      "ep 4263: ep_len:570 episode reward: total was -19.720000. running mean: -6.657565\n",
      "ep 4263: ep_len:585 episode reward: total was 23.870000. running mean: -6.352290\n",
      "ep 4263: ep_len:555 episode reward: total was 2.460000. running mean: -6.264167\n",
      "ep 4263: ep_len:510 episode reward: total was -3.610000. running mean: -6.237625\n",
      "ep 4263: ep_len:3 episode reward: total was 0.000000. running mean: -6.175249\n",
      "ep 4263: ep_len:595 episode reward: total was -24.040000. running mean: -6.353896\n",
      "ep 4263: ep_len:585 episode reward: total was -5.600000. running mean: -6.346357\n",
      "epsilon:0.010000 episode_count: 29848. steps_count: 13165236.000000\n",
      "ep 4264: ep_len:211 episode reward: total was -4.900000. running mean: -6.331894\n",
      "ep 4264: ep_len:530 episode reward: total was -2.630000. running mean: -6.294875\n",
      "ep 4264: ep_len:585 episode reward: total was -46.910000. running mean: -6.701026\n",
      "ep 4264: ep_len:610 episode reward: total was -3.390000. running mean: -6.667916\n",
      "ep 4264: ep_len:52 episode reward: total was 5.000000. running mean: -6.551237\n",
      "ep 4264: ep_len:540 episode reward: total was 4.430000. running mean: -6.441424\n",
      "ep 4264: ep_len:640 episode reward: total was -0.690000. running mean: -6.383910\n",
      "epsilon:0.010000 episode_count: 29855. steps_count: 13168404.000000\n",
      "ep 4265: ep_len:555 episode reward: total was 6.090000. running mean: -6.259171\n",
      "ep 4265: ep_len:500 episode reward: total was 17.210000. running mean: -6.024479\n",
      "ep 4265: ep_len:565 episode reward: total was -8.400000. running mean: -6.048235\n",
      "ep 4265: ep_len:575 episode reward: total was -17.990000. running mean: -6.167652\n",
      "ep 4265: ep_len:3 episode reward: total was 0.000000. running mean: -6.105976\n",
      "ep 4265: ep_len:233 episode reward: total was 8.170000. running mean: -5.963216\n",
      "ep 4265: ep_len:630 episode reward: total was 2.050000. running mean: -5.883084\n",
      "epsilon:0.010000 episode_count: 29862. steps_count: 13171465.000000\n",
      "ep 4266: ep_len:555 episode reward: total was 6.500000. running mean: -5.759253\n",
      "ep 4266: ep_len:259 episode reward: total was -20.860000. running mean: -5.910260\n",
      "ep 4266: ep_len:625 episode reward: total was -5.530000. running mean: -5.906458\n",
      "ep 4266: ep_len:500 episode reward: total was -15.080000. running mean: -5.998193\n",
      "ep 4266: ep_len:90 episode reward: total was -10.440000. running mean: -6.042611\n",
      "ep 4266: ep_len:326 episode reward: total was 5.190000. running mean: -5.930285\n",
      "ep 4266: ep_len:570 episode reward: total was -3.800000. running mean: -5.908982\n",
      "epsilon:0.010000 episode_count: 29869. steps_count: 13174390.000000\n",
      "ep 4267: ep_len:550 episode reward: total was -21.240000. running mean: -6.062293\n",
      "ep 4267: ep_len:500 episode reward: total was 13.150000. running mean: -5.870170\n",
      "ep 4267: ep_len:500 episode reward: total was -3.050000. running mean: -5.841968\n",
      "ep 4267: ep_len:605 episode reward: total was 16.010000. running mean: -5.623448\n",
      "ep 4267: ep_len:93 episode reward: total was -8.970000. running mean: -5.656914\n",
      "ep 4267: ep_len:535 episode reward: total was -32.520000. running mean: -5.925545\n",
      "ep 4267: ep_len:555 episode reward: total was -9.060000. running mean: -5.956889\n",
      "epsilon:0.010000 episode_count: 29876. steps_count: 13177728.000000\n",
      "ep 4268: ep_len:500 episode reward: total was 14.320000. running mean: -5.754120\n",
      "ep 4268: ep_len:520 episode reward: total was -14.870000. running mean: -5.845279\n",
      "ep 4268: ep_len:625 episode reward: total was -16.760000. running mean: -5.954426\n",
      "ep 4268: ep_len:605 episode reward: total was 3.510000. running mean: -5.859782\n",
      "ep 4268: ep_len:128 episode reward: total was 6.050000. running mean: -5.740684\n",
      "ep 4268: ep_len:625 episode reward: total was -10.070000. running mean: -5.783977\n",
      "ep 4268: ep_len:600 episode reward: total was -3.830000. running mean: -5.764438\n",
      "epsilon:0.010000 episode_count: 29883. steps_count: 13181331.000000\n",
      "ep 4269: ep_len:510 episode reward: total was -15.890000. running mean: -5.865693\n",
      "ep 4269: ep_len:555 episode reward: total was 3.720000. running mean: -5.769836\n",
      "ep 4269: ep_len:585 episode reward: total was 1.480000. running mean: -5.697338\n",
      "ep 4269: ep_len:500 episode reward: total was -2.520000. running mean: -5.665565\n",
      "ep 4269: ep_len:76 episode reward: total was 3.040000. running mean: -5.578509\n",
      "ep 4269: ep_len:570 episode reward: total was 6.640000. running mean: -5.456324\n",
      "ep 4269: ep_len:500 episode reward: total was -8.410000. running mean: -5.485861\n",
      "epsilon:0.010000 episode_count: 29890. steps_count: 13184627.000000\n",
      "ep 4270: ep_len:590 episode reward: total was 2.580000. running mean: -5.405202\n",
      "ep 4270: ep_len:555 episode reward: total was 16.870000. running mean: -5.182450\n",
      "ep 4270: ep_len:500 episode reward: total was -0.580000. running mean: -5.136425\n",
      "ep 4270: ep_len:152 episode reward: total was 7.100000. running mean: -5.014061\n",
      "ep 4270: ep_len:3 episode reward: total was 0.000000. running mean: -4.963921\n",
      "ep 4270: ep_len:530 episode reward: total was -1.580000. running mean: -4.930081\n",
      "ep 4270: ep_len:590 episode reward: total was -27.880000. running mean: -5.159581\n",
      "epsilon:0.010000 episode_count: 29897. steps_count: 13187547.000000\n",
      "ep 4271: ep_len:227 episode reward: total was 2.100000. running mean: -5.086985\n",
      "ep 4271: ep_len:560 episode reward: total was -1.600000. running mean: -5.052115\n",
      "ep 4271: ep_len:430 episode reward: total was 0.220000. running mean: -4.999394\n",
      "ep 4271: ep_len:505 episode reward: total was -22.050000. running mean: -5.169900\n",
      "ep 4271: ep_len:3 episode reward: total was 0.000000. running mean: -5.118201\n",
      "ep 4271: ep_len:540 episode reward: total was 6.610000. running mean: -5.000919\n",
      "ep 4271: ep_len:290 episode reward: total was -4.310000. running mean: -4.994010\n",
      "epsilon:0.010000 episode_count: 29904. steps_count: 13190102.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4272: ep_len:530 episode reward: total was -20.390000. running mean: -5.147970\n",
      "ep 4272: ep_len:184 episode reward: total was -6.920000. running mean: -5.165690\n",
      "ep 4272: ep_len:555 episode reward: total was -13.070000. running mean: -5.244733\n",
      "ep 4272: ep_len:620 episode reward: total was -1.410000. running mean: -5.206386\n",
      "ep 4272: ep_len:3 episode reward: total was 0.000000. running mean: -5.154322\n",
      "ep 4272: ep_len:570 episode reward: total was -4.590000. running mean: -5.148679\n",
      "ep 4272: ep_len:545 episode reward: total was 2.480000. running mean: -5.072392\n",
      "epsilon:0.010000 episode_count: 29911. steps_count: 13193109.000000\n",
      "ep 4273: ep_len:610 episode reward: total was -9.370000. running mean: -5.115368\n",
      "ep 4273: ep_len:585 episode reward: total was -43.000000. running mean: -5.494214\n",
      "ep 4273: ep_len:660 episode reward: total was -19.940000. running mean: -5.638672\n",
      "ep 4273: ep_len:39 episode reward: total was 1.030000. running mean: -5.571985\n",
      "ep 4273: ep_len:108 episode reward: total was -5.940000. running mean: -5.575665\n",
      "ep 4273: ep_len:500 episode reward: total was -3.750000. running mean: -5.557409\n",
      "ep 4273: ep_len:500 episode reward: total was -14.750000. running mean: -5.649335\n",
      "epsilon:0.010000 episode_count: 29918. steps_count: 13196111.000000\n",
      "ep 4274: ep_len:153 episode reward: total was 0.080000. running mean: -5.592041\n",
      "ep 4274: ep_len:655 episode reward: total was 21.600000. running mean: -5.320121\n",
      "ep 4274: ep_len:730 episode reward: total was -52.770000. running mean: -5.794620\n",
      "ep 4274: ep_len:560 episode reward: total was 4.120000. running mean: -5.695474\n",
      "ep 4274: ep_len:3 episode reward: total was 0.000000. running mean: -5.638519\n",
      "ep 4274: ep_len:565 episode reward: total was 1.380000. running mean: -5.568334\n",
      "ep 4274: ep_len:288 episode reward: total was -9.840000. running mean: -5.611050\n",
      "epsilon:0.010000 episode_count: 29925. steps_count: 13199065.000000\n",
      "ep 4275: ep_len:565 episode reward: total was -2.390000. running mean: -5.578840\n",
      "ep 4275: ep_len:630 episode reward: total was -7.130000. running mean: -5.594351\n",
      "ep 4275: ep_len:615 episode reward: total was -7.640000. running mean: -5.614808\n",
      "ep 4275: ep_len:500 episode reward: total was 10.400000. running mean: -5.454660\n",
      "ep 4275: ep_len:116 episode reward: total was -15.990000. running mean: -5.560013\n",
      "ep 4275: ep_len:580 episode reward: total was -26.430000. running mean: -5.768713\n",
      "ep 4275: ep_len:211 episode reward: total was -7.880000. running mean: -5.789826\n",
      "epsilon:0.010000 episode_count: 29932. steps_count: 13202282.000000\n",
      "ep 4276: ep_len:575 episode reward: total was -5.640000. running mean: -5.788328\n",
      "ep 4276: ep_len:580 episode reward: total was -2.310000. running mean: -5.753544\n",
      "ep 4276: ep_len:500 episode reward: total was 0.900000. running mean: -5.687009\n",
      "ep 4276: ep_len:56 episode reward: total was 1.560000. running mean: -5.614539\n",
      "ep 4276: ep_len:3 episode reward: total was 0.000000. running mean: -5.558393\n",
      "ep 4276: ep_len:565 episode reward: total was -14.090000. running mean: -5.643710\n",
      "ep 4276: ep_len:307 episode reward: total was -9.350000. running mean: -5.680772\n",
      "epsilon:0.010000 episode_count: 29939. steps_count: 13204868.000000\n",
      "ep 4277: ep_len:635 episode reward: total was -7.220000. running mean: -5.696165\n",
      "ep 4277: ep_len:570 episode reward: total was 2.920000. running mean: -5.610003\n",
      "ep 4277: ep_len:565 episode reward: total was 6.980000. running mean: -5.484103\n",
      "ep 4277: ep_len:530 episode reward: total was 1.410000. running mean: -5.415162\n",
      "ep 4277: ep_len:105 episode reward: total was 4.520000. running mean: -5.315810\n",
      "ep 4277: ep_len:535 episode reward: total was -10.570000. running mean: -5.368352\n",
      "ep 4277: ep_len:540 episode reward: total was -15.230000. running mean: -5.466969\n",
      "epsilon:0.010000 episode_count: 29946. steps_count: 13208348.000000\n",
      "ep 4278: ep_len:520 episode reward: total was -11.160000. running mean: -5.523899\n",
      "ep 4278: ep_len:500 episode reward: total was -2.610000. running mean: -5.494760\n",
      "ep 4278: ep_len:555 episode reward: total was -3.110000. running mean: -5.470912\n",
      "ep 4278: ep_len:515 episode reward: total was -3.440000. running mean: -5.450603\n",
      "ep 4278: ep_len:3 episode reward: total was 0.000000. running mean: -5.396097\n",
      "ep 4278: ep_len:500 episode reward: total was 15.060000. running mean: -5.191536\n",
      "ep 4278: ep_len:600 episode reward: total was -33.110000. running mean: -5.470721\n",
      "epsilon:0.010000 episode_count: 29953. steps_count: 13211541.000000\n",
      "ep 4279: ep_len:685 episode reward: total was -8.590000. running mean: -5.501914\n",
      "ep 4279: ep_len:545 episode reward: total was 18.380000. running mean: -5.263095\n",
      "ep 4279: ep_len:74 episode reward: total was -0.460000. running mean: -5.215064\n",
      "ep 4279: ep_len:585 episode reward: total was 3.490000. running mean: -5.128013\n",
      "ep 4279: ep_len:3 episode reward: total was 0.000000. running mean: -5.076733\n",
      "ep 4279: ep_len:500 episode reward: total was -1.090000. running mean: -5.036866\n",
      "ep 4279: ep_len:284 episode reward: total was -8.800000. running mean: -5.074497\n",
      "epsilon:0.010000 episode_count: 29960. steps_count: 13214217.000000\n",
      "ep 4280: ep_len:670 episode reward: total was -13.640000. running mean: -5.160152\n",
      "ep 4280: ep_len:277 episode reward: total was -19.360000. running mean: -5.302150\n",
      "ep 4280: ep_len:680 episode reward: total was -1.680000. running mean: -5.265929\n",
      "ep 4280: ep_len:570 episode reward: total was 3.430000. running mean: -5.178970\n",
      "ep 4280: ep_len:3 episode reward: total was 0.000000. running mean: -5.127180\n",
      "ep 4280: ep_len:520 episode reward: total was -11.780000. running mean: -5.193708\n",
      "ep 4280: ep_len:610 episode reward: total was -7.760000. running mean: -5.219371\n",
      "epsilon:0.010000 episode_count: 29967. steps_count: 13217547.000000\n",
      "ep 4281: ep_len:635 episode reward: total was -0.790000. running mean: -5.175077\n",
      "ep 4281: ep_len:500 episode reward: total was 2.550000. running mean: -5.097827\n",
      "ep 4281: ep_len:424 episode reward: total was 2.250000. running mean: -5.024348\n",
      "ep 4281: ep_len:500 episode reward: total was -1.750000. running mean: -4.991605\n",
      "ep 4281: ep_len:104 episode reward: total was 2.510000. running mean: -4.916589\n",
      "ep 4281: ep_len:540 episode reward: total was -13.160000. running mean: -4.999023\n",
      "ep 4281: ep_len:595 episode reward: total was -30.070000. running mean: -5.249733\n",
      "epsilon:0.010000 episode_count: 29974. steps_count: 13220845.000000\n",
      "ep 4282: ep_len:650 episode reward: total was -11.910000. running mean: -5.316335\n",
      "ep 4282: ep_len:510 episode reward: total was -3.530000. running mean: -5.298472\n",
      "ep 4282: ep_len:655 episode reward: total was -7.930000. running mean: -5.324787\n",
      "ep 4282: ep_len:599 episode reward: total was -30.450000. running mean: -5.576039\n",
      "ep 4282: ep_len:1 episode reward: total was 0.000000. running mean: -5.520279\n",
      "ep 4282: ep_len:500 episode reward: total was -16.620000. running mean: -5.631276\n",
      "ep 4282: ep_len:610 episode reward: total was -9.320000. running mean: -5.668163\n",
      "epsilon:0.010000 episode_count: 29981. steps_count: 13224370.000000\n",
      "ep 4283: ep_len:505 episode reward: total was -2.120000. running mean: -5.632682\n",
      "ep 4283: ep_len:515 episode reward: total was 19.320000. running mean: -5.383155\n",
      "ep 4283: ep_len:365 episode reward: total was -3.320000. running mean: -5.362523\n",
      "ep 4283: ep_len:500 episode reward: total was 7.890000. running mean: -5.229998\n",
      "ep 4283: ep_len:97 episode reward: total was -2.460000. running mean: -5.202298\n",
      "ep 4283: ep_len:520 episode reward: total was -0.870000. running mean: -5.158975\n",
      "ep 4283: ep_len:525 episode reward: total was -23.480000. running mean: -5.342185\n",
      "epsilon:0.010000 episode_count: 29988. steps_count: 13227397.000000\n",
      "ep 4284: ep_len:595 episode reward: total was 9.460000. running mean: -5.194164\n",
      "ep 4284: ep_len:655 episode reward: total was 12.090000. running mean: -5.021322\n",
      "ep 4284: ep_len:575 episode reward: total was -19.240000. running mean: -5.163509\n",
      "ep 4284: ep_len:582 episode reward: total was -35.990000. running mean: -5.471774\n",
      "ep 4284: ep_len:3 episode reward: total was 0.000000. running mean: -5.417056\n",
      "ep 4284: ep_len:525 episode reward: total was -0.770000. running mean: -5.370585\n",
      "ep 4284: ep_len:500 episode reward: total was -9.240000. running mean: -5.409280\n",
      "epsilon:0.010000 episode_count: 29995. steps_count: 13230832.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4285: ep_len:580 episode reward: total was -2.520000. running mean: -5.380387\n",
      "ep 4285: ep_len:500 episode reward: total was 14.180000. running mean: -5.184783\n",
      "ep 4285: ep_len:540 episode reward: total was -9.260000. running mean: -5.225535\n",
      "ep 4285: ep_len:585 episode reward: total was -15.120000. running mean: -5.324480\n",
      "ep 4285: ep_len:3 episode reward: total was 0.000000. running mean: -5.271235\n",
      "ep 4285: ep_len:186 episode reward: total was 7.120000. running mean: -5.147323\n",
      "ep 4285: ep_len:500 episode reward: total was -18.050000. running mean: -5.276349\n",
      "epsilon:0.010000 episode_count: 30002. steps_count: 13233726.000000\n",
      "ep 4286: ep_len:505 episode reward: total was 2.840000. running mean: -5.195186\n",
      "ep 4286: ep_len:620 episode reward: total was -60.560000. running mean: -5.748834\n",
      "ep 4286: ep_len:615 episode reward: total was -3.390000. running mean: -5.725246\n",
      "ep 4286: ep_len:505 episode reward: total was 1.300000. running mean: -5.654993\n",
      "ep 4286: ep_len:3 episode reward: total was 0.000000. running mean: -5.598443\n",
      "ep 4286: ep_len:500 episode reward: total was -1.990000. running mean: -5.562359\n",
      "ep 4286: ep_len:515 episode reward: total was -12.860000. running mean: -5.635335\n",
      "epsilon:0.010000 episode_count: 30009. steps_count: 13236989.000000\n",
      "ep 4287: ep_len:660 episode reward: total was -7.110000. running mean: -5.650082\n",
      "ep 4287: ep_len:545 episode reward: total was 6.120000. running mean: -5.532381\n",
      "ep 4287: ep_len:500 episode reward: total was 0.130000. running mean: -5.475757\n",
      "ep 4287: ep_len:505 episode reward: total was 10.030000. running mean: -5.320700\n",
      "ep 4287: ep_len:44 episode reward: total was 4.000000. running mean: -5.227493\n",
      "ep 4287: ep_len:535 episode reward: total was 0.660000. running mean: -5.168618\n",
      "ep 4287: ep_len:535 episode reward: total was -7.960000. running mean: -5.196532\n",
      "epsilon:0.010000 episode_count: 30016. steps_count: 13240313.000000\n",
      "ep 4288: ep_len:500 episode reward: total was 12.790000. running mean: -5.016666\n",
      "ep 4288: ep_len:505 episode reward: total was 4.670000. running mean: -4.919800\n",
      "ep 4288: ep_len:409 episode reward: total was -3.780000. running mean: -4.908402\n",
      "ep 4288: ep_len:510 episode reward: total was -15.550000. running mean: -5.014818\n",
      "ep 4288: ep_len:79 episode reward: total was 5.030000. running mean: -4.914369\n",
      "ep 4288: ep_len:500 episode reward: total was 4.530000. running mean: -4.819926\n",
      "ep 4288: ep_len:324 episode reward: total was -12.320000. running mean: -4.894926\n",
      "epsilon:0.010000 episode_count: 30023. steps_count: 13243140.000000\n",
      "ep 4289: ep_len:630 episode reward: total was 4.660000. running mean: -4.799377\n",
      "ep 4289: ep_len:595 episode reward: total was -34.430000. running mean: -5.095683\n",
      "ep 4289: ep_len:377 episode reward: total was 6.190000. running mean: -4.982827\n",
      "ep 4289: ep_len:152 episode reward: total was -1.390000. running mean: -4.946898\n",
      "ep 4289: ep_len:99 episode reward: total was 3.020000. running mean: -4.867229\n",
      "ep 4289: ep_len:535 episode reward: total was -8.910000. running mean: -4.907657\n",
      "ep 4289: ep_len:269 episode reward: total was -2.350000. running mean: -4.882080\n",
      "epsilon:0.010000 episode_count: 30030. steps_count: 13245797.000000\n",
      "ep 4290: ep_len:615 episode reward: total was 3.100000. running mean: -4.802260\n",
      "ep 4290: ep_len:745 episode reward: total was -36.700000. running mean: -5.121237\n",
      "ep 4290: ep_len:500 episode reward: total was 3.860000. running mean: -5.031425\n",
      "ep 4290: ep_len:56 episode reward: total was 2.570000. running mean: -4.955410\n",
      "ep 4290: ep_len:3 episode reward: total was 0.000000. running mean: -4.905856\n",
      "ep 4290: ep_len:500 episode reward: total was -11.240000. running mean: -4.969198\n",
      "ep 4290: ep_len:500 episode reward: total was -8.260000. running mean: -5.002106\n",
      "epsilon:0.010000 episode_count: 30037. steps_count: 13248716.000000\n",
      "ep 4291: ep_len:500 episode reward: total was 10.750000. running mean: -4.844585\n",
      "ep 4291: ep_len:175 episode reward: total was -9.420000. running mean: -4.890339\n",
      "ep 4291: ep_len:540 episode reward: total was -16.070000. running mean: -5.002136\n",
      "ep 4291: ep_len:500 episode reward: total was -19.590000. running mean: -5.148014\n",
      "ep 4291: ep_len:3 episode reward: total was 0.000000. running mean: -5.096534\n",
      "ep 4291: ep_len:500 episode reward: total was 1.280000. running mean: -5.032769\n",
      "ep 4291: ep_len:535 episode reward: total was -21.180000. running mean: -5.194241\n",
      "epsilon:0.010000 episode_count: 30044. steps_count: 13251469.000000\n",
      "ep 4292: ep_len:605 episode reward: total was 7.960000. running mean: -5.062699\n",
      "ep 4292: ep_len:610 episode reward: total was 11.610000. running mean: -4.895972\n",
      "ep 4292: ep_len:540 episode reward: total was -3.540000. running mean: -4.882412\n",
      "ep 4292: ep_len:525 episode reward: total was -40.660000. running mean: -5.240188\n",
      "ep 4292: ep_len:3 episode reward: total was 0.000000. running mean: -5.187786\n",
      "ep 4292: ep_len:560 episode reward: total was -27.600000. running mean: -5.411908\n",
      "ep 4292: ep_len:710 episode reward: total was -32.810000. running mean: -5.685889\n",
      "epsilon:0.010000 episode_count: 30051. steps_count: 13255022.000000\n",
      "ep 4293: ep_len:510 episode reward: total was 10.320000. running mean: -5.525830\n",
      "ep 4293: ep_len:500 episode reward: total was -1.370000. running mean: -5.484272\n",
      "ep 4293: ep_len:630 episode reward: total was -7.950000. running mean: -5.508929\n",
      "ep 4293: ep_len:555 episode reward: total was -4.590000. running mean: -5.499740\n",
      "ep 4293: ep_len:1 episode reward: total was 0.000000. running mean: -5.444742\n",
      "ep 4293: ep_len:300 episode reward: total was 1.190000. running mean: -5.378395\n",
      "ep 4293: ep_len:500 episode reward: total was -3.620000. running mean: -5.360811\n",
      "epsilon:0.010000 episode_count: 30058. steps_count: 13258018.000000\n",
      "ep 4294: ep_len:500 episode reward: total was -0.660000. running mean: -5.313803\n",
      "ep 4294: ep_len:595 episode reward: total was 11.070000. running mean: -5.149965\n",
      "ep 4294: ep_len:79 episode reward: total was 0.040000. running mean: -5.098065\n",
      "ep 4294: ep_len:535 episode reward: total was 6.920000. running mean: -4.977885\n",
      "ep 4294: ep_len:78 episode reward: total was -3.970000. running mean: -4.967806\n",
      "ep 4294: ep_len:535 episode reward: total was -0.840000. running mean: -4.926528\n",
      "ep 4294: ep_len:505 episode reward: total was -15.620000. running mean: -5.033462\n",
      "epsilon:0.010000 episode_count: 30065. steps_count: 13260845.000000\n",
      "ep 4295: ep_len:530 episode reward: total was -1.000000. running mean: -4.993128\n",
      "ep 4295: ep_len:535 episode reward: total was 3.600000. running mean: -4.907196\n",
      "ep 4295: ep_len:640 episode reward: total was -37.510000. running mean: -5.233224\n",
      "ep 4295: ep_len:514 episode reward: total was -23.900000. running mean: -5.419892\n",
      "ep 4295: ep_len:3 episode reward: total was 0.000000. running mean: -5.365693\n",
      "ep 4295: ep_len:645 episode reward: total was -18.450000. running mean: -5.496536\n",
      "ep 4295: ep_len:510 episode reward: total was -10.320000. running mean: -5.544771\n",
      "epsilon:0.010000 episode_count: 30072. steps_count: 13264222.000000\n",
      "ep 4296: ep_len:545 episode reward: total was 4.890000. running mean: -5.440423\n",
      "ep 4296: ep_len:590 episode reward: total was -12.330000. running mean: -5.509319\n",
      "ep 4296: ep_len:500 episode reward: total was -0.910000. running mean: -5.463326\n",
      "ep 4296: ep_len:605 episode reward: total was -28.410000. running mean: -5.692793\n",
      "ep 4296: ep_len:87 episode reward: total was 4.040000. running mean: -5.595465\n",
      "ep 4296: ep_len:510 episode reward: total was -5.960000. running mean: -5.599110\n",
      "ep 4296: ep_len:535 episode reward: total was -22.340000. running mean: -5.766519\n",
      "epsilon:0.010000 episode_count: 30079. steps_count: 13267594.000000\n",
      "ep 4297: ep_len:229 episode reward: total was 4.120000. running mean: -5.667654\n",
      "ep 4297: ep_len:575 episode reward: total was -4.880000. running mean: -5.659777\n",
      "ep 4297: ep_len:372 episode reward: total was 0.210000. running mean: -5.601079\n",
      "ep 4297: ep_len:500 episode reward: total was -17.560000. running mean: -5.720669\n",
      "ep 4297: ep_len:3 episode reward: total was 0.000000. running mean: -5.663462\n",
      "ep 4297: ep_len:298 episode reward: total was 3.200000. running mean: -5.574827\n",
      "ep 4297: ep_len:620 episode reward: total was -40.930000. running mean: -5.928379\n",
      "epsilon:0.010000 episode_count: 30086. steps_count: 13270191.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4298: ep_len:565 episode reward: total was 4.180000. running mean: -5.827295\n",
      "ep 4298: ep_len:1140 episode reward: total was -159.690000. running mean: -7.365922\n",
      "ep 4298: ep_len:500 episode reward: total was -37.280000. running mean: -7.665063\n",
      "ep 4298: ep_len:540 episode reward: total was -5.020000. running mean: -7.638612\n",
      "ep 4298: ep_len:3 episode reward: total was 0.000000. running mean: -7.562226\n",
      "ep 4298: ep_len:600 episode reward: total was -8.530000. running mean: -7.571904\n",
      "ep 4298: ep_len:334 episode reward: total was -5.230000. running mean: -7.548485\n",
      "epsilon:0.010000 episode_count: 30093. steps_count: 13273873.000000\n",
      "ep 4299: ep_len:645 episode reward: total was 4.660000. running mean: -7.426400\n",
      "ep 4299: ep_len:500 episode reward: total was 9.240000. running mean: -7.259736\n",
      "ep 4299: ep_len:530 episode reward: total was -5.150000. running mean: -7.238639\n",
      "ep 4299: ep_len:505 episode reward: total was 11.480000. running mean: -7.051452\n",
      "ep 4299: ep_len:74 episode reward: total was -1.960000. running mean: -7.000538\n",
      "ep 4299: ep_len:615 episode reward: total was -1.540000. running mean: -6.945933\n",
      "ep 4299: ep_len:327 episode reward: total was 2.720000. running mean: -6.849273\n",
      "epsilon:0.010000 episode_count: 30100. steps_count: 13277069.000000\n",
      "ep 4300: ep_len:595 episode reward: total was -7.700000. running mean: -6.857780\n",
      "ep 4300: ep_len:500 episode reward: total was 3.210000. running mean: -6.757103\n",
      "ep 4300: ep_len:560 episode reward: total was 5.640000. running mean: -6.633132\n",
      "ep 4300: ep_len:157 episode reward: total was 1.620000. running mean: -6.550600\n",
      "ep 4300: ep_len:117 episode reward: total was 0.550000. running mean: -6.479594\n",
      "ep 4300: ep_len:580 episode reward: total was -0.150000. running mean: -6.416298\n",
      "ep 4300: ep_len:535 episode reward: total was -14.080000. running mean: -6.492935\n",
      "epsilon:0.010000 episode_count: 30107. steps_count: 13280113.000000\n",
      "ep 4301: ep_len:505 episode reward: total was 1.370000. running mean: -6.414306\n",
      "ep 4301: ep_len:550 episode reward: total was 11.060000. running mean: -6.239563\n",
      "ep 4301: ep_len:695 episode reward: total was -7.620000. running mean: -6.253367\n",
      "ep 4301: ep_len:600 episode reward: total was -19.110000. running mean: -6.381934\n",
      "ep 4301: ep_len:43 episode reward: total was 4.000000. running mean: -6.278114\n",
      "ep 4301: ep_len:935 episode reward: total was -104.440000. running mean: -7.259733\n",
      "ep 4301: ep_len:585 episode reward: total was -35.690000. running mean: -7.544036\n",
      "epsilon:0.010000 episode_count: 30114. steps_count: 13284026.000000\n",
      "ep 4302: ep_len:560 episode reward: total was -17.240000. running mean: -7.640996\n",
      "ep 4302: ep_len:600 episode reward: total was 0.920000. running mean: -7.555386\n",
      "ep 4302: ep_len:392 episode reward: total was -34.860000. running mean: -7.828432\n",
      "ep 4302: ep_len:119 episode reward: total was 0.580000. running mean: -7.744347\n",
      "ep 4302: ep_len:3 episode reward: total was 0.000000. running mean: -7.666904\n",
      "ep 4302: ep_len:261 episode reward: total was 8.660000. running mean: -7.503635\n",
      "ep 4302: ep_len:292 episode reward: total was 1.200000. running mean: -7.416599\n",
      "epsilon:0.010000 episode_count: 30121. steps_count: 13286253.000000\n",
      "ep 4303: ep_len:500 episode reward: total was 15.300000. running mean: -7.189433\n",
      "ep 4303: ep_len:565 episode reward: total was 11.640000. running mean: -7.001138\n",
      "ep 4303: ep_len:431 episode reward: total was 4.260000. running mean: -6.888527\n",
      "ep 4303: ep_len:56 episode reward: total was 2.570000. running mean: -6.793942\n",
      "ep 4303: ep_len:3 episode reward: total was 0.000000. running mean: -6.726002\n",
      "ep 4303: ep_len:535 episode reward: total was -23.030000. running mean: -6.889042\n",
      "ep 4303: ep_len:575 episode reward: total was -0.520000. running mean: -6.825352\n",
      "epsilon:0.010000 episode_count: 30128. steps_count: 13288918.000000\n",
      "ep 4304: ep_len:555 episode reward: total was 8.960000. running mean: -6.667498\n",
      "ep 4304: ep_len:500 episode reward: total was 10.210000. running mean: -6.498723\n",
      "ep 4304: ep_len:640 episode reward: total was 2.210000. running mean: -6.411636\n",
      "ep 4304: ep_len:500 episode reward: total was 6.910000. running mean: -6.278420\n",
      "ep 4304: ep_len:3 episode reward: total was 0.000000. running mean: -6.215635\n",
      "ep 4304: ep_len:500 episode reward: total was -11.510000. running mean: -6.268579\n",
      "ep 4304: ep_len:335 episode reward: total was -6.290000. running mean: -6.268793\n",
      "epsilon:0.010000 episode_count: 30135. steps_count: 13291951.000000\n",
      "ep 4305: ep_len:580 episode reward: total was 11.000000. running mean: -6.096105\n",
      "ep 4305: ep_len:855 episode reward: total was -67.970000. running mean: -6.714844\n",
      "ep 4305: ep_len:640 episode reward: total was -3.430000. running mean: -6.681996\n",
      "ep 4305: ep_len:595 episode reward: total was -2.550000. running mean: -6.640676\n",
      "ep 4305: ep_len:59 episode reward: total was 1.010000. running mean: -6.564169\n",
      "ep 4305: ep_len:520 episode reward: total was -6.030000. running mean: -6.558827\n",
      "ep 4305: ep_len:590 episode reward: total was -19.510000. running mean: -6.688339\n",
      "epsilon:0.010000 episode_count: 30142. steps_count: 13295790.000000\n",
      "ep 4306: ep_len:244 episode reward: total was -2.890000. running mean: -6.650356\n",
      "ep 4306: ep_len:500 episode reward: total was 15.720000. running mean: -6.426652\n",
      "ep 4306: ep_len:605 episode reward: total was -3.850000. running mean: -6.400886\n",
      "ep 4306: ep_len:590 episode reward: total was 8.420000. running mean: -6.252677\n",
      "ep 4306: ep_len:3 episode reward: total was 0.000000. running mean: -6.190150\n",
      "ep 4306: ep_len:282 episode reward: total was -15.330000. running mean: -6.281549\n",
      "ep 4306: ep_len:530 episode reward: total was -30.730000. running mean: -6.526033\n",
      "epsilon:0.010000 episode_count: 30149. steps_count: 13298544.000000\n",
      "ep 4307: ep_len:525 episode reward: total was -0.880000. running mean: -6.469573\n",
      "ep 4307: ep_len:540 episode reward: total was -0.290000. running mean: -6.407777\n",
      "ep 4307: ep_len:74 episode reward: total was -1.960000. running mean: -6.363299\n",
      "ep 4307: ep_len:565 episode reward: total was -13.500000. running mean: -6.434666\n",
      "ep 4307: ep_len:48 episode reward: total was 4.500000. running mean: -6.325320\n",
      "ep 4307: ep_len:505 episode reward: total was -25.040000. running mean: -6.512466\n",
      "ep 4307: ep_len:298 episode reward: total was -11.310000. running mean: -6.560442\n",
      "epsilon:0.010000 episode_count: 30156. steps_count: 13301099.000000\n",
      "ep 4308: ep_len:216 episode reward: total was -3.920000. running mean: -6.534037\n",
      "ep 4308: ep_len:500 episode reward: total was 6.560000. running mean: -6.403097\n",
      "ep 4308: ep_len:640 episode reward: total was -28.040000. running mean: -6.619466\n",
      "ep 4308: ep_len:550 episode reward: total was -14.480000. running mean: -6.698071\n",
      "ep 4308: ep_len:91 episode reward: total was 4.540000. running mean: -6.585691\n",
      "ep 4308: ep_len:252 episode reward: total was 3.650000. running mean: -6.483334\n",
      "ep 4308: ep_len:585 episode reward: total was -8.330000. running mean: -6.501800\n",
      "epsilon:0.010000 episode_count: 30163. steps_count: 13303933.000000\n",
      "ep 4309: ep_len:208 episode reward: total was 4.600000. running mean: -6.390782\n",
      "ep 4309: ep_len:500 episode reward: total was -17.870000. running mean: -6.505575\n",
      "ep 4309: ep_len:660 episode reward: total was -14.730000. running mean: -6.587819\n",
      "ep 4309: ep_len:515 episode reward: total was 3.550000. running mean: -6.486441\n",
      "ep 4309: ep_len:3 episode reward: total was 0.000000. running mean: -6.421576\n",
      "ep 4309: ep_len:690 episode reward: total was -53.310000. running mean: -6.890460\n",
      "ep 4309: ep_len:610 episode reward: total was -9.400000. running mean: -6.915556\n",
      "epsilon:0.010000 episode_count: 30170. steps_count: 13307119.000000\n",
      "ep 4310: ep_len:615 episode reward: total was -27.860000. running mean: -7.125000\n",
      "ep 4310: ep_len:525 episode reward: total was 0.780000. running mean: -7.045950\n",
      "ep 4310: ep_len:665 episode reward: total was -19.390000. running mean: -7.169391\n",
      "ep 4310: ep_len:520 episode reward: total was 12.440000. running mean: -6.973297\n",
      "ep 4310: ep_len:101 episode reward: total was 4.040000. running mean: -6.863164\n",
      "ep 4310: ep_len:500 episode reward: total was -5.560000. running mean: -6.850132\n",
      "ep 4310: ep_len:615 episode reward: total was -16.320000. running mean: -6.944831\n",
      "epsilon:0.010000 episode_count: 30177. steps_count: 13310660.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4311: ep_len:610 episode reward: total was 9.500000. running mean: -6.780383\n",
      "ep 4311: ep_len:590 episode reward: total was 0.710000. running mean: -6.705479\n",
      "ep 4311: ep_len:620 episode reward: total was 4.070000. running mean: -6.597724\n",
      "ep 4311: ep_len:396 episode reward: total was -26.130000. running mean: -6.793047\n",
      "ep 4311: ep_len:108 episode reward: total was 2.520000. running mean: -6.699916\n",
      "ep 4311: ep_len:500 episode reward: total was -27.630000. running mean: -6.909217\n",
      "ep 4311: ep_len:595 episode reward: total was -23.580000. running mean: -7.075925\n",
      "epsilon:0.010000 episode_count: 30184. steps_count: 13314079.000000\n",
      "ep 4312: ep_len:520 episode reward: total was -18.910000. running mean: -7.194266\n",
      "ep 4312: ep_len:500 episode reward: total was 13.210000. running mean: -6.990223\n",
      "ep 4312: ep_len:550 episode reward: total was -2.580000. running mean: -6.946121\n",
      "ep 4312: ep_len:610 episode reward: total was -13.530000. running mean: -7.011960\n",
      "ep 4312: ep_len:3 episode reward: total was 0.000000. running mean: -6.941840\n",
      "ep 4312: ep_len:540 episode reward: total was -3.480000. running mean: -6.907222\n",
      "ep 4312: ep_len:600 episode reward: total was -5.020000. running mean: -6.888349\n",
      "epsilon:0.010000 episode_count: 30191. steps_count: 13317402.000000\n",
      "ep 4313: ep_len:520 episode reward: total was -34.630000. running mean: -7.165766\n",
      "ep 4313: ep_len:187 episode reward: total was -6.380000. running mean: -7.157908\n",
      "ep 4313: ep_len:515 episode reward: total was -9.970000. running mean: -7.186029\n",
      "ep 4313: ep_len:500 episode reward: total was -18.510000. running mean: -7.299269\n",
      "ep 4313: ep_len:3 episode reward: total was 0.000000. running mean: -7.226276\n",
      "ep 4313: ep_len:670 episode reward: total was -36.300000. running mean: -7.517013\n",
      "ep 4313: ep_len:570 episode reward: total was -1.780000. running mean: -7.459643\n",
      "epsilon:0.010000 episode_count: 30198. steps_count: 13320367.000000\n",
      "ep 4314: ep_len:119 episode reward: total was -1.950000. running mean: -7.404547\n",
      "ep 4314: ep_len:321 episode reward: total was -9.350000. running mean: -7.424001\n",
      "ep 4314: ep_len:525 episode reward: total was -3.560000. running mean: -7.385361\n",
      "ep 4314: ep_len:500 episode reward: total was 5.530000. running mean: -7.256208\n",
      "ep 4314: ep_len:92 episode reward: total was 6.530000. running mean: -7.118346\n",
      "ep 4314: ep_len:685 episode reward: total was 7.970000. running mean: -6.967462\n",
      "ep 4314: ep_len:535 episode reward: total was 1.690000. running mean: -6.880888\n",
      "epsilon:0.010000 episode_count: 30205. steps_count: 13323144.000000\n",
      "ep 4315: ep_len:510 episode reward: total was -4.960000. running mean: -6.861679\n",
      "ep 4315: ep_len:545 episode reward: total was 18.920000. running mean: -6.603862\n",
      "ep 4315: ep_len:448 episode reward: total was -14.250000. running mean: -6.680323\n",
      "ep 4315: ep_len:505 episode reward: total was -18.120000. running mean: -6.794720\n",
      "ep 4315: ep_len:55 episode reward: total was 5.010000. running mean: -6.676673\n",
      "ep 4315: ep_len:500 episode reward: total was 7.200000. running mean: -6.537906\n",
      "ep 4315: ep_len:550 episode reward: total was -15.430000. running mean: -6.626827\n",
      "epsilon:0.010000 episode_count: 30212. steps_count: 13326257.000000\n",
      "ep 4316: ep_len:515 episode reward: total was -11.880000. running mean: -6.679359\n",
      "ep 4316: ep_len:545 episode reward: total was 3.350000. running mean: -6.579065\n",
      "ep 4316: ep_len:79 episode reward: total was 0.040000. running mean: -6.512875\n",
      "ep 4316: ep_len:500 episode reward: total was -28.670000. running mean: -6.734446\n",
      "ep 4316: ep_len:3 episode reward: total was 0.000000. running mean: -6.667101\n",
      "ep 4316: ep_len:500 episode reward: total was 4.940000. running mean: -6.551030\n",
      "ep 4316: ep_len:555 episode reward: total was -11.600000. running mean: -6.601520\n",
      "epsilon:0.010000 episode_count: 30219. steps_count: 13328954.000000\n",
      "ep 4317: ep_len:595 episode reward: total was 5.450000. running mean: -6.481005\n",
      "ep 4317: ep_len:500 episode reward: total was -8.290000. running mean: -6.499095\n",
      "ep 4317: ep_len:455 episode reward: total was 2.260000. running mean: -6.411504\n",
      "ep 4317: ep_len:540 episode reward: total was -7.460000. running mean: -6.421989\n",
      "ep 4317: ep_len:73 episode reward: total was -11.000000. running mean: -6.467769\n",
      "ep 4317: ep_len:700 episode reward: total was -41.790000. running mean: -6.820991\n",
      "ep 4317: ep_len:500 episode reward: total was -11.230000. running mean: -6.865081\n",
      "epsilon:0.010000 episode_count: 30226. steps_count: 13332317.000000\n",
      "ep 4318: ep_len:500 episode reward: total was 10.940000. running mean: -6.687031\n",
      "ep 4318: ep_len:525 episode reward: total was -94.950000. running mean: -7.569660\n",
      "ep 4318: ep_len:610 episode reward: total was 1.530000. running mean: -7.478664\n",
      "ep 4318: ep_len:500 episode reward: total was 7.600000. running mean: -7.327877\n",
      "ep 4318: ep_len:3 episode reward: total was 0.000000. running mean: -7.254598\n",
      "ep 4318: ep_len:500 episode reward: total was 0.260000. running mean: -7.179452\n",
      "ep 4318: ep_len:251 episode reward: total was 3.690000. running mean: -7.070758\n",
      "epsilon:0.010000 episode_count: 30233. steps_count: 13335206.000000\n",
      "ep 4319: ep_len:660 episode reward: total was -8.640000. running mean: -7.086450\n",
      "ep 4319: ep_len:605 episode reward: total was -27.940000. running mean: -7.294986\n",
      "ep 4319: ep_len:585 episode reward: total was -16.750000. running mean: -7.389536\n",
      "ep 4319: ep_len:520 episode reward: total was -21.130000. running mean: -7.526940\n",
      "ep 4319: ep_len:3 episode reward: total was 0.000000. running mean: -7.451671\n",
      "ep 4319: ep_len:500 episode reward: total was -1.550000. running mean: -7.392654\n",
      "ep 4319: ep_len:1065 episode reward: total was -156.710000. running mean: -8.885828\n",
      "epsilon:0.010000 episode_count: 30240. steps_count: 13339144.000000\n",
      "ep 4320: ep_len:630 episode reward: total was 8.610000. running mean: -8.710869\n",
      "ep 4320: ep_len:625 episode reward: total was 14.640000. running mean: -8.477361\n",
      "ep 4320: ep_len:595 episode reward: total was 3.860000. running mean: -8.353987\n",
      "ep 4320: ep_len:505 episode reward: total was -5.980000. running mean: -8.330247\n",
      "ep 4320: ep_len:3 episode reward: total was 0.000000. running mean: -8.246945\n",
      "ep 4320: ep_len:505 episode reward: total was -34.290000. running mean: -8.507375\n",
      "ep 4320: ep_len:500 episode reward: total was -10.570000. running mean: -8.528002\n",
      "epsilon:0.010000 episode_count: 30247. steps_count: 13342507.000000\n",
      "ep 4321: ep_len:535 episode reward: total was -2.950000. running mean: -8.472222\n",
      "ep 4321: ep_len:560 episode reward: total was -0.320000. running mean: -8.390699\n",
      "ep 4321: ep_len:660 episode reward: total was -20.960000. running mean: -8.516392\n",
      "ep 4321: ep_len:56 episode reward: total was 1.560000. running mean: -8.415628\n",
      "ep 4321: ep_len:3 episode reward: total was 0.000000. running mean: -8.331472\n",
      "ep 4321: ep_len:600 episode reward: total was -7.120000. running mean: -8.319357\n",
      "ep 4321: ep_len:580 episode reward: total was -3.990000. running mean: -8.276064\n",
      "epsilon:0.010000 episode_count: 30254. steps_count: 13345501.000000\n",
      "ep 4322: ep_len:194 episode reward: total was -16.840000. running mean: -8.361703\n",
      "ep 4322: ep_len:555 episode reward: total was -6.130000. running mean: -8.339386\n",
      "ep 4322: ep_len:540 episode reward: total was -42.040000. running mean: -8.676392\n",
      "ep 4322: ep_len:520 episode reward: total was -1.610000. running mean: -8.605728\n",
      "ep 4322: ep_len:99 episode reward: total was -9.960000. running mean: -8.619271\n",
      "ep 4322: ep_len:500 episode reward: total was 8.090000. running mean: -8.452178\n",
      "ep 4322: ep_len:550 episode reward: total was -6.430000. running mean: -8.431957\n",
      "epsilon:0.010000 episode_count: 30261. steps_count: 13348459.000000\n",
      "ep 4323: ep_len:500 episode reward: total was 9.300000. running mean: -8.254637\n",
      "ep 4323: ep_len:356 episode reward: total was -28.360000. running mean: -8.455691\n",
      "ep 4323: ep_len:595 episode reward: total was -4.880000. running mean: -8.419934\n",
      "ep 4323: ep_len:412 episode reward: total was 8.890000. running mean: -8.246834\n",
      "ep 4323: ep_len:3 episode reward: total was 0.000000. running mean: -8.164366\n",
      "ep 4323: ep_len:300 episode reward: total was 4.190000. running mean: -8.040822\n",
      "ep 4323: ep_len:500 episode reward: total was -14.980000. running mean: -8.110214\n",
      "epsilon:0.010000 episode_count: 30268. steps_count: 13351125.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4324: ep_len:615 episode reward: total was 0.100000. running mean: -8.028112\n",
      "ep 4324: ep_len:505 episode reward: total was 21.700000. running mean: -7.730831\n",
      "ep 4324: ep_len:393 episode reward: total was 9.280000. running mean: -7.560723\n",
      "ep 4324: ep_len:545 episode reward: total was 4.380000. running mean: -7.441315\n",
      "ep 4324: ep_len:3 episode reward: total was 0.000000. running mean: -7.366902\n",
      "ep 4324: ep_len:158 episode reward: total was 8.620000. running mean: -7.207033\n",
      "ep 4324: ep_len:515 episode reward: total was -13.600000. running mean: -7.270963\n",
      "epsilon:0.010000 episode_count: 30275. steps_count: 13353859.000000\n",
      "ep 4325: ep_len:645 episode reward: total was -12.130000. running mean: -7.319553\n",
      "ep 4325: ep_len:500 episode reward: total was 3.980000. running mean: -7.206558\n",
      "ep 4325: ep_len:595 episode reward: total was 1.440000. running mean: -7.120092\n",
      "ep 4325: ep_len:132 episode reward: total was 5.610000. running mean: -6.992791\n",
      "ep 4325: ep_len:3 episode reward: total was 0.000000. running mean: -6.922863\n",
      "ep 4325: ep_len:640 episode reward: total was -3.490000. running mean: -6.888535\n",
      "ep 4325: ep_len:600 episode reward: total was -15.820000. running mean: -6.977849\n",
      "epsilon:0.010000 episode_count: 30282. steps_count: 13356974.000000\n",
      "ep 4326: ep_len:865 episode reward: total was -61.160000. running mean: -7.519671\n",
      "ep 4326: ep_len:274 episode reward: total was -4.410000. running mean: -7.488574\n",
      "ep 4326: ep_len:540 episode reward: total was -1.570000. running mean: -7.429388\n",
      "ep 4326: ep_len:500 episode reward: total was 11.410000. running mean: -7.240995\n",
      "ep 4326: ep_len:3 episode reward: total was 0.000000. running mean: -7.168585\n",
      "ep 4326: ep_len:500 episode reward: total was 5.290000. running mean: -7.043999\n",
      "ep 4326: ep_len:179 episode reward: total was -3.870000. running mean: -7.012259\n",
      "epsilon:0.010000 episode_count: 30289. steps_count: 13359835.000000\n",
      "ep 4327: ep_len:195 episode reward: total was 3.600000. running mean: -6.906136\n",
      "ep 4327: ep_len:174 episode reward: total was -8.910000. running mean: -6.926175\n",
      "ep 4327: ep_len:680 episode reward: total was -7.100000. running mean: -6.927913\n",
      "ep 4327: ep_len:500 episode reward: total was 6.100000. running mean: -6.797634\n",
      "ep 4327: ep_len:3 episode reward: total was 0.000000. running mean: -6.729658\n",
      "ep 4327: ep_len:565 episode reward: total was -9.240000. running mean: -6.754761\n",
      "ep 4327: ep_len:630 episode reward: total was -18.030000. running mean: -6.867513\n",
      "epsilon:0.010000 episode_count: 30296. steps_count: 13362582.000000\n",
      "ep 4328: ep_len:500 episode reward: total was -10.360000. running mean: -6.902438\n",
      "ep 4328: ep_len:610 episode reward: total was -21.110000. running mean: -7.044514\n",
      "ep 4328: ep_len:417 episode reward: total was -6.740000. running mean: -7.041469\n",
      "ep 4328: ep_len:500 episode reward: total was 5.440000. running mean: -6.916654\n",
      "ep 4328: ep_len:119 episode reward: total was -12.440000. running mean: -6.971888\n",
      "ep 4328: ep_len:620 episode reward: total was -9.810000. running mean: -7.000269\n",
      "ep 4328: ep_len:335 episode reward: total was -31.350000. running mean: -7.243766\n",
      "epsilon:0.010000 episode_count: 30303. steps_count: 13365683.000000\n",
      "ep 4329: ep_len:530 episode reward: total was -11.440000. running mean: -7.285728\n",
      "ep 4329: ep_len:655 episode reward: total was -44.200000. running mean: -7.654871\n",
      "ep 4329: ep_len:500 episode reward: total was -7.410000. running mean: -7.652422\n",
      "ep 4329: ep_len:625 episode reward: total was 14.640000. running mean: -7.429498\n",
      "ep 4329: ep_len:3 episode reward: total was 0.000000. running mean: -7.355203\n",
      "ep 4329: ep_len:235 episode reward: total was 10.140000. running mean: -7.180251\n",
      "ep 4329: ep_len:500 episode reward: total was -20.550000. running mean: -7.313949\n",
      "epsilon:0.010000 episode_count: 30310. steps_count: 13368731.000000\n",
      "ep 4330: ep_len:530 episode reward: total was -10.200000. running mean: -7.342809\n",
      "ep 4330: ep_len:530 episode reward: total was 11.250000. running mean: -7.156881\n",
      "ep 4330: ep_len:585 episode reward: total was 3.940000. running mean: -7.045912\n",
      "ep 4330: ep_len:760 episode reward: total was -126.430000. running mean: -8.239753\n",
      "ep 4330: ep_len:3 episode reward: total was 0.000000. running mean: -8.157356\n",
      "ep 4330: ep_len:535 episode reward: total was -14.480000. running mean: -8.220582\n",
      "ep 4330: ep_len:605 episode reward: total was -18.040000. running mean: -8.318776\n",
      "epsilon:0.010000 episode_count: 30317. steps_count: 13372279.000000\n",
      "ep 4331: ep_len:247 episode reward: total was 0.090000. running mean: -8.234688\n",
      "ep 4331: ep_len:545 episode reward: total was -2.390000. running mean: -8.176242\n",
      "ep 4331: ep_len:560 episode reward: total was -28.810000. running mean: -8.382579\n",
      "ep 4331: ep_len:505 episode reward: total was 8.450000. running mean: -8.214253\n",
      "ep 4331: ep_len:54 episode reward: total was -10.000000. running mean: -8.232111\n",
      "ep 4331: ep_len:625 episode reward: total was -26.630000. running mean: -8.416090\n",
      "ep 4331: ep_len:500 episode reward: total was -16.770000. running mean: -8.499629\n",
      "epsilon:0.010000 episode_count: 30324. steps_count: 13375315.000000\n",
      "ep 4332: ep_len:565 episode reward: total was -15.720000. running mean: -8.571832\n",
      "ep 4332: ep_len:525 episode reward: total was -5.430000. running mean: -8.540414\n",
      "ep 4332: ep_len:620 episode reward: total was -3.700000. running mean: -8.492010\n",
      "ep 4332: ep_len:520 episode reward: total was -38.180000. running mean: -8.788890\n",
      "ep 4332: ep_len:85 episode reward: total was -2.450000. running mean: -8.725501\n",
      "ep 4332: ep_len:545 episode reward: total was -14.850000. running mean: -8.786746\n",
      "ep 4332: ep_len:500 episode reward: total was -7.740000. running mean: -8.776279\n",
      "epsilon:0.010000 episode_count: 30331. steps_count: 13378675.000000\n",
      "ep 4333: ep_len:565 episode reward: total was -14.230000. running mean: -8.830816\n",
      "ep 4333: ep_len:630 episode reward: total was -11.580000. running mean: -8.858308\n",
      "ep 4333: ep_len:500 episode reward: total was 2.030000. running mean: -8.749425\n",
      "ep 4333: ep_len:560 episode reward: total was 4.070000. running mean: -8.621230\n",
      "ep 4333: ep_len:40 episode reward: total was -3.990000. running mean: -8.574918\n",
      "ep 4333: ep_len:555 episode reward: total was 4.070000. running mean: -8.448469\n",
      "ep 4333: ep_len:590 episode reward: total was -7.530000. running mean: -8.439284\n",
      "epsilon:0.010000 episode_count: 30338. steps_count: 13382115.000000\n",
      "ep 4334: ep_len:222 episode reward: total was 7.140000. running mean: -8.283491\n",
      "ep 4334: ep_len:520 episode reward: total was -4.670000. running mean: -8.247356\n",
      "ep 4334: ep_len:505 episode reward: total was -2.480000. running mean: -8.189683\n",
      "ep 4334: ep_len:500 episode reward: total was 13.440000. running mean: -7.973386\n",
      "ep 4334: ep_len:114 episode reward: total was -3.960000. running mean: -7.933252\n",
      "ep 4334: ep_len:505 episode reward: total was -19.860000. running mean: -8.052520\n",
      "ep 4334: ep_len:550 episode reward: total was -21.480000. running mean: -8.186794\n",
      "epsilon:0.010000 episode_count: 30345. steps_count: 13385031.000000\n",
      "ep 4335: ep_len:124 episode reward: total was -13.920000. running mean: -8.244126\n",
      "ep 4335: ep_len:780 episode reward: total was -143.440000. running mean: -9.596085\n",
      "ep 4335: ep_len:598 episode reward: total was -40.540000. running mean: -9.905524\n",
      "ep 4335: ep_len:565 episode reward: total was 2.080000. running mean: -9.785669\n",
      "ep 4335: ep_len:71 episode reward: total was 4.500000. running mean: -9.642812\n",
      "ep 4335: ep_len:158 episode reward: total was 7.580000. running mean: -9.470584\n",
      "ep 4335: ep_len:750 episode reward: total was -56.480000. running mean: -9.940678\n",
      "epsilon:0.010000 episode_count: 30352. steps_count: 13388077.000000\n",
      "ep 4336: ep_len:580 episode reward: total was 14.970000. running mean: -9.691572\n",
      "ep 4336: ep_len:346 episode reward: total was -41.370000. running mean: -10.008356\n",
      "ep 4336: ep_len:660 episode reward: total was -6.680000. running mean: -9.975072\n",
      "ep 4336: ep_len:540 episode reward: total was -8.960000. running mean: -9.964922\n",
      "ep 4336: ep_len:3 episode reward: total was 0.000000. running mean: -9.865272\n",
      "ep 4336: ep_len:535 episode reward: total was 10.120000. running mean: -9.665420\n",
      "ep 4336: ep_len:615 episode reward: total was -18.550000. running mean: -9.754266\n",
      "epsilon:0.010000 episode_count: 30359. steps_count: 13391356.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4337: ep_len:122 episode reward: total was 1.080000. running mean: -9.645923\n",
      "ep 4337: ep_len:530 episode reward: total was 19.390000. running mean: -9.355564\n",
      "ep 4337: ep_len:635 episode reward: total was -2.850000. running mean: -9.290508\n",
      "ep 4337: ep_len:500 episode reward: total was 3.540000. running mean: -9.162203\n",
      "ep 4337: ep_len:3 episode reward: total was 0.000000. running mean: -9.070581\n",
      "ep 4337: ep_len:580 episode reward: total was -19.790000. running mean: -9.177775\n",
      "ep 4337: ep_len:605 episode reward: total was -30.480000. running mean: -9.390797\n",
      "epsilon:0.010000 episode_count: 30366. steps_count: 13394331.000000\n",
      "ep 4338: ep_len:91 episode reward: total was 4.020000. running mean: -9.256689\n",
      "ep 4338: ep_len:730 episode reward: total was -130.470000. running mean: -10.468822\n",
      "ep 4338: ep_len:655 episode reward: total was 2.860000. running mean: -10.335534\n",
      "ep 4338: ep_len:545 episode reward: total was 4.410000. running mean: -10.188079\n",
      "ep 4338: ep_len:3 episode reward: total was 0.000000. running mean: -10.086198\n",
      "ep 4338: ep_len:670 episode reward: total was -7.150000. running mean: -10.056836\n",
      "ep 4338: ep_len:311 episode reward: total was -4.820000. running mean: -10.004468\n",
      "epsilon:0.010000 episode_count: 30373. steps_count: 13397336.000000\n",
      "ep 4339: ep_len:125 episode reward: total was 3.080000. running mean: -9.873623\n",
      "ep 4339: ep_len:500 episode reward: total was -7.930000. running mean: -9.854187\n",
      "ep 4339: ep_len:79 episode reward: total was 0.040000. running mean: -9.755245\n",
      "ep 4339: ep_len:585 episode reward: total was -8.500000. running mean: -9.742693\n",
      "ep 4339: ep_len:103 episode reward: total was 5.540000. running mean: -9.589866\n",
      "ep 4339: ep_len:881 episode reward: total was -59.680000. running mean: -10.090767\n",
      "ep 4339: ep_len:500 episode reward: total was -10.950000. running mean: -10.099359\n",
      "epsilon:0.010000 episode_count: 30380. steps_count: 13400109.000000\n",
      "ep 4340: ep_len:565 episode reward: total was -9.410000. running mean: -10.092466\n",
      "ep 4340: ep_len:540 episode reward: total was 3.090000. running mean: -9.960641\n",
      "ep 4340: ep_len:585 episode reward: total was -1.880000. running mean: -9.879835\n",
      "ep 4340: ep_len:500 episode reward: total was -8.000000. running mean: -9.861036\n",
      "ep 4340: ep_len:83 episode reward: total was 2.530000. running mean: -9.737126\n",
      "ep 4340: ep_len:180 episode reward: total was 5.610000. running mean: -9.583655\n",
      "ep 4340: ep_len:178 episode reward: total was -26.470000. running mean: -9.752518\n",
      "epsilon:0.010000 episode_count: 30387. steps_count: 13402740.000000\n",
      "ep 4341: ep_len:555 episode reward: total was -0.110000. running mean: -9.656093\n",
      "ep 4341: ep_len:150 episode reward: total was -3.930000. running mean: -9.598832\n",
      "ep 4341: ep_len:580 episode reward: total was 0.260000. running mean: -9.500244\n",
      "ep 4341: ep_len:570 episode reward: total was 10.950000. running mean: -9.295741\n",
      "ep 4341: ep_len:51 episode reward: total was 5.000000. running mean: -9.152784\n",
      "ep 4341: ep_len:600 episode reward: total was 3.500000. running mean: -9.026256\n",
      "ep 4341: ep_len:525 episode reward: total was -1.540000. running mean: -8.951393\n",
      "epsilon:0.010000 episode_count: 30394. steps_count: 13405771.000000\n",
      "ep 4342: ep_len:230 episode reward: total was -1.410000. running mean: -8.875979\n",
      "ep 4342: ep_len:500 episode reward: total was 18.860000. running mean: -8.598620\n",
      "ep 4342: ep_len:605 episode reward: total was 1.960000. running mean: -8.493033\n",
      "ep 4342: ep_len:132 episode reward: total was 3.590000. running mean: -8.372203\n",
      "ep 4342: ep_len:3 episode reward: total was 0.000000. running mean: -8.288481\n",
      "ep 4342: ep_len:680 episode reward: total was 2.850000. running mean: -8.177096\n",
      "ep 4342: ep_len:555 episode reward: total was -9.070000. running mean: -8.186025\n",
      "epsilon:0.010000 episode_count: 30401. steps_count: 13408476.000000\n",
      "ep 4343: ep_len:670 episode reward: total was -12.660000. running mean: -8.230765\n",
      "ep 4343: ep_len:555 episode reward: total was -1.130000. running mean: -8.159757\n",
      "ep 4343: ep_len:630 episode reward: total was 4.630000. running mean: -8.031860\n",
      "ep 4343: ep_len:545 episode reward: total was -20.000000. running mean: -8.151541\n",
      "ep 4343: ep_len:3 episode reward: total was 0.000000. running mean: -8.070026\n",
      "ep 4343: ep_len:520 episode reward: total was -6.650000. running mean: -8.055826\n",
      "ep 4343: ep_len:595 episode reward: total was -16.440000. running mean: -8.139667\n",
      "epsilon:0.010000 episode_count: 30408. steps_count: 13411994.000000\n",
      "ep 4344: ep_len:545 episode reward: total was 2.390000. running mean: -8.034371\n",
      "ep 4344: ep_len:500 episode reward: total was -23.350000. running mean: -8.187527\n",
      "ep 4344: ep_len:720 episode reward: total was -13.710000. running mean: -8.242752\n",
      "ep 4344: ep_len:595 episode reward: total was -0.490000. running mean: -8.165224\n",
      "ep 4344: ep_len:3 episode reward: total was 0.000000. running mean: -8.083572\n",
      "ep 4344: ep_len:555 episode reward: total was 4.070000. running mean: -7.962036\n",
      "ep 4344: ep_len:545 episode reward: total was -13.870000. running mean: -8.021116\n",
      "epsilon:0.010000 episode_count: 30415. steps_count: 13415457.000000\n",
      "ep 4345: ep_len:525 episode reward: total was 5.420000. running mean: -7.886705\n",
      "ep 4345: ep_len:500 episode reward: total was 5.660000. running mean: -7.751238\n",
      "ep 4345: ep_len:384 episode reward: total was 13.270000. running mean: -7.541025\n",
      "ep 4345: ep_len:520 episode reward: total was -4.060000. running mean: -7.506215\n",
      "ep 4345: ep_len:130 episode reward: total was 4.560000. running mean: -7.385553\n",
      "ep 4345: ep_len:500 episode reward: total was -15.980000. running mean: -7.471497\n",
      "ep 4345: ep_len:331 episode reward: total was -15.300000. running mean: -7.549782\n",
      "epsilon:0.010000 episode_count: 30422. steps_count: 13418347.000000\n",
      "ep 4346: ep_len:500 episode reward: total was -29.000000. running mean: -7.764285\n",
      "ep 4346: ep_len:570 episode reward: total was -4.760000. running mean: -7.734242\n",
      "ep 4346: ep_len:575 episode reward: total was 4.460000. running mean: -7.612299\n",
      "ep 4346: ep_len:500 episode reward: total was 9.560000. running mean: -7.440576\n",
      "ep 4346: ep_len:51 episode reward: total was 3.500000. running mean: -7.331171\n",
      "ep 4346: ep_len:670 episode reward: total was 0.350000. running mean: -7.254359\n",
      "ep 4346: ep_len:605 episode reward: total was -17.350000. running mean: -7.355315\n",
      "epsilon:0.010000 episode_count: 30429. steps_count: 13421818.000000\n",
      "ep 4347: ep_len:510 episode reward: total was 4.930000. running mean: -7.232462\n",
      "ep 4347: ep_len:500 episode reward: total was -1.930000. running mean: -7.179437\n",
      "ep 4347: ep_len:565 episode reward: total was -11.000000. running mean: -7.217643\n",
      "ep 4347: ep_len:500 episode reward: total was -6.510000. running mean: -7.210567\n",
      "ep 4347: ep_len:125 episode reward: total was 3.050000. running mean: -7.107961\n",
      "ep 4347: ep_len:670 episode reward: total was -11.150000. running mean: -7.148381\n",
      "ep 4347: ep_len:535 episode reward: total was -7.580000. running mean: -7.152698\n",
      "epsilon:0.010000 episode_count: 30436. steps_count: 13425223.000000\n",
      "ep 4348: ep_len:515 episode reward: total was -1.490000. running mean: -7.096071\n",
      "ep 4348: ep_len:630 episode reward: total was 4.560000. running mean: -6.979510\n",
      "ep 4348: ep_len:670 episode reward: total was -10.670000. running mean: -7.016415\n",
      "ep 4348: ep_len:500 episode reward: total was 13.110000. running mean: -6.815151\n",
      "ep 4348: ep_len:3 episode reward: total was 0.000000. running mean: -6.746999\n",
      "ep 4348: ep_len:595 episode reward: total was 0.490000. running mean: -6.674629\n",
      "ep 4348: ep_len:535 episode reward: total was -9.580000. running mean: -6.703683\n",
      "epsilon:0.010000 episode_count: 30443. steps_count: 13428671.000000\n",
      "ep 4349: ep_len:505 episode reward: total was -7.480000. running mean: -6.711446\n",
      "ep 4349: ep_len:640 episode reward: total was -35.380000. running mean: -6.998132\n",
      "ep 4349: ep_len:545 episode reward: total was -11.890000. running mean: -7.047050\n",
      "ep 4349: ep_len:555 episode reward: total was 7.430000. running mean: -6.902280\n",
      "ep 4349: ep_len:3 episode reward: total was 0.000000. running mean: -6.833257\n",
      "ep 4349: ep_len:615 episode reward: total was -19.790000. running mean: -6.962824\n",
      "ep 4349: ep_len:555 episode reward: total was -21.540000. running mean: -7.108596\n",
      "epsilon:0.010000 episode_count: 30450. steps_count: 13432089.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4350: ep_len:685 episode reward: total was -12.690000. running mean: -7.164410\n",
      "ep 4350: ep_len:745 episode reward: total was -30.750000. running mean: -7.400266\n",
      "ep 4350: ep_len:595 episode reward: total was -12.780000. running mean: -7.454063\n",
      "ep 4350: ep_len:500 episode reward: total was -1.620000. running mean: -7.395723\n",
      "ep 4350: ep_len:85 episode reward: total was -1.960000. running mean: -7.341366\n",
      "ep 4350: ep_len:213 episode reward: total was 0.690000. running mean: -7.261052\n",
      "ep 4350: ep_len:510 episode reward: total was -27.610000. running mean: -7.464541\n",
      "epsilon:0.010000 episode_count: 30457. steps_count: 13435422.000000\n",
      "ep 4351: ep_len:570 episode reward: total was -0.430000. running mean: -7.394196\n",
      "ep 4351: ep_len:560 episode reward: total was -11.230000. running mean: -7.432554\n",
      "ep 4351: ep_len:675 episode reward: total was 2.380000. running mean: -7.334428\n",
      "ep 4351: ep_len:685 episode reward: total was -48.660000. running mean: -7.747684\n",
      "ep 4351: ep_len:3 episode reward: total was 0.000000. running mean: -7.670207\n",
      "ep 4351: ep_len:555 episode reward: total was 3.610000. running mean: -7.557405\n",
      "ep 4351: ep_len:515 episode reward: total was -4.900000. running mean: -7.530831\n",
      "epsilon:0.010000 episode_count: 30464. steps_count: 13438985.000000\n",
      "ep 4352: ep_len:500 episode reward: total was 8.730000. running mean: -7.368223\n",
      "ep 4352: ep_len:600 episode reward: total was -22.580000. running mean: -7.520341\n",
      "ep 4352: ep_len:444 episode reward: total was -8.280000. running mean: -7.527937\n",
      "ep 4352: ep_len:515 episode reward: total was -4.520000. running mean: -7.497858\n",
      "ep 4352: ep_len:3 episode reward: total was 0.000000. running mean: -7.422879\n",
      "ep 4352: ep_len:329 episode reward: total was 2.680000. running mean: -7.321851\n",
      "ep 4352: ep_len:535 episode reward: total was 0.250000. running mean: -7.246132\n",
      "epsilon:0.010000 episode_count: 30471. steps_count: 13441911.000000\n",
      "ep 4353: ep_len:222 episode reward: total was -4.370000. running mean: -7.217371\n",
      "ep 4353: ep_len:560 episode reward: total was -0.770000. running mean: -7.152897\n",
      "ep 4353: ep_len:79 episode reward: total was 0.040000. running mean: -7.080968\n",
      "ep 4353: ep_len:605 episode reward: total was -18.410000. running mean: -7.194258\n",
      "ep 4353: ep_len:93 episode reward: total was 3.530000. running mean: -7.087016\n",
      "ep 4353: ep_len:175 episode reward: total was 10.130000. running mean: -6.914846\n",
      "ep 4353: ep_len:326 episode reward: total was -11.850000. running mean: -6.964197\n",
      "epsilon:0.010000 episode_count: 30478. steps_count: 13443971.000000\n",
      "ep 4354: ep_len:229 episode reward: total was 3.110000. running mean: -6.863455\n",
      "ep 4354: ep_len:605 episode reward: total was -27.890000. running mean: -7.073721\n",
      "ep 4354: ep_len:630 episode reward: total was 6.510000. running mean: -6.937883\n",
      "ep 4354: ep_len:610 episode reward: total was -0.000000. running mean: -6.868505\n",
      "ep 4354: ep_len:3 episode reward: total was 0.000000. running mean: -6.799820\n",
      "ep 4354: ep_len:730 episode reward: total was -35.300000. running mean: -7.084821\n",
      "ep 4354: ep_len:520 episode reward: total was -14.100000. running mean: -7.154973\n",
      "epsilon:0.010000 episode_count: 30485. steps_count: 13447298.000000\n",
      "ep 4355: ep_len:565 episode reward: total was -16.190000. running mean: -7.245323\n",
      "ep 4355: ep_len:550 episode reward: total was 1.780000. running mean: -7.155070\n",
      "ep 4355: ep_len:500 episode reward: total was -39.130000. running mean: -7.474819\n",
      "ep 4355: ep_len:500 episode reward: total was -11.560000. running mean: -7.515671\n",
      "ep 4355: ep_len:76 episode reward: total was 5.520000. running mean: -7.385315\n",
      "ep 4355: ep_len:585 episode reward: total was -7.050000. running mean: -7.381961\n",
      "ep 4355: ep_len:590 episode reward: total was -3.270000. running mean: -7.340842\n",
      "epsilon:0.010000 episode_count: 30492. steps_count: 13450664.000000\n",
      "ep 4356: ep_len:126 episode reward: total was 2.070000. running mean: -7.246733\n",
      "ep 4356: ep_len:585 episode reward: total was 4.440000. running mean: -7.129866\n",
      "ep 4356: ep_len:550 episode reward: total was -22.890000. running mean: -7.287467\n",
      "ep 4356: ep_len:500 episode reward: total was 10.110000. running mean: -7.113493\n",
      "ep 4356: ep_len:3 episode reward: total was 0.000000. running mean: -7.042358\n",
      "ep 4356: ep_len:585 episode reward: total was -12.650000. running mean: -7.098434\n",
      "ep 4356: ep_len:525 episode reward: total was -9.620000. running mean: -7.123650\n",
      "epsilon:0.010000 episode_count: 30499. steps_count: 13453538.000000\n",
      "ep 4357: ep_len:223 episode reward: total was -15.390000. running mean: -7.206313\n",
      "ep 4357: ep_len:500 episode reward: total was 14.230000. running mean: -6.991950\n",
      "ep 4357: ep_len:605 episode reward: total was -28.320000. running mean: -7.205231\n",
      "ep 4357: ep_len:540 episode reward: total was -0.910000. running mean: -7.142278\n",
      "ep 4357: ep_len:108 episode reward: total was 6.040000. running mean: -7.010456\n",
      "ep 4357: ep_len:610 episode reward: total was -5.630000. running mean: -6.996651\n",
      "ep 4357: ep_len:500 episode reward: total was -19.930000. running mean: -7.125985\n",
      "epsilon:0.010000 episode_count: 30506. steps_count: 13456624.000000\n",
      "ep 4358: ep_len:530 episode reward: total was -8.990000. running mean: -7.144625\n",
      "ep 4358: ep_len:500 episode reward: total was 1.530000. running mean: -7.057878\n",
      "ep 4358: ep_len:380 episode reward: total was 1.750000. running mean: -6.969800\n",
      "ep 4358: ep_len:530 episode reward: total was 4.160000. running mean: -6.858502\n",
      "ep 4358: ep_len:3 episode reward: total was 0.000000. running mean: -6.789917\n",
      "ep 4358: ep_len:173 episode reward: total was 7.610000. running mean: -6.645918\n",
      "ep 4358: ep_len:560 episode reward: total was -6.040000. running mean: -6.639858\n",
      "epsilon:0.010000 episode_count: 30513. steps_count: 13459300.000000\n",
      "ep 4359: ep_len:242 episode reward: total was 3.600000. running mean: -6.537460\n",
      "ep 4359: ep_len:352 episode reward: total was -1.790000. running mean: -6.489985\n",
      "ep 4359: ep_len:460 episode reward: total was -7.760000. running mean: -6.502685\n",
      "ep 4359: ep_len:570 episode reward: total was 1.020000. running mean: -6.427458\n",
      "ep 4359: ep_len:87 episode reward: total was 6.550000. running mean: -6.297684\n",
      "ep 4359: ep_len:535 episode reward: total was -11.060000. running mean: -6.345307\n",
      "ep 4359: ep_len:500 episode reward: total was -3.400000. running mean: -6.315854\n",
      "epsilon:0.010000 episode_count: 30520. steps_count: 13462046.000000\n",
      "ep 4360: ep_len:530 episode reward: total was -3.390000. running mean: -6.286595\n",
      "ep 4360: ep_len:500 episode reward: total was 5.760000. running mean: -6.166129\n",
      "ep 4360: ep_len:605 episode reward: total was -1.860000. running mean: -6.123068\n",
      "ep 4360: ep_len:505 episode reward: total was 6.880000. running mean: -5.993037\n",
      "ep 4360: ep_len:3 episode reward: total was 0.000000. running mean: -5.933107\n",
      "ep 4360: ep_len:800 episode reward: total was -33.180000. running mean: -6.205576\n",
      "ep 4360: ep_len:635 episode reward: total was -19.800000. running mean: -6.341520\n",
      "epsilon:0.010000 episode_count: 30527. steps_count: 13465624.000000\n",
      "ep 4361: ep_len:620 episode reward: total was -1.950000. running mean: -6.297605\n",
      "ep 4361: ep_len:500 episode reward: total was 14.170000. running mean: -6.092929\n",
      "ep 4361: ep_len:595 episode reward: total was -4.430000. running mean: -6.076300\n",
      "ep 4361: ep_len:505 episode reward: total was 3.570000. running mean: -5.979837\n",
      "ep 4361: ep_len:3 episode reward: total was 0.000000. running mean: -5.920038\n",
      "ep 4361: ep_len:510 episode reward: total was 9.120000. running mean: -5.769638\n",
      "ep 4361: ep_len:530 episode reward: total was -11.110000. running mean: -5.823042\n",
      "epsilon:0.010000 episode_count: 30534. steps_count: 13468887.000000\n",
      "ep 4362: ep_len:570 episode reward: total was -0.140000. running mean: -5.766211\n",
      "ep 4362: ep_len:535 episode reward: total was -1.960000. running mean: -5.728149\n",
      "ep 4362: ep_len:630 episode reward: total was -6.890000. running mean: -5.739768\n",
      "ep 4362: ep_len:565 episode reward: total was 1.980000. running mean: -5.662570\n",
      "ep 4362: ep_len:3 episode reward: total was 0.000000. running mean: -5.605944\n",
      "ep 4362: ep_len:640 episode reward: total was 6.920000. running mean: -5.480685\n",
      "ep 4362: ep_len:515 episode reward: total was 5.230000. running mean: -5.373578\n",
      "epsilon:0.010000 episode_count: 30541. steps_count: 13472345.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4363: ep_len:590 episode reward: total was -11.640000. running mean: -5.436242\n",
      "ep 4363: ep_len:580 episode reward: total was -9.210000. running mean: -5.473980\n",
      "ep 4363: ep_len:630 episode reward: total was -27.070000. running mean: -5.689940\n",
      "ep 4363: ep_len:575 episode reward: total was -8.070000. running mean: -5.713741\n",
      "ep 4363: ep_len:91 episode reward: total was -9.970000. running mean: -5.756303\n",
      "ep 4363: ep_len:500 episode reward: total was -7.790000. running mean: -5.776640\n",
      "ep 4363: ep_len:257 episode reward: total was -1.320000. running mean: -5.732074\n",
      "epsilon:0.010000 episode_count: 30548. steps_count: 13475568.000000\n",
      "ep 4364: ep_len:615 episode reward: total was -6.980000. running mean: -5.744553\n",
      "ep 4364: ep_len:605 episode reward: total was 4.810000. running mean: -5.639007\n",
      "ep 4364: ep_len:680 episode reward: total was 3.950000. running mean: -5.543117\n",
      "ep 4364: ep_len:163 episode reward: total was 1.130000. running mean: -5.476386\n",
      "ep 4364: ep_len:3 episode reward: total was 0.000000. running mean: -5.421622\n",
      "ep 4364: ep_len:655 episode reward: total was 4.720000. running mean: -5.320206\n",
      "ep 4364: ep_len:685 episode reward: total was -14.770000. running mean: -5.414704\n",
      "epsilon:0.010000 episode_count: 30555. steps_count: 13478974.000000\n",
      "ep 4365: ep_len:500 episode reward: total was -3.140000. running mean: -5.391957\n",
      "ep 4365: ep_len:500 episode reward: total was -12.520000. running mean: -5.463237\n",
      "ep 4365: ep_len:386 episode reward: total was 4.220000. running mean: -5.366405\n",
      "ep 4365: ep_len:500 episode reward: total was 13.110000. running mean: -5.181641\n",
      "ep 4365: ep_len:3 episode reward: total was 0.000000. running mean: -5.129825\n",
      "ep 4365: ep_len:1125 episode reward: total was -138.200000. running mean: -6.460526\n",
      "ep 4365: ep_len:565 episode reward: total was -4.770000. running mean: -6.443621\n",
      "epsilon:0.010000 episode_count: 30562. steps_count: 13482553.000000\n",
      "ep 4366: ep_len:1060 episode reward: total was -123.720000. running mean: -7.616385\n",
      "ep 4366: ep_len:500 episode reward: total was 9.260000. running mean: -7.447621\n",
      "ep 4366: ep_len:505 episode reward: total was -13.020000. running mean: -7.503345\n",
      "ep 4366: ep_len:575 episode reward: total was -94.930000. running mean: -8.377611\n",
      "ep 4366: ep_len:3 episode reward: total was 0.000000. running mean: -8.293835\n",
      "ep 4366: ep_len:665 episode reward: total was 1.380000. running mean: -8.197097\n",
      "ep 4366: ep_len:565 episode reward: total was -34.460000. running mean: -8.459726\n",
      "epsilon:0.010000 episode_count: 30569. steps_count: 13486426.000000\n",
      "ep 4367: ep_len:255 episode reward: total was 6.140000. running mean: -8.313729\n",
      "ep 4367: ep_len:600 episode reward: total was 16.310000. running mean: -8.067491\n",
      "ep 4367: ep_len:525 episode reward: total was 4.850000. running mean: -7.938316\n",
      "ep 4367: ep_len:575 episode reward: total was 10.440000. running mean: -7.754533\n",
      "ep 4367: ep_len:51 episode reward: total was 2.000000. running mean: -7.656988\n",
      "ep 4367: ep_len:515 episode reward: total was -4.430000. running mean: -7.624718\n",
      "ep 4367: ep_len:555 episode reward: total was -19.400000. running mean: -7.742471\n",
      "epsilon:0.010000 episode_count: 30576. steps_count: 13489502.000000\n",
      "ep 4368: ep_len:625 episode reward: total was -6.970000. running mean: -7.734746\n",
      "ep 4368: ep_len:530 episode reward: total was 10.790000. running mean: -7.549499\n",
      "ep 4368: ep_len:645 episode reward: total was 0.330000. running mean: -7.470704\n",
      "ep 4368: ep_len:500 episode reward: total was -14.600000. running mean: -7.541997\n",
      "ep 4368: ep_len:33 episode reward: total was 3.000000. running mean: -7.436577\n",
      "ep 4368: ep_len:157 episode reward: total was 5.130000. running mean: -7.310911\n",
      "ep 4368: ep_len:550 episode reward: total was -12.940000. running mean: -7.367202\n",
      "epsilon:0.010000 episode_count: 30583. steps_count: 13492542.000000\n",
      "ep 4369: ep_len:570 episode reward: total was 6.010000. running mean: -7.233430\n",
      "ep 4369: ep_len:555 episode reward: total was 17.880000. running mean: -6.982296\n",
      "ep 4369: ep_len:565 episode reward: total was -7.390000. running mean: -6.986373\n",
      "ep 4369: ep_len:530 episode reward: total was -34.780000. running mean: -7.264309\n",
      "ep 4369: ep_len:89 episode reward: total was 2.020000. running mean: -7.171466\n",
      "ep 4369: ep_len:540 episode reward: total was -5.530000. running mean: -7.155051\n",
      "ep 4369: ep_len:525 episode reward: total was -37.290000. running mean: -7.456401\n",
      "epsilon:0.010000 episode_count: 30590. steps_count: 13495916.000000\n",
      "ep 4370: ep_len:615 episode reward: total was 0.360000. running mean: -7.378237\n",
      "ep 4370: ep_len:725 episode reward: total was -37.400000. running mean: -7.678454\n",
      "ep 4370: ep_len:570 episode reward: total was -8.420000. running mean: -7.685870\n",
      "ep 4370: ep_len:170 episode reward: total was 6.660000. running mean: -7.542411\n",
      "ep 4370: ep_len:3 episode reward: total was 0.000000. running mean: -7.466987\n",
      "ep 4370: ep_len:525 episode reward: total was -22.250000. running mean: -7.614817\n",
      "ep 4370: ep_len:560 episode reward: total was -7.260000. running mean: -7.611269\n",
      "epsilon:0.010000 episode_count: 30597. steps_count: 13499084.000000\n",
      "ep 4371: ep_len:540 episode reward: total was 13.970000. running mean: -7.395456\n",
      "ep 4371: ep_len:645 episode reward: total was 15.110000. running mean: -7.170402\n",
      "ep 4371: ep_len:640 episode reward: total was -16.390000. running mean: -7.262598\n",
      "ep 4371: ep_len:397 episode reward: total was 0.350000. running mean: -7.186472\n",
      "ep 4371: ep_len:3 episode reward: total was 0.000000. running mean: -7.114607\n",
      "ep 4371: ep_len:500 episode reward: total was -5.240000. running mean: -7.095861\n",
      "ep 4371: ep_len:585 episode reward: total was -18.890000. running mean: -7.213802\n",
      "epsilon:0.010000 episode_count: 30604. steps_count: 13502394.000000\n",
      "ep 4372: ep_len:520 episode reward: total was -3.010000. running mean: -7.171764\n",
      "ep 4372: ep_len:520 episode reward: total was -9.270000. running mean: -7.192747\n",
      "ep 4372: ep_len:49 episode reward: total was 0.530000. running mean: -7.115519\n",
      "ep 4372: ep_len:525 episode reward: total was 4.540000. running mean: -6.998964\n",
      "ep 4372: ep_len:3 episode reward: total was 0.000000. running mean: -6.928974\n",
      "ep 4372: ep_len:710 episode reward: total was -2.110000. running mean: -6.880785\n",
      "ep 4372: ep_len:570 episode reward: total was -10.570000. running mean: -6.917677\n",
      "epsilon:0.010000 episode_count: 30611. steps_count: 13505291.000000\n",
      "ep 4373: ep_len:680 episode reward: total was -19.190000. running mean: -7.040400\n",
      "ep 4373: ep_len:600 episode reward: total was 8.620000. running mean: -6.883796\n",
      "ep 4373: ep_len:630 episode reward: total was -6.450000. running mean: -6.879458\n",
      "ep 4373: ep_len:387 episode reward: total was 7.880000. running mean: -6.731863\n",
      "ep 4373: ep_len:90 episode reward: total was 7.540000. running mean: -6.589145\n",
      "ep 4373: ep_len:615 episode reward: total was 1.330000. running mean: -6.509953\n",
      "ep 4373: ep_len:590 episode reward: total was -3.820000. running mean: -6.483054\n",
      "epsilon:0.010000 episode_count: 30618. steps_count: 13508883.000000\n",
      "ep 4374: ep_len:830 episode reward: total was -53.200000. running mean: -6.950223\n",
      "ep 4374: ep_len:605 episode reward: total was 1.940000. running mean: -6.861321\n",
      "ep 4374: ep_len:720 episode reward: total was -36.250000. running mean: -7.155208\n",
      "ep 4374: ep_len:570 episode reward: total was 1.880000. running mean: -7.064856\n",
      "ep 4374: ep_len:3 episode reward: total was 0.000000. running mean: -6.994207\n",
      "ep 4374: ep_len:585 episode reward: total was -93.320000. running mean: -7.857465\n",
      "ep 4374: ep_len:505 episode reward: total was -6.820000. running mean: -7.847090\n",
      "epsilon:0.010000 episode_count: 30625. steps_count: 13512701.000000\n",
      "ep 4375: ep_len:500 episode reward: total was 12.960000. running mean: -7.639020\n",
      "ep 4375: ep_len:500 episode reward: total was 0.560000. running mean: -7.557029\n",
      "ep 4375: ep_len:463 episode reward: total was -9.790000. running mean: -7.579359\n",
      "ep 4375: ep_len:104 episode reward: total was 2.110000. running mean: -7.482465\n",
      "ep 4375: ep_len:3 episode reward: total was 0.000000. running mean: -7.407641\n",
      "ep 4375: ep_len:580 episode reward: total was -7.610000. running mean: -7.409664\n",
      "ep 4375: ep_len:535 episode reward: total was -7.450000. running mean: -7.410068\n",
      "epsilon:0.010000 episode_count: 30632. steps_count: 13515386.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4376: ep_len:505 episode reward: total was 9.740000. running mean: -7.238567\n",
      "ep 4376: ep_len:505 episode reward: total was -9.970000. running mean: -7.265881\n",
      "ep 4376: ep_len:407 episode reward: total was -9.290000. running mean: -7.286123\n",
      "ep 4376: ep_len:535 episode reward: total was -11.080000. running mean: -7.324061\n",
      "ep 4376: ep_len:49 episode reward: total was 4.500000. running mean: -7.205821\n",
      "ep 4376: ep_len:645 episode reward: total was 1.610000. running mean: -7.117663\n",
      "ep 4376: ep_len:550 episode reward: total was 5.490000. running mean: -6.991586\n",
      "epsilon:0.010000 episode_count: 30639. steps_count: 13518582.000000\n",
      "ep 4377: ep_len:695 episode reward: total was -17.660000. running mean: -7.098270\n",
      "ep 4377: ep_len:505 episode reward: total was 0.040000. running mean: -7.026887\n",
      "ep 4377: ep_len:630 episode reward: total was -33.540000. running mean: -7.292018\n",
      "ep 4377: ep_len:525 episode reward: total was -12.540000. running mean: -7.344498\n",
      "ep 4377: ep_len:3 episode reward: total was 0.000000. running mean: -7.271053\n",
      "ep 4377: ep_len:500 episode reward: total was -9.480000. running mean: -7.293143\n",
      "ep 4377: ep_len:570 episode reward: total was -5.850000. running mean: -7.278711\n",
      "epsilon:0.010000 episode_count: 30646. steps_count: 13522010.000000\n",
      "ep 4378: ep_len:206 episode reward: total was 3.650000. running mean: -7.169424\n",
      "ep 4378: ep_len:605 episode reward: total was 19.850000. running mean: -6.899230\n",
      "ep 4378: ep_len:615 episode reward: total was -27.080000. running mean: -7.101038\n",
      "ep 4378: ep_len:500 episode reward: total was 17.260000. running mean: -6.857427\n",
      "ep 4378: ep_len:3 episode reward: total was 0.000000. running mean: -6.788853\n",
      "ep 4378: ep_len:600 episode reward: total was -2.540000. running mean: -6.746364\n",
      "ep 4378: ep_len:585 episode reward: total was -34.320000. running mean: -7.022101\n",
      "epsilon:0.010000 episode_count: 30653. steps_count: 13525124.000000\n",
      "ep 4379: ep_len:695 episode reward: total was -33.270000. running mean: -7.284580\n",
      "ep 4379: ep_len:590 episode reward: total was 20.400000. running mean: -7.007734\n",
      "ep 4379: ep_len:500 episode reward: total was -10.520000. running mean: -7.042857\n",
      "ep 4379: ep_len:56 episode reward: total was -0.930000. running mean: -6.981728\n",
      "ep 4379: ep_len:3 episode reward: total was 0.000000. running mean: -6.911911\n",
      "ep 4379: ep_len:590 episode reward: total was -1.600000. running mean: -6.858792\n",
      "ep 4379: ep_len:535 episode reward: total was -8.080000. running mean: -6.871004\n",
      "epsilon:0.010000 episode_count: 30660. steps_count: 13528093.000000\n",
      "ep 4380: ep_len:525 episode reward: total was 7.480000. running mean: -6.727494\n",
      "ep 4380: ep_len:580 episode reward: total was -3.760000. running mean: -6.697819\n",
      "ep 4380: ep_len:625 episode reward: total was -27.790000. running mean: -6.908741\n",
      "ep 4380: ep_len:153 episode reward: total was 2.130000. running mean: -6.818353\n",
      "ep 4380: ep_len:3 episode reward: total was 0.000000. running mean: -6.750170\n",
      "ep 4380: ep_len:725 episode reward: total was -50.300000. running mean: -7.185668\n",
      "ep 4380: ep_len:211 episode reward: total was -0.320000. running mean: -7.117011\n",
      "epsilon:0.010000 episode_count: 30667. steps_count: 13530915.000000\n",
      "ep 4381: ep_len:545 episode reward: total was 1.600000. running mean: -7.029841\n",
      "ep 4381: ep_len:595 episode reward: total was -3.840000. running mean: -6.997943\n",
      "ep 4381: ep_len:660 episode reward: total was 0.910000. running mean: -6.918863\n",
      "ep 4381: ep_len:515 episode reward: total was -15.970000. running mean: -7.009375\n",
      "ep 4381: ep_len:3 episode reward: total was 0.000000. running mean: -6.939281\n",
      "ep 4381: ep_len:600 episode reward: total was -10.630000. running mean: -6.976188\n",
      "ep 4381: ep_len:505 episode reward: total was -19.940000. running mean: -7.105826\n",
      "epsilon:0.010000 episode_count: 30674. steps_count: 13534338.000000\n",
      "ep 4382: ep_len:765 episode reward: total was -37.230000. running mean: -7.407068\n",
      "ep 4382: ep_len:530 episode reward: total was -6.140000. running mean: -7.394397\n",
      "ep 4382: ep_len:680 episode reward: total was -7.100000. running mean: -7.391453\n",
      "ep 4382: ep_len:500 episode reward: total was 9.180000. running mean: -7.225739\n",
      "ep 4382: ep_len:99 episode reward: total was 7.550000. running mean: -7.077981\n",
      "ep 4382: ep_len:595 episode reward: total was 3.660000. running mean: -6.970602\n",
      "ep 4382: ep_len:520 episode reward: total was -7.500000. running mean: -6.975896\n",
      "epsilon:0.010000 episode_count: 30681. steps_count: 13538027.000000\n",
      "ep 4383: ep_len:500 episode reward: total was -26.330000. running mean: -7.169437\n",
      "ep 4383: ep_len:500 episode reward: total was 8.600000. running mean: -7.011742\n",
      "ep 4383: ep_len:645 episode reward: total was -9.420000. running mean: -7.035825\n",
      "ep 4383: ep_len:56 episode reward: total was -0.930000. running mean: -6.974767\n",
      "ep 4383: ep_len:3 episode reward: total was 0.000000. running mean: -6.905019\n",
      "ep 4383: ep_len:640 episode reward: total was -27.660000. running mean: -7.112569\n",
      "ep 4383: ep_len:580 episode reward: total was -13.500000. running mean: -7.176443\n",
      "epsilon:0.010000 episode_count: 30688. steps_count: 13540951.000000\n",
      "ep 4384: ep_len:595 episode reward: total was 7.410000. running mean: -7.030579\n",
      "ep 4384: ep_len:610 episode reward: total was -20.070000. running mean: -7.160973\n",
      "ep 4384: ep_len:555 episode reward: total was -17.810000. running mean: -7.267463\n",
      "ep 4384: ep_len:565 episode reward: total was -7.960000. running mean: -7.274389\n",
      "ep 4384: ep_len:3 episode reward: total was 0.000000. running mean: -7.201645\n",
      "ep 4384: ep_len:655 episode reward: total was -7.170000. running mean: -7.201328\n",
      "ep 4384: ep_len:550 episode reward: total was 2.160000. running mean: -7.107715\n",
      "epsilon:0.010000 episode_count: 30695. steps_count: 13544484.000000\n",
      "ep 4385: ep_len:660 episode reward: total was -56.860000. running mean: -7.605238\n",
      "ep 4385: ep_len:585 episode reward: total was 17.070000. running mean: -7.358485\n",
      "ep 4385: ep_len:71 episode reward: total was 1.530000. running mean: -7.269601\n",
      "ep 4385: ep_len:545 episode reward: total was -19.540000. running mean: -7.392305\n",
      "ep 4385: ep_len:3 episode reward: total was 0.000000. running mean: -7.318381\n",
      "ep 4385: ep_len:625 episode reward: total was -5.430000. running mean: -7.299498\n",
      "ep 4385: ep_len:510 episode reward: total was -8.400000. running mean: -7.310503\n",
      "epsilon:0.010000 episode_count: 30702. steps_count: 13547483.000000\n",
      "ep 4386: ep_len:665 episode reward: total was -15.730000. running mean: -7.394698\n",
      "ep 4386: ep_len:645 episode reward: total was 3.440000. running mean: -7.286351\n",
      "ep 4386: ep_len:570 episode reward: total was -19.730000. running mean: -7.410787\n",
      "ep 4386: ep_len:590 episode reward: total was -10.440000. running mean: -7.441079\n",
      "ep 4386: ep_len:3 episode reward: total was 0.000000. running mean: -7.366669\n",
      "ep 4386: ep_len:515 episode reward: total was -39.560000. running mean: -7.688602\n",
      "ep 4386: ep_len:535 episode reward: total was -20.460000. running mean: -7.816316\n",
      "epsilon:0.010000 episode_count: 30709. steps_count: 13551006.000000\n",
      "ep 4387: ep_len:540 episode reward: total was -29.860000. running mean: -8.036753\n",
      "ep 4387: ep_len:525 episode reward: total was 0.720000. running mean: -7.949185\n",
      "ep 4387: ep_len:565 episode reward: total was -6.700000. running mean: -7.936693\n",
      "ep 4387: ep_len:530 episode reward: total was 10.120000. running mean: -7.756126\n",
      "ep 4387: ep_len:3 episode reward: total was 0.000000. running mean: -7.678565\n",
      "ep 4387: ep_len:500 episode reward: total was -9.780000. running mean: -7.699579\n",
      "ep 4387: ep_len:306 episode reward: total was -6.290000. running mean: -7.685484\n",
      "epsilon:0.010000 episode_count: 30716. steps_count: 13553975.000000\n",
      "ep 4388: ep_len:610 episode reward: total was -44.460000. running mean: -8.053229\n",
      "ep 4388: ep_len:500 episode reward: total was 9.170000. running mean: -7.880997\n",
      "ep 4388: ep_len:392 episode reward: total was 2.730000. running mean: -7.774887\n",
      "ep 4388: ep_len:500 episode reward: total was -25.180000. running mean: -7.948938\n",
      "ep 4388: ep_len:3 episode reward: total was 0.000000. running mean: -7.869448\n",
      "ep 4388: ep_len:555 episode reward: total was 2.370000. running mean: -7.767054\n",
      "ep 4388: ep_len:570 episode reward: total was -6.010000. running mean: -7.749483\n",
      "epsilon:0.010000 episode_count: 30723. steps_count: 13557105.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4389: ep_len:555 episode reward: total was -28.850000. running mean: -7.960488\n",
      "ep 4389: ep_len:575 episode reward: total was 15.860000. running mean: -7.722284\n",
      "ep 4389: ep_len:555 episode reward: total was 2.120000. running mean: -7.623861\n",
      "ep 4389: ep_len:500 episode reward: total was 4.490000. running mean: -7.502722\n",
      "ep 4389: ep_len:100 episode reward: total was 4.040000. running mean: -7.387295\n",
      "ep 4389: ep_len:530 episode reward: total was 10.020000. running mean: -7.213222\n",
      "ep 4389: ep_len:351 episode reward: total was -7.290000. running mean: -7.213990\n",
      "epsilon:0.010000 episode_count: 30730. steps_count: 13560271.000000\n",
      "ep 4390: ep_len:570 episode reward: total was 1.940000. running mean: -7.122450\n",
      "ep 4390: ep_len:500 episode reward: total was -12.340000. running mean: -7.174625\n",
      "ep 4390: ep_len:560 episode reward: total was -9.790000. running mean: -7.200779\n",
      "ep 4390: ep_len:545 episode reward: total was 4.930000. running mean: -7.079471\n",
      "ep 4390: ep_len:114 episode reward: total was -1.970000. running mean: -7.028377\n",
      "ep 4390: ep_len:500 episode reward: total was -27.780000. running mean: -7.235893\n",
      "ep 4390: ep_len:535 episode reward: total was -11.150000. running mean: -7.275034\n",
      "epsilon:0.010000 episode_count: 30737. steps_count: 13563595.000000\n",
      "ep 4391: ep_len:239 episode reward: total was 3.620000. running mean: -7.166084\n",
      "ep 4391: ep_len:500 episode reward: total was 0.870000. running mean: -7.085723\n",
      "ep 4391: ep_len:500 episode reward: total was -19.320000. running mean: -7.208065\n",
      "ep 4391: ep_len:48 episode reward: total was 2.550000. running mean: -7.110485\n",
      "ep 4391: ep_len:3 episode reward: total was 0.000000. running mean: -7.039380\n",
      "ep 4391: ep_len:500 episode reward: total was -15.280000. running mean: -7.121786\n",
      "ep 4391: ep_len:555 episode reward: total was -8.550000. running mean: -7.136068\n",
      "epsilon:0.010000 episode_count: 30744. steps_count: 13565940.000000\n",
      "ep 4392: ep_len:565 episode reward: total was -14.090000. running mean: -7.205608\n",
      "ep 4392: ep_len:510 episode reward: total was 6.300000. running mean: -7.070552\n",
      "ep 4392: ep_len:447 episode reward: total was -3.750000. running mean: -7.037346\n",
      "ep 4392: ep_len:500 episode reward: total was 6.360000. running mean: -6.903373\n",
      "ep 4392: ep_len:103 episode reward: total was 4.040000. running mean: -6.793939\n",
      "ep 4392: ep_len:510 episode reward: total was -22.880000. running mean: -6.954799\n",
      "ep 4392: ep_len:625 episode reward: total was -3.530000. running mean: -6.920551\n",
      "epsilon:0.010000 episode_count: 30751. steps_count: 13569200.000000\n",
      "ep 4393: ep_len:550 episode reward: total was -31.570000. running mean: -7.167046\n",
      "ep 4393: ep_len:500 episode reward: total was 1.280000. running mean: -7.082576\n",
      "ep 4393: ep_len:500 episode reward: total was -0.480000. running mean: -7.016550\n",
      "ep 4393: ep_len:610 episode reward: total was -2.100000. running mean: -6.967384\n",
      "ep 4393: ep_len:86 episode reward: total was 5.540000. running mean: -6.842310\n",
      "ep 4393: ep_len:238 episode reward: total was 4.660000. running mean: -6.727287\n",
      "ep 4393: ep_len:206 episode reward: total was -3.360000. running mean: -6.693614\n",
      "epsilon:0.010000 episode_count: 30758. steps_count: 13571890.000000\n",
      "ep 4394: ep_len:710 episode reward: total was -34.280000. running mean: -6.969478\n",
      "ep 4394: ep_len:525 episode reward: total was -9.840000. running mean: -6.998184\n",
      "ep 4394: ep_len:438 episode reward: total was -3.730000. running mean: -6.965502\n",
      "ep 4394: ep_len:500 episode reward: total was -25.120000. running mean: -7.147047\n",
      "ep 4394: ep_len:3 episode reward: total was 0.000000. running mean: -7.075576\n",
      "ep 4394: ep_len:500 episode reward: total was 9.610000. running mean: -6.908720\n",
      "ep 4394: ep_len:500 episode reward: total was -4.950000. running mean: -6.889133\n",
      "epsilon:0.010000 episode_count: 30765. steps_count: 13575066.000000\n",
      "ep 4395: ep_len:116 episode reward: total was -6.920000. running mean: -6.889442\n",
      "ep 4395: ep_len:271 episode reward: total was -21.330000. running mean: -7.033847\n",
      "ep 4395: ep_len:565 episode reward: total was -15.870000. running mean: -7.122209\n",
      "ep 4395: ep_len:500 episode reward: total was -15.690000. running mean: -7.207887\n",
      "ep 4395: ep_len:3 episode reward: total was 0.000000. running mean: -7.135808\n",
      "ep 4395: ep_len:595 episode reward: total was 3.140000. running mean: -7.033050\n",
      "ep 4395: ep_len:500 episode reward: total was -18.780000. running mean: -7.150519\n",
      "epsilon:0.010000 episode_count: 30772. steps_count: 13577616.000000\n",
      "ep 4396: ep_len:218 episode reward: total was 6.640000. running mean: -7.012614\n",
      "ep 4396: ep_len:500 episode reward: total was 13.670000. running mean: -6.805788\n",
      "ep 4396: ep_len:620 episode reward: total was 1.500000. running mean: -6.722730\n",
      "ep 4396: ep_len:615 episode reward: total was -0.520000. running mean: -6.660703\n",
      "ep 4396: ep_len:3 episode reward: total was 0.000000. running mean: -6.594096\n",
      "ep 4396: ep_len:590 episode reward: total was 13.630000. running mean: -6.391855\n",
      "ep 4396: ep_len:286 episode reward: total was -2.790000. running mean: -6.355836\n",
      "epsilon:0.010000 episode_count: 30779. steps_count: 13580448.000000\n",
      "ep 4397: ep_len:500 episode reward: total was 14.790000. running mean: -6.144378\n",
      "ep 4397: ep_len:610 episode reward: total was 13.030000. running mean: -5.952634\n",
      "ep 4397: ep_len:560 episode reward: total was -10.680000. running mean: -5.999908\n",
      "ep 4397: ep_len:510 episode reward: total was -20.630000. running mean: -6.146209\n",
      "ep 4397: ep_len:73 episode reward: total was -2.970000. running mean: -6.114447\n",
      "ep 4397: ep_len:740 episode reward: total was -32.610000. running mean: -6.379402\n",
      "ep 4397: ep_len:500 episode reward: total was -5.300000. running mean: -6.368608\n",
      "epsilon:0.010000 episode_count: 30786. steps_count: 13583941.000000\n",
      "ep 4398: ep_len:685 episode reward: total was -20.120000. running mean: -6.506122\n",
      "ep 4398: ep_len:635 episode reward: total was -27.820000. running mean: -6.719261\n",
      "ep 4398: ep_len:560 episode reward: total was -13.340000. running mean: -6.785468\n",
      "ep 4398: ep_len:500 episode reward: total was 8.980000. running mean: -6.627814\n",
      "ep 4398: ep_len:53 episode reward: total was 5.000000. running mean: -6.511536\n",
      "ep 4398: ep_len:680 episode reward: total was -37.290000. running mean: -6.819320\n",
      "ep 4398: ep_len:585 episode reward: total was -5.250000. running mean: -6.803627\n",
      "epsilon:0.010000 episode_count: 30793. steps_count: 13587639.000000\n",
      "ep 4399: ep_len:560 episode reward: total was 13.950000. running mean: -6.596091\n",
      "ep 4399: ep_len:500 episode reward: total was 15.350000. running mean: -6.376630\n",
      "ep 4399: ep_len:635 episode reward: total was -1.520000. running mean: -6.328063\n",
      "ep 4399: ep_len:548 episode reward: total was -22.390000. running mean: -6.488683\n",
      "ep 4399: ep_len:3 episode reward: total was 0.000000. running mean: -6.423796\n",
      "ep 4399: ep_len:500 episode reward: total was -3.680000. running mean: -6.396358\n",
      "ep 4399: ep_len:500 episode reward: total was -0.360000. running mean: -6.335994\n",
      "epsilon:0.010000 episode_count: 30800. steps_count: 13590885.000000\n",
      "ep 4400: ep_len:505 episode reward: total was 4.580000. running mean: -6.226835\n",
      "ep 4400: ep_len:600 episode reward: total was 0.210000. running mean: -6.162466\n",
      "ep 4400: ep_len:570 episode reward: total was 4.450000. running mean: -6.056342\n",
      "ep 4400: ep_len:505 episode reward: total was -5.190000. running mean: -6.047678\n",
      "ep 4400: ep_len:3 episode reward: total was 0.000000. running mean: -5.987201\n",
      "ep 4400: ep_len:535 episode reward: total was 0.140000. running mean: -5.925929\n",
      "ep 4400: ep_len:600 episode reward: total was -5.080000. running mean: -5.917470\n",
      "epsilon:0.010000 episode_count: 30807. steps_count: 13594203.000000\n",
      "ep 4401: ep_len:560 episode reward: total was -2.870000. running mean: -5.886995\n",
      "ep 4401: ep_len:590 episode reward: total was -11.850000. running mean: -5.946625\n",
      "ep 4401: ep_len:396 episode reward: total was 9.230000. running mean: -5.794859\n",
      "ep 4401: ep_len:535 episode reward: total was -4.970000. running mean: -5.786611\n",
      "ep 4401: ep_len:1 episode reward: total was 0.000000. running mean: -5.728744\n",
      "ep 4401: ep_len:540 episode reward: total was 8.720000. running mean: -5.584257\n",
      "ep 4401: ep_len:510 episode reward: total was -15.430000. running mean: -5.682714\n",
      "epsilon:0.010000 episode_count: 30814. steps_count: 13597335.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4402: ep_len:206 episode reward: total was -13.860000. running mean: -5.764487\n",
      "ep 4402: ep_len:505 episode reward: total was 2.900000. running mean: -5.677842\n",
      "ep 4402: ep_len:600 episode reward: total was 1.980000. running mean: -5.601264\n",
      "ep 4402: ep_len:505 episode reward: total was -12.990000. running mean: -5.675151\n",
      "ep 4402: ep_len:126 episode reward: total was 9.060000. running mean: -5.527800\n",
      "ep 4402: ep_len:605 episode reward: total was 4.860000. running mean: -5.423922\n",
      "ep 4402: ep_len:535 episode reward: total was -2.340000. running mean: -5.393083\n",
      "epsilon:0.010000 episode_count: 30821. steps_count: 13600417.000000\n",
      "ep 4403: ep_len:620 episode reward: total was -2.080000. running mean: -5.359952\n",
      "ep 4403: ep_len:535 episode reward: total was 2.550000. running mean: -5.280852\n",
      "ep 4403: ep_len:423 episode reward: total was 1.280000. running mean: -5.215244\n",
      "ep 4403: ep_len:56 episode reward: total was -4.450000. running mean: -5.207591\n",
      "ep 4403: ep_len:102 episode reward: total was 8.540000. running mean: -5.070115\n",
      "ep 4403: ep_len:630 episode reward: total was -2.020000. running mean: -5.039614\n",
      "ep 4403: ep_len:327 episode reward: total was -1.780000. running mean: -5.007018\n",
      "epsilon:0.010000 episode_count: 30828. steps_count: 13603110.000000\n",
      "ep 4404: ep_len:500 episode reward: total was -5.230000. running mean: -5.009248\n",
      "ep 4404: ep_len:500 episode reward: total was 6.660000. running mean: -4.892555\n",
      "ep 4404: ep_len:500 episode reward: total was 3.930000. running mean: -4.804330\n",
      "ep 4404: ep_len:515 episode reward: total was -5.990000. running mean: -4.816187\n",
      "ep 4404: ep_len:3 episode reward: total was 0.000000. running mean: -4.768025\n",
      "ep 4404: ep_len:545 episode reward: total was 9.510000. running mean: -4.625244\n",
      "ep 4404: ep_len:710 episode reward: total was -36.210000. running mean: -4.941092\n",
      "epsilon:0.010000 episode_count: 30835. steps_count: 13606383.000000\n",
      "ep 4405: ep_len:131 episode reward: total was 2.080000. running mean: -4.870881\n",
      "ep 4405: ep_len:515 episode reward: total was 2.360000. running mean: -4.798572\n",
      "ep 4405: ep_len:458 episode reward: total was 9.270000. running mean: -4.657887\n",
      "ep 4405: ep_len:535 episode reward: total was -3.630000. running mean: -4.647608\n",
      "ep 4405: ep_len:3 episode reward: total was 0.000000. running mean: -4.601132\n",
      "ep 4405: ep_len:555 episode reward: total was -11.110000. running mean: -4.666220\n",
      "ep 4405: ep_len:605 episode reward: total was -17.520000. running mean: -4.794758\n",
      "epsilon:0.010000 episode_count: 30842. steps_count: 13609185.000000\n",
      "ep 4406: ep_len:202 episode reward: total was 3.610000. running mean: -4.710711\n",
      "ep 4406: ep_len:500 episode reward: total was -1.030000. running mean: -4.673903\n",
      "ep 4406: ep_len:520 episode reward: total was -20.000000. running mean: -4.827164\n",
      "ep 4406: ep_len:625 episode reward: total was 0.110000. running mean: -4.777793\n",
      "ep 4406: ep_len:3 episode reward: total was 0.000000. running mean: -4.730015\n",
      "ep 4406: ep_len:555 episode reward: total was -8.220000. running mean: -4.764915\n",
      "ep 4406: ep_len:525 episode reward: total was -10.060000. running mean: -4.817866\n",
      "epsilon:0.010000 episode_count: 30849. steps_count: 13612115.000000\n",
      "ep 4407: ep_len:545 episode reward: total was -12.510000. running mean: -4.894787\n",
      "ep 4407: ep_len:575 episode reward: total was 10.080000. running mean: -4.745039\n",
      "ep 4407: ep_len:500 episode reward: total was 1.850000. running mean: -4.679089\n",
      "ep 4407: ep_len:505 episode reward: total was 12.400000. running mean: -4.508298\n",
      "ep 4407: ep_len:100 episode reward: total was 2.020000. running mean: -4.443015\n",
      "ep 4407: ep_len:560 episode reward: total was -12.600000. running mean: -4.524585\n",
      "ep 4407: ep_len:590 episode reward: total was -5.840000. running mean: -4.537739\n",
      "epsilon:0.010000 episode_count: 30856. steps_count: 13615490.000000\n",
      "ep 4408: ep_len:535 episode reward: total was -7.120000. running mean: -4.563561\n",
      "ep 4408: ep_len:550 episode reward: total was 18.350000. running mean: -4.334426\n",
      "ep 4408: ep_len:443 episode reward: total was -17.760000. running mean: -4.468681\n",
      "ep 4408: ep_len:515 episode reward: total was 6.850000. running mean: -4.355495\n",
      "ep 4408: ep_len:97 episode reward: total was 3.020000. running mean: -4.281740\n",
      "ep 4408: ep_len:510 episode reward: total was -31.960000. running mean: -4.558522\n",
      "ep 4408: ep_len:211 episode reward: total was -3.350000. running mean: -4.546437\n",
      "epsilon:0.010000 episode_count: 30863. steps_count: 13618351.000000\n",
      "ep 4409: ep_len:640 episode reward: total was -15.170000. running mean: -4.652673\n",
      "ep 4409: ep_len:590 episode reward: total was -0.730000. running mean: -4.613446\n",
      "ep 4409: ep_len:695 episode reward: total was -3.670000. running mean: -4.604012\n",
      "ep 4409: ep_len:550 episode reward: total was 13.470000. running mean: -4.423271\n",
      "ep 4409: ep_len:54 episode reward: total was 5.000000. running mean: -4.329039\n",
      "ep 4409: ep_len:685 episode reward: total was 3.480000. running mean: -4.250948\n",
      "ep 4409: ep_len:500 episode reward: total was -15.550000. running mean: -4.363939\n",
      "epsilon:0.010000 episode_count: 30870. steps_count: 13622065.000000\n",
      "ep 4410: ep_len:710 episode reward: total was -28.710000. running mean: -4.607399\n",
      "ep 4410: ep_len:615 episode reward: total was 9.630000. running mean: -4.465025\n",
      "ep 4410: ep_len:575 episode reward: total was 0.590000. running mean: -4.414475\n",
      "ep 4410: ep_len:552 episode reward: total was -38.480000. running mean: -4.755130\n",
      "ep 4410: ep_len:3 episode reward: total was 0.000000. running mean: -4.707579\n",
      "ep 4410: ep_len:505 episode reward: total was 3.690000. running mean: -4.623603\n",
      "ep 4410: ep_len:575 episode reward: total was -7.560000. running mean: -4.652967\n",
      "epsilon:0.010000 episode_count: 30877. steps_count: 13625600.000000\n",
      "ep 4411: ep_len:237 episode reward: total was 1.600000. running mean: -4.590438\n",
      "ep 4411: ep_len:550 episode reward: total was 8.810000. running mean: -4.456433\n",
      "ep 4411: ep_len:565 episode reward: total was -9.290000. running mean: -4.504769\n",
      "ep 4411: ep_len:170 episode reward: total was 0.630000. running mean: -4.453421\n",
      "ep 4411: ep_len:107 episode reward: total was 4.020000. running mean: -4.368687\n",
      "ep 4411: ep_len:680 episode reward: total was -40.810000. running mean: -4.733100\n",
      "ep 4411: ep_len:565 episode reward: total was -11.620000. running mean: -4.801969\n",
      "epsilon:0.010000 episode_count: 30884. steps_count: 13628474.000000\n",
      "ep 4412: ep_len:500 episode reward: total was -0.310000. running mean: -4.757049\n",
      "ep 4412: ep_len:500 episode reward: total was -3.980000. running mean: -4.749279\n",
      "ep 4412: ep_len:595 episode reward: total was 5.980000. running mean: -4.641986\n",
      "ep 4412: ep_len:626 episode reward: total was -19.830000. running mean: -4.793866\n",
      "ep 4412: ep_len:1 episode reward: total was 0.000000. running mean: -4.745928\n",
      "ep 4412: ep_len:505 episode reward: total was -12.790000. running mean: -4.826368\n",
      "ep 4412: ep_len:590 episode reward: total was -4.340000. running mean: -4.821505\n",
      "epsilon:0.010000 episode_count: 30891. steps_count: 13631791.000000\n",
      "ep 4413: ep_len:134 episode reward: total was 2.570000. running mean: -4.747590\n",
      "ep 4413: ep_len:500 episode reward: total was 15.680000. running mean: -4.543314\n",
      "ep 4413: ep_len:665 episode reward: total was -3.640000. running mean: -4.534281\n",
      "ep 4413: ep_len:382 episode reward: total was 7.380000. running mean: -4.415138\n",
      "ep 4413: ep_len:3 episode reward: total was 0.000000. running mean: -4.370986\n",
      "ep 4413: ep_len:670 episode reward: total was -45.820000. running mean: -4.785477\n",
      "ep 4413: ep_len:640 episode reward: total was -39.330000. running mean: -5.130922\n",
      "epsilon:0.010000 episode_count: 30898. steps_count: 13634785.000000\n",
      "ep 4414: ep_len:590 episode reward: total was 7.580000. running mean: -5.003813\n",
      "ep 4414: ep_len:715 episode reward: total was -28.710000. running mean: -5.240874\n",
      "ep 4414: ep_len:555 episode reward: total was -6.740000. running mean: -5.255866\n",
      "ep 4414: ep_len:500 episode reward: total was -23.620000. running mean: -5.439507\n",
      "ep 4414: ep_len:3 episode reward: total was 0.000000. running mean: -5.385112\n",
      "ep 4414: ep_len:500 episode reward: total was 2.220000. running mean: -5.309061\n",
      "ep 4414: ep_len:291 episode reward: total was -16.290000. running mean: -5.418870\n",
      "epsilon:0.010000 episode_count: 30905. steps_count: 13637939.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4415: ep_len:535 episode reward: total was 2.560000. running mean: -5.339082\n",
      "ep 4415: ep_len:530 episode reward: total was -11.330000. running mean: -5.398991\n",
      "ep 4415: ep_len:525 episode reward: total was -4.990000. running mean: -5.394901\n",
      "ep 4415: ep_len:550 episode reward: total was 1.450000. running mean: -5.326452\n",
      "ep 4415: ep_len:3 episode reward: total was 0.000000. running mean: -5.273187\n",
      "ep 4415: ep_len:500 episode reward: total was -7.810000. running mean: -5.298555\n",
      "ep 4415: ep_len:625 episode reward: total was -25.570000. running mean: -5.501270\n",
      "epsilon:0.010000 episode_count: 30912. steps_count: 13641207.000000\n",
      "ep 4416: ep_len:500 episode reward: total was 1.250000. running mean: -5.433757\n",
      "ep 4416: ep_len:182 episode reward: total was -6.380000. running mean: -5.443220\n",
      "ep 4416: ep_len:575 episode reward: total was -4.230000. running mean: -5.431087\n",
      "ep 4416: ep_len:595 episode reward: total was -8.470000. running mean: -5.461477\n",
      "ep 4416: ep_len:3 episode reward: total was 0.000000. running mean: -5.406862\n",
      "ep 4416: ep_len:500 episode reward: total was 11.570000. running mean: -5.237093\n",
      "ep 4416: ep_len:302 episode reward: total was -1.260000. running mean: -5.197322\n",
      "epsilon:0.010000 episode_count: 30919. steps_count: 13643864.000000\n",
      "ep 4417: ep_len:500 episode reward: total was 10.250000. running mean: -5.042849\n",
      "ep 4417: ep_len:665 episode reward: total was 13.680000. running mean: -4.855621\n",
      "ep 4417: ep_len:640 episode reward: total was -4.180000. running mean: -4.848864\n",
      "ep 4417: ep_len:500 episode reward: total was 0.370000. running mean: -4.796676\n",
      "ep 4417: ep_len:3 episode reward: total was 0.000000. running mean: -4.748709\n",
      "ep 4417: ep_len:570 episode reward: total was 17.140000. running mean: -4.529822\n",
      "ep 4417: ep_len:194 episode reward: total was -1.820000. running mean: -4.502724\n",
      "epsilon:0.010000 episode_count: 30926. steps_count: 13646936.000000\n",
      "ep 4418: ep_len:525 episode reward: total was 1.440000. running mean: -4.443296\n",
      "ep 4418: ep_len:500 episode reward: total was 14.690000. running mean: -4.251963\n",
      "ep 4418: ep_len:500 episode reward: total was -4.230000. running mean: -4.251744\n",
      "ep 4418: ep_len:56 episode reward: total was 1.560000. running mean: -4.193626\n",
      "ep 4418: ep_len:3 episode reward: total was 0.000000. running mean: -4.151690\n",
      "ep 4418: ep_len:500 episode reward: total was -12.810000. running mean: -4.238273\n",
      "ep 4418: ep_len:560 episode reward: total was -10.350000. running mean: -4.299390\n",
      "epsilon:0.010000 episode_count: 30933. steps_count: 13649580.000000\n",
      "ep 4419: ep_len:530 episode reward: total was 4.080000. running mean: -4.215597\n",
      "ep 4419: ep_len:595 episode reward: total was -1.210000. running mean: -4.185541\n",
      "ep 4419: ep_len:605 episode reward: total was -8.230000. running mean: -4.225985\n",
      "ep 4419: ep_len:500 episode reward: total was -11.100000. running mean: -4.294725\n",
      "ep 4419: ep_len:54 episode reward: total was 3.500000. running mean: -4.216778\n",
      "ep 4419: ep_len:520 episode reward: total was 1.150000. running mean: -4.163110\n",
      "ep 4419: ep_len:580 episode reward: total was -3.970000. running mean: -4.161179\n",
      "epsilon:0.010000 episode_count: 30940. steps_count: 13652964.000000\n",
      "ep 4420: ep_len:555 episode reward: total was 6.900000. running mean: -4.050567\n",
      "ep 4420: ep_len:615 episode reward: total was -35.580000. running mean: -4.365862\n",
      "ep 4420: ep_len:540 episode reward: total was -6.740000. running mean: -4.389603\n",
      "ep 4420: ep_len:500 episode reward: total was -15.030000. running mean: -4.496007\n",
      "ep 4420: ep_len:3 episode reward: total was 0.000000. running mean: -4.451047\n",
      "ep 4420: ep_len:645 episode reward: total was 7.950000. running mean: -4.327037\n",
      "ep 4420: ep_len:730 episode reward: total was -55.880000. running mean: -4.842566\n",
      "epsilon:0.010000 episode_count: 30947. steps_count: 13656552.000000\n",
      "ep 4421: ep_len:116 episode reward: total was 4.070000. running mean: -4.753440\n",
      "ep 4421: ep_len:550 episode reward: total was 18.930000. running mean: -4.516606\n",
      "ep 4421: ep_len:540 episode reward: total was 8.010000. running mean: -4.391340\n",
      "ep 4421: ep_len:515 episode reward: total was -35.300000. running mean: -4.700427\n",
      "ep 4421: ep_len:3 episode reward: total was 0.000000. running mean: -4.653422\n",
      "ep 4421: ep_len:181 episode reward: total was 6.130000. running mean: -4.545588\n",
      "ep 4421: ep_len:500 episode reward: total was -6.370000. running mean: -4.563832\n",
      "epsilon:0.010000 episode_count: 30954. steps_count: 13658957.000000\n",
      "ep 4422: ep_len:530 episode reward: total was -21.920000. running mean: -4.737394\n",
      "ep 4422: ep_len:500 episode reward: total was -2.680000. running mean: -4.716820\n",
      "ep 4422: ep_len:550 episode reward: total was -9.870000. running mean: -4.768352\n",
      "ep 4422: ep_len:610 episode reward: total was 2.640000. running mean: -4.694268\n",
      "ep 4422: ep_len:3 episode reward: total was 0.000000. running mean: -4.647326\n",
      "ep 4422: ep_len:535 episode reward: total was -14.290000. running mean: -4.743752\n",
      "ep 4422: ep_len:595 episode reward: total was -28.180000. running mean: -4.978115\n",
      "epsilon:0.010000 episode_count: 30961. steps_count: 13662280.000000\n",
      "ep 4423: ep_len:500 episode reward: total was 0.080000. running mean: -4.927534\n",
      "ep 4423: ep_len:295 episode reward: total was -29.290000. running mean: -5.171158\n",
      "ep 4423: ep_len:79 episode reward: total was 1.050000. running mean: -5.108947\n",
      "ep 4423: ep_len:386 episode reward: total was -1.680000. running mean: -5.074657\n",
      "ep 4423: ep_len:104 episode reward: total was 4.530000. running mean: -4.978611\n",
      "ep 4423: ep_len:500 episode reward: total was 11.600000. running mean: -4.812825\n",
      "ep 4423: ep_len:211 episode reward: total was -0.320000. running mean: -4.767896\n",
      "epsilon:0.010000 episode_count: 30968. steps_count: 13664355.000000\n",
      "ep 4424: ep_len:500 episode reward: total was 9.300000. running mean: -4.627217\n",
      "ep 4424: ep_len:500 episode reward: total was -1.960000. running mean: -4.600545\n",
      "ep 4424: ep_len:515 episode reward: total was 6.410000. running mean: -4.490440\n",
      "ep 4424: ep_len:510 episode reward: total was 8.400000. running mean: -4.361535\n",
      "ep 4424: ep_len:3 episode reward: total was 0.000000. running mean: -4.317920\n",
      "ep 4424: ep_len:500 episode reward: total was -2.300000. running mean: -4.297741\n",
      "ep 4424: ep_len:630 episode reward: total was -31.900000. running mean: -4.573763\n",
      "epsilon:0.010000 episode_count: 30975. steps_count: 13667513.000000\n",
      "ep 4425: ep_len:239 episode reward: total was -3.390000. running mean: -4.561926\n",
      "ep 4425: ep_len:187 episode reward: total was -2.880000. running mean: -4.545107\n",
      "ep 4425: ep_len:565 episode reward: total was 3.400000. running mean: -4.465655\n",
      "ep 4425: ep_len:546 episode reward: total was -7.510000. running mean: -4.496099\n",
      "ep 4425: ep_len:3 episode reward: total was 0.000000. running mean: -4.451138\n",
      "ep 4425: ep_len:545 episode reward: total was -4.690000. running mean: -4.453527\n",
      "ep 4425: ep_len:595 episode reward: total was -11.370000. running mean: -4.522691\n",
      "epsilon:0.010000 episode_count: 30982. steps_count: 13670193.000000\n",
      "ep 4426: ep_len:122 episode reward: total was -2.430000. running mean: -4.501764\n",
      "ep 4426: ep_len:359 episode reward: total was -7.810000. running mean: -4.534847\n",
      "ep 4426: ep_len:605 episode reward: total was 1.960000. running mean: -4.469898\n",
      "ep 4426: ep_len:570 episode reward: total was 10.890000. running mean: -4.316299\n",
      "ep 4426: ep_len:3 episode reward: total was 0.000000. running mean: -4.273136\n",
      "ep 4426: ep_len:620 episode reward: total was 2.840000. running mean: -4.202005\n",
      "ep 4426: ep_len:540 episode reward: total was -6.090000. running mean: -4.220885\n",
      "epsilon:0.010000 episode_count: 30989. steps_count: 13673012.000000\n",
      "ep 4427: ep_len:500 episode reward: total was 12.480000. running mean: -4.053876\n",
      "ep 4427: ep_len:500 episode reward: total was 6.090000. running mean: -3.952437\n",
      "ep 4427: ep_len:429 episode reward: total was -1.790000. running mean: -3.930813\n",
      "ep 4427: ep_len:500 episode reward: total was -18.600000. running mean: -4.077505\n",
      "ep 4427: ep_len:3 episode reward: total was 0.000000. running mean: -4.036730\n",
      "ep 4427: ep_len:635 episode reward: total was -20.980000. running mean: -4.206162\n",
      "ep 4427: ep_len:345 episode reward: total was -4.770000. running mean: -4.211801\n",
      "epsilon:0.010000 episode_count: 30996. steps_count: 13675924.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4428: ep_len:258 episode reward: total was 6.660000. running mean: -4.103083\n",
      "ep 4428: ep_len:500 episode reward: total was 3.610000. running mean: -4.025952\n",
      "ep 4428: ep_len:605 episode reward: total was -0.000000. running mean: -3.985692\n",
      "ep 4428: ep_len:56 episode reward: total was 1.560000. running mean: -3.930235\n",
      "ep 4428: ep_len:3 episode reward: total was 0.000000. running mean: -3.890933\n",
      "ep 4428: ep_len:525 episode reward: total was -18.120000. running mean: -4.033224\n",
      "ep 4428: ep_len:585 episode reward: total was -5.330000. running mean: -4.046192\n",
      "epsilon:0.010000 episode_count: 31003. steps_count: 13678456.000000\n",
      "ep 4429: ep_len:650 episode reward: total was 0.660000. running mean: -3.999130\n",
      "ep 4429: ep_len:500 episode reward: total was 4.510000. running mean: -3.914038\n",
      "ep 4429: ep_len:62 episode reward: total was -2.470000. running mean: -3.899598\n",
      "ep 4429: ep_len:575 episode reward: total was 13.960000. running mean: -3.721002\n",
      "ep 4429: ep_len:131 episode reward: total was 9.060000. running mean: -3.593192\n",
      "ep 4429: ep_len:555 episode reward: total was -3.510000. running mean: -3.592360\n",
      "ep 4429: ep_len:625 episode reward: total was -7.540000. running mean: -3.631836\n",
      "epsilon:0.010000 episode_count: 31010. steps_count: 13681554.000000\n",
      "ep 4430: ep_len:695 episode reward: total was -17.140000. running mean: -3.766918\n",
      "ep 4430: ep_len:281 episode reward: total was -1.770000. running mean: -3.746949\n",
      "ep 4430: ep_len:580 episode reward: total was -7.760000. running mean: -3.787079\n",
      "ep 4430: ep_len:500 episode reward: total was -15.600000. running mean: -3.905209\n",
      "ep 4430: ep_len:3 episode reward: total was 0.000000. running mean: -3.866157\n",
      "ep 4430: ep_len:715 episode reward: total was 0.380000. running mean: -3.823695\n",
      "ep 4430: ep_len:520 episode reward: total was -36.620000. running mean: -4.151658\n",
      "epsilon:0.010000 episode_count: 31017. steps_count: 13684848.000000\n",
      "ep 4431: ep_len:630 episode reward: total was -31.830000. running mean: -4.428441\n",
      "ep 4431: ep_len:530 episode reward: total was -8.770000. running mean: -4.471857\n",
      "ep 4431: ep_len:605 episode reward: total was -23.660000. running mean: -4.663738\n",
      "ep 4431: ep_len:500 episode reward: total was 2.430000. running mean: -4.592801\n",
      "ep 4431: ep_len:3 episode reward: total was 0.000000. running mean: -4.546873\n",
      "ep 4431: ep_len:505 episode reward: total was -18.670000. running mean: -4.688104\n",
      "ep 4431: ep_len:565 episode reward: total was -22.030000. running mean: -4.861523\n",
      "epsilon:0.010000 episode_count: 31024. steps_count: 13688186.000000\n",
      "ep 4432: ep_len:172 episode reward: total was -1.930000. running mean: -4.832208\n",
      "ep 4432: ep_len:505 episode reward: total was -14.610000. running mean: -4.929986\n",
      "ep 4432: ep_len:434 episode reward: total was 3.250000. running mean: -4.848186\n",
      "ep 4432: ep_len:500 episode reward: total was 8.410000. running mean: -4.715604\n",
      "ep 4432: ep_len:91 episode reward: total was -10.960000. running mean: -4.778048\n",
      "ep 4432: ep_len:580 episode reward: total was 2.910000. running mean: -4.701168\n",
      "ep 4432: ep_len:595 episode reward: total was -16.990000. running mean: -4.824056\n",
      "epsilon:0.010000 episode_count: 31031. steps_count: 13691063.000000\n",
      "ep 4433: ep_len:605 episode reward: total was -13.460000. running mean: -4.910415\n",
      "ep 4433: ep_len:535 episode reward: total was -21.930000. running mean: -5.080611\n",
      "ep 4433: ep_len:570 episode reward: total was -23.520000. running mean: -5.265005\n",
      "ep 4433: ep_len:500 episode reward: total was -12.510000. running mean: -5.337455\n",
      "ep 4433: ep_len:3 episode reward: total was 0.000000. running mean: -5.284081\n",
      "ep 4433: ep_len:660 episode reward: total was -89.790000. running mean: -6.129140\n",
      "ep 4433: ep_len:500 episode reward: total was -19.610000. running mean: -6.263948\n",
      "epsilon:0.010000 episode_count: 31038. steps_count: 13694436.000000\n",
      "ep 4434: ep_len:600 episode reward: total was 2.170000. running mean: -6.179609\n",
      "ep 4434: ep_len:590 episode reward: total was -6.140000. running mean: -6.179213\n",
      "ep 4434: ep_len:500 episode reward: total was 12.410000. running mean: -5.993321\n",
      "ep 4434: ep_len:585 episode reward: total was 0.980000. running mean: -5.923588\n",
      "ep 4434: ep_len:3 episode reward: total was 0.000000. running mean: -5.864352\n",
      "ep 4434: ep_len:500 episode reward: total was -9.260000. running mean: -5.898308\n",
      "ep 4434: ep_len:560 episode reward: total was -78.290000. running mean: -6.622225\n",
      "epsilon:0.010000 episode_count: 31045. steps_count: 13697774.000000\n",
      "ep 4435: ep_len:515 episode reward: total was 4.800000. running mean: -6.508003\n",
      "ep 4435: ep_len:515 episode reward: total was 14.420000. running mean: -6.298723\n",
      "ep 4435: ep_len:500 episode reward: total was -3.730000. running mean: -6.273036\n",
      "ep 4435: ep_len:375 episode reward: total was 5.870000. running mean: -6.151605\n",
      "ep 4435: ep_len:102 episode reward: total was 6.550000. running mean: -6.024589\n",
      "ep 4435: ep_len:520 episode reward: total was -3.070000. running mean: -5.995043\n",
      "ep 4435: ep_len:590 episode reward: total was -19.430000. running mean: -6.129393\n",
      "epsilon:0.010000 episode_count: 31052. steps_count: 13700891.000000\n",
      "ep 4436: ep_len:890 episode reward: total was -99.690000. running mean: -7.064999\n",
      "ep 4436: ep_len:280 episode reward: total was -8.370000. running mean: -7.078049\n",
      "ep 4436: ep_len:620 episode reward: total was -11.420000. running mean: -7.121468\n",
      "ep 4436: ep_len:500 episode reward: total was -10.580000. running mean: -7.156054\n",
      "ep 4436: ep_len:3 episode reward: total was 0.000000. running mean: -7.084493\n",
      "ep 4436: ep_len:500 episode reward: total was -23.280000. running mean: -7.246448\n",
      "ep 4436: ep_len:500 episode reward: total was 1.460000. running mean: -7.159384\n",
      "epsilon:0.010000 episode_count: 31059. steps_count: 13704184.000000\n",
      "ep 4437: ep_len:605 episode reward: total was -5.560000. running mean: -7.143390\n",
      "ep 4437: ep_len:525 episode reward: total was -10.270000. running mean: -7.174656\n",
      "ep 4437: ep_len:500 episode reward: total was -2.580000. running mean: -7.128709\n",
      "ep 4437: ep_len:575 episode reward: total was -41.270000. running mean: -7.470122\n",
      "ep 4437: ep_len:3 episode reward: total was 0.000000. running mean: -7.395421\n",
      "ep 4437: ep_len:620 episode reward: total was -30.120000. running mean: -7.622667\n",
      "ep 4437: ep_len:500 episode reward: total was -23.150000. running mean: -7.777940\n",
      "epsilon:0.010000 episode_count: 31066. steps_count: 13707512.000000\n",
      "ep 4438: ep_len:675 episode reward: total was -17.700000. running mean: -7.877161\n",
      "ep 4438: ep_len:510 episode reward: total was -3.290000. running mean: -7.831289\n",
      "ep 4438: ep_len:500 episode reward: total was -4.260000. running mean: -7.795576\n",
      "ep 4438: ep_len:500 episode reward: total was -13.140000. running mean: -7.849021\n",
      "ep 4438: ep_len:3 episode reward: total was 0.000000. running mean: -7.770530\n",
      "ep 4438: ep_len:336 episode reward: total was -0.820000. running mean: -7.701025\n",
      "ep 4438: ep_len:1030 episode reward: total was -84.470000. running mean: -8.468715\n",
      "epsilon:0.010000 episode_count: 31073. steps_count: 13711066.000000\n",
      "ep 4439: ep_len:500 episode reward: total was -0.660000. running mean: -8.390628\n",
      "ep 4439: ep_len:640 episode reward: total was -30.350000. running mean: -8.610221\n",
      "ep 4439: ep_len:500 episode reward: total was -2.100000. running mean: -8.545119\n",
      "ep 4439: ep_len:560 episode reward: total was -16.600000. running mean: -8.625668\n",
      "ep 4439: ep_len:3 episode reward: total was 0.000000. running mean: -8.539411\n",
      "ep 4439: ep_len:500 episode reward: total was 9.450000. running mean: -8.359517\n",
      "ep 4439: ep_len:500 episode reward: total was -0.100000. running mean: -8.276922\n",
      "epsilon:0.010000 episode_count: 31080. steps_count: 13714269.000000\n",
      "ep 4440: ep_len:535 episode reward: total was -29.400000. running mean: -8.488153\n",
      "ep 4440: ep_len:180 episode reward: total was -2.920000. running mean: -8.432471\n",
      "ep 4440: ep_len:630 episode reward: total was -5.040000. running mean: -8.398547\n",
      "ep 4440: ep_len:500 episode reward: total was -4.660000. running mean: -8.361161\n",
      "ep 4440: ep_len:3 episode reward: total was 0.000000. running mean: -8.277549\n",
      "ep 4440: ep_len:510 episode reward: total was 0.120000. running mean: -8.193574\n",
      "ep 4440: ep_len:500 episode reward: total was -17.100000. running mean: -8.282638\n",
      "epsilon:0.010000 episode_count: 31087. steps_count: 13717127.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4441: ep_len:237 episode reward: total was 1.630000. running mean: -8.183512\n",
      "ep 4441: ep_len:500 episode reward: total was 10.660000. running mean: -7.995077\n",
      "ep 4441: ep_len:54 episode reward: total was -1.970000. running mean: -7.934826\n",
      "ep 4441: ep_len:595 episode reward: total was -0.560000. running mean: -7.861078\n",
      "ep 4441: ep_len:3 episode reward: total was 0.000000. running mean: -7.782467\n",
      "ep 4441: ep_len:610 episode reward: total was -26.140000. running mean: -7.966042\n",
      "ep 4441: ep_len:500 episode reward: total was -16.000000. running mean: -8.046382\n",
      "epsilon:0.010000 episode_count: 31094. steps_count: 13719626.000000\n",
      "ep 4442: ep_len:690 episode reward: total was -30.250000. running mean: -8.268418\n",
      "ep 4442: ep_len:585 episode reward: total was -2.600000. running mean: -8.211734\n",
      "ep 4442: ep_len:575 episode reward: total was -12.730000. running mean: -8.256917\n",
      "ep 4442: ep_len:510 episode reward: total was 6.600000. running mean: -8.108347\n",
      "ep 4442: ep_len:95 episode reward: total was 3.050000. running mean: -7.996764\n",
      "ep 4442: ep_len:605 episode reward: total was -41.020000. running mean: -8.326996\n",
      "ep 4442: ep_len:500 episode reward: total was -31.530000. running mean: -8.559026\n",
      "epsilon:0.010000 episode_count: 31101. steps_count: 13723186.000000\n",
      "ep 4443: ep_len:203 episode reward: total was 4.100000. running mean: -8.432436\n",
      "ep 4443: ep_len:178 episode reward: total was 2.150000. running mean: -8.326612\n",
      "ep 4443: ep_len:575 episode reward: total was -5.100000. running mean: -8.294346\n",
      "ep 4443: ep_len:595 episode reward: total was 3.420000. running mean: -8.177202\n",
      "ep 4443: ep_len:96 episode reward: total was 5.040000. running mean: -8.045030\n",
      "ep 4443: ep_len:595 episode reward: total was 0.430000. running mean: -7.960280\n",
      "ep 4443: ep_len:299 episode reward: total was -33.770000. running mean: -8.218377\n",
      "epsilon:0.010000 episode_count: 31108. steps_count: 13725727.000000\n",
      "ep 4444: ep_len:600 episode reward: total was -8.500000. running mean: -8.221193\n",
      "ep 4444: ep_len:575 episode reward: total was 3.560000. running mean: -8.103381\n",
      "ep 4444: ep_len:79 episode reward: total was 0.040000. running mean: -8.021947\n",
      "ep 4444: ep_len:525 episode reward: total was 10.450000. running mean: -7.837228\n",
      "ep 4444: ep_len:48 episode reward: total was 4.500000. running mean: -7.713856\n",
      "ep 4444: ep_len:670 episode reward: total was -15.200000. running mean: -7.788717\n",
      "ep 4444: ep_len:297 episode reward: total was -4.320000. running mean: -7.754030\n",
      "epsilon:0.010000 episode_count: 31115. steps_count: 13728521.000000\n",
      "ep 4445: ep_len:230 episode reward: total was 0.090000. running mean: -7.675590\n",
      "ep 4445: ep_len:590 episode reward: total was 6.130000. running mean: -7.537534\n",
      "ep 4445: ep_len:560 episode reward: total was -1.630000. running mean: -7.478458\n",
      "ep 4445: ep_len:505 episode reward: total was -2.700000. running mean: -7.430674\n",
      "ep 4445: ep_len:3 episode reward: total was 0.000000. running mean: -7.356367\n",
      "ep 4445: ep_len:530 episode reward: total was -3.800000. running mean: -7.320803\n",
      "ep 4445: ep_len:283 episode reward: total was -7.850000. running mean: -7.326095\n",
      "epsilon:0.010000 episode_count: 31122. steps_count: 13731222.000000\n",
      "ep 4446: ep_len:600 episode reward: total was 7.940000. running mean: -7.173434\n",
      "ep 4446: ep_len:555 episode reward: total was 22.380000. running mean: -6.877900\n",
      "ep 4446: ep_len:620 episode reward: total was -4.220000. running mean: -6.851321\n",
      "ep 4446: ep_len:525 episode reward: total was -4.690000. running mean: -6.829708\n",
      "ep 4446: ep_len:3 episode reward: total was 0.000000. running mean: -6.761411\n",
      "ep 4446: ep_len:610 episode reward: total was 0.860000. running mean: -6.685197\n",
      "ep 4446: ep_len:550 episode reward: total was -19.620000. running mean: -6.814545\n",
      "epsilon:0.010000 episode_count: 31129. steps_count: 13734685.000000\n",
      "ep 4447: ep_len:715 episode reward: total was -15.170000. running mean: -6.898099\n",
      "ep 4447: ep_len:500 episode reward: total was 14.670000. running mean: -6.682418\n",
      "ep 4447: ep_len:505 episode reward: total was -13.420000. running mean: -6.749794\n",
      "ep 4447: ep_len:500 episode reward: total was 12.560000. running mean: -6.556696\n",
      "ep 4447: ep_len:3 episode reward: total was 0.000000. running mean: -6.491129\n",
      "ep 4447: ep_len:665 episode reward: total was -52.890000. running mean: -6.955118\n",
      "ep 4447: ep_len:545 episode reward: total was -12.090000. running mean: -7.006467\n",
      "epsilon:0.010000 episode_count: 31136. steps_count: 13738118.000000\n",
      "ep 4448: ep_len:605 episode reward: total was 0.560000. running mean: -6.930802\n",
      "ep 4448: ep_len:530 episode reward: total was -4.930000. running mean: -6.910794\n",
      "ep 4448: ep_len:510 episode reward: total was -5.760000. running mean: -6.899286\n",
      "ep 4448: ep_len:620 episode reward: total was 9.920000. running mean: -6.731093\n",
      "ep 4448: ep_len:3 episode reward: total was 0.000000. running mean: -6.663782\n",
      "ep 4448: ep_len:585 episode reward: total was -4.120000. running mean: -6.638345\n",
      "ep 4448: ep_len:560 episode reward: total was -3.550000. running mean: -6.607461\n",
      "epsilon:0.010000 episode_count: 31143. steps_count: 13741531.000000\n",
      "ep 4449: ep_len:580 episode reward: total was -26.400000. running mean: -6.805386\n",
      "ep 4449: ep_len:520 episode reward: total was 7.630000. running mean: -6.661033\n",
      "ep 4449: ep_len:595 episode reward: total was -21.130000. running mean: -6.805722\n",
      "ep 4449: ep_len:615 episode reward: total was 4.080000. running mean: -6.696865\n",
      "ep 4449: ep_len:98 episode reward: total was 5.530000. running mean: -6.574596\n",
      "ep 4449: ep_len:585 episode reward: total was -27.350000. running mean: -6.782350\n",
      "ep 4449: ep_len:575 episode reward: total was -14.080000. running mean: -6.855327\n",
      "epsilon:0.010000 episode_count: 31150. steps_count: 13745099.000000\n",
      "ep 4450: ep_len:650 episode reward: total was -36.390000. running mean: -7.150674\n",
      "ep 4450: ep_len:500 episode reward: total was -2.910000. running mean: -7.108267\n",
      "ep 4450: ep_len:384 episode reward: total was -15.790000. running mean: -7.195084\n",
      "ep 4450: ep_len:505 episode reward: total was -14.120000. running mean: -7.264333\n",
      "ep 4450: ep_len:44 episode reward: total was 2.500000. running mean: -7.166690\n",
      "ep 4450: ep_len:560 episode reward: total was -42.540000. running mean: -7.520423\n",
      "ep 4450: ep_len:298 episode reward: total was -0.780000. running mean: -7.453019\n",
      "epsilon:0.010000 episode_count: 31157. steps_count: 13748040.000000\n",
      "ep 4451: ep_len:520 episode reward: total was 0.110000. running mean: -7.377389\n",
      "ep 4451: ep_len:269 episode reward: total was -32.850000. running mean: -7.632115\n",
      "ep 4451: ep_len:500 episode reward: total was -10.610000. running mean: -7.661894\n",
      "ep 4451: ep_len:510 episode reward: total was 4.950000. running mean: -7.535775\n",
      "ep 4451: ep_len:3 episode reward: total was 0.000000. running mean: -7.460417\n",
      "ep 4451: ep_len:625 episode reward: total was -21.510000. running mean: -7.600913\n",
      "ep 4451: ep_len:315 episode reward: total was -6.790000. running mean: -7.592804\n",
      "epsilon:0.010000 episode_count: 31164. steps_count: 13750782.000000\n",
      "ep 4452: ep_len:705 episode reward: total was -26.730000. running mean: -7.784176\n",
      "ep 4452: ep_len:500 episode reward: total was -37.210000. running mean: -8.078434\n",
      "ep 4452: ep_len:625 episode reward: total was -9.430000. running mean: -8.091950\n",
      "ep 4452: ep_len:615 episode reward: total was 2.090000. running mean: -7.990130\n",
      "ep 4452: ep_len:3 episode reward: total was 0.000000. running mean: -7.910229\n",
      "ep 4452: ep_len:500 episode reward: total was 4.040000. running mean: -7.790727\n",
      "ep 4452: ep_len:590 episode reward: total was -17.550000. running mean: -7.888319\n",
      "epsilon:0.010000 episode_count: 31171. steps_count: 13754320.000000\n",
      "ep 4453: ep_len:229 episode reward: total was 6.630000. running mean: -7.743136\n",
      "ep 4453: ep_len:505 episode reward: total was -9.420000. running mean: -7.759905\n",
      "ep 4453: ep_len:625 episode reward: total was -27.420000. running mean: -7.956506\n",
      "ep 4453: ep_len:505 episode reward: total was -16.110000. running mean: -8.038041\n",
      "ep 4453: ep_len:3 episode reward: total was 0.000000. running mean: -7.957660\n",
      "ep 4453: ep_len:235 episode reward: total was 5.150000. running mean: -7.826584\n",
      "ep 4453: ep_len:555 episode reward: total was -9.510000. running mean: -7.843418\n",
      "epsilon:0.010000 episode_count: 31178. steps_count: 13756977.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4454: ep_len:560 episode reward: total was -3.640000. running mean: -7.801384\n",
      "ep 4454: ep_len:359 episode reward: total was -45.910000. running mean: -8.182470\n",
      "ep 4454: ep_len:500 episode reward: total was 3.060000. running mean: -8.070045\n",
      "ep 4454: ep_len:500 episode reward: total was -2.470000. running mean: -8.014045\n",
      "ep 4454: ep_len:115 episode reward: total was 4.530000. running mean: -7.888604\n",
      "ep 4454: ep_len:690 episode reward: total was -30.230000. running mean: -8.112018\n",
      "ep 4454: ep_len:585 episode reward: total was -9.340000. running mean: -8.124298\n",
      "epsilon:0.010000 episode_count: 31185. steps_count: 13760286.000000\n",
      "ep 4455: ep_len:500 episode reward: total was 6.300000. running mean: -7.980055\n",
      "ep 4455: ep_len:650 episode reward: total was -12.820000. running mean: -8.028454\n",
      "ep 4455: ep_len:565 episode reward: total was -4.530000. running mean: -7.993470\n",
      "ep 4455: ep_len:535 episode reward: total was -15.460000. running mean: -8.068135\n",
      "ep 4455: ep_len:3 episode reward: total was 0.000000. running mean: -7.987454\n",
      "ep 4455: ep_len:570 episode reward: total was 3.350000. running mean: -7.874079\n",
      "ep 4455: ep_len:630 episode reward: total was -59.560000. running mean: -8.390938\n",
      "epsilon:0.010000 episode_count: 31192. steps_count: 13763739.000000\n",
      "ep 4456: ep_len:635 episode reward: total was -13.710000. running mean: -8.444129\n",
      "ep 4456: ep_len:605 episode reward: total was -38.450000. running mean: -8.744188\n",
      "ep 4456: ep_len:525 episode reward: total was -0.230000. running mean: -8.659046\n",
      "ep 4456: ep_len:520 episode reward: total was 6.590000. running mean: -8.506555\n",
      "ep 4456: ep_len:95 episode reward: total was 5.040000. running mean: -8.371090\n",
      "ep 4456: ep_len:500 episode reward: total was -6.050000. running mean: -8.347879\n",
      "ep 4456: ep_len:520 episode reward: total was -17.630000. running mean: -8.440700\n",
      "epsilon:0.010000 episode_count: 31199. steps_count: 13767139.000000\n",
      "ep 4457: ep_len:685 episode reward: total was -20.680000. running mean: -8.563093\n",
      "ep 4457: ep_len:180 episode reward: total was -3.930000. running mean: -8.516762\n",
      "ep 4457: ep_len:530 episode reward: total was 1.430000. running mean: -8.417295\n",
      "ep 4457: ep_len:500 episode reward: total was -17.110000. running mean: -8.504222\n",
      "ep 4457: ep_len:3 episode reward: total was 0.000000. running mean: -8.419179\n",
      "ep 4457: ep_len:500 episode reward: total was -14.930000. running mean: -8.484288\n",
      "ep 4457: ep_len:515 episode reward: total was -9.560000. running mean: -8.495045\n",
      "epsilon:0.010000 episode_count: 31206. steps_count: 13770052.000000\n",
      "ep 4458: ep_len:500 episode reward: total was -34.710000. running mean: -8.757194\n",
      "ep 4458: ep_len:329 episode reward: total was -9.340000. running mean: -8.763022\n",
      "ep 4458: ep_len:595 episode reward: total was 1.580000. running mean: -8.659592\n",
      "ep 4458: ep_len:500 episode reward: total was -6.000000. running mean: -8.632996\n",
      "ep 4458: ep_len:3 episode reward: total was 0.000000. running mean: -8.546666\n",
      "ep 4458: ep_len:645 episode reward: total was 1.830000. running mean: -8.442900\n",
      "ep 4458: ep_len:555 episode reward: total was -14.040000. running mean: -8.498871\n",
      "epsilon:0.010000 episode_count: 31213. steps_count: 13773179.000000\n",
      "ep 4459: ep_len:595 episode reward: total was -37.650000. running mean: -8.790382\n",
      "ep 4459: ep_len:500 episode reward: total was -16.230000. running mean: -8.864778\n",
      "ep 4459: ep_len:560 episode reward: total was -9.450000. running mean: -8.870630\n",
      "ep 4459: ep_len:56 episode reward: total was 2.570000. running mean: -8.756224\n",
      "ep 4459: ep_len:3 episode reward: total was 0.000000. running mean: -8.668662\n",
      "ep 4459: ep_len:600 episode reward: total was -8.100000. running mean: -8.662975\n",
      "ep 4459: ep_len:595 episode reward: total was -16.510000. running mean: -8.741445\n",
      "epsilon:0.010000 episode_count: 31220. steps_count: 13776088.000000\n",
      "ep 4460: ep_len:740 episode reward: total was -40.220000. running mean: -9.056231\n",
      "ep 4460: ep_len:157 episode reward: total was 0.060000. running mean: -8.965069\n",
      "ep 4460: ep_len:393 episode reward: total was -27.400000. running mean: -9.149418\n",
      "ep 4460: ep_len:500 episode reward: total was 6.890000. running mean: -8.989024\n",
      "ep 4460: ep_len:3 episode reward: total was 0.000000. running mean: -8.899134\n",
      "ep 4460: ep_len:610 episode reward: total was -28.880000. running mean: -9.098942\n",
      "ep 4460: ep_len:590 episode reward: total was -15.080000. running mean: -9.158753\n",
      "epsilon:0.010000 episode_count: 31227. steps_count: 13779081.000000\n",
      "ep 4461: ep_len:605 episode reward: total was -36.740000. running mean: -9.434565\n",
      "ep 4461: ep_len:500 episode reward: total was 3.130000. running mean: -9.308920\n",
      "ep 4461: ep_len:525 episode reward: total was -19.330000. running mean: -9.409130\n",
      "ep 4461: ep_len:535 episode reward: total was -19.590000. running mean: -9.510939\n",
      "ep 4461: ep_len:3 episode reward: total was 0.000000. running mean: -9.415830\n",
      "ep 4461: ep_len:500 episode reward: total was 1.400000. running mean: -9.307671\n",
      "ep 4461: ep_len:510 episode reward: total was -44.680000. running mean: -9.661395\n",
      "epsilon:0.010000 episode_count: 31234. steps_count: 13782259.000000\n",
      "ep 4462: ep_len:500 episode reward: total was 3.930000. running mean: -9.525481\n",
      "ep 4462: ep_len:500 episode reward: total was 5.190000. running mean: -9.378326\n",
      "ep 4462: ep_len:645 episode reward: total was -29.330000. running mean: -9.577843\n",
      "ep 4462: ep_len:500 episode reward: total was -3.970000. running mean: -9.521764\n",
      "ep 4462: ep_len:3 episode reward: total was 0.000000. running mean: -9.426547\n",
      "ep 4462: ep_len:715 episode reward: total was -42.840000. running mean: -9.760681\n",
      "ep 4462: ep_len:565 episode reward: total was -31.130000. running mean: -9.974374\n",
      "epsilon:0.010000 episode_count: 31241. steps_count: 13785687.000000\n",
      "ep 4463: ep_len:535 episode reward: total was 3.660000. running mean: -9.838031\n",
      "ep 4463: ep_len:500 episode reward: total was 6.120000. running mean: -9.678450\n",
      "ep 4463: ep_len:555 episode reward: total was -9.310000. running mean: -9.674766\n",
      "ep 4463: ep_len:520 episode reward: total was 7.110000. running mean: -9.506918\n",
      "ep 4463: ep_len:1 episode reward: total was 0.000000. running mean: -9.411849\n",
      "ep 4463: ep_len:230 episode reward: total was 3.640000. running mean: -9.281330\n",
      "ep 4463: ep_len:198 episode reward: total was -11.400000. running mean: -9.302517\n",
      "epsilon:0.010000 episode_count: 31248. steps_count: 13788226.000000\n",
      "ep 4464: ep_len:218 episode reward: total was -8.880000. running mean: -9.298292\n",
      "ep 4464: ep_len:540 episode reward: total was 13.850000. running mean: -9.066809\n",
      "ep 4464: ep_len:500 episode reward: total was -6.940000. running mean: -9.045541\n",
      "ep 4464: ep_len:500 episode reward: total was 14.930000. running mean: -8.805786\n",
      "ep 4464: ep_len:3 episode reward: total was 0.000000. running mean: -8.717728\n",
      "ep 4464: ep_len:580 episode reward: total was -5.940000. running mean: -8.689950\n",
      "ep 4464: ep_len:207 episode reward: total was -5.380000. running mean: -8.656851\n",
      "epsilon:0.010000 episode_count: 31255. steps_count: 13790774.000000\n",
      "ep 4465: ep_len:121 episode reward: total was -0.450000. running mean: -8.574782\n",
      "ep 4465: ep_len:500 episode reward: total was 17.190000. running mean: -8.317135\n",
      "ep 4465: ep_len:585 episode reward: total was -3.050000. running mean: -8.264463\n",
      "ep 4465: ep_len:590 episode reward: total was 7.410000. running mean: -8.107719\n",
      "ep 4465: ep_len:129 episode reward: total was 5.560000. running mean: -7.971041\n",
      "ep 4465: ep_len:500 episode reward: total was 4.250000. running mean: -7.848831\n",
      "ep 4465: ep_len:530 episode reward: total was -3.340000. running mean: -7.803743\n",
      "epsilon:0.010000 episode_count: 31262. steps_count: 13793729.000000\n",
      "ep 4466: ep_len:665 episode reward: total was -18.180000. running mean: -7.907505\n",
      "ep 4466: ep_len:272 episode reward: total was -34.340000. running mean: -8.171830\n",
      "ep 4466: ep_len:530 episode reward: total was -0.570000. running mean: -8.095812\n",
      "ep 4466: ep_len:500 episode reward: total was 8.990000. running mean: -7.924954\n",
      "ep 4466: ep_len:3 episode reward: total was 0.000000. running mean: -7.845704\n",
      "ep 4466: ep_len:510 episode reward: total was -43.020000. running mean: -8.197447\n",
      "ep 4466: ep_len:570 episode reward: total was -0.090000. running mean: -8.116373\n",
      "epsilon:0.010000 episode_count: 31269. steps_count: 13796779.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4467: ep_len:500 episode reward: total was 9.270000. running mean: -7.942509\n",
      "ep 4467: ep_len:505 episode reward: total was 1.690000. running mean: -7.846184\n",
      "ep 4467: ep_len:560 episode reward: total was -4.020000. running mean: -7.807922\n",
      "ep 4467: ep_len:525 episode reward: total was 4.460000. running mean: -7.685243\n",
      "ep 4467: ep_len:3 episode reward: total was 0.000000. running mean: -7.608390\n",
      "ep 4467: ep_len:311 episode reward: total was -1.850000. running mean: -7.550807\n",
      "ep 4467: ep_len:515 episode reward: total was 2.780000. running mean: -7.447498\n",
      "epsilon:0.010000 episode_count: 31276. steps_count: 13799698.000000\n",
      "ep 4468: ep_len:680 episode reward: total was -9.610000. running mean: -7.469124\n",
      "ep 4468: ep_len:500 episode reward: total was -14.850000. running mean: -7.542932\n",
      "ep 4468: ep_len:500 episode reward: total was 1.450000. running mean: -7.453003\n",
      "ep 4468: ep_len:119 episode reward: total was 5.610000. running mean: -7.322373\n",
      "ep 4468: ep_len:3 episode reward: total was 0.000000. running mean: -7.249149\n",
      "ep 4468: ep_len:305 episode reward: total was -2.870000. running mean: -7.205358\n",
      "ep 4468: ep_len:550 episode reward: total was -43.980000. running mean: -7.573104\n",
      "epsilon:0.010000 episode_count: 31283. steps_count: 13802355.000000\n",
      "ep 4469: ep_len:500 episode reward: total was 3.300000. running mean: -7.464373\n",
      "ep 4469: ep_len:605 episode reward: total was -1.550000. running mean: -7.405229\n",
      "ep 4469: ep_len:79 episode reward: total was 0.040000. running mean: -7.330777\n",
      "ep 4469: ep_len:500 episode reward: total was 4.870000. running mean: -7.208769\n",
      "ep 4469: ep_len:3 episode reward: total was 0.000000. running mean: -7.136682\n",
      "ep 4469: ep_len:575 episode reward: total was 9.650000. running mean: -6.968815\n",
      "ep 4469: ep_len:600 episode reward: total was -39.520000. running mean: -7.294327\n",
      "epsilon:0.010000 episode_count: 31290. steps_count: 13805217.000000\n",
      "ep 4470: ep_len:560 episode reward: total was -20.790000. running mean: -7.429283\n",
      "ep 4470: ep_len:610 episode reward: total was -6.340000. running mean: -7.418391\n",
      "ep 4470: ep_len:530 episode reward: total was -7.050000. running mean: -7.414707\n",
      "ep 4470: ep_len:500 episode reward: total was -11.040000. running mean: -7.450960\n",
      "ep 4470: ep_len:3 episode reward: total was 0.000000. running mean: -7.376450\n",
      "ep 4470: ep_len:500 episode reward: total was -14.270000. running mean: -7.445385\n",
      "ep 4470: ep_len:173 episode reward: total was -6.430000. running mean: -7.435232\n",
      "epsilon:0.010000 episode_count: 31297. steps_count: 13808093.000000\n",
      "ep 4471: ep_len:525 episode reward: total was -7.980000. running mean: -7.440679\n",
      "ep 4471: ep_len:600 episode reward: total was -9.750000. running mean: -7.463772\n",
      "ep 4471: ep_len:660 episode reward: total was -20.240000. running mean: -7.591535\n",
      "ep 4471: ep_len:535 episode reward: total was 1.910000. running mean: -7.496519\n",
      "ep 4471: ep_len:98 episode reward: total was 5.040000. running mean: -7.371154\n",
      "ep 4471: ep_len:735 episode reward: total was -9.100000. running mean: -7.388443\n",
      "ep 4471: ep_len:500 episode reward: total was -10.280000. running mean: -7.417358\n",
      "epsilon:0.010000 episode_count: 31304. steps_count: 13811746.000000\n",
      "ep 4472: ep_len:550 episode reward: total was 3.110000. running mean: -7.312085\n",
      "ep 4472: ep_len:615 episode reward: total was 8.290000. running mean: -7.156064\n",
      "ep 4472: ep_len:550 episode reward: total was -4.670000. running mean: -7.131203\n",
      "ep 4472: ep_len:515 episode reward: total was 1.540000. running mean: -7.044491\n",
      "ep 4472: ep_len:79 episode reward: total was 4.540000. running mean: -6.928646\n",
      "ep 4472: ep_len:500 episode reward: total was -6.240000. running mean: -6.921760\n",
      "ep 4472: ep_len:500 episode reward: total was -8.610000. running mean: -6.938642\n",
      "epsilon:0.010000 episode_count: 31311. steps_count: 13815055.000000\n",
      "ep 4473: ep_len:525 episode reward: total was -0.370000. running mean: -6.872956\n",
      "ep 4473: ep_len:500 episode reward: total was 1.560000. running mean: -6.788626\n",
      "ep 4473: ep_len:555 episode reward: total was -9.830000. running mean: -6.819040\n",
      "ep 4473: ep_len:520 episode reward: total was -11.490000. running mean: -6.865750\n",
      "ep 4473: ep_len:3 episode reward: total was 0.000000. running mean: -6.797092\n",
      "ep 4473: ep_len:570 episode reward: total was 0.440000. running mean: -6.724721\n",
      "ep 4473: ep_len:595 episode reward: total was -7.520000. running mean: -6.732674\n",
      "epsilon:0.010000 episode_count: 31318. steps_count: 13818323.000000\n",
      "ep 4474: ep_len:134 episode reward: total was 3.580000. running mean: -6.629547\n",
      "ep 4474: ep_len:620 episode reward: total was 7.960000. running mean: -6.483652\n",
      "ep 4474: ep_len:665 episode reward: total was -1.300000. running mean: -6.431815\n",
      "ep 4474: ep_len:510 episode reward: total was -9.050000. running mean: -6.457997\n",
      "ep 4474: ep_len:3 episode reward: total was 0.000000. running mean: -6.393417\n",
      "ep 4474: ep_len:500 episode reward: total was -5.650000. running mean: -6.385983\n",
      "ep 4474: ep_len:585 episode reward: total was -7.590000. running mean: -6.398023\n",
      "epsilon:0.010000 episode_count: 31325. steps_count: 13821340.000000\n",
      "ep 4475: ep_len:715 episode reward: total was -20.100000. running mean: -6.535043\n",
      "ep 4475: ep_len:645 episode reward: total was -30.840000. running mean: -6.778092\n",
      "ep 4475: ep_len:466 episode reward: total was -0.230000. running mean: -6.712611\n",
      "ep 4475: ep_len:56 episode reward: total was 2.570000. running mean: -6.619785\n",
      "ep 4475: ep_len:79 episode reward: total was 5.030000. running mean: -6.503288\n",
      "ep 4475: ep_len:610 episode reward: total was -7.120000. running mean: -6.509455\n",
      "ep 4475: ep_len:500 episode reward: total was -28.640000. running mean: -6.730760\n",
      "epsilon:0.010000 episode_count: 31332. steps_count: 13824411.000000\n",
      "ep 4476: ep_len:227 episode reward: total was 3.600000. running mean: -6.627452\n",
      "ep 4476: ep_len:500 episode reward: total was -36.680000. running mean: -6.927978\n",
      "ep 4476: ep_len:408 episode reward: total was 8.240000. running mean: -6.776298\n",
      "ep 4476: ep_len:535 episode reward: total was 13.090000. running mean: -6.577635\n",
      "ep 4476: ep_len:104 episode reward: total was -9.440000. running mean: -6.606259\n",
      "ep 4476: ep_len:520 episode reward: total was -11.000000. running mean: -6.650196\n",
      "ep 4476: ep_len:545 episode reward: total was -11.110000. running mean: -6.694794\n",
      "epsilon:0.010000 episode_count: 31339. steps_count: 13827250.000000\n",
      "ep 4477: ep_len:555 episode reward: total was -13.210000. running mean: -6.759946\n",
      "ep 4477: ep_len:510 episode reward: total was 14.230000. running mean: -6.550047\n",
      "ep 4477: ep_len:555 episode reward: total was -0.140000. running mean: -6.485946\n",
      "ep 4477: ep_len:161 episode reward: total was 1.100000. running mean: -6.410087\n",
      "ep 4477: ep_len:3 episode reward: total was 0.000000. running mean: -6.345986\n",
      "ep 4477: ep_len:500 episode reward: total was -12.800000. running mean: -6.410526\n",
      "ep 4477: ep_len:500 episode reward: total was -4.810000. running mean: -6.394521\n",
      "epsilon:0.010000 episode_count: 31346. steps_count: 13830034.000000\n",
      "ep 4478: ep_len:545 episode reward: total was -2.930000. running mean: -6.359876\n",
      "ep 4478: ep_len:585 episode reward: total was -2.630000. running mean: -6.322577\n",
      "ep 4478: ep_len:79 episode reward: total was 0.040000. running mean: -6.258951\n",
      "ep 4478: ep_len:500 episode reward: total was -37.710000. running mean: -6.573462\n",
      "ep 4478: ep_len:95 episode reward: total was 5.040000. running mean: -6.457327\n",
      "ep 4478: ep_len:570 episode reward: total was -22.150000. running mean: -6.614254\n",
      "ep 4478: ep_len:570 episode reward: total was -7.600000. running mean: -6.624111\n",
      "epsilon:0.010000 episode_count: 31353. steps_count: 13832978.000000\n",
      "ep 4479: ep_len:500 episode reward: total was 12.730000. running mean: -6.430570\n",
      "ep 4479: ep_len:555 episode reward: total was -29.920000. running mean: -6.665464\n",
      "ep 4479: ep_len:69 episode reward: total was -0.960000. running mean: -6.608410\n",
      "ep 4479: ep_len:525 episode reward: total was 6.900000. running mean: -6.473326\n",
      "ep 4479: ep_len:54 episode reward: total was 5.000000. running mean: -6.358592\n",
      "ep 4479: ep_len:291 episode reward: total was -0.360000. running mean: -6.298607\n",
      "ep 4479: ep_len:281 episode reward: total was -7.880000. running mean: -6.314420\n",
      "epsilon:0.010000 episode_count: 31360. steps_count: 13835253.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4480: ep_len:500 episode reward: total was -30.340000. running mean: -6.554676\n",
      "ep 4480: ep_len:636 episode reward: total was -40.880000. running mean: -6.897930\n",
      "ep 4480: ep_len:650 episode reward: total was -4.610000. running mean: -6.875050\n",
      "ep 4480: ep_len:500 episode reward: total was 8.580000. running mean: -6.720500\n",
      "ep 4480: ep_len:3 episode reward: total was 0.000000. running mean: -6.653295\n",
      "ep 4480: ep_len:550 episode reward: total was 2.620000. running mean: -6.560562\n",
      "ep 4480: ep_len:500 episode reward: total was -38.110000. running mean: -6.876056\n",
      "epsilon:0.010000 episode_count: 31367. steps_count: 13838592.000000\n",
      "ep 4481: ep_len:615 episode reward: total was -15.110000. running mean: -6.958396\n",
      "ep 4481: ep_len:625 episode reward: total was -1.070000. running mean: -6.899512\n",
      "ep 4481: ep_len:625 episode reward: total was -37.360000. running mean: -7.204117\n",
      "ep 4481: ep_len:500 episode reward: total was -16.490000. running mean: -7.296975\n",
      "ep 4481: ep_len:117 episode reward: total was 4.040000. running mean: -7.183606\n",
      "ep 4481: ep_len:317 episode reward: total was -16.360000. running mean: -7.275370\n",
      "ep 4481: ep_len:595 episode reward: total was -3.510000. running mean: -7.237716\n",
      "epsilon:0.010000 episode_count: 31374. steps_count: 13841986.000000\n",
      "ep 4482: ep_len:500 episode reward: total was -1.210000. running mean: -7.177439\n",
      "ep 4482: ep_len:565 episode reward: total was -3.970000. running mean: -7.145364\n",
      "ep 4482: ep_len:635 episode reward: total was -21.960000. running mean: -7.293511\n",
      "ep 4482: ep_len:525 episode reward: total was -16.010000. running mean: -7.380676\n",
      "ep 4482: ep_len:3 episode reward: total was 0.000000. running mean: -7.306869\n",
      "ep 4482: ep_len:595 episode reward: total was -13.610000. running mean: -7.369900\n",
      "ep 4482: ep_len:545 episode reward: total was -48.160000. running mean: -7.777801\n",
      "epsilon:0.010000 episode_count: 31381. steps_count: 13845354.000000\n",
      "ep 4483: ep_len:585 episode reward: total was -3.420000. running mean: -7.734223\n",
      "ep 4483: ep_len:645 episode reward: total was -42.990000. running mean: -8.086781\n",
      "ep 4483: ep_len:565 episode reward: total was -9.380000. running mean: -8.099713\n",
      "ep 4483: ep_len:500 episode reward: total was -3.510000. running mean: -8.053816\n",
      "ep 4483: ep_len:3 episode reward: total was 0.000000. running mean: -7.973278\n",
      "ep 4483: ep_len:595 episode reward: total was -1.590000. running mean: -7.909445\n",
      "ep 4483: ep_len:560 episode reward: total was -32.850000. running mean: -8.158851\n",
      "epsilon:0.010000 episode_count: 31388. steps_count: 13848807.000000\n",
      "ep 4484: ep_len:555 episode reward: total was 4.450000. running mean: -8.032762\n",
      "ep 4484: ep_len:530 episode reward: total was 16.240000. running mean: -7.790034\n",
      "ep 4484: ep_len:79 episode reward: total was 0.040000. running mean: -7.711734\n",
      "ep 4484: ep_len:555 episode reward: total was 3.360000. running mean: -7.601017\n",
      "ep 4484: ep_len:3 episode reward: total was 0.000000. running mean: -7.525007\n",
      "ep 4484: ep_len:645 episode reward: total was -7.880000. running mean: -7.528557\n",
      "ep 4484: ep_len:540 episode reward: total was -4.070000. running mean: -7.493971\n",
      "epsilon:0.010000 episode_count: 31395. steps_count: 13851714.000000\n",
      "ep 4485: ep_len:500 episode reward: total was 6.390000. running mean: -7.355131\n",
      "ep 4485: ep_len:500 episode reward: total was 1.210000. running mean: -7.269480\n",
      "ep 4485: ep_len:630 episode reward: total was -4.420000. running mean: -7.240985\n",
      "ep 4485: ep_len:595 episode reward: total was 6.420000. running mean: -7.104375\n",
      "ep 4485: ep_len:3 episode reward: total was 0.000000. running mean: -7.033332\n",
      "ep 4485: ep_len:680 episode reward: total was -13.150000. running mean: -7.094498\n",
      "ep 4485: ep_len:328 episode reward: total was -12.830000. running mean: -7.151853\n",
      "epsilon:0.010000 episode_count: 31402. steps_count: 13854950.000000\n",
      "ep 4486: ep_len:675 episode reward: total was -20.180000. running mean: -7.282135\n",
      "ep 4486: ep_len:500 episode reward: total was 3.850000. running mean: -7.170813\n",
      "ep 4486: ep_len:443 episode reward: total was 9.760000. running mean: -7.001505\n",
      "ep 4486: ep_len:585 episode reward: total was -32.480000. running mean: -7.256290\n",
      "ep 4486: ep_len:3 episode reward: total was 0.000000. running mean: -7.183727\n",
      "ep 4486: ep_len:540 episode reward: total was -45.990000. running mean: -7.571790\n",
      "ep 4486: ep_len:300 episode reward: total was -14.320000. running mean: -7.639272\n",
      "epsilon:0.010000 episode_count: 31409. steps_count: 13857996.000000\n",
      "ep 4487: ep_len:123 episode reward: total was 3.080000. running mean: -7.532079\n",
      "ep 4487: ep_len:510 episode reward: total was -9.870000. running mean: -7.555459\n",
      "ep 4487: ep_len:510 episode reward: total was -23.420000. running mean: -7.714104\n",
      "ep 4487: ep_len:595 episode reward: total was 12.470000. running mean: -7.512263\n",
      "ep 4487: ep_len:3 episode reward: total was 0.000000. running mean: -7.437140\n",
      "ep 4487: ep_len:635 episode reward: total was -29.580000. running mean: -7.658569\n",
      "ep 4487: ep_len:510 episode reward: total was -7.100000. running mean: -7.652983\n",
      "epsilon:0.010000 episode_count: 31416. steps_count: 13860882.000000\n",
      "ep 4488: ep_len:590 episode reward: total was -9.490000. running mean: -7.671353\n",
      "ep 4488: ep_len:620 episode reward: total was 4.530000. running mean: -7.549340\n",
      "ep 4488: ep_len:575 episode reward: total was -29.410000. running mean: -7.767946\n",
      "ep 4488: ep_len:395 episode reward: total was 2.340000. running mean: -7.666867\n",
      "ep 4488: ep_len:3 episode reward: total was 0.000000. running mean: -7.590198\n",
      "ep 4488: ep_len:500 episode reward: total was 7.530000. running mean: -7.438996\n",
      "ep 4488: ep_len:590 episode reward: total was -28.980000. running mean: -7.654406\n",
      "epsilon:0.010000 episode_count: 31423. steps_count: 13864155.000000\n",
      "ep 4489: ep_len:216 episode reward: total was 5.110000. running mean: -7.526762\n",
      "ep 4489: ep_len:605 episode reward: total was -5.290000. running mean: -7.504395\n",
      "ep 4489: ep_len:655 episode reward: total was 0.900000. running mean: -7.420351\n",
      "ep 4489: ep_len:550 episode reward: total was 14.500000. running mean: -7.201147\n",
      "ep 4489: ep_len:3 episode reward: total was 0.000000. running mean: -7.129136\n",
      "ep 4489: ep_len:595 episode reward: total was -20.840000. running mean: -7.266244\n",
      "ep 4489: ep_len:303 episode reward: total was -13.840000. running mean: -7.331982\n",
      "epsilon:0.010000 episode_count: 31430. steps_count: 13867082.000000\n",
      "ep 4490: ep_len:640 episode reward: total was -44.540000. running mean: -7.704062\n",
      "ep 4490: ep_len:705 episode reward: total was -8.700000. running mean: -7.714022\n",
      "ep 4490: ep_len:585 episode reward: total was 5.910000. running mean: -7.577781\n",
      "ep 4490: ep_len:545 episode reward: total was -10.950000. running mean: -7.611503\n",
      "ep 4490: ep_len:1 episode reward: total was 0.000000. running mean: -7.535388\n",
      "ep 4490: ep_len:226 episode reward: total was 4.180000. running mean: -7.418235\n",
      "ep 4490: ep_len:550 episode reward: total was -13.960000. running mean: -7.483652\n",
      "epsilon:0.010000 episode_count: 31437. steps_count: 13870334.000000\n",
      "ep 4491: ep_len:635 episode reward: total was -2.850000. running mean: -7.437316\n",
      "ep 4491: ep_len:500 episode reward: total was -0.340000. running mean: -7.366343\n",
      "ep 4491: ep_len:535 episode reward: total was -17.390000. running mean: -7.466579\n",
      "ep 4491: ep_len:590 episode reward: total was 8.590000. running mean: -7.306013\n",
      "ep 4491: ep_len:3 episode reward: total was 0.000000. running mean: -7.232953\n",
      "ep 4491: ep_len:575 episode reward: total was -30.680000. running mean: -7.467424\n",
      "ep 4491: ep_len:305 episode reward: total was -2.280000. running mean: -7.415549\n",
      "epsilon:0.010000 episode_count: 31444. steps_count: 13873477.000000\n",
      "ep 4492: ep_len:690 episode reward: total was -29.760000. running mean: -7.638994\n",
      "ep 4492: ep_len:555 episode reward: total was -2.790000. running mean: -7.590504\n",
      "ep 4492: ep_len:446 episode reward: total was 10.780000. running mean: -7.406799\n",
      "ep 4492: ep_len:500 episode reward: total was -24.690000. running mean: -7.579631\n",
      "ep 4492: ep_len:3 episode reward: total was 0.000000. running mean: -7.503835\n",
      "ep 4492: ep_len:515 episode reward: total was -3.600000. running mean: -7.464796\n",
      "ep 4492: ep_len:560 episode reward: total was -10.560000. running mean: -7.495748\n",
      "epsilon:0.010000 episode_count: 31451. steps_count: 13876746.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4493: ep_len:620 episode reward: total was -5.660000. running mean: -7.477391\n",
      "ep 4493: ep_len:575 episode reward: total was -16.510000. running mean: -7.567717\n",
      "ep 4493: ep_len:670 episode reward: total was -0.570000. running mean: -7.497740\n",
      "ep 4493: ep_len:510 episode reward: total was 2.910000. running mean: -7.393662\n",
      "ep 4493: ep_len:109 episode reward: total was 6.040000. running mean: -7.259326\n",
      "ep 4493: ep_len:560 episode reward: total was -7.940000. running mean: -7.266133\n",
      "ep 4493: ep_len:715 episode reward: total was -55.430000. running mean: -7.747771\n",
      "epsilon:0.010000 episode_count: 31458. steps_count: 13880505.000000\n",
      "ep 4494: ep_len:755 episode reward: total was -56.730000. running mean: -8.237593\n",
      "ep 4494: ep_len:530 episode reward: total was 6.200000. running mean: -8.093218\n",
      "ep 4494: ep_len:615 episode reward: total was 0.140000. running mean: -8.010885\n",
      "ep 4494: ep_len:520 episode reward: total was 0.830000. running mean: -7.922477\n",
      "ep 4494: ep_len:103 episode reward: total was 5.540000. running mean: -7.787852\n",
      "ep 4494: ep_len:570 episode reward: total was 3.410000. running mean: -7.675873\n",
      "ep 4494: ep_len:570 episode reward: total was 0.740000. running mean: -7.591714\n",
      "epsilon:0.010000 episode_count: 31465. steps_count: 13884168.000000\n",
      "ep 4495: ep_len:540 episode reward: total was 5.860000. running mean: -7.457197\n",
      "ep 4495: ep_len:625 episode reward: total was 2.420000. running mean: -7.358425\n",
      "ep 4495: ep_len:500 episode reward: total was 1.390000. running mean: -7.270941\n",
      "ep 4495: ep_len:530 episode reward: total was 6.830000. running mean: -7.129932\n",
      "ep 4495: ep_len:92 episode reward: total was 4.540000. running mean: -7.013232\n",
      "ep 4495: ep_len:585 episode reward: total was 6.050000. running mean: -6.882600\n",
      "ep 4495: ep_len:620 episode reward: total was -9.920000. running mean: -6.912974\n",
      "epsilon:0.010000 episode_count: 31472. steps_count: 13887660.000000\n",
      "ep 4496: ep_len:100 episode reward: total was 2.060000. running mean: -6.823244\n",
      "ep 4496: ep_len:520 episode reward: total was -0.240000. running mean: -6.757412\n",
      "ep 4496: ep_len:376 episode reward: total was -0.820000. running mean: -6.698038\n",
      "ep 4496: ep_len:105 episode reward: total was 0.590000. running mean: -6.625157\n",
      "ep 4496: ep_len:3 episode reward: total was 0.000000. running mean: -6.558906\n",
      "ep 4496: ep_len:540 episode reward: total was -17.280000. running mean: -6.666117\n",
      "ep 4496: ep_len:500 episode reward: total was -8.530000. running mean: -6.684756\n",
      "epsilon:0.010000 episode_count: 31479. steps_count: 13889804.000000\n",
      "ep 4497: ep_len:675 episode reward: total was -23.700000. running mean: -6.854908\n",
      "ep 4497: ep_len:500 episode reward: total was 18.340000. running mean: -6.602959\n",
      "ep 4497: ep_len:620 episode reward: total was -9.410000. running mean: -6.631029\n",
      "ep 4497: ep_len:153 episode reward: total was 2.100000. running mean: -6.543719\n",
      "ep 4497: ep_len:3 episode reward: total was 0.000000. running mean: -6.478282\n",
      "ep 4497: ep_len:520 episode reward: total was -5.110000. running mean: -6.464599\n",
      "ep 4497: ep_len:540 episode reward: total was -18.400000. running mean: -6.583953\n",
      "epsilon:0.010000 episode_count: 31486. steps_count: 13892815.000000\n",
      "ep 4498: ep_len:540 episode reward: total was -8.800000. running mean: -6.606114\n",
      "ep 4498: ep_len:505 episode reward: total was -20.110000. running mean: -6.741152\n",
      "ep 4498: ep_len:620 episode reward: total was -0.580000. running mean: -6.679541\n",
      "ep 4498: ep_len:615 episode reward: total was -22.350000. running mean: -6.836245\n",
      "ep 4498: ep_len:88 episode reward: total was 2.540000. running mean: -6.742483\n",
      "ep 4498: ep_len:500 episode reward: total was -2.780000. running mean: -6.702858\n",
      "ep 4498: ep_len:555 episode reward: total was -11.940000. running mean: -6.755230\n",
      "epsilon:0.010000 episode_count: 31493. steps_count: 13896238.000000\n",
      "ep 4499: ep_len:605 episode reward: total was 3.430000. running mean: -6.653377\n",
      "ep 4499: ep_len:580 episode reward: total was 9.570000. running mean: -6.491144\n",
      "ep 4499: ep_len:680 episode reward: total was -4.680000. running mean: -6.473032\n",
      "ep 4499: ep_len:515 episode reward: total was -8.470000. running mean: -6.493002\n",
      "ep 4499: ep_len:51 episode reward: total was 3.500000. running mean: -6.393072\n",
      "ep 4499: ep_len:525 episode reward: total was -7.890000. running mean: -6.408041\n",
      "ep 4499: ep_len:565 episode reward: total was -25.070000. running mean: -6.594661\n",
      "epsilon:0.010000 episode_count: 31500. steps_count: 13899759.000000\n",
      "ep 4500: ep_len:605 episode reward: total was 13.980000. running mean: -6.388914\n",
      "ep 4500: ep_len:610 episode reward: total was 2.230000. running mean: -6.302725\n",
      "ep 4500: ep_len:42 episode reward: total was 1.510000. running mean: -6.224598\n",
      "ep 4500: ep_len:530 episode reward: total was 9.500000. running mean: -6.067352\n",
      "ep 4500: ep_len:3 episode reward: total was 0.000000. running mean: -6.006678\n",
      "ep 4500: ep_len:500 episode reward: total was 2.280000. running mean: -5.923811\n",
      "ep 4500: ep_len:585 episode reward: total was -4.200000. running mean: -5.906573\n",
      "epsilon:0.010000 episode_count: 31507. steps_count: 13902634.000000\n",
      "ep 4501: ep_len:500 episode reward: total was -10.250000. running mean: -5.950008\n",
      "ep 4501: ep_len:560 episode reward: total was 22.410000. running mean: -5.666407\n",
      "ep 4501: ep_len:575 episode reward: total was 5.970000. running mean: -5.550043\n",
      "ep 4501: ep_len:500 episode reward: total was 9.420000. running mean: -5.400343\n",
      "ep 4501: ep_len:124 episode reward: total was 5.550000. running mean: -5.290839\n",
      "ep 4501: ep_len:540 episode reward: total was -4.550000. running mean: -5.283431\n",
      "ep 4501: ep_len:550 episode reward: total was 4.320000. running mean: -5.187397\n",
      "epsilon:0.010000 episode_count: 31514. steps_count: 13905983.000000\n",
      "ep 4502: ep_len:590 episode reward: total was -15.810000. running mean: -5.293623\n",
      "ep 4502: ep_len:590 episode reward: total was -0.080000. running mean: -5.241487\n",
      "ep 4502: ep_len:359 episode reward: total was 2.710000. running mean: -5.161972\n",
      "ep 4502: ep_len:630 episode reward: total was 8.470000. running mean: -5.025652\n",
      "ep 4502: ep_len:3 episode reward: total was 0.000000. running mean: -4.975395\n",
      "ep 4502: ep_len:615 episode reward: total was -0.570000. running mean: -4.931342\n",
      "ep 4502: ep_len:500 episode reward: total was -3.210000. running mean: -4.914128\n",
      "epsilon:0.010000 episode_count: 31521. steps_count: 13909270.000000\n",
      "ep 4503: ep_len:252 episode reward: total was 3.620000. running mean: -4.828787\n",
      "ep 4503: ep_len:525 episode reward: total was -14.810000. running mean: -4.928599\n",
      "ep 4503: ep_len:79 episode reward: total was 2.060000. running mean: -4.858713\n",
      "ep 4503: ep_len:132 episode reward: total was 7.110000. running mean: -4.739026\n",
      "ep 4503: ep_len:3 episode reward: total was 0.000000. running mean: -4.691636\n",
      "ep 4503: ep_len:500 episode reward: total was -8.010000. running mean: -4.724819\n",
      "ep 4503: ep_len:500 episode reward: total was -3.560000. running mean: -4.713171\n",
      "epsilon:0.010000 episode_count: 31528. steps_count: 13911261.000000\n",
      "ep 4504: ep_len:510 episode reward: total was -1.240000. running mean: -4.678439\n",
      "ep 4504: ep_len:530 episode reward: total was 18.830000. running mean: -4.443355\n",
      "ep 4504: ep_len:67 episode reward: total was 0.050000. running mean: -4.398421\n",
      "ep 4504: ep_len:540 episode reward: total was 7.620000. running mean: -4.278237\n",
      "ep 4504: ep_len:91 episode reward: total was -1.460000. running mean: -4.250055\n",
      "ep 4504: ep_len:246 episode reward: total was 8.140000. running mean: -4.126154\n",
      "ep 4504: ep_len:505 episode reward: total was -10.860000. running mean: -4.193493\n",
      "epsilon:0.010000 episode_count: 31535. steps_count: 13913750.000000\n",
      "ep 4505: ep_len:560 episode reward: total was -16.860000. running mean: -4.320158\n",
      "ep 4505: ep_len:500 episode reward: total was 7.590000. running mean: -4.201056\n",
      "ep 4505: ep_len:575 episode reward: total was -1.130000. running mean: -4.170346\n",
      "ep 4505: ep_len:403 episode reward: total was 0.330000. running mean: -4.125342\n",
      "ep 4505: ep_len:96 episode reward: total was 6.540000. running mean: -4.018689\n",
      "ep 4505: ep_len:515 episode reward: total was 0.970000. running mean: -3.968802\n",
      "ep 4505: ep_len:580 episode reward: total was -3.540000. running mean: -3.964514\n",
      "epsilon:0.010000 episode_count: 31542. steps_count: 13916979.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4506: ep_len:585 episode reward: total was 5.420000. running mean: -3.870669\n",
      "ep 4506: ep_len:520 episode reward: total was 5.230000. running mean: -3.779662\n",
      "ep 4506: ep_len:685 episode reward: total was -46.280000. running mean: -4.204665\n",
      "ep 4506: ep_len:56 episode reward: total was -1.940000. running mean: -4.182019\n",
      "ep 4506: ep_len:123 episode reward: total was -4.950000. running mean: -4.189699\n",
      "ep 4506: ep_len:500 episode reward: total was 12.940000. running mean: -4.018402\n",
      "ep 4506: ep_len:600 episode reward: total was -34.660000. running mean: -4.324818\n",
      "epsilon:0.010000 episode_count: 31549. steps_count: 13920048.000000\n",
      "ep 4507: ep_len:134 episode reward: total was 2.570000. running mean: -4.255869\n",
      "ep 4507: ep_len:590 episode reward: total was -30.890000. running mean: -4.522211\n",
      "ep 4507: ep_len:474 episode reward: total was 3.760000. running mean: -4.439389\n",
      "ep 4507: ep_len:615 episode reward: total was 11.140000. running mean: -4.283595\n",
      "ep 4507: ep_len:87 episode reward: total was 4.040000. running mean: -4.200359\n",
      "ep 4507: ep_len:335 episode reward: total was 4.690000. running mean: -4.111455\n",
      "ep 4507: ep_len:525 episode reward: total was -21.920000. running mean: -4.289541\n",
      "epsilon:0.010000 episode_count: 31556. steps_count: 13922808.000000\n",
      "ep 4508: ep_len:585 episode reward: total was 0.380000. running mean: -4.242845\n",
      "ep 4508: ep_len:500 episode reward: total was 7.620000. running mean: -4.124217\n",
      "ep 4508: ep_len:500 episode reward: total was 1.420000. running mean: -4.068775\n",
      "ep 4508: ep_len:565 episode reward: total was 6.550000. running mean: -3.962587\n",
      "ep 4508: ep_len:3 episode reward: total was 0.000000. running mean: -3.922961\n",
      "ep 4508: ep_len:535 episode reward: total was -6.860000. running mean: -3.952331\n",
      "ep 4508: ep_len:555 episode reward: total was -10.640000. running mean: -4.019208\n",
      "epsilon:0.010000 episode_count: 31563. steps_count: 13926051.000000\n",
      "ep 4509: ep_len:115 episode reward: total was -5.940000. running mean: -4.038416\n",
      "ep 4509: ep_len:500 episode reward: total was 3.610000. running mean: -3.961932\n",
      "ep 4509: ep_len:595 episode reward: total was -39.370000. running mean: -4.316013\n",
      "ep 4509: ep_len:530 episode reward: total was 11.000000. running mean: -4.162852\n",
      "ep 4509: ep_len:84 episode reward: total was 4.030000. running mean: -4.080924\n",
      "ep 4509: ep_len:620 episode reward: total was -27.790000. running mean: -4.318015\n",
      "ep 4509: ep_len:590 episode reward: total was -9.550000. running mean: -4.370334\n",
      "epsilon:0.010000 episode_count: 31570. steps_count: 13929085.000000\n",
      "ep 4510: ep_len:223 episode reward: total was 5.120000. running mean: -4.275431\n",
      "ep 4510: ep_len:535 episode reward: total was -79.800000. running mean: -5.030677\n",
      "ep 4510: ep_len:61 episode reward: total was 1.510000. running mean: -4.965270\n",
      "ep 4510: ep_len:600 episode reward: total was -21.400000. running mean: -5.129617\n",
      "ep 4510: ep_len:3 episode reward: total was 0.000000. running mean: -5.078321\n",
      "ep 4510: ep_len:570 episode reward: total was -3.050000. running mean: -5.058038\n",
      "ep 4510: ep_len:500 episode reward: total was -26.190000. running mean: -5.269358\n",
      "epsilon:0.010000 episode_count: 31577. steps_count: 13931577.000000\n",
      "ep 4511: ep_len:565 episode reward: total was 1.580000. running mean: -5.200864\n",
      "ep 4511: ep_len:570 episode reward: total was -5.340000. running mean: -5.202255\n",
      "ep 4511: ep_len:540 episode reward: total was -7.960000. running mean: -5.229833\n",
      "ep 4511: ep_len:170 episode reward: total was 3.140000. running mean: -5.146134\n",
      "ep 4511: ep_len:3 episode reward: total was 0.000000. running mean: -5.094673\n",
      "ep 4511: ep_len:680 episode reward: total was 8.910000. running mean: -4.954626\n",
      "ep 4511: ep_len:500 episode reward: total was -9.160000. running mean: -4.996680\n",
      "epsilon:0.010000 episode_count: 31584. steps_count: 13934605.000000\n",
      "ep 4512: ep_len:134 episode reward: total was 4.590000. running mean: -4.900813\n",
      "ep 4512: ep_len:520 episode reward: total was -1.280000. running mean: -4.864605\n",
      "ep 4512: ep_len:570 episode reward: total was 3.610000. running mean: -4.779859\n",
      "ep 4512: ep_len:550 episode reward: total was -5.520000. running mean: -4.787261\n",
      "ep 4512: ep_len:3 episode reward: total was 0.000000. running mean: -4.739388\n",
      "ep 4512: ep_len:625 episode reward: total was -34.360000. running mean: -5.035594\n",
      "ep 4512: ep_len:500 episode reward: total was -12.890000. running mean: -5.114138\n",
      "epsilon:0.010000 episode_count: 31591. steps_count: 13937507.000000\n",
      "ep 4513: ep_len:500 episode reward: total was 6.270000. running mean: -5.000297\n",
      "ep 4513: ep_len:615 episode reward: total was 0.460000. running mean: -4.945694\n",
      "ep 4513: ep_len:740 episode reward: total was -33.620000. running mean: -5.232437\n",
      "ep 4513: ep_len:535 episode reward: total was -11.050000. running mean: -5.290612\n",
      "ep 4513: ep_len:3 episode reward: total was 0.000000. running mean: -5.237706\n",
      "ep 4513: ep_len:305 episode reward: total was -5.840000. running mean: -5.243729\n",
      "ep 4513: ep_len:620 episode reward: total was -9.510000. running mean: -5.286392\n",
      "epsilon:0.010000 episode_count: 31598. steps_count: 13940825.000000\n",
      "ep 4514: ep_len:500 episode reward: total was 9.420000. running mean: -5.139328\n",
      "ep 4514: ep_len:595 episode reward: total was 3.230000. running mean: -5.055635\n",
      "ep 4514: ep_len:500 episode reward: total was -1.460000. running mean: -5.019678\n",
      "ep 4514: ep_len:500 episode reward: total was 11.910000. running mean: -4.850382\n",
      "ep 4514: ep_len:3 episode reward: total was 0.000000. running mean: -4.801878\n",
      "ep 4514: ep_len:500 episode reward: total was -16.240000. running mean: -4.916259\n",
      "ep 4514: ep_len:500 episode reward: total was -24.860000. running mean: -5.115696\n",
      "epsilon:0.010000 episode_count: 31605. steps_count: 13943923.000000\n",
      "ep 4515: ep_len:580 episode reward: total was -4.210000. running mean: -5.106640\n",
      "ep 4515: ep_len:535 episode reward: total was 6.400000. running mean: -4.991573\n",
      "ep 4515: ep_len:720 episode reward: total was -19.030000. running mean: -5.131957\n",
      "ep 4515: ep_len:500 episode reward: total was -11.530000. running mean: -5.195938\n",
      "ep 4515: ep_len:3 episode reward: total was 0.000000. running mean: -5.143978\n",
      "ep 4515: ep_len:500 episode reward: total was -6.450000. running mean: -5.157039\n",
      "ep 4515: ep_len:580 episode reward: total was -2.830000. running mean: -5.133768\n",
      "epsilon:0.010000 episode_count: 31612. steps_count: 13947341.000000\n",
      "ep 4516: ep_len:645 episode reward: total was -11.170000. running mean: -5.194131\n",
      "ep 4516: ep_len:505 episode reward: total was -39.610000. running mean: -5.538289\n",
      "ep 4516: ep_len:500 episode reward: total was 7.010000. running mean: -5.412806\n",
      "ep 4516: ep_len:505 episode reward: total was 4.980000. running mean: -5.308878\n",
      "ep 4516: ep_len:88 episode reward: total was 4.010000. running mean: -5.215690\n",
      "ep 4516: ep_len:570 episode reward: total was 5.890000. running mean: -5.104633\n",
      "ep 4516: ep_len:540 episode reward: total was -37.680000. running mean: -5.430386\n",
      "epsilon:0.010000 episode_count: 31619. steps_count: 13950694.000000\n",
      "ep 4517: ep_len:167 episode reward: total was -10.370000. running mean: -5.479782\n",
      "ep 4517: ep_len:165 episode reward: total was -2.890000. running mean: -5.453885\n",
      "ep 4517: ep_len:590 episode reward: total was 0.490000. running mean: -5.394446\n",
      "ep 4517: ep_len:500 episode reward: total was 8.410000. running mean: -5.256401\n",
      "ep 4517: ep_len:3 episode reward: total was 0.000000. running mean: -5.203837\n",
      "ep 4517: ep_len:560 episode reward: total was -0.710000. running mean: -5.158899\n",
      "ep 4517: ep_len:500 episode reward: total was -16.520000. running mean: -5.272510\n",
      "epsilon:0.010000 episode_count: 31626. steps_count: 13953179.000000\n",
      "ep 4518: ep_len:505 episode reward: total was 2.930000. running mean: -5.190485\n",
      "ep 4518: ep_len:520 episode reward: total was 1.750000. running mean: -5.121080\n",
      "ep 4518: ep_len:795 episode reward: total was -31.630000. running mean: -5.386169\n",
      "ep 4518: ep_len:500 episode reward: total was -4.060000. running mean: -5.372908\n",
      "ep 4518: ep_len:3 episode reward: total was 0.000000. running mean: -5.319178\n",
      "ep 4518: ep_len:505 episode reward: total was 5.470000. running mean: -5.211287\n",
      "ep 4518: ep_len:570 episode reward: total was 0.400000. running mean: -5.155174\n",
      "epsilon:0.010000 episode_count: 31633. steps_count: 13956577.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4519: ep_len:500 episode reward: total was -1.130000. running mean: -5.114922\n",
      "ep 4519: ep_len:530 episode reward: total was 14.910000. running mean: -4.914673\n",
      "ep 4519: ep_len:660 episode reward: total was 0.860000. running mean: -4.856926\n",
      "ep 4519: ep_len:535 episode reward: total was 19.560000. running mean: -4.612757\n",
      "ep 4519: ep_len:3 episode reward: total was 0.000000. running mean: -4.566629\n",
      "ep 4519: ep_len:555 episode reward: total was -1.870000. running mean: -4.539663\n",
      "ep 4519: ep_len:615 episode reward: total was -29.540000. running mean: -4.789666\n",
      "epsilon:0.010000 episode_count: 31640. steps_count: 13959975.000000\n",
      "ep 4520: ep_len:660 episode reward: total was 1.890000. running mean: -4.722870\n",
      "ep 4520: ep_len:363 episode reward: total was -19.860000. running mean: -4.874241\n",
      "ep 4520: ep_len:655 episode reward: total was 3.410000. running mean: -4.791399\n",
      "ep 4520: ep_len:585 episode reward: total was 10.520000. running mean: -4.638285\n",
      "ep 4520: ep_len:3 episode reward: total was 0.000000. running mean: -4.591902\n",
      "ep 4520: ep_len:500 episode reward: total was -12.710000. running mean: -4.673083\n",
      "ep 4520: ep_len:610 episode reward: total was -8.880000. running mean: -4.715152\n",
      "epsilon:0.010000 episode_count: 31647. steps_count: 13963351.000000\n",
      "ep 4521: ep_len:505 episode reward: total was -11.350000. running mean: -4.781500\n",
      "ep 4521: ep_len:500 episode reward: total was -6.340000. running mean: -4.797085\n",
      "ep 4521: ep_len:675 episode reward: total was 4.920000. running mean: -4.699915\n",
      "ep 4521: ep_len:500 episode reward: total was -7.570000. running mean: -4.728615\n",
      "ep 4521: ep_len:3 episode reward: total was 0.000000. running mean: -4.681329\n",
      "ep 4521: ep_len:535 episode reward: total was -9.070000. running mean: -4.725216\n",
      "ep 4521: ep_len:500 episode reward: total was -10.380000. running mean: -4.781764\n",
      "epsilon:0.010000 episode_count: 31654. steps_count: 13966569.000000\n",
      "ep 4522: ep_len:500 episode reward: total was -16.880000. running mean: -4.902746\n",
      "ep 4522: ep_len:505 episode reward: total was -38.540000. running mean: -5.239119\n",
      "ep 4522: ep_len:500 episode reward: total was 1.540000. running mean: -5.171327\n",
      "ep 4522: ep_len:500 episode reward: total was -9.480000. running mean: -5.214414\n",
      "ep 4522: ep_len:3 episode reward: total was 0.000000. running mean: -5.162270\n",
      "ep 4522: ep_len:545 episode reward: total was -6.910000. running mean: -5.179747\n",
      "ep 4522: ep_len:555 episode reward: total was -11.050000. running mean: -5.238450\n",
      "epsilon:0.010000 episode_count: 31661. steps_count: 13969677.000000\n",
      "ep 4523: ep_len:585 episode reward: total was -2.520000. running mean: -5.211265\n",
      "ep 4523: ep_len:635 episode reward: total was 18.100000. running mean: -4.978153\n",
      "ep 4523: ep_len:780 episode reward: total was -31.090000. running mean: -5.239271\n",
      "ep 4523: ep_len:132 episode reward: total was 2.110000. running mean: -5.165779\n",
      "ep 4523: ep_len:3 episode reward: total was 0.000000. running mean: -5.114121\n",
      "ep 4523: ep_len:515 episode reward: total was 14.010000. running mean: -4.922880\n",
      "ep 4523: ep_len:500 episode reward: total was -2.820000. running mean: -4.901851\n",
      "epsilon:0.010000 episode_count: 31668. steps_count: 13972827.000000\n",
      "ep 4524: ep_len:221 episode reward: total was 2.120000. running mean: -4.831632\n",
      "ep 4524: ep_len:515 episode reward: total was 0.440000. running mean: -4.778916\n",
      "ep 4524: ep_len:500 episode reward: total was -1.950000. running mean: -4.750627\n",
      "ep 4524: ep_len:505 episode reward: total was -12.500000. running mean: -4.828120\n",
      "ep 4524: ep_len:107 episode reward: total was -12.460000. running mean: -4.904439\n",
      "ep 4524: ep_len:500 episode reward: total was -5.560000. running mean: -4.910995\n",
      "ep 4524: ep_len:535 episode reward: total was -5.080000. running mean: -4.912685\n",
      "epsilon:0.010000 episode_count: 31675. steps_count: 13975710.000000\n",
      "ep 4525: ep_len:580 episode reward: total was -16.190000. running mean: -5.025458\n",
      "ep 4525: ep_len:635 episode reward: total was 18.100000. running mean: -4.794203\n",
      "ep 4525: ep_len:555 episode reward: total was -21.800000. running mean: -4.964261\n",
      "ep 4525: ep_len:510 episode reward: total was 8.110000. running mean: -4.833519\n",
      "ep 4525: ep_len:69 episode reward: total was 1.550000. running mean: -4.769684\n",
      "ep 4525: ep_len:580 episode reward: total was 8.360000. running mean: -4.638387\n",
      "ep 4525: ep_len:530 episode reward: total was -12.220000. running mean: -4.714203\n",
      "epsilon:0.010000 episode_count: 31682. steps_count: 13979169.000000\n",
      "ep 4526: ep_len:585 episode reward: total was 7.400000. running mean: -4.593061\n",
      "ep 4526: ep_len:500 episode reward: total was -33.070000. running mean: -4.877830\n",
      "ep 4526: ep_len:625 episode reward: total was -13.270000. running mean: -4.961752\n",
      "ep 4526: ep_len:500 episode reward: total was -27.570000. running mean: -5.187834\n",
      "ep 4526: ep_len:107 episode reward: total was 3.040000. running mean: -5.105556\n",
      "ep 4526: ep_len:316 episode reward: total was -15.840000. running mean: -5.212901\n",
      "ep 4526: ep_len:525 episode reward: total was -13.120000. running mean: -5.291972\n",
      "epsilon:0.010000 episode_count: 31689. steps_count: 13982327.000000\n",
      "ep 4527: ep_len:191 episode reward: total was -1.430000. running mean: -5.253352\n",
      "ep 4527: ep_len:365 episode reward: total was -44.850000. running mean: -5.649318\n",
      "ep 4527: ep_len:640 episode reward: total was -20.740000. running mean: -5.800225\n",
      "ep 4527: ep_len:555 episode reward: total was -9.440000. running mean: -5.836623\n",
      "ep 4527: ep_len:2 episode reward: total was 0.000000. running mean: -5.778257\n",
      "ep 4527: ep_len:580 episode reward: total was 2.420000. running mean: -5.696274\n",
      "ep 4527: ep_len:500 episode reward: total was -2.760000. running mean: -5.666911\n",
      "epsilon:0.010000 episode_count: 31696. steps_count: 13985160.000000\n",
      "ep 4528: ep_len:545 episode reward: total was -9.540000. running mean: -5.705642\n",
      "ep 4528: ep_len:525 episode reward: total was 20.360000. running mean: -5.444986\n",
      "ep 4528: ep_len:590 episode reward: total was 4.310000. running mean: -5.347436\n",
      "ep 4528: ep_len:540 episode reward: total was -5.150000. running mean: -5.345462\n",
      "ep 4528: ep_len:71 episode reward: total was -12.990000. running mean: -5.421907\n",
      "ep 4528: ep_len:520 episode reward: total was -32.150000. running mean: -5.689188\n",
      "ep 4528: ep_len:302 episode reward: total was -4.290000. running mean: -5.675196\n",
      "epsilon:0.010000 episode_count: 31703. steps_count: 13988253.000000\n",
      "ep 4529: ep_len:530 episode reward: total was 4.280000. running mean: -5.575644\n",
      "ep 4529: ep_len:540 episode reward: total was 5.190000. running mean: -5.467988\n",
      "ep 4529: ep_len:400 episode reward: total was -5.320000. running mean: -5.466508\n",
      "ep 4529: ep_len:56 episode reward: total was -5.980000. running mean: -5.471643\n",
      "ep 4529: ep_len:3 episode reward: total was 0.000000. running mean: -5.416926\n",
      "ep 4529: ep_len:530 episode reward: total was 2.000000. running mean: -5.342757\n",
      "ep 4529: ep_len:500 episode reward: total was -18.130000. running mean: -5.470629\n",
      "epsilon:0.010000 episode_count: 31710. steps_count: 13990812.000000\n",
      "ep 4530: ep_len:104 episode reward: total was -2.480000. running mean: -5.440723\n",
      "ep 4530: ep_len:334 episode reward: total was -19.860000. running mean: -5.584916\n",
      "ep 4530: ep_len:500 episode reward: total was 3.900000. running mean: -5.490067\n",
      "ep 4530: ep_len:418 episode reward: total was 8.920000. running mean: -5.345966\n",
      "ep 4530: ep_len:101 episode reward: total was 2.050000. running mean: -5.272006\n",
      "ep 4530: ep_len:500 episode reward: total was -1.250000. running mean: -5.231786\n",
      "ep 4530: ep_len:515 episode reward: total was -30.100000. running mean: -5.480468\n",
      "epsilon:0.010000 episode_count: 31717. steps_count: 13993284.000000\n",
      "ep 4531: ep_len:515 episode reward: total was 3.380000. running mean: -5.391864\n",
      "ep 4531: ep_len:645 episode reward: total was 16.580000. running mean: -5.172145\n",
      "ep 4531: ep_len:555 episode reward: total was -7.290000. running mean: -5.193324\n",
      "ep 4531: ep_len:555 episode reward: total was -1.140000. running mean: -5.152790\n",
      "ep 4531: ep_len:3 episode reward: total was 0.000000. running mean: -5.101263\n",
      "ep 4531: ep_len:500 episode reward: total was -16.990000. running mean: -5.220150\n",
      "ep 4531: ep_len:600 episode reward: total was -1.720000. running mean: -5.185148\n",
      "epsilon:0.010000 episode_count: 31724. steps_count: 13996657.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4532: ep_len:600 episode reward: total was -1.400000. running mean: -5.147297\n",
      "ep 4532: ep_len:630 episode reward: total was -37.370000. running mean: -5.469524\n",
      "ep 4532: ep_len:560 episode reward: total was 7.960000. running mean: -5.335229\n",
      "ep 4532: ep_len:500 episode reward: total was -5.130000. running mean: -5.333176\n",
      "ep 4532: ep_len:55 episode reward: total was 4.000000. running mean: -5.239845\n",
      "ep 4532: ep_len:261 episode reward: total was 8.660000. running mean: -5.100846\n",
      "ep 4532: ep_len:515 episode reward: total was -3.220000. running mean: -5.082038\n",
      "epsilon:0.010000 episode_count: 31731. steps_count: 13999778.000000\n",
      "ep 4533: ep_len:625 episode reward: total was 3.160000. running mean: -4.999617\n",
      "ep 4533: ep_len:530 episode reward: total was 13.320000. running mean: -4.816421\n",
      "ep 4533: ep_len:695 episode reward: total was -5.660000. running mean: -4.824857\n",
      "ep 4533: ep_len:132 episode reward: total was 3.100000. running mean: -4.745608\n",
      "ep 4533: ep_len:3 episode reward: total was 0.000000. running mean: -4.698152\n",
      "ep 4533: ep_len:590 episode reward: total was 4.970000. running mean: -4.601471\n",
      "ep 4533: ep_len:500 episode reward: total was -5.090000. running mean: -4.606356\n",
      "epsilon:0.010000 episode_count: 31738. steps_count: 14002853.000000\n",
      "ep 4534: ep_len:555 episode reward: total was 7.880000. running mean: -4.481493\n",
      "ep 4534: ep_len:520 episode reward: total was -20.030000. running mean: -4.636978\n",
      "ep 4534: ep_len:424 episode reward: total was 9.750000. running mean: -4.493108\n",
      "ep 4534: ep_len:414 episode reward: total was 10.930000. running mean: -4.338877\n",
      "ep 4534: ep_len:3 episode reward: total was 0.000000. running mean: -4.295488\n",
      "ep 4534: ep_len:620 episode reward: total was -6.670000. running mean: -4.319233\n",
      "ep 4534: ep_len:550 episode reward: total was -30.200000. running mean: -4.578041\n",
      "epsilon:0.010000 episode_count: 31745. steps_count: 14005939.000000\n",
      "ep 4535: ep_len:525 episode reward: total was -19.420000. running mean: -4.726460\n",
      "ep 4535: ep_len:180 episode reward: total was -7.390000. running mean: -4.753096\n",
      "ep 4535: ep_len:590 episode reward: total was -17.820000. running mean: -4.883765\n",
      "ep 4535: ep_len:530 episode reward: total was -30.170000. running mean: -5.136627\n",
      "ep 4535: ep_len:3 episode reward: total was 0.000000. running mean: -5.085261\n",
      "ep 4535: ep_len:167 episode reward: total was 8.610000. running mean: -4.948308\n",
      "ep 4535: ep_len:525 episode reward: total was -10.090000. running mean: -4.999725\n",
      "epsilon:0.010000 episode_count: 31752. steps_count: 14008459.000000\n",
      "ep 4536: ep_len:119 episode reward: total was 2.540000. running mean: -4.924328\n",
      "ep 4536: ep_len:565 episode reward: total was 17.370000. running mean: -4.701385\n",
      "ep 4536: ep_len:520 episode reward: total was -41.660000. running mean: -5.070971\n",
      "ep 4536: ep_len:610 episode reward: total was -4.960000. running mean: -5.069861\n",
      "ep 4536: ep_len:3 episode reward: total was 0.000000. running mean: -5.019163\n",
      "ep 4536: ep_len:603 episode reward: total was -73.590000. running mean: -5.704871\n",
      "ep 4536: ep_len:500 episode reward: total was -32.710000. running mean: -5.974922\n",
      "epsilon:0.010000 episode_count: 31759. steps_count: 14011379.000000\n",
      "ep 4537: ep_len:500 episode reward: total was -17.430000. running mean: -6.089473\n",
      "ep 4537: ep_len:535 episode reward: total was -2.960000. running mean: -6.058178\n",
      "ep 4537: ep_len:535 episode reward: total was -5.450000. running mean: -6.052096\n",
      "ep 4537: ep_len:605 episode reward: total was -1.480000. running mean: -6.006376\n",
      "ep 4537: ep_len:3 episode reward: total was 0.000000. running mean: -5.946312\n",
      "ep 4537: ep_len:515 episode reward: total was -21.000000. running mean: -6.096849\n",
      "ep 4537: ep_len:610 episode reward: total was -19.120000. running mean: -6.227080\n",
      "epsilon:0.010000 episode_count: 31766. steps_count: 14014682.000000\n",
      "ep 4538: ep_len:515 episode reward: total was -16.410000. running mean: -6.328909\n",
      "ep 4538: ep_len:500 episode reward: total was -7.890000. running mean: -6.344520\n",
      "ep 4538: ep_len:500 episode reward: total was -12.370000. running mean: -6.404775\n",
      "ep 4538: ep_len:500 episode reward: total was 17.410000. running mean: -6.166627\n",
      "ep 4538: ep_len:3 episode reward: total was 0.000000. running mean: -6.104961\n",
      "ep 4538: ep_len:179 episode reward: total was 5.110000. running mean: -5.992811\n",
      "ep 4538: ep_len:500 episode reward: total was -22.580000. running mean: -6.158683\n",
      "epsilon:0.010000 episode_count: 31773. steps_count: 14017379.000000\n",
      "ep 4539: ep_len:520 episode reward: total was 0.350000. running mean: -6.093596\n",
      "ep 4539: ep_len:500 episode reward: total was -4.460000. running mean: -6.077260\n",
      "ep 4539: ep_len:365 episode reward: total was -1.850000. running mean: -6.034988\n",
      "ep 4539: ep_len:530 episode reward: total was -20.590000. running mean: -6.180538\n",
      "ep 4539: ep_len:109 episode reward: total was 5.550000. running mean: -6.063233\n",
      "ep 4539: ep_len:500 episode reward: total was -16.760000. running mean: -6.170200\n",
      "ep 4539: ep_len:347 episode reward: total was -31.350000. running mean: -6.421998\n",
      "epsilon:0.010000 episode_count: 31780. steps_count: 14020250.000000\n",
      "ep 4540: ep_len:580 episode reward: total was -18.730000. running mean: -6.545078\n",
      "ep 4540: ep_len:515 episode reward: total was 12.860000. running mean: -6.351028\n",
      "ep 4540: ep_len:454 episode reward: total was -10.800000. running mean: -6.395517\n",
      "ep 4540: ep_len:56 episode reward: total was -0.950000. running mean: -6.341062\n",
      "ep 4540: ep_len:103 episode reward: total was 7.040000. running mean: -6.207251\n",
      "ep 4540: ep_len:675 episode reward: total was -23.730000. running mean: -6.382479\n",
      "ep 4540: ep_len:500 episode reward: total was -10.900000. running mean: -6.427654\n",
      "epsilon:0.010000 episode_count: 31787. steps_count: 14023133.000000\n",
      "ep 4541: ep_len:237 episode reward: total was -17.820000. running mean: -6.541578\n",
      "ep 4541: ep_len:515 episode reward: total was -3.310000. running mean: -6.509262\n",
      "ep 4541: ep_len:685 episode reward: total was -0.630000. running mean: -6.450469\n",
      "ep 4541: ep_len:510 episode reward: total was -16.050000. running mean: -6.546465\n",
      "ep 4541: ep_len:3 episode reward: total was 0.000000. running mean: -6.481000\n",
      "ep 4541: ep_len:575 episode reward: total was -5.090000. running mean: -6.467090\n",
      "ep 4541: ep_len:625 episode reward: total was -44.000000. running mean: -6.842419\n",
      "epsilon:0.010000 episode_count: 31794. steps_count: 14026283.000000\n",
      "ep 4542: ep_len:655 episode reward: total was -16.270000. running mean: -6.936695\n",
      "ep 4542: ep_len:525 episode reward: total was 20.410000. running mean: -6.663228\n",
      "ep 4542: ep_len:560 episode reward: total was 0.160000. running mean: -6.594996\n",
      "ep 4542: ep_len:555 episode reward: total was 7.930000. running mean: -6.449746\n",
      "ep 4542: ep_len:38 episode reward: total was 3.500000. running mean: -6.350248\n",
      "ep 4542: ep_len:535 episode reward: total was -6.590000. running mean: -6.352646\n",
      "ep 4542: ep_len:585 episode reward: total was -12.510000. running mean: -6.414219\n",
      "epsilon:0.010000 episode_count: 31801. steps_count: 14029736.000000\n",
      "ep 4543: ep_len:635 episode reward: total was 4.180000. running mean: -6.308277\n",
      "ep 4543: ep_len:500 episode reward: total was 0.200000. running mean: -6.243194\n",
      "ep 4543: ep_len:575 episode reward: total was -23.010000. running mean: -6.410862\n",
      "ep 4543: ep_len:56 episode reward: total was 2.570000. running mean: -6.321054\n",
      "ep 4543: ep_len:51 episode reward: total was 3.500000. running mean: -6.222843\n",
      "ep 4543: ep_len:535 episode reward: total was 6.140000. running mean: -6.099215\n",
      "ep 4543: ep_len:500 episode reward: total was -13.600000. running mean: -6.174223\n",
      "epsilon:0.010000 episode_count: 31808. steps_count: 14032588.000000\n",
      "ep 4544: ep_len:225 episode reward: total was 2.600000. running mean: -6.086480\n",
      "ep 4544: ep_len:500 episode reward: total was 17.700000. running mean: -5.848616\n",
      "ep 4544: ep_len:520 episode reward: total was -1.600000. running mean: -5.806129\n",
      "ep 4544: ep_len:500 episode reward: total was -21.170000. running mean: -5.959768\n",
      "ep 4544: ep_len:92 episode reward: total was 3.040000. running mean: -5.869770\n",
      "ep 4544: ep_len:500 episode reward: total was 7.460000. running mean: -5.736473\n",
      "ep 4544: ep_len:550 episode reward: total was -14.340000. running mean: -5.822508\n",
      "epsilon:0.010000 episode_count: 31815. steps_count: 14035475.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4545: ep_len:580 episode reward: total was -20.130000. running mean: -5.965583\n",
      "ep 4545: ep_len:530 episode reward: total was 19.940000. running mean: -5.706527\n",
      "ep 4545: ep_len:545 episode reward: total was -4.170000. running mean: -5.691162\n",
      "ep 4545: ep_len:535 episode reward: total was 16.580000. running mean: -5.468450\n",
      "ep 4545: ep_len:72 episode reward: total was -0.980000. running mean: -5.423566\n",
      "ep 4545: ep_len:505 episode reward: total was -6.490000. running mean: -5.434230\n",
      "ep 4545: ep_len:500 episode reward: total was -8.390000. running mean: -5.463788\n",
      "epsilon:0.010000 episode_count: 31822. steps_count: 14038742.000000\n",
      "ep 4546: ep_len:650 episode reward: total was -18.700000. running mean: -5.596150\n",
      "ep 4546: ep_len:580 episode reward: total was -9.810000. running mean: -5.638288\n",
      "ep 4546: ep_len:380 episode reward: total was 9.750000. running mean: -5.484405\n",
      "ep 4546: ep_len:56 episode reward: total was 2.570000. running mean: -5.403861\n",
      "ep 4546: ep_len:51 episode reward: total was 0.500000. running mean: -5.344823\n",
      "ep 4546: ep_len:575 episode reward: total was 3.850000. running mean: -5.252875\n",
      "ep 4546: ep_len:540 episode reward: total was -12.110000. running mean: -5.321446\n",
      "epsilon:0.010000 episode_count: 31829. steps_count: 14041574.000000\n",
      "ep 4547: ep_len:635 episode reward: total was -12.730000. running mean: -5.395531\n",
      "ep 4547: ep_len:550 episode reward: total was 23.890000. running mean: -5.102676\n",
      "ep 4547: ep_len:670 episode reward: total was -20.970000. running mean: -5.261349\n",
      "ep 4547: ep_len:510 episode reward: total was -7.490000. running mean: -5.283636\n",
      "ep 4547: ep_len:3 episode reward: total was 0.000000. running mean: -5.230799\n",
      "ep 4547: ep_len:500 episode reward: total was 0.120000. running mean: -5.177291\n",
      "ep 4547: ep_len:515 episode reward: total was -2.240000. running mean: -5.147919\n",
      "epsilon:0.010000 episode_count: 31836. steps_count: 14044957.000000\n",
      "ep 4548: ep_len:535 episode reward: total was 5.870000. running mean: -5.037739\n",
      "ep 4548: ep_len:525 episode reward: total was 15.390000. running mean: -4.833462\n",
      "ep 4548: ep_len:414 episode reward: total was -7.810000. running mean: -4.863227\n",
      "ep 4548: ep_len:500 episode reward: total was -1.260000. running mean: -4.827195\n",
      "ep 4548: ep_len:3 episode reward: total was 0.000000. running mean: -4.778923\n",
      "ep 4548: ep_len:520 episode reward: total was 5.140000. running mean: -4.679734\n",
      "ep 4548: ep_len:555 episode reward: total was -10.140000. running mean: -4.734337\n",
      "epsilon:0.010000 episode_count: 31843. steps_count: 14048009.000000\n",
      "ep 4549: ep_len:500 episode reward: total was 6.270000. running mean: -4.624293\n",
      "ep 4549: ep_len:500 episode reward: total was 8.080000. running mean: -4.497250\n",
      "ep 4549: ep_len:392 episode reward: total was 6.220000. running mean: -4.390078\n",
      "ep 4549: ep_len:525 episode reward: total was -3.160000. running mean: -4.377777\n",
      "ep 4549: ep_len:3 episode reward: total was 0.000000. running mean: -4.333999\n",
      "ep 4549: ep_len:304 episode reward: total was 7.190000. running mean: -4.218759\n",
      "ep 4549: ep_len:525 episode reward: total was -4.930000. running mean: -4.225872\n",
      "epsilon:0.010000 episode_count: 31850. steps_count: 14050758.000000\n",
      "ep 4550: ep_len:500 episode reward: total was -3.690000. running mean: -4.220513\n",
      "ep 4550: ep_len:346 episode reward: total was -18.360000. running mean: -4.361908\n",
      "ep 4550: ep_len:500 episode reward: total was 1.050000. running mean: -4.307789\n",
      "ep 4550: ep_len:500 episode reward: total was -8.080000. running mean: -4.345511\n",
      "ep 4550: ep_len:78 episode reward: total was -9.950000. running mean: -4.401556\n",
      "ep 4550: ep_len:530 episode reward: total was -40.550000. running mean: -4.763040\n",
      "ep 4550: ep_len:510 episode reward: total was -9.600000. running mean: -4.811410\n",
      "epsilon:0.010000 episode_count: 31857. steps_count: 14053722.000000\n",
      "ep 4551: ep_len:214 episode reward: total was -21.850000. running mean: -4.981796\n",
      "ep 4551: ep_len:500 episode reward: total was -3.820000. running mean: -4.970178\n",
      "ep 4551: ep_len:550 episode reward: total was -1.610000. running mean: -4.936576\n",
      "ep 4551: ep_len:520 episode reward: total was -23.100000. running mean: -5.118210\n",
      "ep 4551: ep_len:3 episode reward: total was 0.000000. running mean: -5.067028\n",
      "ep 4551: ep_len:585 episode reward: total was -41.510000. running mean: -5.431458\n",
      "ep 4551: ep_len:500 episode reward: total was -9.260000. running mean: -5.469743\n",
      "epsilon:0.010000 episode_count: 31864. steps_count: 14056594.000000\n",
      "ep 4552: ep_len:580 episode reward: total was 11.970000. running mean: -5.295346\n",
      "ep 4552: ep_len:530 episode reward: total was -8.410000. running mean: -5.326492\n",
      "ep 4552: ep_len:450 episode reward: total was -36.770000. running mean: -5.640927\n",
      "ep 4552: ep_len:123 episode reward: total was 3.110000. running mean: -5.553418\n",
      "ep 4552: ep_len:3 episode reward: total was 0.000000. running mean: -5.497884\n",
      "ep 4552: ep_len:655 episode reward: total was -54.030000. running mean: -5.983205\n",
      "ep 4552: ep_len:199 episode reward: total was -4.880000. running mean: -5.972173\n",
      "epsilon:0.010000 episode_count: 31871. steps_count: 14059134.000000\n",
      "ep 4553: ep_len:188 episode reward: total was -16.880000. running mean: -6.081251\n",
      "ep 4553: ep_len:750 episode reward: total was -18.710000. running mean: -6.207539\n",
      "ep 4553: ep_len:655 episode reward: total was -2.100000. running mean: -6.166463\n",
      "ep 4553: ep_len:44 episode reward: total was 2.540000. running mean: -6.079399\n",
      "ep 4553: ep_len:3 episode reward: total was 0.000000. running mean: -6.018605\n",
      "ep 4553: ep_len:565 episode reward: total was 4.960000. running mean: -5.908819\n",
      "ep 4553: ep_len:610 episode reward: total was -36.060000. running mean: -6.210331\n",
      "epsilon:0.010000 episode_count: 31878. steps_count: 14061949.000000\n",
      "ep 4554: ep_len:545 episode reward: total was 2.150000. running mean: -6.126727\n",
      "ep 4554: ep_len:299 episode reward: total was -36.830000. running mean: -6.433760\n",
      "ep 4554: ep_len:540 episode reward: total was 4.050000. running mean: -6.328922\n",
      "ep 4554: ep_len:500 episode reward: total was -12.090000. running mean: -6.386533\n",
      "ep 4554: ep_len:3 episode reward: total was 0.000000. running mean: -6.322668\n",
      "ep 4554: ep_len:500 episode reward: total was 0.680000. running mean: -6.252641\n",
      "ep 4554: ep_len:610 episode reward: total was -8.880000. running mean: -6.278915\n",
      "epsilon:0.010000 episode_count: 31885. steps_count: 14064946.000000\n",
      "ep 4555: ep_len:103 episode reward: total was -0.950000. running mean: -6.225626\n",
      "ep 4555: ep_len:565 episode reward: total was 8.440000. running mean: -6.078969\n",
      "ep 4555: ep_len:550 episode reward: total was -4.100000. running mean: -6.059180\n",
      "ep 4555: ep_len:126 episode reward: total was 5.600000. running mean: -5.942588\n",
      "ep 4555: ep_len:3 episode reward: total was 0.000000. running mean: -5.883162\n",
      "ep 4555: ep_len:505 episode reward: total was -12.100000. running mean: -5.945330\n",
      "ep 4555: ep_len:585 episode reward: total was -2.490000. running mean: -5.910777\n",
      "epsilon:0.010000 episode_count: 31892. steps_count: 14067383.000000\n",
      "ep 4556: ep_len:212 episode reward: total was 4.090000. running mean: -5.810769\n",
      "ep 4556: ep_len:755 episode reward: total was -39.820000. running mean: -6.150862\n",
      "ep 4556: ep_len:351 episode reward: total was 12.200000. running mean: -5.967353\n",
      "ep 4556: ep_len:500 episode reward: total was -19.670000. running mean: -6.104379\n",
      "ep 4556: ep_len:3 episode reward: total was 0.000000. running mean: -6.043336\n",
      "ep 4556: ep_len:575 episode reward: total was 7.860000. running mean: -5.904302\n",
      "ep 4556: ep_len:595 episode reward: total was 3.780000. running mean: -5.807459\n",
      "epsilon:0.010000 episode_count: 31899. steps_count: 14070374.000000\n",
      "ep 4557: ep_len:645 episode reward: total was -12.680000. running mean: -5.876185\n",
      "ep 4557: ep_len:500 episode reward: total was 22.690000. running mean: -5.590523\n",
      "ep 4557: ep_len:580 episode reward: total was -1.820000. running mean: -5.552818\n",
      "ep 4557: ep_len:555 episode reward: total was 14.090000. running mean: -5.356389\n",
      "ep 4557: ep_len:102 episode reward: total was 4.530000. running mean: -5.257526\n",
      "ep 4557: ep_len:535 episode reward: total was -5.470000. running mean: -5.259650\n",
      "ep 4557: ep_len:500 episode reward: total was -2.060000. running mean: -5.227654\n",
      "epsilon:0.010000 episode_count: 31906. steps_count: 14073791.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4558: ep_len:134 episode reward: total was -14.940000. running mean: -5.324777\n",
      "ep 4558: ep_len:500 episode reward: total was -36.620000. running mean: -5.637729\n",
      "ep 4558: ep_len:560 episode reward: total was -49.670000. running mean: -6.078052\n",
      "ep 4558: ep_len:500 episode reward: total was -13.560000. running mean: -6.152872\n",
      "ep 4558: ep_len:128 episode reward: total was 4.060000. running mean: -6.050743\n",
      "ep 4558: ep_len:525 episode reward: total was -0.430000. running mean: -5.994535\n",
      "ep 4558: ep_len:590 episode reward: total was -29.060000. running mean: -6.225190\n",
      "epsilon:0.010000 episode_count: 31913. steps_count: 14076728.000000\n",
      "ep 4559: ep_len:550 episode reward: total was -22.770000. running mean: -6.390638\n",
      "ep 4559: ep_len:500 episode reward: total was 9.060000. running mean: -6.236132\n",
      "ep 4559: ep_len:580 episode reward: total was -9.470000. running mean: -6.268471\n",
      "ep 4559: ep_len:505 episode reward: total was -15.530000. running mean: -6.361086\n",
      "ep 4559: ep_len:46 episode reward: total was 4.500000. running mean: -6.252475\n",
      "ep 4559: ep_len:590 episode reward: total was -8.500000. running mean: -6.274950\n",
      "ep 4559: ep_len:585 episode reward: total was -3.310000. running mean: -6.245301\n",
      "epsilon:0.010000 episode_count: 31920. steps_count: 14080084.000000\n",
      "ep 4560: ep_len:665 episode reward: total was -16.680000. running mean: -6.349648\n",
      "ep 4560: ep_len:201 episode reward: total was -5.360000. running mean: -6.339751\n",
      "ep 4560: ep_len:605 episode reward: total was -2.690000. running mean: -6.303254\n",
      "ep 4560: ep_len:540 episode reward: total was -17.470000. running mean: -6.414921\n",
      "ep 4560: ep_len:3 episode reward: total was 0.000000. running mean: -6.350772\n",
      "ep 4560: ep_len:545 episode reward: total was -14.520000. running mean: -6.432464\n",
      "ep 4560: ep_len:500 episode reward: total was -12.240000. running mean: -6.490540\n",
      "epsilon:0.010000 episode_count: 31927. steps_count: 14083143.000000\n",
      "ep 4561: ep_len:615 episode reward: total was 12.480000. running mean: -6.300834\n",
      "ep 4561: ep_len:560 episode reward: total was -0.830000. running mean: -6.246126\n",
      "ep 4561: ep_len:393 episode reward: total was -2.790000. running mean: -6.211565\n",
      "ep 4561: ep_len:378 episode reward: total was -23.110000. running mean: -6.380549\n",
      "ep 4561: ep_len:3 episode reward: total was 0.000000. running mean: -6.316743\n",
      "ep 4561: ep_len:525 episode reward: total was 2.660000. running mean: -6.226976\n",
      "ep 4561: ep_len:530 episode reward: total was -33.550000. running mean: -6.500206\n",
      "epsilon:0.010000 episode_count: 31934. steps_count: 14086147.000000\n",
      "ep 4562: ep_len:211 episode reward: total was 2.100000. running mean: -6.414204\n",
      "ep 4562: ep_len:595 episode reward: total was 0.360000. running mean: -6.346462\n",
      "ep 4562: ep_len:500 episode reward: total was -6.020000. running mean: -6.343198\n",
      "ep 4562: ep_len:56 episode reward: total was 1.560000. running mean: -6.264166\n",
      "ep 4562: ep_len:89 episode reward: total was 4.040000. running mean: -6.161124\n",
      "ep 4562: ep_len:590 episode reward: total was -52.060000. running mean: -6.620113\n",
      "ep 4562: ep_len:505 episode reward: total was 3.440000. running mean: -6.519512\n",
      "epsilon:0.010000 episode_count: 31941. steps_count: 14088693.000000\n",
      "ep 4563: ep_len:640 episode reward: total was 11.200000. running mean: -6.342316\n",
      "ep 4563: ep_len:500 episode reward: total was 5.370000. running mean: -6.225193\n",
      "ep 4563: ep_len:540 episode reward: total was -5.620000. running mean: -6.219141\n",
      "ep 4563: ep_len:615 episode reward: total was 12.130000. running mean: -6.035650\n",
      "ep 4563: ep_len:45 episode reward: total was 4.010000. running mean: -5.935193\n",
      "ep 4563: ep_len:600 episode reward: total was -11.730000. running mean: -5.993141\n",
      "ep 4563: ep_len:500 episode reward: total was -8.160000. running mean: -6.014810\n",
      "epsilon:0.010000 episode_count: 31948. steps_count: 14092133.000000\n",
      "ep 4564: ep_len:650 episode reward: total was -16.220000. running mean: -6.116862\n",
      "ep 4564: ep_len:162 episode reward: total was -21.950000. running mean: -6.275193\n",
      "ep 4564: ep_len:605 episode reward: total was -5.370000. running mean: -6.266141\n",
      "ep 4564: ep_len:540 episode reward: total was 10.010000. running mean: -6.103380\n",
      "ep 4564: ep_len:55 episode reward: total was 5.010000. running mean: -5.992246\n",
      "ep 4564: ep_len:500 episode reward: total was -6.780000. running mean: -6.000124\n",
      "ep 4564: ep_len:500 episode reward: total was -22.780000. running mean: -6.167923\n",
      "epsilon:0.010000 episode_count: 31955. steps_count: 14095145.000000\n",
      "ep 4565: ep_len:595 episode reward: total was 5.480000. running mean: -6.051443\n",
      "ep 4565: ep_len:500 episode reward: total was -20.840000. running mean: -6.199329\n",
      "ep 4565: ep_len:555 episode reward: total was 3.350000. running mean: -6.103836\n",
      "ep 4565: ep_len:525 episode reward: total was -89.950000. running mean: -6.942297\n",
      "ep 4565: ep_len:80 episode reward: total was 4.060000. running mean: -6.832274\n",
      "ep 4565: ep_len:233 episode reward: total was 4.620000. running mean: -6.717751\n",
      "ep 4565: ep_len:194 episode reward: total was -4.890000. running mean: -6.699474\n",
      "epsilon:0.010000 episode_count: 31962. steps_count: 14097827.000000\n",
      "ep 4566: ep_len:565 episode reward: total was -1.100000. running mean: -6.643479\n",
      "ep 4566: ep_len:555 episode reward: total was 14.160000. running mean: -6.435444\n",
      "ep 4566: ep_len:575 episode reward: total was -0.620000. running mean: -6.377290\n",
      "ep 4566: ep_len:535 episode reward: total was 1.460000. running mean: -6.298917\n",
      "ep 4566: ep_len:3 episode reward: total was 0.000000. running mean: -6.235928\n",
      "ep 4566: ep_len:585 episode reward: total was -0.590000. running mean: -6.179469\n",
      "ep 4566: ep_len:570 episode reward: total was -34.010000. running mean: -6.457774\n",
      "epsilon:0.010000 episode_count: 31969. steps_count: 14101215.000000\n",
      "ep 4567: ep_len:660 episode reward: total was -11.150000. running mean: -6.504696\n",
      "ep 4567: ep_len:760 episode reward: total was -48.320000. running mean: -6.922849\n",
      "ep 4567: ep_len:565 episode reward: total was 3.430000. running mean: -6.819321\n",
      "ep 4567: ep_len:505 episode reward: total was 13.440000. running mean: -6.616728\n",
      "ep 4567: ep_len:3 episode reward: total was 0.000000. running mean: -6.550560\n",
      "ep 4567: ep_len:625 episode reward: total was -0.550000. running mean: -6.490555\n",
      "ep 4567: ep_len:520 episode reward: total was -12.880000. running mean: -6.554449\n",
      "epsilon:0.010000 episode_count: 31976. steps_count: 14104853.000000\n",
      "ep 4568: ep_len:585 episode reward: total was -25.760000. running mean: -6.746505\n",
      "ep 4568: ep_len:515 episode reward: total was 2.050000. running mean: -6.658540\n",
      "ep 4568: ep_len:545 episode reward: total was -4.370000. running mean: -6.635654\n",
      "ep 4568: ep_len:530 episode reward: total was 14.090000. running mean: -6.428398\n",
      "ep 4568: ep_len:76 episode reward: total was 3.500000. running mean: -6.329114\n",
      "ep 4568: ep_len:645 episode reward: total was -15.220000. running mean: -6.418023\n",
      "ep 4568: ep_len:179 episode reward: total was -6.390000. running mean: -6.417742\n",
      "epsilon:0.010000 episode_count: 31983. steps_count: 14107928.000000\n",
      "ep 4569: ep_len:510 episode reward: total was -13.910000. running mean: -6.492665\n",
      "ep 4569: ep_len:505 episode reward: total was 9.370000. running mean: -6.334038\n",
      "ep 4569: ep_len:79 episode reward: total was 0.040000. running mean: -6.270298\n",
      "ep 4569: ep_len:545 episode reward: total was 6.050000. running mean: -6.147095\n",
      "ep 4569: ep_len:3 episode reward: total was 0.000000. running mean: -6.085624\n",
      "ep 4569: ep_len:640 episode reward: total was 2.540000. running mean: -5.999368\n",
      "ep 4569: ep_len:530 episode reward: total was -3.340000. running mean: -5.972774\n",
      "epsilon:0.010000 episode_count: 31990. steps_count: 14110740.000000\n",
      "ep 4570: ep_len:500 episode reward: total was 7.970000. running mean: -5.833346\n",
      "ep 4570: ep_len:655 episode reward: total was 12.110000. running mean: -5.653913\n",
      "ep 4570: ep_len:35 episode reward: total was -0.500000. running mean: -5.602374\n",
      "ep 4570: ep_len:56 episode reward: total was 1.560000. running mean: -5.530750\n",
      "ep 4570: ep_len:3 episode reward: total was 0.000000. running mean: -5.475442\n",
      "ep 4570: ep_len:173 episode reward: total was 6.630000. running mean: -5.354388\n",
      "ep 4570: ep_len:500 episode reward: total was -38.720000. running mean: -5.688044\n",
      "epsilon:0.010000 episode_count: 31997. steps_count: 14112662.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4571: ep_len:605 episode reward: total was -14.200000. running mean: -5.773164\n",
      "ep 4571: ep_len:620 episode reward: total was -33.990000. running mean: -6.055332\n",
      "ep 4571: ep_len:560 episode reward: total was 11.470000. running mean: -5.880079\n",
      "ep 4571: ep_len:550 episode reward: total was -3.910000. running mean: -5.860378\n",
      "ep 4571: ep_len:128 episode reward: total was 4.550000. running mean: -5.756274\n",
      "ep 4571: ep_len:505 episode reward: total was 3.430000. running mean: -5.664411\n",
      "ep 4571: ep_len:296 episode reward: total was -13.820000. running mean: -5.745967\n",
      "epsilon:0.010000 episode_count: 32004. steps_count: 14115926.000000\n",
      "ep 4572: ep_len:605 episode reward: total was -35.060000. running mean: -6.039108\n",
      "ep 4572: ep_len:595 episode reward: total was -33.530000. running mean: -6.314017\n",
      "ep 4572: ep_len:600 episode reward: total was -4.870000. running mean: -6.299576\n",
      "ep 4572: ep_len:585 episode reward: total was -3.020000. running mean: -6.266781\n",
      "ep 4572: ep_len:87 episode reward: total was 3.030000. running mean: -6.173813\n",
      "ep 4572: ep_len:505 episode reward: total was -10.480000. running mean: -6.216875\n",
      "ep 4572: ep_len:610 episode reward: total was -34.640000. running mean: -6.501106\n",
      "epsilon:0.010000 episode_count: 32011. steps_count: 14119513.000000\n",
      "ep 4573: ep_len:214 episode reward: total was 3.580000. running mean: -6.400295\n",
      "ep 4573: ep_len:500 episode reward: total was 10.200000. running mean: -6.234292\n",
      "ep 4573: ep_len:79 episode reward: total was 0.040000. running mean: -6.171549\n",
      "ep 4573: ep_len:625 episode reward: total was -25.420000. running mean: -6.364034\n",
      "ep 4573: ep_len:3 episode reward: total was 0.000000. running mean: -6.300393\n",
      "ep 4573: ep_len:515 episode reward: total was -12.770000. running mean: -6.365089\n",
      "ep 4573: ep_len:555 episode reward: total was -10.410000. running mean: -6.405538\n",
      "epsilon:0.010000 episode_count: 32018. steps_count: 14122004.000000\n",
      "ep 4574: ep_len:600 episode reward: total was 6.910000. running mean: -6.272383\n",
      "ep 4574: ep_len:500 episode reward: total was 12.660000. running mean: -6.083059\n",
      "ep 4574: ep_len:555 episode reward: total was -11.820000. running mean: -6.140429\n",
      "ep 4574: ep_len:56 episode reward: total was 1.560000. running mean: -6.063424\n",
      "ep 4574: ep_len:118 episode reward: total was 7.560000. running mean: -5.927190\n",
      "ep 4574: ep_len:545 episode reward: total was -9.390000. running mean: -5.961818\n",
      "ep 4574: ep_len:525 episode reward: total was -0.290000. running mean: -5.905100\n",
      "epsilon:0.010000 episode_count: 32025. steps_count: 14124903.000000\n",
      "ep 4575: ep_len:216 episode reward: total was -3.400000. running mean: -5.880049\n",
      "ep 4575: ep_len:510 episode reward: total was 1.070000. running mean: -5.810548\n",
      "ep 4575: ep_len:550 episode reward: total was -6.260000. running mean: -5.815043\n",
      "ep 4575: ep_len:565 episode reward: total was 9.470000. running mean: -5.662193\n",
      "ep 4575: ep_len:3 episode reward: total was 0.000000. running mean: -5.605571\n",
      "ep 4575: ep_len:500 episode reward: total was 6.410000. running mean: -5.485415\n",
      "ep 4575: ep_len:600 episode reward: total was -9.310000. running mean: -5.523661\n",
      "epsilon:0.010000 episode_count: 32032. steps_count: 14127847.000000\n",
      "ep 4576: ep_len:650 episode reward: total was -37.910000. running mean: -5.847524\n",
      "ep 4576: ep_len:350 episode reward: total was -37.810000. running mean: -6.167149\n",
      "ep 4576: ep_len:79 episode reward: total was 0.040000. running mean: -6.105077\n",
      "ep 4576: ep_len:510 episode reward: total was 9.930000. running mean: -5.944727\n",
      "ep 4576: ep_len:53 episode reward: total was 5.000000. running mean: -5.835279\n",
      "ep 4576: ep_len:635 episode reward: total was -16.450000. running mean: -5.941427\n",
      "ep 4576: ep_len:199 episode reward: total was -12.440000. running mean: -6.006412\n",
      "epsilon:0.010000 episode_count: 32039. steps_count: 14130323.000000\n",
      "ep 4577: ep_len:179 episode reward: total was -18.890000. running mean: -6.135248\n",
      "ep 4577: ep_len:500 episode reward: total was 0.060000. running mean: -6.073296\n",
      "ep 4577: ep_len:855 episode reward: total was -44.640000. running mean: -6.458963\n",
      "ep 4577: ep_len:500 episode reward: total was -7.010000. running mean: -6.464473\n",
      "ep 4577: ep_len:3 episode reward: total was 0.000000. running mean: -6.399828\n",
      "ep 4577: ep_len:680 episode reward: total was -18.670000. running mean: -6.522530\n",
      "ep 4577: ep_len:540 episode reward: total was -12.000000. running mean: -6.577305\n",
      "epsilon:0.010000 episode_count: 32046. steps_count: 14133580.000000\n",
      "ep 4578: ep_len:505 episode reward: total was -1.690000. running mean: -6.528432\n",
      "ep 4578: ep_len:575 episode reward: total was -87.750000. running mean: -7.340647\n",
      "ep 4578: ep_len:550 episode reward: total was -3.230000. running mean: -7.299541\n",
      "ep 4578: ep_len:605 episode reward: total was 8.570000. running mean: -7.140846\n",
      "ep 4578: ep_len:3 episode reward: total was 0.000000. running mean: -7.069437\n",
      "ep 4578: ep_len:500 episode reward: total was -6.890000. running mean: -7.067643\n",
      "ep 4578: ep_len:166 episode reward: total was -7.420000. running mean: -7.071166\n",
      "epsilon:0.010000 episode_count: 32053. steps_count: 14136484.000000\n",
      "ep 4579: ep_len:201 episode reward: total was -17.880000. running mean: -7.179255\n",
      "ep 4579: ep_len:665 episode reward: total was -23.380000. running mean: -7.341262\n",
      "ep 4579: ep_len:500 episode reward: total was -1.390000. running mean: -7.281749\n",
      "ep 4579: ep_len:104 episode reward: total was 4.570000. running mean: -7.163232\n",
      "ep 4579: ep_len:117 episode reward: total was 0.060000. running mean: -7.091000\n",
      "ep 4579: ep_len:515 episode reward: total was -13.260000. running mean: -7.152690\n",
      "ep 4579: ep_len:545 episode reward: total was -4.780000. running mean: -7.128963\n",
      "epsilon:0.010000 episode_count: 32060. steps_count: 14139131.000000\n",
      "ep 4580: ep_len:500 episode reward: total was 9.790000. running mean: -6.959773\n",
      "ep 4580: ep_len:605 episode reward: total was 1.200000. running mean: -6.878175\n",
      "ep 4580: ep_len:645 episode reward: total was -10.190000. running mean: -6.911294\n",
      "ep 4580: ep_len:515 episode reward: total was 7.920000. running mean: -6.762981\n",
      "ep 4580: ep_len:3 episode reward: total was 0.000000. running mean: -6.695351\n",
      "ep 4580: ep_len:500 episode reward: total was -7.260000. running mean: -6.700997\n",
      "ep 4580: ep_len:575 episode reward: total was -7.940000. running mean: -6.713387\n",
      "epsilon:0.010000 episode_count: 32067. steps_count: 14142474.000000\n",
      "ep 4581: ep_len:505 episode reward: total was -2.400000. running mean: -6.670254\n",
      "ep 4581: ep_len:550 episode reward: total was 16.910000. running mean: -6.434451\n",
      "ep 4581: ep_len:575 episode reward: total was -16.280000. running mean: -6.532907\n",
      "ep 4581: ep_len:154 episode reward: total was 4.090000. running mean: -6.426677\n",
      "ep 4581: ep_len:3 episode reward: total was 0.000000. running mean: -6.362411\n",
      "ep 4581: ep_len:570 episode reward: total was -4.610000. running mean: -6.344887\n",
      "ep 4581: ep_len:745 episode reward: total was -31.700000. running mean: -6.598438\n",
      "epsilon:0.010000 episode_count: 32074. steps_count: 14145576.000000\n",
      "ep 4582: ep_len:620 episode reward: total was -28.570000. running mean: -6.818153\n",
      "ep 4582: ep_len:500 episode reward: total was 16.720000. running mean: -6.582772\n",
      "ep 4582: ep_len:575 episode reward: total was -18.510000. running mean: -6.702044\n",
      "ep 4582: ep_len:117 episode reward: total was 2.060000. running mean: -6.614424\n",
      "ep 4582: ep_len:3 episode reward: total was 0.000000. running mean: -6.548279\n",
      "ep 4582: ep_len:505 episode reward: total was -35.780000. running mean: -6.840597\n",
      "ep 4582: ep_len:505 episode reward: total was -35.730000. running mean: -7.129491\n",
      "epsilon:0.010000 episode_count: 32081. steps_count: 14148401.000000\n",
      "ep 4583: ep_len:945 episode reward: total was -158.800000. running mean: -8.646196\n",
      "ep 4583: ep_len:605 episode reward: total was -28.590000. running mean: -8.845634\n",
      "ep 4583: ep_len:500 episode reward: total was 3.930000. running mean: -8.717877\n",
      "ep 4583: ep_len:515 episode reward: total was -53.230000. running mean: -9.162999\n",
      "ep 4583: ep_len:3 episode reward: total was 0.000000. running mean: -9.071369\n",
      "ep 4583: ep_len:720 episode reward: total was -17.120000. running mean: -9.151855\n",
      "ep 4583: ep_len:595 episode reward: total was -24.550000. running mean: -9.305836\n",
      "epsilon:0.010000 episode_count: 32088. steps_count: 14152284.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4584: ep_len:580 episode reward: total was -2.600000. running mean: -9.238778\n",
      "ep 4584: ep_len:500 episode reward: total was 13.210000. running mean: -9.014290\n",
      "ep 4584: ep_len:451 episode reward: total was -14.780000. running mean: -9.071947\n",
      "ep 4584: ep_len:500 episode reward: total was 6.420000. running mean: -8.917028\n",
      "ep 4584: ep_len:121 episode reward: total was -2.930000. running mean: -8.857158\n",
      "ep 4584: ep_len:670 episode reward: total was -36.350000. running mean: -9.132086\n",
      "ep 4584: ep_len:525 episode reward: total was -6.620000. running mean: -9.106965\n",
      "epsilon:0.010000 episode_count: 32095. steps_count: 14155631.000000\n",
      "ep 4585: ep_len:530 episode reward: total was -6.300000. running mean: -9.078896\n",
      "ep 4585: ep_len:520 episode reward: total was -0.110000. running mean: -8.989207\n",
      "ep 4585: ep_len:515 episode reward: total was -14.790000. running mean: -9.047215\n",
      "ep 4585: ep_len:515 episode reward: total was 11.000000. running mean: -8.846742\n",
      "ep 4585: ep_len:3 episode reward: total was 0.000000. running mean: -8.758275\n",
      "ep 4585: ep_len:610 episode reward: total was 3.950000. running mean: -8.631192\n",
      "ep 4585: ep_len:585 episode reward: total was -12.070000. running mean: -8.665580\n",
      "epsilon:0.010000 episode_count: 32102. steps_count: 14158909.000000\n",
      "ep 4586: ep_len:505 episode reward: total was -21.330000. running mean: -8.792224\n",
      "ep 4586: ep_len:183 episode reward: total was -13.400000. running mean: -8.838302\n",
      "ep 4586: ep_len:600 episode reward: total was 5.470000. running mean: -8.695219\n",
      "ep 4586: ep_len:600 episode reward: total was -31.360000. running mean: -8.921867\n",
      "ep 4586: ep_len:102 episode reward: total was 6.550000. running mean: -8.767148\n",
      "ep 4586: ep_len:610 episode reward: total was -0.520000. running mean: -8.684677\n",
      "ep 4586: ep_len:505 episode reward: total was -8.050000. running mean: -8.678330\n",
      "epsilon:0.010000 episode_count: 32109. steps_count: 14162014.000000\n",
      "ep 4587: ep_len:535 episode reward: total was 11.430000. running mean: -8.477247\n",
      "ep 4587: ep_len:540 episode reward: total was 4.300000. running mean: -8.349474\n",
      "ep 4587: ep_len:620 episode reward: total was 8.550000. running mean: -8.180480\n",
      "ep 4587: ep_len:560 episode reward: total was 2.420000. running mean: -8.074475\n",
      "ep 4587: ep_len:3 episode reward: total was 0.000000. running mean: -7.993730\n",
      "ep 4587: ep_len:660 episode reward: total was 3.390000. running mean: -7.879893\n",
      "ep 4587: ep_len:580 episode reward: total was -0.010000. running mean: -7.801194\n",
      "epsilon:0.010000 episode_count: 32116. steps_count: 14165512.000000\n",
      "ep 4588: ep_len:535 episode reward: total was -2.750000. running mean: -7.750682\n",
      "ep 4588: ep_len:595 episode reward: total was 24.440000. running mean: -7.428775\n",
      "ep 4588: ep_len:535 episode reward: total was 3.860000. running mean: -7.315887\n",
      "ep 4588: ep_len:500 episode reward: total was -8.530000. running mean: -7.328028\n",
      "ep 4588: ep_len:3 episode reward: total was 0.000000. running mean: -7.254748\n",
      "ep 4588: ep_len:170 episode reward: total was 6.600000. running mean: -7.116201\n",
      "ep 4588: ep_len:530 episode reward: total was -7.080000. running mean: -7.115839\n",
      "epsilon:0.010000 episode_count: 32123. steps_count: 14168380.000000\n",
      "ep 4589: ep_len:500 episode reward: total was -4.320000. running mean: -7.087880\n",
      "ep 4589: ep_len:500 episode reward: total was 14.160000. running mean: -6.875401\n",
      "ep 4589: ep_len:525 episode reward: total was -4.810000. running mean: -6.854747\n",
      "ep 4589: ep_len:50 episode reward: total was -0.940000. running mean: -6.795600\n",
      "ep 4589: ep_len:3 episode reward: total was 0.000000. running mean: -6.727644\n",
      "ep 4589: ep_len:585 episode reward: total was -21.660000. running mean: -6.876968\n",
      "ep 4589: ep_len:525 episode reward: total was 0.750000. running mean: -6.800698\n",
      "epsilon:0.010000 episode_count: 32130. steps_count: 14171068.000000\n",
      "ep 4590: ep_len:505 episode reward: total was -18.710000. running mean: -6.919791\n",
      "ep 4590: ep_len:266 episode reward: total was -34.870000. running mean: -7.199293\n",
      "ep 4590: ep_len:600 episode reward: total was -5.550000. running mean: -7.182800\n",
      "ep 4590: ep_len:510 episode reward: total was -8.570000. running mean: -7.196672\n",
      "ep 4590: ep_len:3 episode reward: total was 0.000000. running mean: -7.124705\n",
      "ep 4590: ep_len:610 episode reward: total was -2.470000. running mean: -7.078158\n",
      "ep 4590: ep_len:267 episode reward: total was -8.900000. running mean: -7.096377\n",
      "epsilon:0.010000 episode_count: 32137. steps_count: 14173829.000000\n",
      "ep 4591: ep_len:500 episode reward: total was 10.250000. running mean: -6.922913\n",
      "ep 4591: ep_len:500 episode reward: total was 15.190000. running mean: -6.701784\n",
      "ep 4591: ep_len:500 episode reward: total was 1.990000. running mean: -6.614866\n",
      "ep 4591: ep_len:500 episode reward: total was -13.580000. running mean: -6.684517\n",
      "ep 4591: ep_len:3 episode reward: total was 0.000000. running mean: -6.617672\n",
      "ep 4591: ep_len:585 episode reward: total was -1.640000. running mean: -6.567895\n",
      "ep 4591: ep_len:575 episode reward: total was -12.610000. running mean: -6.628316\n",
      "epsilon:0.010000 episode_count: 32144. steps_count: 14176992.000000\n",
      "ep 4592: ep_len:580 episode reward: total was -21.200000. running mean: -6.774033\n",
      "ep 4592: ep_len:605 episode reward: total was -12.680000. running mean: -6.833093\n",
      "ep 4592: ep_len:550 episode reward: total was -17.830000. running mean: -6.943062\n",
      "ep 4592: ep_len:56 episode reward: total was -0.460000. running mean: -6.878231\n",
      "ep 4592: ep_len:3 episode reward: total was 0.000000. running mean: -6.809449\n",
      "ep 4592: ep_len:243 episode reward: total was 9.660000. running mean: -6.644755\n",
      "ep 4592: ep_len:590 episode reward: total was -7.560000. running mean: -6.653907\n",
      "epsilon:0.010000 episode_count: 32151. steps_count: 14179619.000000\n",
      "ep 4593: ep_len:585 episode reward: total was -10.120000. running mean: -6.688568\n",
      "ep 4593: ep_len:505 episode reward: total was 14.190000. running mean: -6.479782\n",
      "ep 4593: ep_len:440 episode reward: total was -2.240000. running mean: -6.437384\n",
      "ep 4593: ep_len:399 episode reward: total was -22.090000. running mean: -6.593911\n",
      "ep 4593: ep_len:3 episode reward: total was 0.000000. running mean: -6.527972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b6bba0695115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# update value_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrrr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdate_criteria\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_memory_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_memory_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplay_memory_cx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_criteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;31m#numUpdateRew.append(rrr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m#if np.random.rand() < 0.05:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-106c733a6f18>\u001b[0m in \u001b[0;36mbackward_network\u001b[0;34m(replay_memory_pos, pos_prob, replay_memory_neg, replay_memory_cx, update_criteria)\u001b[0m\n\u001b[1;32m     26\u001b[0m     y = torch.cat(tuple(reward[i] if minibatch[i][4] \\\n\u001b[1;32m     27\u001b[0m                         \u001b[0;32melse\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                         for i in range(len(minibatch))))\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# extract Q-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-106c733a6f18>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     y = torch.cat(tuple(reward[i] if minibatch[i][4] \\\n\u001b[1;32m     27\u001b[0m                         \u001b[0;32melse\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                         for i in range(len(minibatch))))\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# extract Q-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maxRewSteps = 5\n",
    "max_ep_len = 500\n",
    "seq_len = 20\n",
    "h_len = seq_len\n",
    "\n",
    "while epoch < numEpoch:\n",
    "#for epoch in range(numEpoch):\n",
    "    # repeat for all pedestrians\n",
    "    #disp(pALL)\n",
    "    \n",
    "    for p in range(pALL.shape[0]): #range(pInLoop.shape[0])\n",
    "        \n",
    "        policy_net.train()\n",
    "        \n",
    "        # load p'th person data\n",
    "        ped = np.copy(pALL[p])\n",
    "\n",
    "        # camera index and frame index starts from zero\n",
    "        ped[:,0] -= 1\n",
    "        ped[:,1] -= 1\n",
    "        #print (np.unique(ped[:,0]))\n",
    "        \n",
    "        # check if camera number is correct\n",
    "        if (ped[:,0] >= num_camera).any():\n",
    "            print ('Error in person ', p)\n",
    "            break\n",
    "            \n",
    "        if ped.shape[0] < max_ep_len/4:\n",
    "            continue\n",
    "        \n",
    "        # select a camera uniformly\n",
    "        uniq_cam = np.unique(ped[:,0])\n",
    "        if len(uniq_cam) < 2 and np.random.rand() > 0.4:\n",
    "            if len(np.where(ped[1:,1]-ped[0:-1,1] != 1)[0]) == 0:\n",
    "                continue\n",
    "        rand_cam = uniq_cam[np.random.randint(len(uniq_cam))]\n",
    "        index_of_rand_cam = np.nonzero( ped[:,0]==rand_cam )[0]\n",
    "        len_indices_rand_cam = len(index_of_rand_cam)\n",
    "        \n",
    "        # Initialize with current state with start frame\n",
    "        tranIDX = np.where(ped[1:,0]-ped[0:-1,0])[0]\n",
    "        if len(uniq_cam) < 2 or np.random.rand() < 0.4:\n",
    "            startIDX = np.random.randint( 0,int(ped.shape[0]-ped.shape[0]/2) )\n",
    "        else:\n",
    "            startIDX = np.random.choice(tranIDX)-20\n",
    "        #startIDX = np.random.choice(tranIDX)-20 if np.random.rand(1) < 0.6 else np.random.randint( 0,ped.shape[0]-max_ep_len/2 )\n",
    "        #startIDX = index_of_rand_cam[np.random.randint(len_indices_rand_cam/10)]\n",
    "        myPos = ped[startIDX,0:]\n",
    "        #print (myPos)\n",
    "        \n",
    "        curr_camera = myPos[0]\n",
    "        curr_frame = myPos[1]\n",
    "        \n",
    "        # Initialize history variable (one-hot encoding)\n",
    "        ch = np.zeros((h_len,duke_cam))\n",
    "        prev_ch = ch\n",
    "        \n",
    "        # initialize total time target was occluded\n",
    "        num_steps = 0\n",
    "        occ_len = 0.01\n",
    "        hcount = np.array(10*np.log(occ_len))\n",
    "        CDataEp = []\n",
    "        inCDataEp = []\n",
    "        EpData = []\n",
    "        episodic_seq = []\n",
    "        rewSteps = 1\n",
    "        \n",
    "        tmp_replay = []\n",
    "        tmp_reward = []\n",
    "        tmp_c_seq = []\n",
    "        pivot_cam = curr_camera\n",
    "        prev_box = ped[ np.logical_and(ped[:,0]==curr_camera,ped[:,1]==curr_frame),2:][0]\n",
    "        \n",
    "        # create initial state (ct,rt,tau_t)\n",
    "        #bbox = myPos[2:]\n",
    "        #rt = afc.find_curr_rt(bbox)\n",
    "        x_t,c_t,te_tau,r_t = make_state_vector(ped, curr_camera,curr_frame,ch,occ_len)\n",
    "        prev_rt = r_t[0:4]\n",
    "        stCam = curr_camera\n",
    "        expStC = curr_camera\n",
    "        count_curr_c = 0\n",
    "        prev_camera = curr_camera\n",
    "\n",
    "        if render: # show current location\n",
    "            plt.imshow(x.reshape(input_size))\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "        episode_count += 1\n",
    "        if epsilon > finalEpsilon:\n",
    "            epsilon -= (initialEpsilon - finalEpsilon)/20000\n",
    "        \n",
    "        # select an action from the current state\n",
    "        hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "        #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "        state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "        #state = torch.cat([state_xt, hidden.detach().flatten().view(1,-1)], dim=1)\n",
    "        state = torch.cat([state_xt, hidden[1,].detach()], dim=1)\n",
    "        #print ('State size: ', state.size())\n",
    "        \n",
    "        while(curr_frame <= ped[-1,1]):\n",
    "        \n",
    "            state_in = Variable(state)\n",
    "            value_c = policy_net(state_in)\n",
    "\n",
    "            steps_count += 1\n",
    "            \n",
    "            # generate random steps\n",
    "            if np.random.rand(1) < 0.01:\n",
    "                rsteps = np.random.randint(fpsc,20,1)\n",
    "            else:\n",
    "                rsteps = 1\n",
    "            curr_frame += rsteps*fpsc if rsteps > 1 else fpsc\n",
    "            num_steps += 1\n",
    "                \n",
    "            # initialize action\n",
    "            one_hot_action = torch.zeros([num_camera], dtype=torch.float32)\n",
    "            if use_cuda:  # put on GPU if CUDA is available\n",
    "                one_hot_action = one_hot_action.cuda()\n",
    "\n",
    "            # epsilon greedy exploration\n",
    "            random_action = np.random.random() <= epsilon\n",
    "            camera_index = [torch.randint(num_camera, torch.Size([]), dtype=torch.int)\n",
    "                           if random_action else torch.argmax(value_c)][0]\n",
    "            \n",
    "            if use_cuda:  # put on GPU if CUDA is available\n",
    "                camera_index = camera_index.cuda()\n",
    "\n",
    "            one_hot_action[camera_index] = 1\n",
    "            one_hot_action = one_hot_action.unsqueeze(0)\n",
    "            c = camera_index.detach().cpu().numpy().item()\n",
    "            \n",
    "            # Store the transition explored\n",
    "            #M[stCam,c] += 1\n",
    "            \n",
    "            # get correct label from ground truth\n",
    "            y = afc.find_target_camera(ped, curr_frame)\n",
    "            # get reward (give reward at end of episode)\n",
    "            \n",
    "            #print (c, 'GT box: ', ped[ np.logical_and(ped[:,0]==c,ped[:,1]==curr_frame),2:])\n",
    "            # give reward if rewardSteps reached maxRewardSteps\n",
    "            if rewSteps >= maxRewSteps:\n",
    "                \n",
    "                #if np.random.rand() < 0.5:\n",
    "                #    c = y\n",
    "                \n",
    "                if y == num_camera-1 and y == c:\n",
    "                    reward = 0.01\n",
    "                    wt = 1\n",
    "                elif y == c and occ_len< 20:\n",
    "                    reward = 0.5\n",
    "                    wt = 1\n",
    "                elif y == c:\n",
    "                    reward = 1\n",
    "                    wt = 10\n",
    "                else:\n",
    "                    wt = 1\n",
    "                    reward = -1\n",
    "                reward_sum += reward\n",
    "                rs.append(reward)\n",
    "                \n",
    "                rewSteps = 1\n",
    "                \n",
    "                pivot_cam = c\n",
    "                # take bounding box from GT\n",
    "                # get the current bounding box\n",
    "                bbox = ped[ np.logical_and(ped[:,0]==c,ped[:,1]==curr_frame),2:]\n",
    "                if len(bbox): bbox = bbox[0]\n",
    "            else:\n",
    "                rewSteps += 1\n",
    "                reward = 0\n",
    "                \n",
    "                # take nearest bounding box\n",
    "                if c != num_camera-1 and c == pivot_cam and occ_len < 20:\n",
    "                    bbox = np.array(find_nearest_box(c,curr_frame, prev_box))\n",
    "                else:\n",
    "                    bbox = np.array([])\n",
    "                \n",
    "            #print ('bbox taken: ', bbox)\n",
    "            # get the current bounding box\n",
    "            #bbox = ped[ np.logical_and(ped[:,0]==c,ped[:,1]==curr_frame),2:]\n",
    "            if bbox.shape[0] > 0:\n",
    "                #rt = afc.find_curr_rt(bbox[0]) \n",
    "                #bbox = bbox[0]\n",
    "                rt = np.zeros((8))\n",
    "                rt[0] = bbox[0]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[1] = bbox[1]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[2] = bbox[2]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[3] = bbox[3]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[4] = rt[0] - prev_rt[0] if occ_len < 0.2 else 0\n",
    "                rt[5] = rt[1] - prev_rt[1] if occ_len < 0.2 else 0\n",
    "                rt[6] = rt[2] - prev_rt[2] if occ_len < 0.2 else 0\n",
    "                rt[7] = rt[3] - prev_rt[3] if occ_len < 0.2 else 0\n",
    "                \n",
    "                curr_camera = c\n",
    "                # make next_state vector\n",
    "                this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "                x_t = np.concatenate((this_cam, rt.ravel()))\n",
    "                x_t[x_t==0] = -10\n",
    "                x_t[x_t==1] = 10\n",
    "                x_t = x_t.reshape(1,-1)\n",
    "                if use_cuda:\n",
    "                    x_t = torch.from_numpy(x_t).float().cuda()\n",
    "                \n",
    "                #num_steps = 0\n",
    "                ispresent = 1\n",
    "                stCam = c\n",
    "                \n",
    "                prev_rt = rt[0:4]\n",
    "                prev_box = bbox\n",
    "            else:\n",
    "                ispresent = 0\n",
    "            \n",
    "            #if ispresent and expStC != num_camera-1 and c != num_camera-1:\n",
    "            #    trExplored[str(expStC)+'-'+str(c)].append(occ_len)\n",
    "            #    expStC = c\n",
    "            \n",
    "            \n",
    "            #-----------    reward was here\n",
    "            #EpData.append((list(value_c.detach().cpu().numpy()[0]),hcount.ravel()[0],reward,random_action,y,c,episode_reward))\n",
    "            \n",
    "            #print (np.array([rt, ispresent,c]))\n",
    "            ######################## prepare the next state  #############################\n",
    "            # count the time of prev_camera selection\n",
    "            if ispresent:\n",
    "                occ_len = 0.01\n",
    "            else:\n",
    "                occ_len += rsteps\n",
    "            #hcount = np.array(-occ_max_val + (occ_len/500)*(occ_max_val-(-occ_max_val)))\n",
    "            hcount = np.array(10*np.log(occ_len))\n",
    "            \n",
    "            # get next camera using policy network\n",
    "            this_cam = afc.make_one_hot_camera(c)\n",
    "            c_t = this_cam.reshape(1,-1)\n",
    "            \n",
    "            # update current state and history\n",
    "            prev_ch = ch\n",
    "            ch[1:,] = ch[0:-1,]\n",
    "            ch[0,0:num_camera] = afc.make_one_hot_camera(c)\n",
    "            ch[0,num_camera:] = 0\n",
    "            \n",
    "            #chCuda = torch.from_numpy(ch).float().cuda().unsqueeze(1)\n",
    "            #outSeq = model(chCuda, chCuda)\n",
    "            #print ('Encoding .. ', chCuda[:,0,].argmax(1))\n",
    "            #print ('Decoded ..', outSeq[:,0].argmax(1))\n",
    "            #print ('')\n",
    "            \n",
    "            if use_cuda:\n",
    "                c_t = torch.from_numpy(c_t).float().cuda()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float().cuda()\n",
    "            else:\n",
    "                c_t = torch.from_numpy(c_t).float()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float()\n",
    "            episodic_seq.append((c_t))\n",
    "            \n",
    "            # make next_state vector\n",
    "            hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "            #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "            next_state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "            #next_state = torch.cat([next_state_xt, hidden.detach().flatten().view(1,-1)], dim=1)\n",
    "            next_state = torch.cat([next_state_xt, hidden[1,].detach()], dim=1)\n",
    "            \n",
    "            # save transition to replay memory\n",
    "            tmp_replay.append((state, one_hot_action, next_state, ispresent))\n",
    "            tmp_reward.append(reward)\n",
    "            tmp_c_seq.append(c)\n",
    "            \n",
    "            #state_xt = next_state_xt\n",
    "            state = next_state #torch.cat([state_xt, hidden.detach()], dim=1)\n",
    "            #state = torch.cat([next_state_xt, enc_history], dim=1)\n",
    "            prev_camera = c\n",
    "            \n",
    "            if num_steps >= max_ep_len and y!=num_camera-1 and y==c and reward != 0:  # break the episode\n",
    "                #print ('')\n",
    "                #print (epoch, p, random_action, rsteps)\n",
    "                #print ('x_t: ', c,rt)\n",
    "                ##print ( np.where(ch)[1])\n",
    "                #print ('Q values: ', value_c)\n",
    "                #print (y,c, curr_frame,ped[-1,1], num_steps, hcount)\n",
    "                #print ('isPresent', ispresent)\n",
    "                #print ('Pos Replay length: ', len(replay_memory_pos))\n",
    "                \n",
    "                \n",
    "                #print (pos_prob[:])\n",
    "                break\n",
    "        \n",
    "        #########################################################\n",
    "        # compute reward backward\n",
    "        #epr = np.vstack(tmp_reward)\n",
    "        discounted_epr = discount_rewards(tmp_reward, tmp_c_seq)[-1::-1]\n",
    "        #print (tmp_reward, discounted_epr)\n",
    "\n",
    "        # save transition to replay memory\n",
    "        for rLen in range(len(tmp_replay)):\n",
    "            #state, one_hot_action, next_state, ispresent = tmp_replay[rLen]\n",
    "            state = tmp_replay[rLen][0]\n",
    "            one_hot_action = tmp_replay[rLen][1]\n",
    "            next_state = tmp_replay[rLen][2]\n",
    "            ispresent = tmp_replay[rLen][3]\n",
    "\n",
    "            if use_cuda:\n",
    "                reward = torch.from_numpy(np.array([discounted_epr[rLen]], dtype=np.float32)).unsqueeze(0).cuda()\n",
    "            else:\n",
    "                reward = torch.from_numpy(np.array([discounted_epr[rLen]], dtype=np.float32)).unsqueeze(0)\n",
    "\n",
    "            if reward > 0.1:\n",
    "                replay_memory_pos.append((state, one_hot_action, reward, next_state, ispresent))\n",
    "                if reward > 0.5:\n",
    "                    wt = 10\n",
    "                else:\n",
    "                    wt = 1\n",
    "                pos_prob.append(wt)\n",
    "            elif reward <= 0.01 and reward > 0:\n",
    "                replay_memory_cx.append((state, one_hot_action, reward, next_state, ispresent))\n",
    "            elif reward != 0:\n",
    "                replay_memory_neg.append((state, one_hot_action, reward, next_state, ispresent))\n",
    "\n",
    "            # if replay memory is full, remove the oldest transition\n",
    "            if len(replay_memory_pos) > replay_memory_size:\n",
    "                replay_memory_pos.pop(0)\n",
    "                pos_prob.pop(0)\n",
    "            if len(replay_memory_neg) > replay_memory_size:\n",
    "                replay_memory_neg.pop(0)\n",
    "            if len(replay_memory_cx) > replay_memory_size:\n",
    "                replay_memory_cx.pop(0)\n",
    "        #########################################################\n",
    "\n",
    "        # update value_function\n",
    "        loss,rrr,update_criteria = backward_network(replay_memory_pos, pos_prob[:], replay_memory_neg,replay_memory_cx, update_criteria)\n",
    "        #numUpdateRew.append(rrr)\n",
    "        #if np.random.rand() < 0.05:\n",
    "        #    allEpData.append((np.stack(EpData)))\n",
    "        \n",
    "        # store episodic reward\n",
    "        #numRew.append((sum(np.stack(rs)==100),sum(np.stack(rs)==-100),sum(np.stack(rs)==0.1)))\n",
    "        rs = append_reward(rs,num_steps)\n",
    "        \n",
    "        # boring book-keeping\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print ('ep %d: ep_len:%d episode reward: total was %f. running mean: %f' % (epoch, num_steps, reward_sum, running_reward))\n",
    "        reward_sum = 0\n",
    "        num_steps = 0\n",
    "        rs = []\n",
    "    \n",
    "#     if epoch % 20 == 0: # test on validation set\n",
    "#         _,accV,qv,numTR = test_func(pTest[1:2],iloc='first',eloc='last')\n",
    "#         av = np.stack(accV[0])\n",
    "#         av = sum(av[av[:,0]!=(num_camera-1),0] == av[av[:,0]!=(num_camera-1),1])/sum(av[:,0]!=(num_camera-1))\n",
    "#         validation_reward.append((qv,av,numTR)) \n",
    "    #print (M)\n",
    "    epoch += 1\n",
    "    print ('epsilon:%f episode_count: %d. steps_count: %f' % (epsilon, episode_count,steps_count))\n",
    "    if epoch % 20 == 1 and epoch > 300: \n",
    "        torch.save({'state_dict': policy_net.state_dict()}, './models/policy_db3_semisup_gtBOX_5_'+str(epoch))\n",
    "    #if epoch %200 == 100:\n",
    "        #np.save('./EpData/allEpData_ECCV_db3_pretrAE64_seq20_rp20K_'+str(epoch),np.array(allEpData),allow_pickle=True)\n",
    "        #np.save('./EpData/episode_reward_ECCV_db3_pretrAE64_seq20_rp20K_'+str(epoch), (episode_reward,validation_reward,epsilon,episode_count, steps_count,running_reward))\n",
    "    #allEpData = []\n",
    "    #print ('Time elapsed: ', tt.toc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([282,  95,  25,  90], dtype=uint16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ped[ np.logical_and(ped[:,0]==0,ped[:,1]==100),2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 88, 123, 206] [51, 81, 105, 186]\n",
      "0.5571980896065499\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chCuda = torch.from_numpy(ch).float().cuda().unsqueeze(1)\n",
    "outSeq = model(chCuda, chCuda)\n",
    "print ('Encoding .. ', chCuda[:,0,].argmax(1))\n",
    "print ('Decoded ..', outSeq[:,0].argmax(1))\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-50.0, 0.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reward received during training\n",
    "rpR = np.vstack(episode_reward)\n",
    "from scipy.signal import savgol_filter\n",
    "yhat = savgol_filter(rpR[:,2], 361, 2) # window size 51, polynomial order 3\n",
    "#plt.plot(rpR[:,4])\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(yhat)\n",
    "plt.ylim([-50,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vvr = np.stack(validation_reward)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(vvr[:,1])\n",
    "plt.ylabel('Validation Acc (non-Cx)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of Q values last 10 episodes\n",
    "epn = -1\n",
    "mmat = allEpData\n",
    "episQ = np.stack(mmat[epn][:,0])[:,]\n",
    "episY = mmat[epn][:,4]*10+10\n",
    "episC = mmat[epn][:,5]*10+10\n",
    "episTau = mmat[epn][:,1]\n",
    "# print (episY)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(episY, label='GT')\n",
    "ax1.plot(episTau, label='Tau', color='black')\n",
    "#ax1.plot(episC+1, label='pi(s)', color='cyan')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "ax2.plot(episQ[:,0], label='A1')\n",
    "ax2.plot(episQ[:,1], label='A2')\n",
    "ax2.plot(episQ[:,2], label='A3')\n",
    "ax2.plot(episQ[:,3], label='A4')\n",
    "ax2.plot(episQ[:,4], label='A5')\n",
    "ax2.plot(episQ[:,5], label='Ax')\n",
    "ax2.set_ylim([-4,5])\n",
    "ax2.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(arr): \n",
    "    final_list = [] \n",
    "    gt_tr = []\n",
    "    final_list.append(arr[0])     \n",
    "    for i in range(1,arr.shape[0]): \n",
    "        if arr[i] != arr[i-1]:\n",
    "            final_list.append(arr[i])     \n",
    "            if arr[i] != num_camera-1:\n",
    "                gt_tr.append(arr[i])\n",
    "    return final_list, gt_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2826\n"
     ]
    }
   ],
   "source": [
    "print (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.load_state_dict(torch.load('./models/policy_db4_semisup5601')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial position:  [  2   2 200  85  64 136]\n",
      "Initial position:  [  2   2  17  89  65 146]\n",
      "Initial position:  [   3 1880  190  106   54  133]\n",
      "Initial position:  [   1 1448  120   96   27   81]\n",
      "Initial position:  [   2 2236  219   80   52  133]\n",
      "Initial position:  [  3 630 114  82  82 156]\n",
      "Initial position:  [  2  68   1  94  37 131]\n",
      "Person:  0\n",
      "Transitions:  [2, 4, 1, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 1, 4, 2, 4, 2, 4, 3, 4]\n",
      "GT transitions:  23\n",
      "Transitions captured:  21\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.7540856031128405 0.6990218209179835 0.756514657980456\n",
      "Num frames:  (1329, 333)\n",
      "Accuracy:  0.7540856031128405\n",
      "Person:  1\n",
      "Transitions:  [2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 3, 4, 2, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4]\n",
      "GT transitions:  26\n",
      "Transitions captured:  14\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.5961462839166339 0.41591067690160505 0.5269672855879752\n",
      "Num frames:  (1433, 492)\n",
      "Accuracy:  0.5961462839166339\n",
      "Person:  2\n",
      "Transitions:  [3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 3, 4, 2, 4, 1, 4, 1, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4]\n",
      "GT transitions:  19\n",
      "Transitions captured:  16\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.6192170818505338 0.4213197969543147 0.7021996615905245\n",
      "Num frames:  (985, 466)\n",
      "Accuracy:  0.6192170818505338\n",
      "Person:  3\n",
      "Transitions:  [1, 4, 2, 4]\n",
      "GT transitions:  1\n",
      "Transitions captured:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.78515625 0.5533980582524272 0.8382352941176471\n",
      "Num frames:  (103, 44)\n",
      "Accuracy:  0.78515625\n",
      "Person:  4\n",
      "Transitions:  [2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4]\n",
      "GT transitions:  9\n",
      "Transitions captured:  6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.5596514745308311 0.32049947970863685 0.7096774193548387\n",
      "Num frames:  (961, 531)\n",
      "Accuracy:  0.5596514745308311\n",
      "Person:  5\n",
      "Transitions:  [3, 4, 2, 4]\n",
      "GT transitions:  1\n",
      "Transitions captured:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.7049180327868853 0.7226890756302521 0.9662921348314607\n",
      "Num frames:  (119, 33)\n",
      "Accuracy:  0.7049180327868853\n",
      "Person:  6\n",
      "Transitions:  [2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4]\n",
      "GT transitions:  27\n",
      "Transitions captured:  23\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.7392373923739237 0.6161314413741599 0.7842205323193916\n",
      "Num frames:  (1339, 409)\n",
      "Accuracy:  0.7392373923739237\n",
      "0.6797731597959498\n",
      "Average (only transitions) A,P,R 0.6797731597959498 0.5355671928199114 0.7548724265403278\n",
      "Average (all targets) A,P,R,F, ttr 0.6797731597959498 0.5355671928199114 0.7548724265403278 436.14285714285717 [47, 53, 39, 3, 19, 3, 55]\n",
      "8577\n"
     ]
    }
   ],
   "source": [
    "policy_net.eval()\n",
    "req_inc = 0\n",
    "render = False\n",
    "_,acc,_,numTR = test_func(pTest,iloc='fix',eloc='last', fixLoc=2, isdebug=0, req_inc=req_inc)\n",
    "tr_acc = 0\n",
    "avg_tr_captured = []\n",
    "A,P,R,F, ttr = [],[],[],[],[]\n",
    "A_onlytr,P_onlytr,R_onlytr = [],[],[]\n",
    "nfr = []\n",
    "for i in range(len(acc)):\n",
    "    print ('Person: ',i)\n",
    "    gt = np.array([d[0] for d in acc[i]])\n",
    "    pr = np.array([d[1] for d in acc[i]])\n",
    "    g = gt #t[gt != num_camera-1]\n",
    "    p = pr #r[gt != num_camera-1]\n",
    "    \n",
    "    dups,gt_tr = remove_duplicates(g)\n",
    "    print ('Transitions: ', dups)\n",
    "    print ('GT transitions: ', len(gt_tr))\n",
    "    print ('Transitions captured: ', numTR[i])\n",
    "    if len(gt_tr) != 0:\n",
    "        avg_tr_captured.append((numTR[i],len(gt_tr)))\n",
    "        contains_tr = 1\n",
    "    else:\n",
    "        print ('')\n",
    "        contains_tr = 0\n",
    "        #continue\n",
    "    \n",
    "    # plot transitions\n",
    "    afc.plot_color_transitions(p,g)\n",
    "    # MCTA and number of frames\n",
    "    if req_inc == 1:\n",
    "        ac,pr,re,fr,tr = afc.compute_APRF_one_person_sct_ict(p,g)\n",
    "    else:\n",
    "        ac,pr,re,fr,tr = afc.compute_APRF_one_person_sct_ict(p,g)\n",
    "        \n",
    "    if contains_tr == 1:\n",
    "        A_onlytr.append(ac)\n",
    "        P_onlytr.append(pr)\n",
    "        R_onlytr.append(re)\n",
    "    A.append(ac)\n",
    "    P.append(pr)\n",
    "    R.append(re)\n",
    "    F.append(fr)\n",
    "    ttr.append(tr)\n",
    "    print ('A,P,R: ', ac,pr,re)\n",
    "    f = afc.compute_num_frames(p,g)\n",
    "    nfr.append(f)\n",
    "    print ('Num frames: ', f)\n",
    "    # Accuracy\n",
    "    tacc = np.sum(g==p, dtype=np.float)/g.shape[0]\n",
    "    tr_acc += tacc\n",
    "    print ('Accuracy: ',tacc)\n",
    "print (tr_acc/len(A))\n",
    "print ('Average (only transitions) A,P,R', np.mean(A_onlytr),np.mean(P_onlytr),np.mean(R_onlytr))\n",
    "print ('Average (all targets) A,P,R,F, ttr', np.mean(A),np.mean(P),np.mean(R),np.mean(F), ttr)\n",
    "print (np.sum(nfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8303041140569745\n",
      "0.7735849056603774\n"
     ]
    }
   ],
   "source": [
    "a = np.stack(avg_tr_captured)\n",
    "print (np.mean(a[:,0]/a[:,1]))\n",
    "print (sum(a[:,0])/sum(a[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def eval_policy():\n",
    "    policy_net.eval()\n",
    "    req_inc = 0\n",
    "    render = False\n",
    "    _,acc,_,numTR = test_func(pTest,iloc='fix',eloc='last', fixLoc=2, isdebug=0, req_inc=req_inc)\n",
    "    tr_acc = 0\n",
    "    avg_tr_captured = []\n",
    "    A,P,R,F, ttr = [],[],[],[],[]\n",
    "    Fscore, Fscore_onlytr = [],[]\n",
    "    A_onlytr,P_onlytr,R_onlytr = [],[],[]\n",
    "    nfr = []\n",
    "    for i in range(len(acc)):\n",
    "        print ('Person: ',i)\n",
    "        gt = np.array([d[0] for d in acc[i]])\n",
    "        pr = np.array([d[1] for d in acc[i]])\n",
    "        g = gt #t[gt != num_camera-1]\n",
    "        p = pr #r[gt != num_camera-1]\n",
    "\n",
    "        dups,gt_tr = remove_duplicates(g)\n",
    "        print ('Transitions: ', dups)\n",
    "        print ('GT transitions: ', len(gt_tr))\n",
    "        print ('Transitions captured: ', numTR[i])\n",
    "        if len(gt_tr) != 0:\n",
    "            avg_tr_captured.append((numTR[i],len(gt_tr)))\n",
    "            contains_tr = 1\n",
    "        else:\n",
    "            print ('')\n",
    "            contains_tr = 0\n",
    "            #continue\n",
    "\n",
    "        # plot transitions\n",
    "    #     afc.plot_color_transitions(p,g)\n",
    "        # MCTA and number of frames\n",
    "        if req_inc == 1:\n",
    "            ac,pr,re,fr,tr = afc.compute_APRF_one_person_sct_ict(p,g)\n",
    "        else:\n",
    "            ac,pr,re,fr,tr = afc.compute_APRF_one_person_sct_ict(p,g)\n",
    "\n",
    "        fs = 2*(pr*re)/(pr+re)\n",
    "        if contains_tr == 1:\n",
    "            A_onlytr.append(ac)\n",
    "            P_onlytr.append(pr)\n",
    "            R_onlytr.append(re)\n",
    "            Fscore_onlytr.append(fs)\n",
    "        A.append(ac)\n",
    "        P.append(pr)\n",
    "        R.append(re)\n",
    "        F.append(fr)\n",
    "        Fscore.append(fs) \n",
    "        ttr.append(tr)\n",
    "        print ('A,P,R: ', ac,pr,re)\n",
    "        f = afc.compute_num_frames(p,g)\n",
    "        nfr.append(f)\n",
    "        print ('Num frames: ', f)\n",
    "        # Accuracy\n",
    "        tacc = np.sum(g==p, dtype=np.float)/g.shape[0]\n",
    "        tr_acc += tacc\n",
    "        print ('Accuracy: ',tacc)\n",
    "    print (tr_acc/len(A))\n",
    "    print ('Average (only transitions) A,P,R', np.mean(A_onlytr),np.mean(P_onlytr),np.mean(R_onlytr))\n",
    "    print ('Average (all targets) A,P,R,F, ttr', np.mean(A),np.mean(P),np.mean(R),np.mean(F), ttr)\n",
    "    print ('Fscore (all targets, only transitions) ', np.mean(Fscore),np.mean(Fscore_onlytr))\n",
    "    print (np.sum(nfr))\n",
    "    \n",
    "    a = np.stack(avg_tr_captured)\n",
    "    PCH_1 = np.mean(a[:,0]/a[:,1])\n",
    "    PCH_2 = sum(a[:,0])/sum(a[:,1])\n",
    "    print (PCH_1)\n",
    "    print (PCH_2)\n",
    "    \n",
    "    return np.mean(A),np.mean(P),np.mean(R),np.mean(Fscore),PCH_1,PCH_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/policy_db3_semisup_gtBOX_5_4581\n",
      "Initial position:  [  2   2 200  85  64 136]\n",
      "Initial position:  [  2   2  17  89  65 146]\n",
      "Initial position:  [   3 1880  190  106   54  133]\n",
      "Initial position:  [   1 1448  120   96   27   81]\n",
      "Initial position:  [   2 2236  219   80   52  133]\n",
      "Initial position:  [  3 630 114  82  82 156]\n",
      "Initial position:  [  2  68   1  94  37 131]\n",
      "Person:  0\n",
      "Transitions:  [2, 4, 1, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 1, 4, 2, 4, 2, 4, 3, 4]\n",
      "GT transitions:  23\n",
      "Transitions captured:  21\n",
      "A,P,R:  0.6680933852140077 0.615234375 0.7695439739413681\n",
      "Num frames:  (1536, 570)\n",
      "Accuracy:  0.6680933852140077\n",
      "Person:  1\n",
      "Transitions:  [2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 3, 4, 2, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4]\n",
      "GT transitions:  26\n",
      "Transitions captured:  18\n",
      "A,P,R:  0.6751867872591427 0.5743944636678201 0.7338638373121131\n",
      "Num frames:  (1445, 525)\n",
      "Accuracy:  0.6751867872591427\n",
      "Person:  2\n",
      "Transitions:  [3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 3, 4, 2, 4, 1, 4, 1, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4]\n",
      "GT transitions:  19\n",
      "Transitions captured:  18\n",
      "A,P,R:  0.7443653618030842 0.5747011952191236 0.9763113367174281\n",
      "Num frames:  (1004, 417)\n",
      "Accuracy:  0.7443653618030842\n",
      "Person:  3\n",
      "Transitions:  [1, 4, 2, 4]\n",
      "GT transitions:  1\n",
      "Transitions captured:  1\n",
      "A,P,R:  0.7421875 0.5074626865671642 1.0\n",
      "Num frames:  (134, 66)\n",
      "Accuracy:  0.7421875\n",
      "Person:  4\n",
      "Transitions:  [2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4]\n",
      "GT transitions:  9\n",
      "Transitions captured:  6\n",
      "A,P,R:  0.5026809651474531 0.2712041884816754 0.5967741935483871\n",
      "Num frames:  (955, 567)\n",
      "Accuracy:  0.5026809651474531\n",
      "Person:  5\n",
      "Transitions:  [3, 4, 2, 4]\n",
      "GT transitions:  1\n",
      "Transitions captured:  1\n",
      "A,P,R:  0.7950819672131147 0.7807017543859649 1.0\n",
      "Num frames:  (114, 25)\n",
      "Accuracy:  0.7950819672131147\n",
      "Person:  6\n",
      "Transitions:  [2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4]\n",
      "GT transitions:  27\n",
      "Transitions captured:  23\n",
      "A,P,R:  0.6793767937679377 0.5621925509486999 0.7604562737642585\n",
      "Num frames:  (1423, 530)\n",
      "Accuracy:  0.6793767937679377\n",
      "0.6867103943435343\n",
      "Average (only transitions) A,P,R 0.6867103943435343 0.5551273163243498 0.8338499450405079\n",
      "Average (all targets) A,P,R,F, ttr 0.6867103943435343 0.5551273163243498 0.8338499450405079 434.7142857142857 [47, 53, 39, 3, 19, 3, 55]\n",
      "Fscore (all targets, only transitions)  0.6601745636381546 0.6601745636381546\n",
      "9311\n",
      "0.8673197300199588\n",
      "0.8301886792452831\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_metric_values = []\n",
    "for epoch_i in range(801,epoch,20):\n",
    "    modelname = './models/policy_db3_semisup_gtBOX_5_' + str(epoch_i)\n",
    "    print (modelname)\n",
    "    # load model\n",
    "    policy_net.load_state_dict(torch.load(modelname)['state_dict'])\n",
    "    \n",
    "    A,P,R,F1,PCH1,PCH2 = eval_policy()\n",
    "    all_metric_values.append((epoch_i,A,P,R,F1,PCH1,PCH2))\n",
    "    \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./all_metric_values_db3_5steps', all_metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metric_values_cat = {}\n",
    "all_metric_values_cat['all_metric_values'] = np.stack(all_metric_values)\n",
    "spio.savemat('../../8tb/abstraction/unsup/all_metric_values_db3_5steps.mat', all_metric_values_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_fname = '/media/win/HRLhkl/Q_CamSel_3L_l4_st200_db3_1tCont_2'\n",
    "hkl.dump([[episode_reward, running_reward]], backup_fname+'_variables.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-053482d62e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "1/np.log(600*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 1\n",
    "np.max(pTest[pp][1:,1] - pTest[pp][0:-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
